{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 纯文本模型量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "pretrained_model_dir = \"/home/workspace/model/meta-llama-3-8b-instruct\"\n",
    "quantized_model_dir = \"/home/workspace/model/meta-llama-3-8b-instruct-w4-g128\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\n",
    "examples = [\n",
    "    tokenizer(\n",
    "        \"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,  # quantize model to 4-bit\n",
    "    group_size=128,  # it is recommended to set the value to 128\n",
    "    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    ")\n",
    "\n",
    "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/32...\n",
      "2024-10-30 08:14:41 INFO [auto_gptq.quantization.gptq] duration: 10.037084102630615\n",
      "2024-10-30 08:14:41 INFO [auto_gptq.quantization.gptq] avg loss: 2.1992526054382324\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/32...\n",
      "2024-10-30 08:14:48 INFO [auto_gptq.quantization.gptq] duration: 7.339477300643921\n",
      "2024-10-30 08:14:48 INFO [auto_gptq.quantization.gptq] avg loss: 0.03380981832742691\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/32...\n",
      "2024-10-30 08:14:58 INFO [auto_gptq.quantization.gptq] duration: 9.718609094619751\n",
      "2024-10-30 08:14:58 INFO [auto_gptq.quantization.gptq] avg loss: 3.3699684143066406\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/32...\n",
      "2024-10-30 08:15:08 INFO [auto_gptq.quantization.gptq] duration: 10.15040373802185\n",
      "2024-10-30 08:15:08 INFO [auto_gptq.quantization.gptq] avg loss: 0.0003416052204556763\n",
      "INFO - Quantizing mlp.up_proj in layer 1/32...\n",
      "2024-10-30 08:15:19 INFO [auto_gptq.quantization.gptq] duration: 10.828808546066284\n",
      "2024-10-30 08:15:19 INFO [auto_gptq.quantization.gptq] avg loss: 0.3819321095943451\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/32...\n",
      "2024-10-30 08:15:28 INFO [auto_gptq.quantization.gptq] duration: 9.118736743927002\n",
      "2024-10-30 08:15:28 INFO [auto_gptq.quantization.gptq] avg loss: 0.4896368682384491\n",
      "INFO - Quantizing mlp.down_proj in layer 1/32...\n",
      "2024-10-30 08:16:02 INFO [auto_gptq.quantization.gptq] duration: 34.22894525527954\n",
      "2024-10-30 08:16:02 INFO [auto_gptq.quantization.gptq] avg loss: 0.004767216742038727\n",
      "INFO - Start quantizing layer 2/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/32...\n",
      "2024-10-30 08:16:11 INFO [auto_gptq.quantization.gptq] duration: 8.335230112075806\n",
      "2024-10-30 08:16:11 INFO [auto_gptq.quantization.gptq] avg loss: 1.1905415058135986\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/32...\n",
      "2024-10-30 08:16:22 INFO [auto_gptq.quantization.gptq] duration: 10.828464031219482\n",
      "2024-10-30 08:16:22 INFO [auto_gptq.quantization.gptq] avg loss: 0.06976363062858582\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/32...\n",
      "2024-10-30 08:16:25 INFO [auto_gptq.quantization.gptq] duration: 3.3788704872131348\n",
      "2024-10-30 08:16:25 INFO [auto_gptq.quantization.gptq] avg loss: 2.039285182952881\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/32...\n",
      "2024-10-30 08:16:31 INFO [auto_gptq.quantization.gptq] duration: 5.865533113479614\n",
      "2024-10-30 08:16:31 INFO [auto_gptq.quantization.gptq] avg loss: 0.0007455475861206651\n",
      "INFO - Quantizing mlp.up_proj in layer 2/32...\n",
      "2024-10-30 08:16:41 INFO [auto_gptq.quantization.gptq] duration: 9.759242057800293\n",
      "2024-10-30 08:16:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.7103757858276367\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/32...\n",
      "2024-10-30 08:16:50 INFO [auto_gptq.quantization.gptq] duration: 9.233519554138184\n",
      "2024-10-30 08:16:50 INFO [auto_gptq.quantization.gptq] avg loss: 0.8590945601463318\n",
      "INFO - Quantizing mlp.down_proj in layer 2/32...\n",
      "2024-10-30 08:17:22 INFO [auto_gptq.quantization.gptq] duration: 31.486181497573853\n",
      "2024-10-30 08:17:22 INFO [auto_gptq.quantization.gptq] avg loss: 16.221515655517578\n",
      "INFO - Start quantizing layer 3/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/32...\n",
      "2024-10-30 08:17:31 INFO [auto_gptq.quantization.gptq] duration: 8.975847244262695\n",
      "2024-10-30 08:17:31 INFO [auto_gptq.quantization.gptq] avg loss: 3.683974027633667\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/32...\n",
      "2024-10-30 08:17:38 INFO [auto_gptq.quantization.gptq] duration: 7.537810802459717\n",
      "2024-10-30 08:17:38 INFO [auto_gptq.quantization.gptq] avg loss: 0.17160382866859436\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/32...\n",
      "2024-10-30 08:17:47 INFO [auto_gptq.quantization.gptq] duration: 8.988044738769531\n",
      "2024-10-30 08:17:47 INFO [auto_gptq.quantization.gptq] avg loss: 6.08571720123291\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/32...\n",
      "2024-10-30 08:17:56 INFO [auto_gptq.quantization.gptq] duration: 8.53365683555603\n",
      "2024-10-30 08:17:56 INFO [auto_gptq.quantization.gptq] avg loss: 0.0006117637967690825\n",
      "INFO - Quantizing mlp.up_proj in layer 3/32...\n",
      "2024-10-30 08:18:03 INFO [auto_gptq.quantization.gptq] duration: 7.475140571594238\n",
      "2024-10-30 08:18:03 INFO [auto_gptq.quantization.gptq] avg loss: 1.6254947185516357\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/32...\n",
      "2024-10-30 08:18:12 INFO [auto_gptq.quantization.gptq] duration: 8.30447268486023\n",
      "2024-10-30 08:18:12 INFO [auto_gptq.quantization.gptq] avg loss: 2.1320695877075195\n",
      "INFO - Quantizing mlp.down_proj in layer 3/32...\n",
      "2024-10-30 08:18:40 INFO [auto_gptq.quantization.gptq] duration: 28.691283464431763\n",
      "2024-10-30 08:18:40 INFO [auto_gptq.quantization.gptq] avg loss: 0.00424563605338335\n",
      "INFO - Start quantizing layer 4/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/32...\n",
      "2024-10-30 08:18:49 INFO [auto_gptq.quantization.gptq] duration: 8.147428274154663\n",
      "2024-10-30 08:18:49 INFO [auto_gptq.quantization.gptq] avg loss: 1.921912431716919\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/32...\n",
      "2024-10-30 08:18:57 INFO [auto_gptq.quantization.gptq] duration: 8.485686302185059\n",
      "2024-10-30 08:18:57 INFO [auto_gptq.quantization.gptq] avg loss: 0.15430966019630432\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/32...\n",
      "2024-10-30 08:19:05 INFO [auto_gptq.quantization.gptq] duration: 7.6191229820251465\n",
      "2024-10-30 08:19:05 INFO [auto_gptq.quantization.gptq] avg loss: 3.3717758655548096\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/32...\n",
      "2024-10-30 08:19:13 INFO [auto_gptq.quantization.gptq] duration: 8.217707633972168\n",
      "2024-10-30 08:19:13 INFO [auto_gptq.quantization.gptq] avg loss: 0.0018652137368917465\n",
      "INFO - Quantizing mlp.up_proj in layer 4/32...\n",
      "2024-10-30 08:19:22 INFO [auto_gptq.quantization.gptq] duration: 8.477033853530884\n",
      "2024-10-30 08:19:22 INFO [auto_gptq.quantization.gptq] avg loss: 2.122600793838501\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/32...\n",
      "2024-10-30 08:19:30 INFO [auto_gptq.quantization.gptq] duration: 8.34442400932312\n",
      "2024-10-30 08:19:30 INFO [auto_gptq.quantization.gptq] avg loss: 3.2897701263427734\n",
      "INFO - Quantizing mlp.down_proj in layer 4/32...\n",
      "2024-10-30 08:20:00 INFO [auto_gptq.quantization.gptq] duration: 29.632654428482056\n",
      "2024-10-30 08:20:00 INFO [auto_gptq.quantization.gptq] avg loss: 0.007349357940256596\n",
      "INFO - Start quantizing layer 5/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/32...\n",
      "2024-10-30 08:20:08 INFO [auto_gptq.quantization.gptq] duration: 8.474597692489624\n",
      "2024-10-30 08:20:08 INFO [auto_gptq.quantization.gptq] avg loss: 1.9397143125534058\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/32...\n",
      "2024-10-30 08:20:15 INFO [auto_gptq.quantization.gptq] duration: 7.060450553894043\n",
      "2024-10-30 08:20:15 INFO [auto_gptq.quantization.gptq] avg loss: 0.16960358619689941\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/32...\n",
      "2024-10-30 08:20:24 INFO [auto_gptq.quantization.gptq] duration: 8.526679992675781\n",
      "2024-10-30 08:20:24 INFO [auto_gptq.quantization.gptq] avg loss: 3.2601706981658936\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/32...\n",
      "2024-10-30 08:20:32 INFO [auto_gptq.quantization.gptq] duration: 8.336119174957275\n",
      "2024-10-30 08:20:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.003427261020988226\n",
      "INFO - Quantizing mlp.up_proj in layer 5/32...\n",
      "2024-10-30 08:20:41 INFO [auto_gptq.quantization.gptq] duration: 8.90831971168518\n",
      "2024-10-30 08:20:41 INFO [auto_gptq.quantization.gptq] avg loss: 2.5386962890625\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/32...\n",
      "2024-10-30 08:20:50 INFO [auto_gptq.quantization.gptq] duration: 8.380399703979492\n",
      "2024-10-30 08:20:50 INFO [auto_gptq.quantization.gptq] avg loss: 4.482138633728027\n",
      "INFO - Quantizing mlp.down_proj in layer 5/32...\n",
      "2024-10-30 08:21:19 INFO [auto_gptq.quantization.gptq] duration: 29.324504375457764\n",
      "2024-10-30 08:21:19 INFO [auto_gptq.quantization.gptq] avg loss: 0.012558701448142529\n",
      "INFO - Start quantizing layer 6/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/32...\n",
      "2024-10-30 08:21:27 INFO [auto_gptq.quantization.gptq] duration: 7.340981721878052\n",
      "2024-10-30 08:21:27 INFO [auto_gptq.quantization.gptq] avg loss: 2.871307611465454\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/32...\n",
      "2024-10-30 08:21:35 INFO [auto_gptq.quantization.gptq] duration: 8.701762437820435\n",
      "2024-10-30 08:21:35 INFO [auto_gptq.quantization.gptq] avg loss: 0.16306540369987488\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/32...\n",
      "2024-10-30 08:21:43 INFO [auto_gptq.quantization.gptq] duration: 8.091735363006592\n",
      "2024-10-30 08:21:43 INFO [auto_gptq.quantization.gptq] avg loss: 4.574190616607666\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/32...\n",
      "2024-10-30 08:21:51 INFO [auto_gptq.quantization.gptq] duration: 7.8829026222229\n",
      "2024-10-30 08:21:51 INFO [auto_gptq.quantization.gptq] avg loss: 0.004759425297379494\n",
      "INFO - Quantizing mlp.up_proj in layer 6/32...\n",
      "2024-10-30 08:22:01 INFO [auto_gptq.quantization.gptq] duration: 9.728351354598999\n",
      "2024-10-30 08:22:01 INFO [auto_gptq.quantization.gptq] avg loss: 2.935046434402466\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/32...\n",
      "2024-10-30 08:22:10 INFO [auto_gptq.quantization.gptq] duration: 9.049590110778809\n",
      "2024-10-30 08:22:10 INFO [auto_gptq.quantization.gptq] avg loss: 5.208375453948975\n",
      "INFO - Quantizing mlp.down_proj in layer 6/32...\n",
      "2024-10-30 08:22:38 INFO [auto_gptq.quantization.gptq] duration: 27.521717071533203\n",
      "2024-10-30 08:22:38 INFO [auto_gptq.quantization.gptq] avg loss: 0.01708236336708069\n",
      "INFO - Start quantizing layer 7/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/32...\n",
      "2024-10-30 08:22:47 INFO [auto_gptq.quantization.gptq] duration: 9.297240018844604\n",
      "2024-10-30 08:22:47 INFO [auto_gptq.quantization.gptq] avg loss: 2.17334246635437\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/32...\n",
      "2024-10-30 08:22:56 INFO [auto_gptq.quantization.gptq] duration: 8.46350622177124\n",
      "2024-10-30 08:22:56 INFO [auto_gptq.quantization.gptq] avg loss: 0.16762012243270874\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/32...\n",
      "2024-10-30 08:22:58 INFO [auto_gptq.quantization.gptq] duration: 2.3084716796875\n",
      "2024-10-30 08:22:58 INFO [auto_gptq.quantization.gptq] avg loss: 3.716184616088867\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/32...\n",
      "2024-10-30 08:22:59 INFO [auto_gptq.quantization.gptq] duration: 0.7515566349029541\n",
      "2024-10-30 08:22:59 INFO [auto_gptq.quantization.gptq] avg loss: 0.00757574662566185\n",
      "INFO - Quantizing mlp.up_proj in layer 7/32...\n",
      "2024-10-30 08:23:00 INFO [auto_gptq.quantization.gptq] duration: 0.7646174430847168\n",
      "2024-10-30 08:23:00 INFO [auto_gptq.quantization.gptq] avg loss: 3.3043696880340576\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/32...\n",
      "2024-10-30 08:23:00 INFO [auto_gptq.quantization.gptq] duration: 0.7752165794372559\n",
      "2024-10-30 08:23:00 INFO [auto_gptq.quantization.gptq] avg loss: 5.949164390563965\n",
      "INFO - Quantizing mlp.down_proj in layer 7/32...\n",
      "2024-10-30 08:23:03 INFO [auto_gptq.quantization.gptq] duration: 3.0168724060058594\n",
      "2024-10-30 08:23:03 INFO [auto_gptq.quantization.gptq] avg loss: 0.019619928672909737\n",
      "INFO - Start quantizing layer 8/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/32...\n",
      "2024-10-30 08:23:05 INFO [auto_gptq.quantization.gptq] duration: 1.0251798629760742\n",
      "2024-10-30 08:23:05 INFO [auto_gptq.quantization.gptq] avg loss: 2.1070549488067627\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/32...\n",
      "2024-10-30 08:23:06 INFO [auto_gptq.quantization.gptq] duration: 1.0046427249908447\n",
      "2024-10-30 08:23:06 INFO [auto_gptq.quantization.gptq] avg loss: 0.1636628806591034\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/32...\n",
      "2024-10-30 08:23:07 INFO [auto_gptq.quantization.gptq] duration: 1.036513328552246\n",
      "2024-10-30 08:23:07 INFO [auto_gptq.quantization.gptq] avg loss: 3.366272449493408\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/32...\n",
      "2024-10-30 08:23:08 INFO [auto_gptq.quantization.gptq] duration: 1.0406818389892578\n",
      "2024-10-30 08:23:08 INFO [auto_gptq.quantization.gptq] avg loss: 0.012991133145987988\n",
      "INFO - Quantizing mlp.up_proj in layer 8/32...\n",
      "2024-10-30 08:23:09 INFO [auto_gptq.quantization.gptq] duration: 0.8265926837921143\n",
      "2024-10-30 08:23:09 INFO [auto_gptq.quantization.gptq] avg loss: 3.3469395637512207\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/32...\n",
      "2024-10-30 08:23:09 INFO [auto_gptq.quantization.gptq] duration: 0.6927039623260498\n",
      "2024-10-30 08:23:09 INFO [auto_gptq.quantization.gptq] avg loss: 5.6031951904296875\n",
      "INFO - Quantizing mlp.down_proj in layer 8/32...\n",
      "2024-10-30 08:23:13 INFO [auto_gptq.quantization.gptq] duration: 3.4401893615722656\n",
      "2024-10-30 08:23:13 INFO [auto_gptq.quantization.gptq] avg loss: 0.022065721452236176\n",
      "INFO - Start quantizing layer 9/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/32...\n",
      "2024-10-30 08:23:14 INFO [auto_gptq.quantization.gptq] duration: 1.00180983543396\n",
      "2024-10-30 08:23:14 INFO [auto_gptq.quantization.gptq] avg loss: 2.7274293899536133\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/32...\n",
      "2024-10-30 08:23:15 INFO [auto_gptq.quantization.gptq] duration: 0.6552410125732422\n",
      "2024-10-30 08:23:15 INFO [auto_gptq.quantization.gptq] avg loss: 0.21766620874404907\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/32...\n",
      "2024-10-30 08:23:15 INFO [auto_gptq.quantization.gptq] duration: 0.6730568408966064\n",
      "2024-10-30 08:23:15 INFO [auto_gptq.quantization.gptq] avg loss: 4.318254470825195\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/32...\n",
      "2024-10-30 08:23:16 INFO [auto_gptq.quantization.gptq] duration: 0.669461727142334\n",
      "2024-10-30 08:23:16 INFO [auto_gptq.quantization.gptq] avg loss: 0.01973799802362919\n",
      "INFO - Quantizing mlp.up_proj in layer 9/32...\n",
      "2024-10-30 08:23:17 INFO [auto_gptq.quantization.gptq] duration: 0.6900115013122559\n",
      "2024-10-30 08:23:17 INFO [auto_gptq.quantization.gptq] avg loss: 3.496938467025757\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/32...\n",
      "2024-10-30 08:23:17 INFO [auto_gptq.quantization.gptq] duration: 0.6921286582946777\n",
      "2024-10-30 08:23:17 INFO [auto_gptq.quantization.gptq] avg loss: 5.990079402923584\n",
      "INFO - Quantizing mlp.down_proj in layer 9/32...\n",
      "2024-10-30 08:23:20 INFO [auto_gptq.quantization.gptq] duration: 2.8321006298065186\n",
      "2024-10-30 08:23:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.02571762353181839\n",
      "INFO - Start quantizing layer 10/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/32...\n",
      "2024-10-30 08:23:21 INFO [auto_gptq.quantization.gptq] duration: 0.6327075958251953\n",
      "2024-10-30 08:23:21 INFO [auto_gptq.quantization.gptq] avg loss: 2.4174017906188965\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/32...\n",
      "2024-10-30 08:23:22 INFO [auto_gptq.quantization.gptq] duration: 0.6408584117889404\n",
      "2024-10-30 08:23:22 INFO [auto_gptq.quantization.gptq] avg loss: 0.27908700704574585\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/32...\n",
      "2024-10-30 08:23:22 INFO [auto_gptq.quantization.gptq] duration: 0.6703073978424072\n",
      "2024-10-30 08:23:22 INFO [auto_gptq.quantization.gptq] avg loss: 3.8653459548950195\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/32...\n",
      "2024-10-30 08:23:23 INFO [auto_gptq.quantization.gptq] duration: 0.6694848537445068\n",
      "2024-10-30 08:23:23 INFO [auto_gptq.quantization.gptq] avg loss: 0.01904677227139473\n",
      "INFO - Quantizing mlp.up_proj in layer 10/32...\n",
      "2024-10-30 08:23:24 INFO [auto_gptq.quantization.gptq] duration: 0.689176082611084\n",
      "2024-10-30 08:23:24 INFO [auto_gptq.quantization.gptq] avg loss: 3.709043025970459\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/32...\n",
      "2024-10-30 08:23:24 INFO [auto_gptq.quantization.gptq] duration: 0.6884393692016602\n",
      "2024-10-30 08:23:24 INFO [auto_gptq.quantization.gptq] avg loss: 6.343160152435303\n",
      "INFO - Quantizing mlp.down_proj in layer 10/32...\n",
      "2024-10-30 08:23:27 INFO [auto_gptq.quantization.gptq] duration: 2.8250367641448975\n",
      "2024-10-30 08:23:27 INFO [auto_gptq.quantization.gptq] avg loss: 0.02898033708333969\n",
      "INFO - Start quantizing layer 11/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/32...\n",
      "2024-10-30 08:23:29 INFO [auto_gptq.quantization.gptq] duration: 0.9903228282928467\n",
      "2024-10-30 08:23:29 INFO [auto_gptq.quantization.gptq] avg loss: 3.3559765815734863\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/32...\n",
      "2024-10-30 08:23:30 INFO [auto_gptq.quantization.gptq] duration: 1.0269114971160889\n",
      "2024-10-30 08:23:30 INFO [auto_gptq.quantization.gptq] avg loss: 0.24244371056556702\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/32...\n",
      "2024-10-30 08:23:31 INFO [auto_gptq.quantization.gptq] duration: 0.9965977668762207\n",
      "2024-10-30 08:23:31 INFO [auto_gptq.quantization.gptq] avg loss: 5.141538619995117\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/32...\n",
      "2024-10-30 08:23:31 INFO [auto_gptq.quantization.gptq] duration: 0.769179105758667\n",
      "2024-10-30 08:23:31 INFO [auto_gptq.quantization.gptq] avg loss: 0.019364133477211\n",
      "INFO - Quantizing mlp.up_proj in layer 11/32...\n",
      "2024-10-30 08:23:32 INFO [auto_gptq.quantization.gptq] duration: 0.6824836730957031\n",
      "2024-10-30 08:23:32 INFO [auto_gptq.quantization.gptq] avg loss: 3.2441835403442383\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/32...\n",
      "2024-10-30 08:23:33 INFO [auto_gptq.quantization.gptq] duration: 0.6846058368682861\n",
      "2024-10-30 08:23:33 INFO [auto_gptq.quantization.gptq] avg loss: 5.129234313964844\n",
      "INFO - Quantizing mlp.down_proj in layer 11/32...\n",
      "2024-10-30 08:23:36 INFO [auto_gptq.quantization.gptq] duration: 2.824298143386841\n",
      "2024-10-30 08:23:36 INFO [auto_gptq.quantization.gptq] avg loss: 0.22699344158172607\n",
      "INFO - Start quantizing layer 12/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/32...\n",
      "2024-10-30 08:23:36 INFO [auto_gptq.quantization.gptq] duration: 0.6340494155883789\n",
      "2024-10-30 08:23:36 INFO [auto_gptq.quantization.gptq] avg loss: 2.790926694869995\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/32...\n",
      "2024-10-30 08:23:37 INFO [auto_gptq.quantization.gptq] duration: 0.6346836090087891\n",
      "2024-10-30 08:23:37 INFO [auto_gptq.quantization.gptq] avg loss: 0.2277548462152481\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/32...\n",
      "2024-10-30 08:23:38 INFO [auto_gptq.quantization.gptq] duration: 0.6625807285308838\n",
      "2024-10-30 08:23:38 INFO [auto_gptq.quantization.gptq] avg loss: 4.091871738433838\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/32...\n",
      "2024-10-30 08:23:38 INFO [auto_gptq.quantization.gptq] duration: 0.6640803813934326\n",
      "2024-10-30 08:23:38 INFO [auto_gptq.quantization.gptq] avg loss: 0.023841027170419693\n",
      "INFO - Quantizing mlp.up_proj in layer 12/32...\n",
      "2024-10-30 08:23:39 INFO [auto_gptq.quantization.gptq] duration: 0.6851658821105957\n",
      "2024-10-30 08:23:39 INFO [auto_gptq.quantization.gptq] avg loss: 3.7377548217773438\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/32...\n",
      "2024-10-30 08:23:40 INFO [auto_gptq.quantization.gptq] duration: 0.6864769458770752\n",
      "2024-10-30 08:23:40 INFO [auto_gptq.quantization.gptq] avg loss: 5.753993511199951\n",
      "INFO - Quantizing mlp.down_proj in layer 12/32...\n",
      "2024-10-30 08:23:43 INFO [auto_gptq.quantization.gptq] duration: 2.8230326175689697\n",
      "2024-10-30 08:23:43 INFO [auto_gptq.quantization.gptq] avg loss: 0.02707657963037491\n",
      "INFO - Start quantizing layer 13/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/32...\n",
      "2024-10-30 08:23:44 INFO [auto_gptq.quantization.gptq] duration: 0.6381351947784424\n",
      "2024-10-30 08:23:44 INFO [auto_gptq.quantization.gptq] avg loss: 2.15982985496521\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/32...\n",
      "2024-10-30 08:23:44 INFO [auto_gptq.quantization.gptq] duration: 0.9032399654388428\n",
      "2024-10-30 08:23:44 INFO [auto_gptq.quantization.gptq] avg loss: 0.2686922550201416\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/32...\n",
      "2024-10-30 08:23:45 INFO [auto_gptq.quantization.gptq] duration: 1.0056190490722656\n",
      "2024-10-30 08:23:45 INFO [auto_gptq.quantization.gptq] avg loss: 3.6072685718536377\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/32...\n",
      "2024-10-30 08:23:46 INFO [auto_gptq.quantization.gptq] duration: 0.6904573440551758\n",
      "2024-10-30 08:23:46 INFO [auto_gptq.quantization.gptq] avg loss: 0.02680094540119171\n",
      "INFO - Quantizing mlp.up_proj in layer 13/32...\n",
      "2024-10-30 08:23:47 INFO [auto_gptq.quantization.gptq] duration: 0.6867432594299316\n",
      "2024-10-30 08:23:47 INFO [auto_gptq.quantization.gptq] avg loss: 4.1996073722839355\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/32...\n",
      "2024-10-30 08:23:48 INFO [auto_gptq.quantization.gptq] duration: 0.6889848709106445\n",
      "2024-10-30 08:23:48 INFO [auto_gptq.quantization.gptq] avg loss: 6.219916343688965\n",
      "INFO - Quantizing mlp.down_proj in layer 13/32...\n",
      "2024-10-30 08:23:50 INFO [auto_gptq.quantization.gptq] duration: 2.817117929458618\n",
      "2024-10-30 08:23:50 INFO [auto_gptq.quantization.gptq] avg loss: 0.031323887407779694\n",
      "INFO - Start quantizing layer 14/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/32...\n",
      "2024-10-30 08:23:51 INFO [auto_gptq.quantization.gptq] duration: 0.6316406726837158\n",
      "2024-10-30 08:23:51 INFO [auto_gptq.quantization.gptq] avg loss: 3.045475721359253\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/32...\n",
      "2024-10-30 08:23:52 INFO [auto_gptq.quantization.gptq] duration: 0.6345148086547852\n",
      "2024-10-30 08:23:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.2771906852722168\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/32...\n",
      "2024-10-30 08:23:53 INFO [auto_gptq.quantization.gptq] duration: 0.6674294471740723\n",
      "2024-10-30 08:23:53 INFO [auto_gptq.quantization.gptq] avg loss: 4.456707954406738\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/32...\n",
      "2024-10-30 08:23:53 INFO [auto_gptq.quantization.gptq] duration: 0.6695916652679443\n",
      "2024-10-30 08:23:53 INFO [auto_gptq.quantization.gptq] avg loss: 0.02957991324365139\n",
      "INFO - Quantizing mlp.up_proj in layer 14/32...\n",
      "2024-10-30 08:23:54 INFO [auto_gptq.quantization.gptq] duration: 0.6873900890350342\n",
      "2024-10-30 08:23:54 INFO [auto_gptq.quantization.gptq] avg loss: 4.3941264152526855\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/32...\n",
      "2024-10-30 08:23:55 INFO [auto_gptq.quantization.gptq] duration: 1.0173759460449219\n",
      "2024-10-30 08:23:55 INFO [auto_gptq.quantization.gptq] avg loss: 6.605837345123291\n",
      "INFO - Quantizing mlp.down_proj in layer 14/32...\n",
      "2024-10-30 08:23:58 INFO [auto_gptq.quantization.gptq] duration: 2.8982298374176025\n",
      "2024-10-30 08:23:58 INFO [auto_gptq.quantization.gptq] avg loss: 0.03599254786968231\n",
      "INFO - Start quantizing layer 15/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/32...\n",
      "2024-10-30 08:23:59 INFO [auto_gptq.quantization.gptq] duration: 0.6431002616882324\n",
      "2024-10-30 08:23:59 INFO [auto_gptq.quantization.gptq] avg loss: 3.7889716625213623\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/32...\n",
      "2024-10-30 08:23:59 INFO [auto_gptq.quantization.gptq] duration: 0.6428864002227783\n",
      "2024-10-30 08:23:59 INFO [auto_gptq.quantization.gptq] avg loss: 0.3350851833820343\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/32...\n",
      "2024-10-30 08:24:00 INFO [auto_gptq.quantization.gptq] duration: 0.6642353534698486\n",
      "2024-10-30 08:24:00 INFO [auto_gptq.quantization.gptq] avg loss: 5.321682929992676\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/32...\n",
      "2024-10-30 08:24:01 INFO [auto_gptq.quantization.gptq] duration: 0.6700839996337891\n",
      "2024-10-30 08:24:01 INFO [auto_gptq.quantization.gptq] avg loss: 0.030687108635902405\n",
      "INFO - Quantizing mlp.up_proj in layer 15/32...\n",
      "2024-10-30 08:24:01 INFO [auto_gptq.quantization.gptq] duration: 0.6833271980285645\n",
      "2024-10-30 08:24:01 INFO [auto_gptq.quantization.gptq] avg loss: 5.0542497634887695\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/32...\n",
      "2024-10-30 08:24:02 INFO [auto_gptq.quantization.gptq] duration: 0.68373703956604\n",
      "2024-10-30 08:24:02 INFO [auto_gptq.quantization.gptq] avg loss: 8.230281829833984\n",
      "INFO - Quantizing mlp.down_proj in layer 15/32...\n",
      "2024-10-30 08:24:05 INFO [auto_gptq.quantization.gptq] duration: 2.8208367824554443\n",
      "2024-10-30 08:24:05 INFO [auto_gptq.quantization.gptq] avg loss: 0.04462345689535141\n",
      "INFO - Start quantizing layer 16/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/32...\n",
      "2024-10-30 08:24:06 INFO [auto_gptq.quantization.gptq] duration: 0.638861894607544\n",
      "2024-10-30 08:24:06 INFO [auto_gptq.quantization.gptq] avg loss: 3.027815818786621\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/32...\n",
      "2024-10-30 08:24:06 INFO [auto_gptq.quantization.gptq] duration: 0.6427297592163086\n",
      "2024-10-30 08:24:06 INFO [auto_gptq.quantization.gptq] avg loss: 0.3707875609397888\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/32...\n",
      "2024-10-30 08:24:07 INFO [auto_gptq.quantization.gptq] duration: 0.6638426780700684\n",
      "2024-10-30 08:24:07 INFO [auto_gptq.quantization.gptq] avg loss: 5.551547050476074\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/32...\n",
      "2024-10-30 08:24:08 INFO [auto_gptq.quantization.gptq] duration: 0.6663141250610352\n",
      "2024-10-30 08:24:08 INFO [auto_gptq.quantization.gptq] avg loss: 0.02362232469022274\n",
      "INFO - Quantizing mlp.up_proj in layer 16/32...\n",
      "2024-10-30 08:24:08 INFO [auto_gptq.quantization.gptq] duration: 0.683795690536499\n",
      "2024-10-30 08:24:08 INFO [auto_gptq.quantization.gptq] avg loss: 4.809642314910889\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/32...\n",
      "2024-10-30 08:24:09 INFO [auto_gptq.quantization.gptq] duration: 0.6837301254272461\n",
      "2024-10-30 08:24:09 INFO [auto_gptq.quantization.gptq] avg loss: 8.196983337402344\n",
      "INFO - Quantizing mlp.down_proj in layer 16/32...\n",
      "2024-10-30 08:24:12 INFO [auto_gptq.quantization.gptq] duration: 2.8177568912506104\n",
      "2024-10-30 08:24:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.05468032509088516\n",
      "INFO - Start quantizing layer 17/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/32...\n",
      "2024-10-30 08:24:13 INFO [auto_gptq.quantization.gptq] duration: 0.6448872089385986\n",
      "2024-10-30 08:24:13 INFO [auto_gptq.quantization.gptq] avg loss: 3.1279354095458984\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/32...\n",
      "2024-10-30 08:24:13 INFO [auto_gptq.quantization.gptq] duration: 0.6390290260314941\n",
      "2024-10-30 08:24:13 INFO [auto_gptq.quantization.gptq] avg loss: 0.3542017936706543\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/32...\n",
      "2024-10-30 08:24:14 INFO [auto_gptq.quantization.gptq] duration: 0.6629433631896973\n",
      "2024-10-30 08:24:14 INFO [auto_gptq.quantization.gptq] avg loss: 5.378959655761719\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/32...\n",
      "2024-10-30 08:24:15 INFO [auto_gptq.quantization.gptq] duration: 0.6657412052154541\n",
      "2024-10-30 08:24:15 INFO [auto_gptq.quantization.gptq] avg loss: 0.02935163490474224\n",
      "INFO - Quantizing mlp.up_proj in layer 17/32...\n",
      "2024-10-30 08:24:16 INFO [auto_gptq.quantization.gptq] duration: 0.6856932640075684\n",
      "2024-10-30 08:24:16 INFO [auto_gptq.quantization.gptq] avg loss: 4.608745098114014\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/32...\n",
      "2024-10-30 08:24:16 INFO [auto_gptq.quantization.gptq] duration: 0.6827833652496338\n",
      "2024-10-30 08:24:16 INFO [auto_gptq.quantization.gptq] avg loss: 8.244406700134277\n",
      "INFO - Quantizing mlp.down_proj in layer 17/32...\n",
      "2024-10-30 08:24:19 INFO [auto_gptq.quantization.gptq] duration: 2.821237087249756\n",
      "2024-10-30 08:24:19 INFO [auto_gptq.quantization.gptq] avg loss: 0.06366308033466339\n",
      "INFO - Start quantizing layer 18/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/32...\n",
      "2024-10-30 08:24:20 INFO [auto_gptq.quantization.gptq] duration: 0.6371912956237793\n",
      "2024-10-30 08:24:20 INFO [auto_gptq.quantization.gptq] avg loss: 3.0500950813293457\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/32...\n",
      "2024-10-30 08:24:21 INFO [auto_gptq.quantization.gptq] duration: 0.6385703086853027\n",
      "2024-10-30 08:24:21 INFO [auto_gptq.quantization.gptq] avg loss: 0.3493248224258423\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/32...\n",
      "2024-10-30 08:24:21 INFO [auto_gptq.quantization.gptq] duration: 0.6648209095001221\n",
      "2024-10-30 08:24:21 INFO [auto_gptq.quantization.gptq] avg loss: 5.062129497528076\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/32...\n",
      "2024-10-30 08:24:22 INFO [auto_gptq.quantization.gptq] duration: 1.0235822200775146\n",
      "2024-10-30 08:24:22 INFO [auto_gptq.quantization.gptq] avg loss: 0.01996644213795662\n",
      "INFO - Quantizing mlp.up_proj in layer 18/32...\n",
      "2024-10-30 08:24:23 INFO [auto_gptq.quantization.gptq] duration: 1.0249526500701904\n",
      "2024-10-30 08:24:23 INFO [auto_gptq.quantization.gptq] avg loss: 4.525355339050293\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/32...\n",
      "2024-10-30 08:24:24 INFO [auto_gptq.quantization.gptq] duration: 0.9934630393981934\n",
      "2024-10-30 08:24:24 INFO [auto_gptq.quantization.gptq] avg loss: 8.311567306518555\n",
      "INFO - Quantizing mlp.down_proj in layer 18/32...\n",
      "2024-10-30 08:24:27 INFO [auto_gptq.quantization.gptq] duration: 3.0210278034210205\n",
      "2024-10-30 08:24:27 INFO [auto_gptq.quantization.gptq] avg loss: 0.08415672183036804\n",
      "INFO - Start quantizing layer 19/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/32...\n",
      "2024-10-30 08:24:28 INFO [auto_gptq.quantization.gptq] duration: 0.637204647064209\n",
      "2024-10-30 08:24:28 INFO [auto_gptq.quantization.gptq] avg loss: 3.4031903743743896\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/32...\n",
      "2024-10-30 08:24:29 INFO [auto_gptq.quantization.gptq] duration: 0.630835771560669\n",
      "2024-10-30 08:24:29 INFO [auto_gptq.quantization.gptq] avg loss: 0.3560396134853363\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/32...\n",
      "2024-10-30 08:24:29 INFO [auto_gptq.quantization.gptq] duration: 0.6643140316009521\n",
      "2024-10-30 08:24:29 INFO [auto_gptq.quantization.gptq] avg loss: 5.068970680236816\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/32...\n",
      "2024-10-30 08:24:30 INFO [auto_gptq.quantization.gptq] duration: 0.6656832695007324\n",
      "2024-10-30 08:24:30 INFO [auto_gptq.quantization.gptq] avg loss: 0.01162052620202303\n",
      "INFO - Quantizing mlp.up_proj in layer 19/32...\n",
      "2024-10-30 08:24:31 INFO [auto_gptq.quantization.gptq] duration: 0.6825823783874512\n",
      "2024-10-30 08:24:31 INFO [auto_gptq.quantization.gptq] avg loss: 4.310252666473389\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/32...\n",
      "2024-10-30 08:24:32 INFO [auto_gptq.quantization.gptq] duration: 0.6825344562530518\n",
      "2024-10-30 08:24:32 INFO [auto_gptq.quantization.gptq] avg loss: 7.924889087677002\n",
      "INFO - Quantizing mlp.down_proj in layer 19/32...\n",
      "2024-10-30 08:24:34 INFO [auto_gptq.quantization.gptq] duration: 2.819945812225342\n",
      "2024-10-30 08:24:34 INFO [auto_gptq.quantization.gptq] avg loss: 0.06767208874225616\n",
      "INFO - Start quantizing layer 20/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/32...\n",
      "2024-10-30 08:24:35 INFO [auto_gptq.quantization.gptq] duration: 0.6377036571502686\n",
      "2024-10-30 08:24:35 INFO [auto_gptq.quantization.gptq] avg loss: 3.2177462577819824\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/32...\n",
      "2024-10-30 08:24:36 INFO [auto_gptq.quantization.gptq] duration: 0.6337978839874268\n",
      "2024-10-30 08:24:36 INFO [auto_gptq.quantization.gptq] avg loss: 0.3803046643733978\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/32...\n",
      "2024-10-30 08:24:37 INFO [auto_gptq.quantization.gptq] duration: 0.6685996055603027\n",
      "2024-10-30 08:24:37 INFO [auto_gptq.quantization.gptq] avg loss: 5.318844795227051\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/32...\n",
      "2024-10-30 08:24:37 INFO [auto_gptq.quantization.gptq] duration: 0.6665921211242676\n",
      "2024-10-30 08:24:37 INFO [auto_gptq.quantization.gptq] avg loss: 0.010474678128957748\n",
      "INFO - Quantizing mlp.up_proj in layer 20/32...\n",
      "2024-10-30 08:24:38 INFO [auto_gptq.quantization.gptq] duration: 0.6838321685791016\n",
      "2024-10-30 08:24:38 INFO [auto_gptq.quantization.gptq] avg loss: 4.528053283691406\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/32...\n",
      "2024-10-30 08:24:39 INFO [auto_gptq.quantization.gptq] duration: 0.6849365234375\n",
      "2024-10-30 08:24:39 INFO [auto_gptq.quantization.gptq] avg loss: 8.470098495483398\n",
      "INFO - Quantizing mlp.down_proj in layer 20/32...\n",
      "2024-10-30 08:24:41 INFO [auto_gptq.quantization.gptq] duration: 2.821955442428589\n",
      "2024-10-30 08:24:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.06649299710988998\n",
      "INFO - Start quantizing layer 21/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/32...\n",
      "2024-10-30 08:24:42 INFO [auto_gptq.quantization.gptq] duration: 0.6388185024261475\n",
      "2024-10-30 08:24:42 INFO [auto_gptq.quantization.gptq] avg loss: 3.4987025260925293\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/32...\n",
      "2024-10-30 08:24:43 INFO [auto_gptq.quantization.gptq] duration: 0.9887664318084717\n",
      "2024-10-30 08:24:43 INFO [auto_gptq.quantization.gptq] avg loss: 0.4227803647518158\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/32...\n",
      "2024-10-30 08:24:44 INFO [auto_gptq.quantization.gptq] duration: 0.9909965991973877\n",
      "2024-10-30 08:24:44 INFO [auto_gptq.quantization.gptq] avg loss: 5.475636959075928\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/32...\n",
      "2024-10-30 08:24:45 INFO [auto_gptq.quantization.gptq] duration: 0.9630591869354248\n",
      "2024-10-30 08:24:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.008370958268642426\n",
      "INFO - Quantizing mlp.up_proj in layer 21/32...\n",
      "2024-10-30 08:24:46 INFO [auto_gptq.quantization.gptq] duration: 0.6820778846740723\n",
      "2024-10-30 08:24:46 INFO [auto_gptq.quantization.gptq] avg loss: 4.668856143951416\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/32...\n",
      "2024-10-30 08:24:47 INFO [auto_gptq.quantization.gptq] duration: 1.0569522380828857\n",
      "2024-10-30 08:24:47 INFO [auto_gptq.quantization.gptq] avg loss: 8.70762825012207\n",
      "INFO - Quantizing mlp.down_proj in layer 21/32...\n",
      "2024-10-30 08:24:50 INFO [auto_gptq.quantization.gptq] duration: 3.290433168411255\n",
      "2024-10-30 08:24:50 INFO [auto_gptq.quantization.gptq] avg loss: 0.08092903345823288\n",
      "INFO - Start quantizing layer 22/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/32...\n",
      "2024-10-30 08:24:51 INFO [auto_gptq.quantization.gptq] duration: 0.6803467273712158\n",
      "2024-10-30 08:24:51 INFO [auto_gptq.quantization.gptq] avg loss: 3.2141809463500977\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/32...\n",
      "2024-10-30 08:24:52 INFO [auto_gptq.quantization.gptq] duration: 1.0341272354125977\n",
      "2024-10-30 08:24:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.4506223797798157\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/32...\n",
      "2024-10-30 08:24:53 INFO [auto_gptq.quantization.gptq] duration: 0.997504711151123\n",
      "2024-10-30 08:24:53 INFO [auto_gptq.quantization.gptq] avg loss: 5.119865417480469\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/32...\n",
      "2024-10-30 08:24:54 INFO [auto_gptq.quantization.gptq] duration: 0.852484941482544\n",
      "2024-10-30 08:24:54 INFO [auto_gptq.quantization.gptq] avg loss: 0.015781033784151077\n",
      "INFO - Quantizing mlp.up_proj in layer 22/32...\n",
      "2024-10-30 08:24:55 INFO [auto_gptq.quantization.gptq] duration: 0.6865391731262207\n",
      "2024-10-30 08:24:55 INFO [auto_gptq.quantization.gptq] avg loss: 4.993688583374023\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/32...\n",
      "2024-10-30 08:24:56 INFO [auto_gptq.quantization.gptq] duration: 1.0961449146270752\n",
      "2024-10-30 08:24:56 INFO [auto_gptq.quantization.gptq] avg loss: 9.444746017456055\n",
      "INFO - Quantizing mlp.down_proj in layer 22/32...\n",
      "2024-10-30 08:25:00 INFO [auto_gptq.quantization.gptq] duration: 3.749769687652588\n",
      "2024-10-30 08:25:00 INFO [auto_gptq.quantization.gptq] avg loss: 0.08386433124542236\n",
      "INFO - Start quantizing layer 23/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/32...\n",
      "2024-10-30 08:25:01 INFO [auto_gptq.quantization.gptq] duration: 1.0289595127105713\n",
      "2024-10-30 08:25:01 INFO [auto_gptq.quantization.gptq] avg loss: 3.515753746032715\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/32...\n",
      "2024-10-30 08:25:02 INFO [auto_gptq.quantization.gptq] duration: 0.8241438865661621\n",
      "2024-10-30 08:25:02 INFO [auto_gptq.quantization.gptq] avg loss: 0.5364612340927124\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/32...\n",
      "2024-10-30 08:25:03 INFO [auto_gptq.quantization.gptq] duration: 0.6926472187042236\n",
      "2024-10-30 08:25:03 INFO [auto_gptq.quantization.gptq] avg loss: 5.255095481872559\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/32...\n",
      "2024-10-30 08:25:03 INFO [auto_gptq.quantization.gptq] duration: 0.6773984432220459\n",
      "2024-10-30 08:25:03 INFO [auto_gptq.quantization.gptq] avg loss: 0.011732600629329681\n",
      "INFO - Quantizing mlp.up_proj in layer 23/32...\n",
      "2024-10-30 08:25:04 INFO [auto_gptq.quantization.gptq] duration: 0.7409934997558594\n",
      "2024-10-30 08:25:04 INFO [auto_gptq.quantization.gptq] avg loss: 5.264748573303223\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/32...\n",
      "2024-10-30 08:25:05 INFO [auto_gptq.quantization.gptq] duration: 0.7238552570343018\n",
      "2024-10-30 08:25:05 INFO [auto_gptq.quantization.gptq] avg loss: 9.779818534851074\n",
      "INFO - Quantizing mlp.down_proj in layer 23/32...\n",
      "2024-10-30 08:25:09 INFO [auto_gptq.quantization.gptq] duration: 3.9880189895629883\n",
      "2024-10-30 08:25:09 INFO [auto_gptq.quantization.gptq] avg loss: 0.08359816670417786\n",
      "INFO - Start quantizing layer 24/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/32...\n",
      "2024-10-30 08:25:10 INFO [auto_gptq.quantization.gptq] duration: 0.6403539180755615\n",
      "2024-10-30 08:25:10 INFO [auto_gptq.quantization.gptq] avg loss: 3.4418718814849854\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/32...\n",
      "2024-10-30 08:25:10 INFO [auto_gptq.quantization.gptq] duration: 0.6408452987670898\n",
      "2024-10-30 08:25:10 INFO [auto_gptq.quantization.gptq] avg loss: 0.5572623014450073\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/32...\n",
      "2024-10-30 08:25:11 INFO [auto_gptq.quantization.gptq] duration: 0.6641359329223633\n",
      "2024-10-30 08:25:11 INFO [auto_gptq.quantization.gptq] avg loss: 5.203144073486328\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/32...\n",
      "2024-10-30 08:25:12 INFO [auto_gptq.quantization.gptq] duration: 0.6637964248657227\n",
      "2024-10-30 08:25:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.009468404576182365\n",
      "INFO - Quantizing mlp.up_proj in layer 24/32...\n",
      "2024-10-30 08:25:12 INFO [auto_gptq.quantization.gptq] duration: 0.6857712268829346\n",
      "2024-10-30 08:25:12 INFO [auto_gptq.quantization.gptq] avg loss: 5.434329509735107\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/32...\n",
      "2024-10-30 08:25:13 INFO [auto_gptq.quantization.gptq] duration: 0.6873073577880859\n",
      "2024-10-30 08:25:13 INFO [auto_gptq.quantization.gptq] avg loss: 9.982233047485352\n",
      "INFO - Quantizing mlp.down_proj in layer 24/32...\n",
      "2024-10-30 08:25:16 INFO [auto_gptq.quantization.gptq] duration: 2.819167137145996\n",
      "2024-10-30 08:25:16 INFO [auto_gptq.quantization.gptq] avg loss: 0.10776354372501373\n",
      "INFO - Start quantizing layer 25/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/32...\n",
      "2024-10-30 08:25:17 INFO [auto_gptq.quantization.gptq] duration: 0.643519401550293\n",
      "2024-10-30 08:25:17 INFO [auto_gptq.quantization.gptq] avg loss: 3.3116140365600586\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/32...\n",
      "2024-10-30 08:25:17 INFO [auto_gptq.quantization.gptq] duration: 0.6523044109344482\n",
      "2024-10-30 08:25:17 INFO [auto_gptq.quantization.gptq] avg loss: 0.7263853549957275\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/32...\n",
      "2024-10-30 08:25:18 INFO [auto_gptq.quantization.gptq] duration: 0.6640760898590088\n",
      "2024-10-30 08:25:18 INFO [auto_gptq.quantization.gptq] avg loss: 5.201694011688232\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/32...\n",
      "2024-10-30 08:25:19 INFO [auto_gptq.quantization.gptq] duration: 0.6663703918457031\n",
      "2024-10-30 08:25:19 INFO [auto_gptq.quantization.gptq] avg loss: 0.013845521956682205\n",
      "INFO - Quantizing mlp.up_proj in layer 25/32...\n",
      "2024-10-30 08:25:20 INFO [auto_gptq.quantization.gptq] duration: 1.0460145473480225\n",
      "2024-10-30 08:25:20 INFO [auto_gptq.quantization.gptq] avg loss: 5.803417205810547\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/32...\n",
      "2024-10-30 08:25:20 INFO [auto_gptq.quantization.gptq] duration: 0.6851811408996582\n",
      "2024-10-30 08:25:20 INFO [auto_gptq.quantization.gptq] avg loss: 10.740999221801758\n",
      "INFO - Quantizing mlp.down_proj in layer 25/32...\n",
      "2024-10-30 08:25:23 INFO [auto_gptq.quantization.gptq] duration: 2.8198721408843994\n",
      "2024-10-30 08:25:23 INFO [auto_gptq.quantization.gptq] avg loss: 0.09952988475561142\n",
      "INFO - Start quantizing layer 26/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/32...\n",
      "2024-10-30 08:25:24 INFO [auto_gptq.quantization.gptq] duration: 0.640430212020874\n",
      "2024-10-30 08:25:24 INFO [auto_gptq.quantization.gptq] avg loss: 3.178253173828125\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/32...\n",
      "2024-10-30 08:25:25 INFO [auto_gptq.quantization.gptq] duration: 0.6389398574829102\n",
      "2024-10-30 08:25:25 INFO [auto_gptq.quantization.gptq] avg loss: 0.7512090802192688\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/32...\n",
      "2024-10-30 08:25:25 INFO [auto_gptq.quantization.gptq] duration: 0.6634540557861328\n",
      "2024-10-30 08:25:25 INFO [auto_gptq.quantization.gptq] avg loss: 5.1941680908203125\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/32...\n",
      "2024-10-30 08:25:26 INFO [auto_gptq.quantization.gptq] duration: 0.6646127700805664\n",
      "2024-10-30 08:25:26 INFO [auto_gptq.quantization.gptq] avg loss: 0.018195414915680885\n",
      "INFO - Quantizing mlp.up_proj in layer 26/32...\n",
      "2024-10-30 08:25:27 INFO [auto_gptq.quantization.gptq] duration: 0.6831169128417969\n",
      "2024-10-30 08:25:27 INFO [auto_gptq.quantization.gptq] avg loss: 6.287962913513184\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/32...\n",
      "2024-10-30 08:25:28 INFO [auto_gptq.quantization.gptq] duration: 0.6848940849304199\n",
      "2024-10-30 08:25:28 INFO [auto_gptq.quantization.gptq] avg loss: 11.5087890625\n",
      "INFO - Quantizing mlp.down_proj in layer 26/32...\n",
      "2024-10-30 08:25:30 INFO [auto_gptq.quantization.gptq] duration: 2.81850266456604\n",
      "2024-10-30 08:25:30 INFO [auto_gptq.quantization.gptq] avg loss: 0.10482949763536453\n",
      "INFO - Start quantizing layer 27/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/32...\n",
      "2024-10-30 08:25:31 INFO [auto_gptq.quantization.gptq] duration: 0.6368556022644043\n",
      "2024-10-30 08:25:31 INFO [auto_gptq.quantization.gptq] avg loss: 3.36843204498291\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/32...\n",
      "2024-10-30 08:25:32 INFO [auto_gptq.quantization.gptq] duration: 0.6367359161376953\n",
      "2024-10-30 08:25:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.7358830571174622\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/32...\n",
      "2024-10-30 08:25:33 INFO [auto_gptq.quantization.gptq] duration: 0.667107105255127\n",
      "2024-10-30 08:25:33 INFO [auto_gptq.quantization.gptq] avg loss: 4.9942779541015625\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/32...\n",
      "2024-10-30 08:25:33 INFO [auto_gptq.quantization.gptq] duration: 0.6665992736816406\n",
      "2024-10-30 08:25:33 INFO [auto_gptq.quantization.gptq] avg loss: 0.02009281888604164\n",
      "INFO - Quantizing mlp.up_proj in layer 27/32...\n",
      "2024-10-30 08:25:34 INFO [auto_gptq.quantization.gptq] duration: 0.6845989227294922\n",
      "2024-10-30 08:25:34 INFO [auto_gptq.quantization.gptq] avg loss: 6.995084285736084\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/32...\n",
      "2024-10-30 08:25:35 INFO [auto_gptq.quantization.gptq] duration: 1.051100492477417\n",
      "2024-10-30 08:25:35 INFO [auto_gptq.quantization.gptq] avg loss: 12.929121971130371\n",
      "INFO - Quantizing mlp.down_proj in layer 27/32...\n",
      "2024-10-30 08:25:38 INFO [auto_gptq.quantization.gptq] duration: 3.2517030239105225\n",
      "2024-10-30 08:25:38 INFO [auto_gptq.quantization.gptq] avg loss: 0.12300819158554077\n",
      "INFO - Start quantizing layer 28/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/32...\n",
      "2024-10-30 08:25:39 INFO [auto_gptq.quantization.gptq] duration: 0.6360998153686523\n",
      "2024-10-30 08:25:39 INFO [auto_gptq.quantization.gptq] avg loss: 3.7245657444000244\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/32...\n",
      "2024-10-30 08:25:40 INFO [auto_gptq.quantization.gptq] duration: 0.6393418312072754\n",
      "2024-10-30 08:25:40 INFO [auto_gptq.quantization.gptq] avg loss: 1.0377029180526733\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/32...\n",
      "2024-10-30 08:25:40 INFO [auto_gptq.quantization.gptq] duration: 0.6702265739440918\n",
      "2024-10-30 08:25:40 INFO [auto_gptq.quantization.gptq] avg loss: 5.300141334533691\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/32...\n",
      "2024-10-30 08:25:41 INFO [auto_gptq.quantization.gptq] duration: 0.6703271865844727\n",
      "2024-10-30 08:25:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.031553056091070175\n",
      "INFO - Quantizing mlp.up_proj in layer 28/32...\n",
      "2024-10-30 08:25:42 INFO [auto_gptq.quantization.gptq] duration: 0.6887867450714111\n",
      "2024-10-30 08:25:42 INFO [auto_gptq.quantization.gptq] avg loss: 7.660228729248047\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/32...\n",
      "2024-10-30 08:25:42 INFO [auto_gptq.quantization.gptq] duration: 0.6890602111816406\n",
      "2024-10-30 08:25:42 INFO [auto_gptq.quantization.gptq] avg loss: 13.831243515014648\n",
      "INFO - Quantizing mlp.down_proj in layer 28/32...\n",
      "2024-10-30 08:25:45 INFO [auto_gptq.quantization.gptq] duration: 2.8346569538116455\n",
      "2024-10-30 08:25:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.17476379871368408\n",
      "INFO - Start quantizing layer 29/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/32...\n",
      "2024-10-30 08:25:46 INFO [auto_gptq.quantization.gptq] duration: 0.6378612518310547\n",
      "2024-10-30 08:25:46 INFO [auto_gptq.quantization.gptq] avg loss: 3.0047378540039062\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/32...\n",
      "2024-10-30 08:25:47 INFO [auto_gptq.quantization.gptq] duration: 0.6373991966247559\n",
      "2024-10-30 08:25:47 INFO [auto_gptq.quantization.gptq] avg loss: 0.9363851547241211\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/32...\n",
      "2024-10-30 08:25:47 INFO [auto_gptq.quantization.gptq] duration: 0.6717967987060547\n",
      "2024-10-30 08:25:47 INFO [auto_gptq.quantization.gptq] avg loss: 4.8604230880737305\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/32...\n",
      "2024-10-30 08:25:48 INFO [auto_gptq.quantization.gptq] duration: 0.6695327758789062\n",
      "2024-10-30 08:25:48 INFO [auto_gptq.quantization.gptq] avg loss: 0.049208927899599075\n",
      "INFO - Quantizing mlp.up_proj in layer 29/32...\n",
      "2024-10-30 08:25:49 INFO [auto_gptq.quantization.gptq] duration: 0.6879699230194092\n",
      "2024-10-30 08:25:49 INFO [auto_gptq.quantization.gptq] avg loss: 8.440662384033203\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/32...\n",
      "2024-10-30 08:25:50 INFO [auto_gptq.quantization.gptq] duration: 0.6942665576934814\n",
      "2024-10-30 08:25:50 INFO [auto_gptq.quantization.gptq] avg loss: 14.636728286743164\n",
      "INFO - Quantizing mlp.down_proj in layer 29/32...\n",
      "2024-10-30 08:25:52 INFO [auto_gptq.quantization.gptq] duration: 2.835653305053711\n",
      "2024-10-30 08:25:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.2249867022037506\n",
      "INFO - Start quantizing layer 30/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/32...\n",
      "2024-10-30 08:25:53 INFO [auto_gptq.quantization.gptq] duration: 0.6398940086364746\n",
      "2024-10-30 08:25:53 INFO [auto_gptq.quantization.gptq] avg loss: 3.177680730819702\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/32...\n",
      "2024-10-30 08:25:54 INFO [auto_gptq.quantization.gptq] duration: 0.6413218975067139\n",
      "2024-10-30 08:25:54 INFO [auto_gptq.quantization.gptq] avg loss: 1.1065967082977295\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/32...\n",
      "2024-10-30 08:25:55 INFO [auto_gptq.quantization.gptq] duration: 0.6757369041442871\n",
      "2024-10-30 08:25:55 INFO [auto_gptq.quantization.gptq] avg loss: 5.337040901184082\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/32...\n",
      "2024-10-30 08:25:55 INFO [auto_gptq.quantization.gptq] duration: 0.6714370250701904\n",
      "2024-10-30 08:25:55 INFO [auto_gptq.quantization.gptq] avg loss: 0.060473158955574036\n",
      "INFO - Quantizing mlp.up_proj in layer 30/32...\n",
      "2024-10-30 08:25:56 INFO [auto_gptq.quantization.gptq] duration: 0.6895837783813477\n",
      "2024-10-30 08:25:56 INFO [auto_gptq.quantization.gptq] avg loss: 9.741785049438477\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/32...\n",
      "2024-10-30 08:25:57 INFO [auto_gptq.quantization.gptq] duration: 0.691828727722168\n",
      "2024-10-30 08:25:57 INFO [auto_gptq.quantization.gptq] avg loss: 15.94802188873291\n",
      "INFO - Quantizing mlp.down_proj in layer 30/32...\n",
      "2024-10-30 08:25:59 INFO [auto_gptq.quantization.gptq] duration: 2.8308465480804443\n",
      "2024-10-30 08:25:59 INFO [auto_gptq.quantization.gptq] avg loss: 0.3754199743270874\n",
      "INFO - Start quantizing layer 31/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/32...\n",
      "2024-10-30 08:26:00 INFO [auto_gptq.quantization.gptq] duration: 0.6346733570098877\n",
      "2024-10-30 08:26:00 INFO [auto_gptq.quantization.gptq] avg loss: 3.111067771911621\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/32...\n",
      "2024-10-30 08:26:01 INFO [auto_gptq.quantization.gptq] duration: 0.6351008415222168\n",
      "2024-10-30 08:26:01 INFO [auto_gptq.quantization.gptq] avg loss: 1.5940788984298706\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/32...\n",
      "2024-10-30 08:26:02 INFO [auto_gptq.quantization.gptq] duration: 0.6678719520568848\n",
      "2024-10-30 08:26:02 INFO [auto_gptq.quantization.gptq] avg loss: 4.6921210289001465\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/32...\n",
      "2024-10-30 08:26:02 INFO [auto_gptq.quantization.gptq] duration: 0.6708347797393799\n",
      "2024-10-30 08:26:02 INFO [auto_gptq.quantization.gptq] avg loss: 0.08755072206258774\n",
      "INFO - Quantizing mlp.up_proj in layer 31/32...\n",
      "2024-10-30 08:26:03 INFO [auto_gptq.quantization.gptq] duration: 0.6883659362792969\n",
      "2024-10-30 08:26:03 INFO [auto_gptq.quantization.gptq] avg loss: 11.385140419006348\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/32...\n",
      "2024-10-30 08:26:04 INFO [auto_gptq.quantization.gptq] duration: 0.6866903305053711\n",
      "2024-10-30 08:26:04 INFO [auto_gptq.quantization.gptq] avg loss: 18.65461540222168\n",
      "INFO - Quantizing mlp.down_proj in layer 31/32...\n",
      "2024-10-30 08:26:07 INFO [auto_gptq.quantization.gptq] duration: 2.8347184658050537\n",
      "2024-10-30 08:26:07 INFO [auto_gptq.quantization.gptq] avg loss: 0.9421476125717163\n",
      "INFO - Start quantizing layer 32/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/32...\n",
      "2024-10-30 08:26:07 INFO [auto_gptq.quantization.gptq] duration: 0.6402497291564941\n",
      "2024-10-30 08:26:07 INFO [auto_gptq.quantization.gptq] avg loss: 2.465703010559082\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/32...\n",
      "2024-10-30 08:26:08 INFO [auto_gptq.quantization.gptq] duration: 0.6428651809692383\n",
      "2024-10-30 08:26:08 INFO [auto_gptq.quantization.gptq] avg loss: 1.0032373666763306\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/32...\n",
      "2024-10-30 08:26:09 INFO [auto_gptq.quantization.gptq] duration: 0.6695475578308105\n",
      "2024-10-30 08:26:09 INFO [auto_gptq.quantization.gptq] avg loss: 4.3938446044921875\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/32...\n",
      "2024-10-30 08:26:09 INFO [auto_gptq.quantization.gptq] duration: 0.6682565212249756\n",
      "2024-10-30 08:26:09 INFO [auto_gptq.quantization.gptq] avg loss: 0.40166765451431274\n",
      "INFO - Quantizing mlp.up_proj in layer 32/32...\n",
      "2024-10-30 08:26:10 INFO [auto_gptq.quantization.gptq] duration: 0.6884582042694092\n",
      "2024-10-30 08:26:10 INFO [auto_gptq.quantization.gptq] avg loss: 14.70301628112793\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/32...\n",
      "2024-10-30 08:26:11 INFO [auto_gptq.quantization.gptq] duration: 0.6912364959716797\n",
      "2024-10-30 08:26:11 INFO [auto_gptq.quantization.gptq] avg loss: 20.083524703979492\n",
      "INFO - Quantizing mlp.down_proj in layer 32/32...\n",
      "2024-10-30 08:26:14 INFO [auto_gptq.quantization.gptq] duration: 2.837254047393799\n",
      "2024-10-30 08:26:14 INFO [auto_gptq.quantization.gptq] avg loss: 10.1775484085083\n",
      "INFO - Packing model...\n",
      "2024-10-30 08:26:14 INFO [auto_gptq.modeling._utils] Packing model...\n",
      "Packing model.layers.31.mlp.down_proj...: 100%|██████████| 224/224 [03:43<00:00,  1.00it/s]   \n",
      "INFO - Model packed.\n",
      "2024-10-30 08:30:01 INFO [auto_gptq.modeling._utils] Model packed.\n",
      "INFO - You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.\n",
      "INFO - The layer lm_head is not quantized.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "/opt/conda/envs/lmquant/lib/python3.11/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "The model 'LlamaGPTQForCausalLM' is not supported for . Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>auto_gptq is a simple Python script that uses the AutoGPT model to generate text\n",
      "auto-gptq is a type of gptq that is generated internally by the system. It\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
    "model.quantize(examples)\n",
    "\n",
    "# save quantized model\n",
    "model.save_quantized(quantized_model_dir)\n",
    "\n",
    "# save quantized model using safetensors\n",
    "model.save_quantized(quantized_model_dir, use_safetensors=True)\n",
    "\n",
    "# push quantized model to Hugging Face Hub.\n",
    "# to use use_auth_token=True, Login first via huggingface-cli login.\n",
    "# or pass explcit token with: use_auth_token=\"hf_xxxxxxx\"\n",
    "# (uncomment the following three lines to enable this feature)\n",
    "# repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
    "# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
    "# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)\n",
    "\n",
    "# alternatively you can save and push at the same time\n",
    "# (uncomment the following three lines to enable this feature)\n",
    "# repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
    "# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
    "# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.\n",
      "INFO - The layer lm_head is not quantized.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The model 'LlamaGPTQForCausalLM' is not supported for . Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>auto_gptq is a simple Python script that uses the AutoGPT model to generate text\n",
      "auto-gptq is a type of gptq that is generated internally by the system. It\n"
     ]
    }
   ],
   "source": [
    "# load quantized model to the first GPU\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device=\"cuda:0\")\n",
    "\n",
    "# download quantized model from Hugging Face Hub and load to the first GPU\n",
    "# model = AutoGPTQForCausalLM.from_quantized(repo_id, device=\"cuda:0\", use_safetensors=True, use_triton=False)\n",
    "\n",
    "# inference with model.generate\n",
    "print(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\", return_tensors=\"pt\").to(model.device))[0]))\n",
    "\n",
    "# or you can also use pipeline\n",
    "pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "print(pipeline(\"auto-gptq is\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多模态模型文本基座量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lmquant/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def get_wikitext2(nsamples, seed, seqlen, tokenizer):\n",
    "    # set seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "\n",
    "    # load dataset and preprocess\n",
    "    # traindata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    # testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    traindata = load_dataset(\"/home/workspace/code/git/FlatQuant_mlm/datasets/wikitext\", split=\"train\")\n",
    "    testdata = load_dataset(\"/home/workspace/code/git/FlatQuant_mlm/datasets/wikitext\", split=\"test\")\n",
    "    trainenc = tokenizer(\"\\n\\n\".join(traindata[\"text\"]), return_tensors=\"pt\")\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    traindataset = []\n",
    "    for _ in range(nsamples):\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        attention_mask = torch.ones_like(inp)\n",
    "        traindataset.append({\"input_ids\": inp, \"attention_mask\": attention_mask})\n",
    "    return traindataset, testenc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniCPM-Llama3-V-2_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-11-06 03:36:13 INFO [transformers_modules.MiniCPM-Llama3-V-2_5.configuration_minicpm] vision_config is None, using default vision config\n",
      "2024-11-06 03:36:13 INFO [transformers_modules.MiniCPM-Llama3-V-2_5.configuration_minicpm] vision_config is None, using default vision config\n",
      "2024-11-06 03:36:13 INFO [transformers_modules.MiniCPM-Llama3-V-2_5.configuration_minicpm] vision_config is None, using default vision config\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:33<00:00,  4.85s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "from auto_gptq import AutoGPTQForCausalMLM, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "pretrained_model_dir = \"/home/workspace/model/MiniCPM-Llama3-V-2_5\"\n",
    "quantized_model_dir = \"/home/workspace/model/MiniCPM-Llama3-V-2_5-w4-g128\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True, trust_remote_code=True)\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,  # quantize model to 4-bit\n",
    "    group_size=128,  # it is recommended to set the value to 128\n",
    "    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    ")\n",
    "traindataset, testenc = get_wikitext2(128, 0, model.seqlen, tokenizer)\n",
    "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "model = AutoGPTQForCausalMLM.from_pretrained(pretrained_model_dir, quantize_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniCPM-3o-1B-sft-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "You are using a model of type minicpmv to instantiate a model of type minicpm. This is not supported for all configurations of models and can yield errors.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "2024-11-12 12:46:25 INFO [auto_gptq.modeling.minicpm.configuration_minicpm] vision_config is None, using default vision config\n",
      "You're using a MiniCPMVTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "from auto_gptq import AutoGPTQForCausalMLM, BaseQuantizeConfig\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "pretrained_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1\"\n",
    "quantized_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1-llm_pc_w4\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True, trust_remote_code=True)\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,  # quantize model to 8-bit\n",
    "    group_size=-1,  # it is recommended to set the value to -1\n",
    "    desc_act=True,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    ")\n",
    "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "model = AutoGPTQForCausalMLM.from_pretrained(pretrained_model_dir, quantize_config)\n",
    "traindataset, testenc = get_wikitext2(128, 0, model.seqlen, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniCPMVGPTQ_Llama3(\n",
       "  (model): MiniCPMV(\n",
       "    (llm): MiniCPMForCausalLM(\n",
       "      (model): MiniCPMModel(\n",
       "        (embed_tokens): Embedding(73464, 1536)\n",
       "        (layers): ModuleList(\n",
       "          (0-51): 52 x MiniCPMDecoderLayer(\n",
       "            (self_attn): MiniCPMSdpaAttention(\n",
       "              (q_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k_proj): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (v_proj): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (rotary_emb): MiniCPMLongRoPE()\n",
       "            )\n",
       "            (mlp): MiniCPMMLP(\n",
       "              (gate_proj): Linear(in_features=1536, out_features=3840, bias=False)\n",
       "              (up_proj): Linear(in_features=1536, out_features=3840, bias=False)\n",
       "              (down_proj): Linear(in_features=3840, out_features=1536, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MiniCPMRMSNorm()\n",
       "            (post_attention_layernorm): MiniCPMRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MiniCPMRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=73464, bias=False)\n",
       "    )\n",
       "    (vpm): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(4900, 1152)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x SiglipEncoderLayer(\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (resampler): Resampler(\n",
       "      (kv_proj): Linear(in_features=1152, out_features=1536, bias=False)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      )\n",
       "      (ln_q): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (ln_kv): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (ln_post): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/52...\n",
      "2024-11-12 12:48:45 INFO [auto_gptq.quantization.gptq] duration: 0.8242471218109131\n",
      "2024-11-12 12:48:45 INFO [auto_gptq.quantization.gptq] avg loss: 502.7901916503906\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/52...\n",
      "2024-11-12 12:48:45 INFO [auto_gptq.quantization.gptq] duration: 0.225905179977417\n",
      "2024-11-12 12:48:45 INFO [auto_gptq.quantization.gptq] avg loss: 264.6324157714844\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/52...\n",
      "2024-11-12 12:48:45 INFO [auto_gptq.quantization.gptq] duration: 0.22806906700134277\n",
      "2024-11-12 12:48:45 INFO [auto_gptq.quantization.gptq] avg loss: 1638.80859375\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/52...\n",
      "2024-11-12 12:49:24 INFO [auto_gptq.quantization.gptq] duration: 0.49829840660095215\n",
      "2024-11-12 12:49:24 INFO [auto_gptq.quantization.gptq] avg loss: 161.2862548828125\n",
      "INFO - Quantizing mlp.up_proj in layer 1/52...\n",
      "2024-11-12 12:50:04 INFO [auto_gptq.quantization.gptq] duration: 0.5235767364501953\n",
      "2024-11-12 12:50:04 INFO [auto_gptq.quantization.gptq] avg loss: 3563.63916015625\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/52...\n",
      "2024-11-12 12:50:05 INFO [auto_gptq.quantization.gptq] duration: 0.38324403762817383\n",
      "2024-11-12 12:50:05 INFO [auto_gptq.quantization.gptq] avg loss: 3168.95166015625\n",
      "INFO - Quantizing mlp.down_proj in layer 1/52...\n",
      "2024-11-12 12:50:50 INFO [auto_gptq.quantization.gptq] duration: 0.8949105739593506\n",
      "2024-11-12 12:50:50 INFO [auto_gptq.quantization.gptq] avg loss: 6784.14404296875\n",
      "INFO - Start quantizing layer 2/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/52...\n",
      "2024-11-12 12:52:06 INFO [auto_gptq.quantization.gptq] duration: 0.5208983421325684\n",
      "2024-11-12 12:52:06 INFO [auto_gptq.quantization.gptq] avg loss: 636.814208984375\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/52...\n",
      "2024-11-12 12:52:06 INFO [auto_gptq.quantization.gptq] duration: 0.22403740882873535\n",
      "2024-11-12 12:52:06 INFO [auto_gptq.quantization.gptq] avg loss: 334.25860595703125\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/52...\n",
      "2024-11-12 12:52:07 INFO [auto_gptq.quantization.gptq] duration: 0.22401833534240723\n",
      "2024-11-12 12:52:07 INFO [auto_gptq.quantization.gptq] avg loss: 1766.6014404296875\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/52...\n",
      "2024-11-12 12:52:43 INFO [auto_gptq.quantization.gptq] duration: 0.5063915252685547\n",
      "2024-11-12 12:52:43 INFO [auto_gptq.quantization.gptq] avg loss: 906.5034790039062\n",
      "INFO - Quantizing mlp.up_proj in layer 2/52...\n",
      "2024-11-12 12:53:20 INFO [auto_gptq.quantization.gptq] duration: 0.5247366428375244\n",
      "2024-11-12 12:53:20 INFO [auto_gptq.quantization.gptq] avg loss: 3334.80810546875\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/52...\n",
      "2024-11-12 12:53:21 INFO [auto_gptq.quantization.gptq] duration: 0.3770754337310791\n",
      "2024-11-12 12:53:21 INFO [auto_gptq.quantization.gptq] avg loss: 2943.499267578125\n",
      "INFO - Quantizing mlp.down_proj in layer 2/52...\n",
      "2024-11-12 12:54:03 INFO [auto_gptq.quantization.gptq] duration: 0.898421049118042\n",
      "2024-11-12 12:54:03 INFO [auto_gptq.quantization.gptq] avg loss: 6590.2958984375\n",
      "INFO - Start quantizing layer 3/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/52...\n",
      "2024-11-12 12:55:17 INFO [auto_gptq.quantization.gptq] duration: 0.5255398750305176\n",
      "2024-11-12 12:55:17 INFO [auto_gptq.quantization.gptq] avg loss: 679.440673828125\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/52...\n",
      "2024-11-12 12:55:17 INFO [auto_gptq.quantization.gptq] duration: 0.22632575035095215\n",
      "2024-11-12 12:55:17 INFO [auto_gptq.quantization.gptq] avg loss: 538.33056640625\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/52...\n",
      "2024-11-12 12:55:18 INFO [auto_gptq.quantization.gptq] duration: 0.2263031005859375\n",
      "2024-11-12 12:55:18 INFO [auto_gptq.quantization.gptq] avg loss: 1580.5582275390625\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/52...\n",
      "2024-11-12 12:55:54 INFO [auto_gptq.quantization.gptq] duration: 0.5064742565155029\n",
      "2024-11-12 12:55:54 INFO [auto_gptq.quantization.gptq] avg loss: 1131.80908203125\n",
      "INFO - Quantizing mlp.up_proj in layer 3/52...\n",
      "2024-11-12 12:56:32 INFO [auto_gptq.quantization.gptq] duration: 0.5175042152404785\n",
      "2024-11-12 12:56:32 INFO [auto_gptq.quantization.gptq] avg loss: 4444.8291015625\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/52...\n",
      "2024-11-12 12:56:32 INFO [auto_gptq.quantization.gptq] duration: 0.23159217834472656\n",
      "2024-11-12 12:56:32 INFO [auto_gptq.quantization.gptq] avg loss: 4070.72509765625\n",
      "INFO - Quantizing mlp.down_proj in layer 3/52...\n",
      "2024-11-12 12:57:14 INFO [auto_gptq.quantization.gptq] duration: 0.9015669822692871\n",
      "2024-11-12 12:57:14 INFO [auto_gptq.quantization.gptq] avg loss: 6160.4765625\n",
      "INFO - Start quantizing layer 4/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/52...\n",
      "2024-11-12 12:58:28 INFO [auto_gptq.quantization.gptq] duration: 0.5257847309112549\n",
      "2024-11-12 12:58:28 INFO [auto_gptq.quantization.gptq] avg loss: 707.952880859375\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/52...\n",
      "2024-11-12 12:58:28 INFO [auto_gptq.quantization.gptq] duration: 0.22600746154785156\n",
      "2024-11-12 12:58:28 INFO [auto_gptq.quantization.gptq] avg loss: 574.9993896484375\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/52...\n",
      "2024-11-12 12:58:28 INFO [auto_gptq.quantization.gptq] duration: 0.22635626792907715\n",
      "2024-11-12 12:58:28 INFO [auto_gptq.quantization.gptq] avg loss: 1515.412353515625\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/52...\n",
      "2024-11-12 12:59:05 INFO [auto_gptq.quantization.gptq] duration: 0.5050582885742188\n",
      "2024-11-12 12:59:05 INFO [auto_gptq.quantization.gptq] avg loss: 1424.50048828125\n",
      "INFO - Quantizing mlp.up_proj in layer 4/52...\n",
      "2024-11-12 12:59:42 INFO [auto_gptq.quantization.gptq] duration: 0.5170469284057617\n",
      "2024-11-12 12:59:42 INFO [auto_gptq.quantization.gptq] avg loss: 4898.640625\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/52...\n",
      "2024-11-12 12:59:42 INFO [auto_gptq.quantization.gptq] duration: 0.23476791381835938\n",
      "2024-11-12 12:59:42 INFO [auto_gptq.quantization.gptq] avg loss: 4335.76806640625\n",
      "INFO - Quantizing mlp.down_proj in layer 4/52...\n",
      "2024-11-12 13:00:25 INFO [auto_gptq.quantization.gptq] duration: 0.8896570205688477\n",
      "2024-11-12 13:00:25 INFO [auto_gptq.quantization.gptq] avg loss: 5766.22998046875\n",
      "INFO - Start quantizing layer 5/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/52...\n",
      "2024-11-12 13:01:38 INFO [auto_gptq.quantization.gptq] duration: 0.525292158126831\n",
      "2024-11-12 13:01:38 INFO [auto_gptq.quantization.gptq] avg loss: 733.9608764648438\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/52...\n",
      "2024-11-12 13:01:39 INFO [auto_gptq.quantization.gptq] duration: 0.22436261177062988\n",
      "2024-11-12 13:01:39 INFO [auto_gptq.quantization.gptq] avg loss: 639.4232177734375\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/52...\n",
      "2024-11-12 13:01:39 INFO [auto_gptq.quantization.gptq] duration: 0.2249469757080078\n",
      "2024-11-12 13:01:39 INFO [auto_gptq.quantization.gptq] avg loss: 1771.0615234375\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/52...\n",
      "2024-11-12 13:02:15 INFO [auto_gptq.quantization.gptq] duration: 0.5029118061065674\n",
      "2024-11-12 13:02:15 INFO [auto_gptq.quantization.gptq] avg loss: 958.9483032226562\n",
      "INFO - Quantizing mlp.up_proj in layer 5/52...\n",
      "2024-11-12 13:02:53 INFO [auto_gptq.quantization.gptq] duration: 0.518883228302002\n",
      "2024-11-12 13:02:53 INFO [auto_gptq.quantization.gptq] avg loss: 5502.9794921875\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/52...\n",
      "2024-11-12 13:02:53 INFO [auto_gptq.quantization.gptq] duration: 0.22806572914123535\n",
      "2024-11-12 13:02:53 INFO [auto_gptq.quantization.gptq] avg loss: 5000.5126953125\n",
      "INFO - Quantizing mlp.down_proj in layer 5/52...\n",
      "2024-11-12 13:03:35 INFO [auto_gptq.quantization.gptq] duration: 0.8947877883911133\n",
      "2024-11-12 13:03:35 INFO [auto_gptq.quantization.gptq] avg loss: 5000.9267578125\n",
      "INFO - Start quantizing layer 6/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/52...\n",
      "2024-11-12 13:04:49 INFO [auto_gptq.quantization.gptq] duration: 0.524705171585083\n",
      "2024-11-12 13:04:49 INFO [auto_gptq.quantization.gptq] avg loss: 755.7747802734375\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/52...\n",
      "2024-11-12 13:04:49 INFO [auto_gptq.quantization.gptq] duration: 0.2245779037475586\n",
      "2024-11-12 13:04:49 INFO [auto_gptq.quantization.gptq] avg loss: 549.6281127929688\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/52...\n",
      "2024-11-12 13:04:49 INFO [auto_gptq.quantization.gptq] duration: 0.22485065460205078\n",
      "2024-11-12 13:04:49 INFO [auto_gptq.quantization.gptq] avg loss: 1539.8780517578125\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/52...\n",
      "2024-11-12 13:05:26 INFO [auto_gptq.quantization.gptq] duration: 0.5023348331451416\n",
      "2024-11-12 13:05:26 INFO [auto_gptq.quantization.gptq] avg loss: 789.5830078125\n",
      "INFO - Quantizing mlp.up_proj in layer 6/52...\n",
      "2024-11-12 13:06:03 INFO [auto_gptq.quantization.gptq] duration: 0.5251274108886719\n",
      "2024-11-12 13:06:03 INFO [auto_gptq.quantization.gptq] avg loss: 6281.853515625\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/52...\n",
      "2024-11-12 13:06:03 INFO [auto_gptq.quantization.gptq] duration: 0.3819999694824219\n",
      "2024-11-12 13:06:03 INFO [auto_gptq.quantization.gptq] avg loss: 5664.6787109375\n",
      "INFO - Quantizing mlp.down_proj in layer 6/52...\n",
      "2024-11-12 13:06:46 INFO [auto_gptq.quantization.gptq] duration: 0.9003429412841797\n",
      "2024-11-12 13:06:46 INFO [auto_gptq.quantization.gptq] avg loss: 5447.89111328125\n",
      "INFO - Start quantizing layer 7/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/52...\n",
      "2024-11-12 13:07:59 INFO [auto_gptq.quantization.gptq] duration: 0.5302426815032959\n",
      "2024-11-12 13:07:59 INFO [auto_gptq.quantization.gptq] avg loss: 896.333740234375\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/52...\n",
      "2024-11-12 13:08:00 INFO [auto_gptq.quantization.gptq] duration: 0.3936502933502197\n",
      "2024-11-12 13:08:00 INFO [auto_gptq.quantization.gptq] avg loss: 646.1832275390625\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/52...\n",
      "2024-11-12 13:08:00 INFO [auto_gptq.quantization.gptq] duration: 0.3577718734741211\n",
      "2024-11-12 13:08:00 INFO [auto_gptq.quantization.gptq] avg loss: 1920.141845703125\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/52...\n",
      "2024-11-12 13:08:36 INFO [auto_gptq.quantization.gptq] duration: 0.5042083263397217\n",
      "2024-11-12 13:08:36 INFO [auto_gptq.quantization.gptq] avg loss: 595.0000610351562\n",
      "INFO - Quantizing mlp.up_proj in layer 7/52...\n",
      "2024-11-12 13:09:14 INFO [auto_gptq.quantization.gptq] duration: 0.5219545364379883\n",
      "2024-11-12 13:09:14 INFO [auto_gptq.quantization.gptq] avg loss: 5995.28662109375\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/52...\n",
      "2024-11-12 13:09:14 INFO [auto_gptq.quantization.gptq] duration: 0.23460173606872559\n",
      "2024-11-12 13:09:14 INFO [auto_gptq.quantization.gptq] avg loss: 5520.1826171875\n",
      "INFO - Quantizing mlp.down_proj in layer 7/52...\n",
      "2024-11-12 13:09:56 INFO [auto_gptq.quantization.gptq] duration: 0.9035320281982422\n",
      "2024-11-12 13:09:56 INFO [auto_gptq.quantization.gptq] avg loss: 5727.607421875\n",
      "INFO - Start quantizing layer 8/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/52...\n",
      "2024-11-12 13:11:10 INFO [auto_gptq.quantization.gptq] duration: 0.5291130542755127\n",
      "2024-11-12 13:11:10 INFO [auto_gptq.quantization.gptq] avg loss: 954.1959228515625\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/52...\n",
      "2024-11-12 13:11:10 INFO [auto_gptq.quantization.gptq] duration: 0.22641921043395996\n",
      "2024-11-12 13:11:10 INFO [auto_gptq.quantization.gptq] avg loss: 751.8585205078125\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/52...\n",
      "2024-11-12 13:11:11 INFO [auto_gptq.quantization.gptq] duration: 0.2259063720703125\n",
      "2024-11-12 13:11:11 INFO [auto_gptq.quantization.gptq] avg loss: 2351.83056640625\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/52...\n",
      "2024-11-12 13:11:47 INFO [auto_gptq.quantization.gptq] duration: 0.5060722827911377\n",
      "2024-11-12 13:11:47 INFO [auto_gptq.quantization.gptq] avg loss: 487.239501953125\n",
      "INFO - Quantizing mlp.up_proj in layer 8/52...\n",
      "2024-11-12 13:12:24 INFO [auto_gptq.quantization.gptq] duration: 0.519989013671875\n",
      "2024-11-12 13:12:24 INFO [auto_gptq.quantization.gptq] avg loss: 6250.76220703125\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/52...\n",
      "2024-11-12 13:12:25 INFO [auto_gptq.quantization.gptq] duration: 0.23650646209716797\n",
      "2024-11-12 13:12:25 INFO [auto_gptq.quantization.gptq] avg loss: 5844.865234375\n",
      "INFO - Quantizing mlp.down_proj in layer 8/52...\n",
      "2024-11-12 13:13:07 INFO [auto_gptq.quantization.gptq] duration: 0.8973560333251953\n",
      "2024-11-12 13:13:07 INFO [auto_gptq.quantization.gptq] avg loss: 5242.6533203125\n",
      "INFO - Start quantizing layer 9/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/52...\n",
      "2024-11-12 13:14:21 INFO [auto_gptq.quantization.gptq] duration: 0.5270581245422363\n",
      "2024-11-12 13:14:21 INFO [auto_gptq.quantization.gptq] avg loss: 820.0810546875\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/52...\n",
      "2024-11-12 13:14:21 INFO [auto_gptq.quantization.gptq] duration: 0.2244715690612793\n",
      "2024-11-12 13:14:21 INFO [auto_gptq.quantization.gptq] avg loss: 660.0848388671875\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/52...\n",
      "2024-11-12 13:14:21 INFO [auto_gptq.quantization.gptq] duration: 0.22604775428771973\n",
      "2024-11-12 13:14:21 INFO [auto_gptq.quantization.gptq] avg loss: 1832.1959228515625\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/52...\n",
      "2024-11-12 13:14:58 INFO [auto_gptq.quantization.gptq] duration: 0.5065228939056396\n",
      "2024-11-12 13:14:58 INFO [auto_gptq.quantization.gptq] avg loss: 769.5872802734375\n",
      "INFO - Quantizing mlp.up_proj in layer 9/52...\n",
      "2024-11-12 13:15:35 INFO [auto_gptq.quantization.gptq] duration: 0.5194029808044434\n",
      "2024-11-12 13:15:35 INFO [auto_gptq.quantization.gptq] avg loss: 6120.203125\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/52...\n",
      "2024-11-12 13:15:35 INFO [auto_gptq.quantization.gptq] duration: 0.22780370712280273\n",
      "2024-11-12 13:15:35 INFO [auto_gptq.quantization.gptq] avg loss: 6027.29150390625\n",
      "INFO - Quantizing mlp.down_proj in layer 9/52...\n",
      "2024-11-12 13:16:18 INFO [auto_gptq.quantization.gptq] duration: 0.89471435546875\n",
      "2024-11-12 13:16:18 INFO [auto_gptq.quantization.gptq] avg loss: 4507.666015625\n",
      "INFO - Start quantizing layer 10/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/52...\n",
      "2024-11-12 13:17:31 INFO [auto_gptq.quantization.gptq] duration: 0.528888463973999\n",
      "2024-11-12 13:17:31 INFO [auto_gptq.quantization.gptq] avg loss: 825.4265747070312\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/52...\n",
      "2024-11-12 13:17:32 INFO [auto_gptq.quantization.gptq] duration: 0.2254021167755127\n",
      "2024-11-12 13:17:32 INFO [auto_gptq.quantization.gptq] avg loss: 675.5047607421875\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/52...\n",
      "2024-11-12 13:17:32 INFO [auto_gptq.quantization.gptq] duration: 0.22751116752624512\n",
      "2024-11-12 13:17:32 INFO [auto_gptq.quantization.gptq] avg loss: 2093.362548828125\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/52...\n",
      "2024-11-12 13:18:08 INFO [auto_gptq.quantization.gptq] duration: 0.5058267116546631\n",
      "2024-11-12 13:18:08 INFO [auto_gptq.quantization.gptq] avg loss: 360.62152099609375\n",
      "INFO - Quantizing mlp.up_proj in layer 10/52...\n",
      "2024-11-12 13:18:46 INFO [auto_gptq.quantization.gptq] duration: 0.5219736099243164\n",
      "2024-11-12 13:18:46 INFO [auto_gptq.quantization.gptq] avg loss: 5924.498046875\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/52...\n",
      "2024-11-12 13:18:46 INFO [auto_gptq.quantization.gptq] duration: 0.23507976531982422\n",
      "2024-11-12 13:18:46 INFO [auto_gptq.quantization.gptq] avg loss: 5839.15869140625\n",
      "INFO - Quantizing mlp.down_proj in layer 10/52...\n",
      "2024-11-12 13:19:28 INFO [auto_gptq.quantization.gptq] duration: 0.897127628326416\n",
      "2024-11-12 13:19:28 INFO [auto_gptq.quantization.gptq] avg loss: 4317.99658203125\n",
      "INFO - Start quantizing layer 11/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/52...\n",
      "2024-11-12 13:20:42 INFO [auto_gptq.quantization.gptq] duration: 0.5242776870727539\n",
      "2024-11-12 13:20:42 INFO [auto_gptq.quantization.gptq] avg loss: 989.764892578125\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/52...\n",
      "2024-11-12 13:20:43 INFO [auto_gptq.quantization.gptq] duration: 0.38121509552001953\n",
      "2024-11-12 13:20:43 INFO [auto_gptq.quantization.gptq] avg loss: 638.324951171875\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/52...\n",
      "2024-11-12 13:20:43 INFO [auto_gptq.quantization.gptq] duration: 0.37647032737731934\n",
      "2024-11-12 13:20:43 INFO [auto_gptq.quantization.gptq] avg loss: 2328.424072265625\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/52...\n",
      "2024-11-12 13:21:19 INFO [auto_gptq.quantization.gptq] duration: 0.507814884185791\n",
      "2024-11-12 13:21:19 INFO [auto_gptq.quantization.gptq] avg loss: 129.07101440429688\n",
      "INFO - Quantizing mlp.up_proj in layer 11/52...\n",
      "2024-11-12 13:21:57 INFO [auto_gptq.quantization.gptq] duration: 0.525719165802002\n",
      "2024-11-12 13:21:57 INFO [auto_gptq.quantization.gptq] avg loss: 5606.4326171875\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/52...\n",
      "2024-11-12 13:21:57 INFO [auto_gptq.quantization.gptq] duration: 0.23652935028076172\n",
      "2024-11-12 13:21:57 INFO [auto_gptq.quantization.gptq] avg loss: 5621.4345703125\n",
      "INFO - Quantizing mlp.down_proj in layer 11/52...\n",
      "2024-11-12 13:22:39 INFO [auto_gptq.quantization.gptq] duration: 0.8973433971405029\n",
      "2024-11-12 13:22:39 INFO [auto_gptq.quantization.gptq] avg loss: 4052.83837890625\n",
      "INFO - Start quantizing layer 12/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/52...\n",
      "2024-11-12 13:23:53 INFO [auto_gptq.quantization.gptq] duration: 0.5248477458953857\n",
      "2024-11-12 13:23:53 INFO [auto_gptq.quantization.gptq] avg loss: 1063.1259765625\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/52...\n",
      "2024-11-12 13:23:54 INFO [auto_gptq.quantization.gptq] duration: 0.3733184337615967\n",
      "2024-11-12 13:23:54 INFO [auto_gptq.quantization.gptq] avg loss: 801.219482421875\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/52...\n",
      "2024-11-12 13:23:54 INFO [auto_gptq.quantization.gptq] duration: 0.34685349464416504\n",
      "2024-11-12 13:23:54 INFO [auto_gptq.quantization.gptq] avg loss: 2473.041748046875\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/52...\n",
      "2024-11-12 13:24:30 INFO [auto_gptq.quantization.gptq] duration: 0.5612931251525879\n",
      "2024-11-12 13:24:30 INFO [auto_gptq.quantization.gptq] avg loss: 366.2694091796875\n",
      "INFO - Quantizing mlp.up_proj in layer 12/52...\n",
      "2024-11-12 13:25:08 INFO [auto_gptq.quantization.gptq] duration: 0.5493259429931641\n",
      "2024-11-12 13:25:08 INFO [auto_gptq.quantization.gptq] avg loss: 5603.00830078125\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/52...\n",
      "2024-11-12 13:25:08 INFO [auto_gptq.quantization.gptq] duration: 0.23567986488342285\n",
      "2024-11-12 13:25:08 INFO [auto_gptq.quantization.gptq] avg loss: 5543.22509765625\n",
      "INFO - Quantizing mlp.down_proj in layer 12/52...\n",
      "2024-11-12 13:25:51 INFO [auto_gptq.quantization.gptq] duration: 0.8949434757232666\n",
      "2024-11-12 13:25:51 INFO [auto_gptq.quantization.gptq] avg loss: 3810.759033203125\n",
      "INFO - Start quantizing layer 13/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/52...\n",
      "2024-11-12 13:27:04 INFO [auto_gptq.quantization.gptq] duration: 0.5257341861724854\n",
      "2024-11-12 13:27:04 INFO [auto_gptq.quantization.gptq] avg loss: 728.3631591796875\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/52...\n",
      "2024-11-12 13:27:05 INFO [auto_gptq.quantization.gptq] duration: 0.37941622734069824\n",
      "2024-11-12 13:27:05 INFO [auto_gptq.quantization.gptq] avg loss: 472.435546875\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/52...\n",
      "2024-11-12 13:27:05 INFO [auto_gptq.quantization.gptq] duration: 0.36754655838012695\n",
      "2024-11-12 13:27:05 INFO [auto_gptq.quantization.gptq] avg loss: 1499.7711181640625\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/52...\n",
      "2024-11-12 13:27:41 INFO [auto_gptq.quantization.gptq] duration: 0.5006203651428223\n",
      "2024-11-12 13:27:41 INFO [auto_gptq.quantization.gptq] avg loss: 555.7730712890625\n",
      "INFO - Quantizing mlp.up_proj in layer 13/52...\n",
      "2024-11-12 13:28:19 INFO [auto_gptq.quantization.gptq] duration: 0.5231356620788574\n",
      "2024-11-12 13:28:19 INFO [auto_gptq.quantization.gptq] avg loss: 5049.873046875\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/52...\n",
      "2024-11-12 13:28:19 INFO [auto_gptq.quantization.gptq] duration: 0.23412775993347168\n",
      "2024-11-12 13:28:19 INFO [auto_gptq.quantization.gptq] avg loss: 4973.470703125\n",
      "INFO - Quantizing mlp.down_proj in layer 13/52...\n",
      "2024-11-12 13:29:01 INFO [auto_gptq.quantization.gptq] duration: 0.8926694393157959\n",
      "2024-11-12 13:29:01 INFO [auto_gptq.quantization.gptq] avg loss: 3520.83837890625\n",
      "INFO - Start quantizing layer 14/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/52...\n",
      "2024-11-12 13:30:15 INFO [auto_gptq.quantization.gptq] duration: 0.5273795127868652\n",
      "2024-11-12 13:30:15 INFO [auto_gptq.quantization.gptq] avg loss: 1000.2040405273438\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/52...\n",
      "2024-11-12 13:30:15 INFO [auto_gptq.quantization.gptq] duration: 0.22418451309204102\n",
      "2024-11-12 13:30:15 INFO [auto_gptq.quantization.gptq] avg loss: 776.0074462890625\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/52...\n",
      "2024-11-12 13:30:16 INFO [auto_gptq.quantization.gptq] duration: 0.22340726852416992\n",
      "2024-11-12 13:30:16 INFO [auto_gptq.quantization.gptq] avg loss: 2422.1357421875\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/52...\n",
      "2024-11-12 13:30:52 INFO [auto_gptq.quantization.gptq] duration: 0.5037190914154053\n",
      "2024-11-12 13:30:52 INFO [auto_gptq.quantization.gptq] avg loss: 499.9173583984375\n",
      "INFO - Quantizing mlp.up_proj in layer 14/52...\n",
      "2024-11-12 13:31:30 INFO [auto_gptq.quantization.gptq] duration: 0.5197877883911133\n",
      "2024-11-12 13:31:30 INFO [auto_gptq.quantization.gptq] avg loss: 4889.734375\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/52...\n",
      "2024-11-12 13:31:30 INFO [auto_gptq.quantization.gptq] duration: 0.2339019775390625\n",
      "2024-11-12 13:31:30 INFO [auto_gptq.quantization.gptq] avg loss: 4873.67724609375\n",
      "INFO - Quantizing mlp.down_proj in layer 14/52...\n",
      "2024-11-12 13:32:12 INFO [auto_gptq.quantization.gptq] duration: 0.8876500129699707\n",
      "2024-11-12 13:32:12 INFO [auto_gptq.quantization.gptq] avg loss: 3581.0673828125\n",
      "INFO - Start quantizing layer 15/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/52...\n",
      "2024-11-12 13:33:26 INFO [auto_gptq.quantization.gptq] duration: 0.5959813594818115\n",
      "2024-11-12 13:33:26 INFO [auto_gptq.quantization.gptq] avg loss: 898.4159545898438\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/52...\n",
      "2024-11-12 13:33:26 INFO [auto_gptq.quantization.gptq] duration: 0.37024569511413574\n",
      "2024-11-12 13:33:26 INFO [auto_gptq.quantization.gptq] avg loss: 614.3258056640625\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/52...\n",
      "2024-11-12 13:33:27 INFO [auto_gptq.quantization.gptq] duration: 0.3663303852081299\n",
      "2024-11-12 13:33:27 INFO [auto_gptq.quantization.gptq] avg loss: 1979.91162109375\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/52...\n",
      "2024-11-12 13:34:03 INFO [auto_gptq.quantization.gptq] duration: 0.5043377876281738\n",
      "2024-11-12 13:34:03 INFO [auto_gptq.quantization.gptq] avg loss: 377.29339599609375\n",
      "INFO - Quantizing mlp.up_proj in layer 15/52...\n",
      "2024-11-12 13:34:41 INFO [auto_gptq.quantization.gptq] duration: 0.5210516452789307\n",
      "2024-11-12 13:34:41 INFO [auto_gptq.quantization.gptq] avg loss: 4627.5595703125\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/52...\n",
      "2024-11-12 13:34:41 INFO [auto_gptq.quantization.gptq] duration: 0.3862442970275879\n",
      "2024-11-12 13:34:41 INFO [auto_gptq.quantization.gptq] avg loss: 4475.3779296875\n",
      "INFO - Quantizing mlp.down_proj in layer 15/52...\n",
      "2024-11-12 13:35:23 INFO [auto_gptq.quantization.gptq] duration: 0.8929107189178467\n",
      "2024-11-12 13:35:23 INFO [auto_gptq.quantization.gptq] avg loss: 2945.292236328125\n",
      "INFO - Start quantizing layer 16/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/52...\n",
      "2024-11-12 13:36:37 INFO [auto_gptq.quantization.gptq] duration: 0.5229811668395996\n",
      "2024-11-12 13:36:37 INFO [auto_gptq.quantization.gptq] avg loss: 905.171875\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/52...\n",
      "2024-11-12 13:36:37 INFO [auto_gptq.quantization.gptq] duration: 0.22352170944213867\n",
      "2024-11-12 13:36:37 INFO [auto_gptq.quantization.gptq] avg loss: 599.0811767578125\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/52...\n",
      "2024-11-12 13:36:38 INFO [auto_gptq.quantization.gptq] duration: 0.22415828704833984\n",
      "2024-11-12 13:36:38 INFO [auto_gptq.quantization.gptq] avg loss: 1991.501708984375\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/52...\n",
      "2024-11-12 13:37:14 INFO [auto_gptq.quantization.gptq] duration: 0.5044491291046143\n",
      "2024-11-12 13:37:14 INFO [auto_gptq.quantization.gptq] avg loss: 356.25677490234375\n",
      "INFO - Quantizing mlp.up_proj in layer 16/52...\n",
      "2024-11-12 13:37:51 INFO [auto_gptq.quantization.gptq] duration: 0.5233614444732666\n",
      "2024-11-12 13:37:51 INFO [auto_gptq.quantization.gptq] avg loss: 4376.7509765625\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/52...\n",
      "2024-11-12 13:37:52 INFO [auto_gptq.quantization.gptq] duration: 0.22818970680236816\n",
      "2024-11-12 13:37:52 INFO [auto_gptq.quantization.gptq] avg loss: 4309.685546875\n",
      "INFO - Quantizing mlp.down_proj in layer 16/52...\n",
      "2024-11-12 13:38:34 INFO [auto_gptq.quantization.gptq] duration: 0.9066212177276611\n",
      "2024-11-12 13:38:34 INFO [auto_gptq.quantization.gptq] avg loss: 2845.123291015625\n",
      "INFO - Start quantizing layer 17/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/52...\n",
      "2024-11-12 13:39:48 INFO [auto_gptq.quantization.gptq] duration: 0.5256249904632568\n",
      "2024-11-12 13:39:48 INFO [auto_gptq.quantization.gptq] avg loss: 894.3782958984375\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/52...\n",
      "2024-11-12 13:39:48 INFO [auto_gptq.quantization.gptq] duration: 0.38707828521728516\n",
      "2024-11-12 13:39:48 INFO [auto_gptq.quantization.gptq] avg loss: 580.0504760742188\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/52...\n",
      "2024-11-12 13:39:49 INFO [auto_gptq.quantization.gptq] duration: 0.37346601486206055\n",
      "2024-11-12 13:39:49 INFO [auto_gptq.quantization.gptq] avg loss: 2161.389404296875\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/52...\n",
      "2024-11-12 13:40:25 INFO [auto_gptq.quantization.gptq] duration: 0.5020387172698975\n",
      "2024-11-12 13:40:25 INFO [auto_gptq.quantization.gptq] avg loss: 378.6213073730469\n",
      "INFO - Quantizing mlp.up_proj in layer 17/52...\n",
      "2024-11-12 13:41:02 INFO [auto_gptq.quantization.gptq] duration: 0.5239236354827881\n",
      "2024-11-12 13:41:02 INFO [auto_gptq.quantization.gptq] avg loss: 4452.2197265625\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/52...\n",
      "2024-11-12 13:41:03 INFO [auto_gptq.quantization.gptq] duration: 0.23369908332824707\n",
      "2024-11-12 13:41:03 INFO [auto_gptq.quantization.gptq] avg loss: 4288.126953125\n",
      "INFO - Quantizing mlp.down_proj in layer 17/52...\n",
      "2024-11-12 13:41:45 INFO [auto_gptq.quantization.gptq] duration: 0.8911073207855225\n",
      "2024-11-12 13:41:45 INFO [auto_gptq.quantization.gptq] avg loss: 2762.736572265625\n",
      "INFO - Start quantizing layer 18/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/52...\n",
      "2024-11-12 13:42:59 INFO [auto_gptq.quantization.gptq] duration: 0.5252923965454102\n",
      "2024-11-12 13:42:59 INFO [auto_gptq.quantization.gptq] avg loss: 828.7828369140625\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/52...\n",
      "2024-11-12 13:42:59 INFO [auto_gptq.quantization.gptq] duration: 0.22484111785888672\n",
      "2024-11-12 13:42:59 INFO [auto_gptq.quantization.gptq] avg loss: 683.5717163085938\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/52...\n",
      "2024-11-12 13:42:59 INFO [auto_gptq.quantization.gptq] duration: 0.22380828857421875\n",
      "2024-11-12 13:42:59 INFO [auto_gptq.quantization.gptq] avg loss: 1978.3028564453125\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/52...\n",
      "2024-11-12 13:43:36 INFO [auto_gptq.quantization.gptq] duration: 0.5042996406555176\n",
      "2024-11-12 13:43:36 INFO [auto_gptq.quantization.gptq] avg loss: 422.734619140625\n",
      "INFO - Quantizing mlp.up_proj in layer 18/52...\n",
      "2024-11-12 13:44:13 INFO [auto_gptq.quantization.gptq] duration: 0.523716926574707\n",
      "2024-11-12 13:44:13 INFO [auto_gptq.quantization.gptq] avg loss: 4106.0869140625\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/52...\n",
      "2024-11-12 13:44:14 INFO [auto_gptq.quantization.gptq] duration: 0.37741899490356445\n",
      "2024-11-12 13:44:14 INFO [auto_gptq.quantization.gptq] avg loss: 3968.8994140625\n",
      "INFO - Quantizing mlp.down_proj in layer 18/52...\n",
      "2024-11-12 13:44:56 INFO [auto_gptq.quantization.gptq] duration: 0.8964443206787109\n",
      "2024-11-12 13:44:56 INFO [auto_gptq.quantization.gptq] avg loss: 2504.635498046875\n",
      "INFO - Start quantizing layer 19/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/52...\n",
      "2024-11-12 13:46:10 INFO [auto_gptq.quantization.gptq] duration: 0.525707483291626\n",
      "2024-11-12 13:46:10 INFO [auto_gptq.quantization.gptq] avg loss: 744.53369140625\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/52...\n",
      "2024-11-12 13:46:10 INFO [auto_gptq.quantization.gptq] duration: 0.22547078132629395\n",
      "2024-11-12 13:46:10 INFO [auto_gptq.quantization.gptq] avg loss: 521.85546875\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/52...\n",
      "2024-11-12 13:46:10 INFO [auto_gptq.quantization.gptq] duration: 0.22605538368225098\n",
      "2024-11-12 13:46:10 INFO [auto_gptq.quantization.gptq] avg loss: 1962.258056640625\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/52...\n",
      "2024-11-12 13:46:46 INFO [auto_gptq.quantization.gptq] duration: 0.503471851348877\n",
      "2024-11-12 13:46:46 INFO [auto_gptq.quantization.gptq] avg loss: 555.9669189453125\n",
      "INFO - Quantizing mlp.up_proj in layer 19/52...\n",
      "2024-11-12 13:47:24 INFO [auto_gptq.quantization.gptq] duration: 0.5194010734558105\n",
      "2024-11-12 13:47:24 INFO [auto_gptq.quantization.gptq] avg loss: 4126.2431640625\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/52...\n",
      "2024-11-12 13:47:24 INFO [auto_gptq.quantization.gptq] duration: 0.23773932456970215\n",
      "2024-11-12 13:47:24 INFO [auto_gptq.quantization.gptq] avg loss: 3897.61865234375\n",
      "INFO - Quantizing mlp.down_proj in layer 19/52...\n",
      "2024-11-12 13:48:06 INFO [auto_gptq.quantization.gptq] duration: 0.9005024433135986\n",
      "2024-11-12 13:48:06 INFO [auto_gptq.quantization.gptq] avg loss: 29794.2890625\n",
      "INFO - Start quantizing layer 20/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/52...\n",
      "2024-11-12 13:49:20 INFO [auto_gptq.quantization.gptq] duration: 0.525019645690918\n",
      "2024-11-12 13:49:20 INFO [auto_gptq.quantization.gptq] avg loss: 516.655029296875\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/52...\n",
      "2024-11-12 13:49:20 INFO [auto_gptq.quantization.gptq] duration: 0.22310733795166016\n",
      "2024-11-12 13:49:20 INFO [auto_gptq.quantization.gptq] avg loss: 518.1114501953125\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/52...\n",
      "2024-11-12 13:49:20 INFO [auto_gptq.quantization.gptq] duration: 0.22490167617797852\n",
      "2024-11-12 13:49:20 INFO [auto_gptq.quantization.gptq] avg loss: 1460.0091552734375\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/52...\n",
      "2024-11-12 13:49:57 INFO [auto_gptq.quantization.gptq] duration: 0.5023729801177979\n",
      "2024-11-12 13:49:57 INFO [auto_gptq.quantization.gptq] avg loss: 442.45068359375\n",
      "INFO - Quantizing mlp.up_proj in layer 20/52...\n",
      "2024-11-12 13:50:34 INFO [auto_gptq.quantization.gptq] duration: 0.5217697620391846\n",
      "2024-11-12 13:50:34 INFO [auto_gptq.quantization.gptq] avg loss: 3711.832763671875\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/52...\n",
      "2024-11-12 13:50:34 INFO [auto_gptq.quantization.gptq] duration: 0.23240303993225098\n",
      "2024-11-12 13:50:34 INFO [auto_gptq.quantization.gptq] avg loss: 3485.71484375\n",
      "INFO - Quantizing mlp.down_proj in layer 20/52...\n",
      "2024-11-12 13:51:16 INFO [auto_gptq.quantization.gptq] duration: 0.889054536819458\n",
      "2024-11-12 13:51:16 INFO [auto_gptq.quantization.gptq] avg loss: 2964.0380859375\n",
      "INFO - Start quantizing layer 21/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/52...\n",
      "2024-11-12 13:52:30 INFO [auto_gptq.quantization.gptq] duration: 0.5225985050201416\n",
      "2024-11-12 13:52:30 INFO [auto_gptq.quantization.gptq] avg loss: 948.3402709960938\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/52...\n",
      "2024-11-12 13:52:30 INFO [auto_gptq.quantization.gptq] duration: 0.2854344844818115\n",
      "2024-11-12 13:52:30 INFO [auto_gptq.quantization.gptq] avg loss: 852.195068359375\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/52...\n",
      "2024-11-12 13:52:31 INFO [auto_gptq.quantization.gptq] duration: 0.3696432113647461\n",
      "2024-11-12 13:52:31 INFO [auto_gptq.quantization.gptq] avg loss: 2798.76318359375\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/52...\n",
      "2024-11-12 13:53:07 INFO [auto_gptq.quantization.gptq] duration: 0.5008077621459961\n",
      "2024-11-12 13:53:07 INFO [auto_gptq.quantization.gptq] avg loss: 662.0775146484375\n",
      "INFO - Quantizing mlp.up_proj in layer 21/52...\n",
      "2024-11-12 13:53:44 INFO [auto_gptq.quantization.gptq] duration: 0.5225181579589844\n",
      "2024-11-12 13:53:44 INFO [auto_gptq.quantization.gptq] avg loss: 3777.3740234375\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/52...\n",
      "2024-11-12 13:53:44 INFO [auto_gptq.quantization.gptq] duration: 0.23406100273132324\n",
      "2024-11-12 13:53:44 INFO [auto_gptq.quantization.gptq] avg loss: 3566.6728515625\n",
      "INFO - Quantizing mlp.down_proj in layer 21/52...\n",
      "2024-11-12 13:54:27 INFO [auto_gptq.quantization.gptq] duration: 0.886214017868042\n",
      "2024-11-12 13:54:27 INFO [auto_gptq.quantization.gptq] avg loss: 2325.84619140625\n",
      "INFO - Start quantizing layer 22/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/52...\n",
      "2024-11-12 13:55:40 INFO [auto_gptq.quantization.gptq] duration: 0.5235848426818848\n",
      "2024-11-12 13:55:40 INFO [auto_gptq.quantization.gptq] avg loss: 842.7196044921875\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/52...\n",
      "2024-11-12 13:55:40 INFO [auto_gptq.quantization.gptq] duration: 0.22339987754821777\n",
      "2024-11-12 13:55:40 INFO [auto_gptq.quantization.gptq] avg loss: 597.199462890625\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/52...\n",
      "2024-11-12 13:55:41 INFO [auto_gptq.quantization.gptq] duration: 0.2237238883972168\n",
      "2024-11-12 13:55:41 INFO [auto_gptq.quantization.gptq] avg loss: 2174.31640625\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/52...\n",
      "2024-11-12 13:56:17 INFO [auto_gptq.quantization.gptq] duration: 0.4998443126678467\n",
      "2024-11-12 13:56:17 INFO [auto_gptq.quantization.gptq] avg loss: 667.1734619140625\n",
      "INFO - Quantizing mlp.up_proj in layer 22/52...\n",
      "2024-11-12 13:56:54 INFO [auto_gptq.quantization.gptq] duration: 0.5156745910644531\n",
      "2024-11-12 13:56:54 INFO [auto_gptq.quantization.gptq] avg loss: 3480.9560546875\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/52...\n",
      "2024-11-12 13:56:54 INFO [auto_gptq.quantization.gptq] duration: 0.23104405403137207\n",
      "2024-11-12 13:56:54 INFO [auto_gptq.quantization.gptq] avg loss: 3249.1064453125\n",
      "INFO - Quantizing mlp.down_proj in layer 22/52...\n",
      "2024-11-12 13:57:36 INFO [auto_gptq.quantization.gptq] duration: 0.8885161876678467\n",
      "2024-11-12 13:57:36 INFO [auto_gptq.quantization.gptq] avg loss: 2133.9482421875\n",
      "INFO - Start quantizing layer 23/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/52...\n",
      "2024-11-12 13:58:50 INFO [auto_gptq.quantization.gptq] duration: 0.524827241897583\n",
      "2024-11-12 13:58:50 INFO [auto_gptq.quantization.gptq] avg loss: 787.7538452148438\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/52...\n",
      "2024-11-12 13:58:50 INFO [auto_gptq.quantization.gptq] duration: 0.22339653968811035\n",
      "2024-11-12 13:58:50 INFO [auto_gptq.quantization.gptq] avg loss: 599.2671508789062\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/52...\n",
      "2024-11-12 13:58:50 INFO [auto_gptq.quantization.gptq] duration: 0.22313594818115234\n",
      "2024-11-12 13:58:50 INFO [auto_gptq.quantization.gptq] avg loss: 1905.5263671875\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/52...\n",
      "2024-11-12 13:59:27 INFO [auto_gptq.quantization.gptq] duration: 0.5026350021362305\n",
      "2024-11-12 13:59:27 INFO [auto_gptq.quantization.gptq] avg loss: 842.5849609375\n",
      "INFO - Quantizing mlp.up_proj in layer 23/52...\n",
      "2024-11-12 14:00:04 INFO [auto_gptq.quantization.gptq] duration: 0.5231494903564453\n",
      "2024-11-12 14:00:04 INFO [auto_gptq.quantization.gptq] avg loss: 3469.985107421875\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/52...\n",
      "2024-11-12 14:00:04 INFO [auto_gptq.quantization.gptq] duration: 0.23250436782836914\n",
      "2024-11-12 14:00:04 INFO [auto_gptq.quantization.gptq] avg loss: 3124.975830078125\n",
      "INFO - Quantizing mlp.down_proj in layer 23/52...\n",
      "2024-11-12 14:00:47 INFO [auto_gptq.quantization.gptq] duration: 0.89156174659729\n",
      "2024-11-12 14:00:47 INFO [auto_gptq.quantization.gptq] avg loss: 1929.2127685546875\n",
      "INFO - Start quantizing layer 24/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/52...\n",
      "2024-11-12 14:02:00 INFO [auto_gptq.quantization.gptq] duration: 0.524162769317627\n",
      "2024-11-12 14:02:00 INFO [auto_gptq.quantization.gptq] avg loss: 1014.5165405273438\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/52...\n",
      "2024-11-12 14:02:00 INFO [auto_gptq.quantization.gptq] duration: 0.22380661964416504\n",
      "2024-11-12 14:02:00 INFO [auto_gptq.quantization.gptq] avg loss: 756.055908203125\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/52...\n",
      "2024-11-12 14:02:01 INFO [auto_gptq.quantization.gptq] duration: 0.22467899322509766\n",
      "2024-11-12 14:02:01 INFO [auto_gptq.quantization.gptq] avg loss: 2483.09814453125\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/52...\n",
      "2024-11-12 14:02:37 INFO [auto_gptq.quantization.gptq] duration: 0.5040366649627686\n",
      "2024-11-12 14:02:37 INFO [auto_gptq.quantization.gptq] avg loss: 698.82373046875\n",
      "INFO - Quantizing mlp.up_proj in layer 24/52...\n",
      "2024-11-12 14:03:14 INFO [auto_gptq.quantization.gptq] duration: 0.522324800491333\n",
      "2024-11-12 14:03:14 INFO [auto_gptq.quantization.gptq] avg loss: 3691.595947265625\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/52...\n",
      "2024-11-12 14:03:15 INFO [auto_gptq.quantization.gptq] duration: 0.23502230644226074\n",
      "2024-11-12 14:03:15 INFO [auto_gptq.quantization.gptq] avg loss: 3286.876953125\n",
      "INFO - Quantizing mlp.down_proj in layer 24/52...\n",
      "2024-11-12 14:03:57 INFO [auto_gptq.quantization.gptq] duration: 0.8913073539733887\n",
      "2024-11-12 14:03:57 INFO [auto_gptq.quantization.gptq] avg loss: 2196.58837890625\n",
      "INFO - Start quantizing layer 25/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/52...\n",
      "2024-11-12 14:05:10 INFO [auto_gptq.quantization.gptq] duration: 0.5237529277801514\n",
      "2024-11-12 14:05:10 INFO [auto_gptq.quantization.gptq] avg loss: 866.1968994140625\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/52...\n",
      "2024-11-12 14:05:11 INFO [auto_gptq.quantization.gptq] duration: 0.28664541244506836\n",
      "2024-11-12 14:05:11 INFO [auto_gptq.quantization.gptq] avg loss: 744.281494140625\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/52...\n",
      "2024-11-12 14:05:11 INFO [auto_gptq.quantization.gptq] duration: 0.3787682056427002\n",
      "2024-11-12 14:05:11 INFO [auto_gptq.quantization.gptq] avg loss: 2237.34423828125\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/52...\n",
      "2024-11-12 14:05:47 INFO [auto_gptq.quantization.gptq] duration: 0.5034885406494141\n",
      "2024-11-12 14:05:47 INFO [auto_gptq.quantization.gptq] avg loss: 933.53857421875\n",
      "INFO - Quantizing mlp.up_proj in layer 25/52...\n",
      "2024-11-12 14:06:25 INFO [auto_gptq.quantization.gptq] duration: 0.52191162109375\n",
      "2024-11-12 14:06:25 INFO [auto_gptq.quantization.gptq] avg loss: 3630.648681640625\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/52...\n",
      "2024-11-12 14:06:25 INFO [auto_gptq.quantization.gptq] duration: 0.23532366752624512\n",
      "2024-11-12 14:06:25 INFO [auto_gptq.quantization.gptq] avg loss: 3175.373046875\n",
      "INFO - Quantizing mlp.down_proj in layer 25/52...\n",
      "2024-11-12 14:07:07 INFO [auto_gptq.quantization.gptq] duration: 0.8890235424041748\n",
      "2024-11-12 14:07:07 INFO [auto_gptq.quantization.gptq] avg loss: 2313.41845703125\n",
      "INFO - Start quantizing layer 26/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/52...\n",
      "2024-11-12 14:08:21 INFO [auto_gptq.quantization.gptq] duration: 0.5219242572784424\n",
      "2024-11-12 14:08:21 INFO [auto_gptq.quantization.gptq] avg loss: 853.08154296875\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/52...\n",
      "2024-11-12 14:08:21 INFO [auto_gptq.quantization.gptq] duration: 0.2232663631439209\n",
      "2024-11-12 14:08:21 INFO [auto_gptq.quantization.gptq] avg loss: 734.1260375976562\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/52...\n",
      "2024-11-12 14:08:21 INFO [auto_gptq.quantization.gptq] duration: 0.2248091697692871\n",
      "2024-11-12 14:08:21 INFO [auto_gptq.quantization.gptq] avg loss: 2184.531494140625\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/52...\n",
      "2024-11-12 14:08:57 INFO [auto_gptq.quantization.gptq] duration: 0.5012335777282715\n",
      "2024-11-12 14:08:57 INFO [auto_gptq.quantization.gptq] avg loss: 903.6305541992188\n",
      "INFO - Quantizing mlp.up_proj in layer 26/52...\n",
      "2024-11-12 14:09:35 INFO [auto_gptq.quantization.gptq] duration: 0.521049976348877\n",
      "2024-11-12 14:09:35 INFO [auto_gptq.quantization.gptq] avg loss: 3792.52099609375\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/52...\n",
      "2024-11-12 14:09:35 INFO [auto_gptq.quantization.gptq] duration: 0.23313546180725098\n",
      "2024-11-12 14:09:35 INFO [auto_gptq.quantization.gptq] avg loss: 3519.9375\n",
      "INFO - Quantizing mlp.down_proj in layer 26/52...\n",
      "2024-11-12 14:10:17 INFO [auto_gptq.quantization.gptq] duration: 0.8922097682952881\n",
      "2024-11-12 14:10:17 INFO [auto_gptq.quantization.gptq] avg loss: 2438.15771484375\n",
      "INFO - Start quantizing layer 27/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/52...\n",
      "2024-11-12 14:11:31 INFO [auto_gptq.quantization.gptq] duration: 0.5237741470336914\n",
      "2024-11-12 14:11:31 INFO [auto_gptq.quantization.gptq] avg loss: 880.186279296875\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/52...\n",
      "2024-11-12 14:11:31 INFO [auto_gptq.quantization.gptq] duration: 0.22322463989257812\n",
      "2024-11-12 14:11:31 INFO [auto_gptq.quantization.gptq] avg loss: 707.9385986328125\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/52...\n",
      "2024-11-12 14:11:31 INFO [auto_gptq.quantization.gptq] duration: 0.38353681564331055\n",
      "2024-11-12 14:11:31 INFO [auto_gptq.quantization.gptq] avg loss: 2362.904541015625\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/52...\n",
      "2024-11-12 14:12:08 INFO [auto_gptq.quantization.gptq] duration: 0.5024707317352295\n",
      "2024-11-12 14:12:08 INFO [auto_gptq.quantization.gptq] avg loss: 814.7291259765625\n",
      "INFO - Quantizing mlp.up_proj in layer 27/52...\n",
      "2024-11-12 14:12:45 INFO [auto_gptq.quantization.gptq] duration: 0.5195808410644531\n",
      "2024-11-12 14:12:45 INFO [auto_gptq.quantization.gptq] avg loss: 3862.654296875\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/52...\n",
      "2024-11-12 14:12:45 INFO [auto_gptq.quantization.gptq] duration: 0.23745059967041016\n",
      "2024-11-12 14:12:45 INFO [auto_gptq.quantization.gptq] avg loss: 4027.30126953125\n",
      "INFO - Quantizing mlp.down_proj in layer 27/52...\n",
      "2024-11-12 14:13:28 INFO [auto_gptq.quantization.gptq] duration: 0.89316725730896\n",
      "2024-11-12 14:13:28 INFO [auto_gptq.quantization.gptq] avg loss: 2303.9677734375\n",
      "INFO - Start quantizing layer 28/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/52...\n",
      "2024-11-12 14:14:41 INFO [auto_gptq.quantization.gptq] duration: 0.5221993923187256\n",
      "2024-11-12 14:14:41 INFO [auto_gptq.quantization.gptq] avg loss: 953.40966796875\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/52...\n",
      "2024-11-12 14:14:41 INFO [auto_gptq.quantization.gptq] duration: 0.22222900390625\n",
      "2024-11-12 14:14:41 INFO [auto_gptq.quantization.gptq] avg loss: 873.4296875\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/52...\n",
      "2024-11-12 14:14:42 INFO [auto_gptq.quantization.gptq] duration: 0.22336292266845703\n",
      "2024-11-12 14:14:42 INFO [auto_gptq.quantization.gptq] avg loss: 2559.732666015625\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/52...\n",
      "2024-11-12 14:15:18 INFO [auto_gptq.quantization.gptq] duration: 0.5046734809875488\n",
      "2024-11-12 14:15:18 INFO [auto_gptq.quantization.gptq] avg loss: 777.7205200195312\n",
      "INFO - Quantizing mlp.up_proj in layer 28/52...\n",
      "2024-11-12 14:15:55 INFO [auto_gptq.quantization.gptq] duration: 0.5169863700866699\n",
      "2024-11-12 14:15:55 INFO [auto_gptq.quantization.gptq] avg loss: 3930.39501953125\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/52...\n",
      "2024-11-12 14:15:56 INFO [auto_gptq.quantization.gptq] duration: 0.2312610149383545\n",
      "2024-11-12 14:15:56 INFO [auto_gptq.quantization.gptq] avg loss: 4306.37841796875\n",
      "INFO - Quantizing mlp.down_proj in layer 28/52...\n",
      "2024-11-12 14:16:38 INFO [auto_gptq.quantization.gptq] duration: 0.8898715972900391\n",
      "2024-11-12 14:16:38 INFO [auto_gptq.quantization.gptq] avg loss: 2500.294189453125\n",
      "INFO - Start quantizing layer 29/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/52...\n",
      "2024-11-12 14:17:51 INFO [auto_gptq.quantization.gptq] duration: 0.5247089862823486\n",
      "2024-11-12 14:17:51 INFO [auto_gptq.quantization.gptq] avg loss: 855.0750732421875\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/52...\n",
      "2024-11-12 14:17:52 INFO [auto_gptq.quantization.gptq] duration: 0.22752761840820312\n",
      "2024-11-12 14:17:52 INFO [auto_gptq.quantization.gptq] avg loss: 574.71044921875\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/52...\n",
      "2024-11-12 14:17:52 INFO [auto_gptq.quantization.gptq] duration: 0.2250652313232422\n",
      "2024-11-12 14:17:52 INFO [auto_gptq.quantization.gptq] avg loss: 2220.8408203125\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/52...\n",
      "2024-11-12 14:18:28 INFO [auto_gptq.quantization.gptq] duration: 0.5047104358673096\n",
      "2024-11-12 14:18:28 INFO [auto_gptq.quantization.gptq] avg loss: 742.399658203125\n",
      "INFO - Quantizing mlp.up_proj in layer 29/52...\n",
      "2024-11-12 14:19:05 INFO [auto_gptq.quantization.gptq] duration: 0.519280195236206\n",
      "2024-11-12 14:19:05 INFO [auto_gptq.quantization.gptq] avg loss: 3570.0732421875\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/52...\n",
      "2024-11-12 14:19:06 INFO [auto_gptq.quantization.gptq] duration: 0.23687314987182617\n",
      "2024-11-12 14:19:06 INFO [auto_gptq.quantization.gptq] avg loss: 3547.49267578125\n",
      "INFO - Quantizing mlp.down_proj in layer 29/52...\n",
      "2024-11-12 14:19:48 INFO [auto_gptq.quantization.gptq] duration: 0.897759199142456\n",
      "2024-11-12 14:19:48 INFO [auto_gptq.quantization.gptq] avg loss: 2279.22509765625\n",
      "INFO - Start quantizing layer 30/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/52...\n",
      "2024-11-12 14:21:01 INFO [auto_gptq.quantization.gptq] duration: 0.5275030136108398\n",
      "2024-11-12 14:21:01 INFO [auto_gptq.quantization.gptq] avg loss: 945.9813842773438\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/52...\n",
      "2024-11-12 14:21:01 INFO [auto_gptq.quantization.gptq] duration: 0.22571730613708496\n",
      "2024-11-12 14:21:01 INFO [auto_gptq.quantization.gptq] avg loss: 705.1788330078125\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/52...\n",
      "2024-11-12 14:21:02 INFO [auto_gptq.quantization.gptq] duration: 0.2267625331878662\n",
      "2024-11-12 14:21:02 INFO [auto_gptq.quantization.gptq] avg loss: 2470.027587890625\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/52...\n",
      "2024-11-12 14:21:38 INFO [auto_gptq.quantization.gptq] duration: 0.5053572654724121\n",
      "2024-11-12 14:21:38 INFO [auto_gptq.quantization.gptq] avg loss: 630.7386474609375\n",
      "INFO - Quantizing mlp.up_proj in layer 30/52...\n",
      "2024-11-12 14:22:15 INFO [auto_gptq.quantization.gptq] duration: 0.5172140598297119\n",
      "2024-11-12 14:22:15 INFO [auto_gptq.quantization.gptq] avg loss: 3626.546630859375\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/52...\n",
      "2024-11-12 14:22:15 INFO [auto_gptq.quantization.gptq] duration: 0.22880244255065918\n",
      "2024-11-12 14:22:15 INFO [auto_gptq.quantization.gptq] avg loss: 3335.463623046875\n",
      "INFO - Quantizing mlp.down_proj in layer 30/52...\n",
      "2024-11-12 14:22:58 INFO [auto_gptq.quantization.gptq] duration: 0.8980517387390137\n",
      "2024-11-12 14:22:58 INFO [auto_gptq.quantization.gptq] avg loss: 2206.27880859375\n",
      "INFO - Start quantizing layer 31/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/52...\n",
      "2024-11-12 14:24:11 INFO [auto_gptq.quantization.gptq] duration: 0.522162675857544\n",
      "2024-11-12 14:24:11 INFO [auto_gptq.quantization.gptq] avg loss: 1159.0146484375\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/52...\n",
      "2024-11-12 14:24:11 INFO [auto_gptq.quantization.gptq] duration: 0.3783836364746094\n",
      "2024-11-12 14:24:11 INFO [auto_gptq.quantization.gptq] avg loss: 767.0120849609375\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/52...\n",
      "2024-11-12 14:24:12 INFO [auto_gptq.quantization.gptq] duration: 0.36842918395996094\n",
      "2024-11-12 14:24:12 INFO [auto_gptq.quantization.gptq] avg loss: 2891.692138671875\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/52...\n",
      "2024-11-12 14:24:48 INFO [auto_gptq.quantization.gptq] duration: 0.5017349720001221\n",
      "2024-11-12 14:24:48 INFO [auto_gptq.quantization.gptq] avg loss: 967.6529541015625\n",
      "INFO - Quantizing mlp.up_proj in layer 31/52...\n",
      "2024-11-12 14:25:25 INFO [auto_gptq.quantization.gptq] duration: 0.5204503536224365\n",
      "2024-11-12 14:25:25 INFO [auto_gptq.quantization.gptq] avg loss: 3561.095703125\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/52...\n",
      "2024-11-12 14:25:26 INFO [auto_gptq.quantization.gptq] duration: 0.23455810546875\n",
      "2024-11-12 14:25:26 INFO [auto_gptq.quantization.gptq] avg loss: 3227.16748046875\n",
      "INFO - Quantizing mlp.down_proj in layer 31/52...\n",
      "2024-11-12 14:26:08 INFO [auto_gptq.quantization.gptq] duration: 0.8909571170806885\n",
      "2024-11-12 14:26:08 INFO [auto_gptq.quantization.gptq] avg loss: 2413.48388671875\n",
      "INFO - Start quantizing layer 32/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/52...\n",
      "2024-11-12 14:27:21 INFO [auto_gptq.quantization.gptq] duration: 0.527101993560791\n",
      "2024-11-12 14:27:21 INFO [auto_gptq.quantization.gptq] avg loss: 827.6321411132812\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/52...\n",
      "2024-11-12 14:27:22 INFO [auto_gptq.quantization.gptq] duration: 0.22557735443115234\n",
      "2024-11-12 14:27:22 INFO [auto_gptq.quantization.gptq] avg loss: 644.1255493164062\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/52...\n",
      "2024-11-12 14:27:22 INFO [auto_gptq.quantization.gptq] duration: 0.22743511199951172\n",
      "2024-11-12 14:27:22 INFO [auto_gptq.quantization.gptq] avg loss: 2097.93798828125\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/52...\n",
      "2024-11-12 14:27:58 INFO [auto_gptq.quantization.gptq] duration: 0.5056581497192383\n",
      "2024-11-12 14:27:58 INFO [auto_gptq.quantization.gptq] avg loss: 1085.931884765625\n",
      "INFO - Quantizing mlp.up_proj in layer 32/52...\n",
      "2024-11-12 14:28:36 INFO [auto_gptq.quantization.gptq] duration: 0.5185501575469971\n",
      "2024-11-12 14:28:36 INFO [auto_gptq.quantization.gptq] avg loss: 3270.190673828125\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/52...\n",
      "2024-11-12 14:28:36 INFO [auto_gptq.quantization.gptq] duration: 0.23615384101867676\n",
      "2024-11-12 14:28:36 INFO [auto_gptq.quantization.gptq] avg loss: 2968.0478515625\n",
      "INFO - Quantizing mlp.down_proj in layer 32/52...\n",
      "2024-11-12 14:29:18 INFO [auto_gptq.quantization.gptq] duration: 0.8934094905853271\n",
      "2024-11-12 14:29:18 INFO [auto_gptq.quantization.gptq] avg loss: 2019.931396484375\n",
      "INFO - Start quantizing layer 33/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 33/52...\n",
      "2024-11-12 14:30:32 INFO [auto_gptq.quantization.gptq] duration: 0.5225944519042969\n",
      "2024-11-12 14:30:32 INFO [auto_gptq.quantization.gptq] avg loss: 1282.3297119140625\n",
      "INFO - Quantizing self_attn.v_proj in layer 33/52...\n",
      "2024-11-12 14:30:32 INFO [auto_gptq.quantization.gptq] duration: 0.2231121063232422\n",
      "2024-11-12 14:30:32 INFO [auto_gptq.quantization.gptq] avg loss: 947.588134765625\n",
      "INFO - Quantizing self_attn.q_proj in layer 33/52...\n",
      "2024-11-12 14:30:32 INFO [auto_gptq.quantization.gptq] duration: 0.22412419319152832\n",
      "2024-11-12 14:30:32 INFO [auto_gptq.quantization.gptq] avg loss: 3263.0078125\n",
      "INFO - Quantizing self_attn.o_proj in layer 33/52...\n",
      "2024-11-12 14:31:08 INFO [auto_gptq.quantization.gptq] duration: 0.502514123916626\n",
      "2024-11-12 14:31:08 INFO [auto_gptq.quantization.gptq] avg loss: 786.5330810546875\n",
      "INFO - Quantizing mlp.up_proj in layer 33/52...\n",
      "2024-11-12 14:31:46 INFO [auto_gptq.quantization.gptq] duration: 0.5155870914459229\n",
      "2024-11-12 14:31:46 INFO [auto_gptq.quantization.gptq] avg loss: 3112.245849609375\n",
      "INFO - Quantizing mlp.gate_proj in layer 33/52...\n",
      "2024-11-12 14:31:46 INFO [auto_gptq.quantization.gptq] duration: 0.23229432106018066\n",
      "2024-11-12 14:31:46 INFO [auto_gptq.quantization.gptq] avg loss: 2859.82958984375\n",
      "INFO - Quantizing mlp.down_proj in layer 33/52...\n",
      "2024-11-12 14:32:28 INFO [auto_gptq.quantization.gptq] duration: 0.8863134384155273\n",
      "2024-11-12 14:32:28 INFO [auto_gptq.quantization.gptq] avg loss: 1901.7916259765625\n",
      "INFO - Start quantizing layer 34/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 34/52...\n",
      "2024-11-12 14:33:42 INFO [auto_gptq.quantization.gptq] duration: 0.5228724479675293\n",
      "2024-11-12 14:33:42 INFO [auto_gptq.quantization.gptq] avg loss: 1540.924560546875\n",
      "INFO - Quantizing self_attn.v_proj in layer 34/52...\n",
      "2024-11-12 14:33:42 INFO [auto_gptq.quantization.gptq] duration: 0.3743441104888916\n",
      "2024-11-12 14:33:42 INFO [auto_gptq.quantization.gptq] avg loss: 1377.1708984375\n",
      "INFO - Quantizing self_attn.q_proj in layer 34/52...\n",
      "2024-11-12 14:33:43 INFO [auto_gptq.quantization.gptq] duration: 0.3674619197845459\n",
      "2024-11-12 14:33:43 INFO [auto_gptq.quantization.gptq] avg loss: 4261.28564453125\n",
      "INFO - Quantizing self_attn.o_proj in layer 34/52...\n",
      "2024-11-12 14:34:19 INFO [auto_gptq.quantization.gptq] duration: 0.5055046081542969\n",
      "2024-11-12 14:34:19 INFO [auto_gptq.quantization.gptq] avg loss: 642.3108520507812\n",
      "INFO - Quantizing mlp.up_proj in layer 34/52...\n",
      "2024-11-12 14:34:56 INFO [auto_gptq.quantization.gptq] duration: 0.5159094333648682\n",
      "2024-11-12 14:34:56 INFO [auto_gptq.quantization.gptq] avg loss: 3473.82666015625\n",
      "INFO - Quantizing mlp.gate_proj in layer 34/52...\n",
      "2024-11-12 14:34:56 INFO [auto_gptq.quantization.gptq] duration: 0.23562383651733398\n",
      "2024-11-12 14:34:56 INFO [auto_gptq.quantization.gptq] avg loss: 3175.27587890625\n",
      "INFO - Quantizing mlp.down_proj in layer 34/52...\n",
      "2024-11-12 14:35:38 INFO [auto_gptq.quantization.gptq] duration: 0.8908681869506836\n",
      "2024-11-12 14:35:38 INFO [auto_gptq.quantization.gptq] avg loss: 2478.05126953125\n",
      "INFO - Start quantizing layer 35/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 35/52...\n",
      "2024-11-12 14:36:52 INFO [auto_gptq.quantization.gptq] duration: 0.5231063365936279\n",
      "2024-11-12 14:36:52 INFO [auto_gptq.quantization.gptq] avg loss: 1016.0984497070312\n",
      "INFO - Quantizing self_attn.v_proj in layer 35/52...\n",
      "2024-11-12 14:36:52 INFO [auto_gptq.quantization.gptq] duration: 0.2239828109741211\n",
      "2024-11-12 14:36:52 INFO [auto_gptq.quantization.gptq] avg loss: 753.8031005859375\n",
      "INFO - Quantizing self_attn.q_proj in layer 35/52...\n",
      "2024-11-12 14:36:52 INFO [auto_gptq.quantization.gptq] duration: 0.2243337631225586\n",
      "2024-11-12 14:36:52 INFO [auto_gptq.quantization.gptq] avg loss: 3290.41357421875\n",
      "INFO - Quantizing self_attn.o_proj in layer 35/52...\n",
      "2024-11-12 14:37:29 INFO [auto_gptq.quantization.gptq] duration: 0.5020782947540283\n",
      "2024-11-12 14:37:29 INFO [auto_gptq.quantization.gptq] avg loss: 774.6890258789062\n",
      "INFO - Quantizing mlp.up_proj in layer 35/52...\n",
      "2024-11-12 14:38:06 INFO [auto_gptq.quantization.gptq] duration: 0.5181624889373779\n",
      "2024-11-12 14:38:06 INFO [auto_gptq.quantization.gptq] avg loss: 3555.445556640625\n",
      "INFO - Quantizing mlp.gate_proj in layer 35/52...\n",
      "2024-11-12 14:38:06 INFO [auto_gptq.quantization.gptq] duration: 0.2277669906616211\n",
      "2024-11-12 14:38:06 INFO [auto_gptq.quantization.gptq] avg loss: 3372.014404296875\n",
      "INFO - Quantizing mlp.down_proj in layer 35/52...\n",
      "2024-11-12 14:38:48 INFO [auto_gptq.quantization.gptq] duration: 0.8941292762756348\n",
      "2024-11-12 14:38:48 INFO [auto_gptq.quantization.gptq] avg loss: 2762.296142578125\n",
      "INFO - Start quantizing layer 36/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 36/52...\n",
      "2024-11-12 14:40:02 INFO [auto_gptq.quantization.gptq] duration: 0.5250296592712402\n",
      "2024-11-12 14:40:02 INFO [auto_gptq.quantization.gptq] avg loss: 1017.3692626953125\n",
      "INFO - Quantizing self_attn.v_proj in layer 36/52...\n",
      "2024-11-12 14:40:02 INFO [auto_gptq.quantization.gptq] duration: 0.22649574279785156\n",
      "2024-11-12 14:40:02 INFO [auto_gptq.quantization.gptq] avg loss: 970.601318359375\n",
      "INFO - Quantizing self_attn.q_proj in layer 36/52...\n",
      "2024-11-12 14:40:02 INFO [auto_gptq.quantization.gptq] duration: 0.22618746757507324\n",
      "2024-11-12 14:40:02 INFO [auto_gptq.quantization.gptq] avg loss: 2757.500244140625\n",
      "INFO - Quantizing self_attn.o_proj in layer 36/52...\n",
      "2024-11-12 14:40:39 INFO [auto_gptq.quantization.gptq] duration: 0.5053696632385254\n",
      "2024-11-12 14:40:39 INFO [auto_gptq.quantization.gptq] avg loss: 899.8406982421875\n",
      "INFO - Quantizing mlp.up_proj in layer 36/52...\n",
      "2024-11-12 14:41:16 INFO [auto_gptq.quantization.gptq] duration: 0.5171048641204834\n",
      "2024-11-12 14:41:16 INFO [auto_gptq.quantization.gptq] avg loss: 3907.831787109375\n",
      "INFO - Quantizing mlp.gate_proj in layer 36/52...\n",
      "2024-11-12 14:41:16 INFO [auto_gptq.quantization.gptq] duration: 0.23530125617980957\n",
      "2024-11-12 14:41:16 INFO [auto_gptq.quantization.gptq] avg loss: 3636.12255859375\n",
      "INFO - Quantizing mlp.down_proj in layer 36/52...\n",
      "2024-11-12 14:41:58 INFO [auto_gptq.quantization.gptq] duration: 0.8974184989929199\n",
      "2024-11-12 14:41:58 INFO [auto_gptq.quantization.gptq] avg loss: 3564.5830078125\n",
      "INFO - Start quantizing layer 37/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 37/52...\n",
      "2024-11-12 14:43:12 INFO [auto_gptq.quantization.gptq] duration: 0.5226826667785645\n",
      "2024-11-12 14:43:12 INFO [auto_gptq.quantization.gptq] avg loss: 1243.85009765625\n",
      "INFO - Quantizing self_attn.v_proj in layer 37/52...\n",
      "2024-11-12 14:43:12 INFO [auto_gptq.quantization.gptq] duration: 0.22326993942260742\n",
      "2024-11-12 14:43:12 INFO [auto_gptq.quantization.gptq] avg loss: 1523.6241455078125\n",
      "INFO - Quantizing self_attn.q_proj in layer 37/52...\n",
      "2024-11-12 14:43:13 INFO [auto_gptq.quantization.gptq] duration: 0.22552824020385742\n",
      "2024-11-12 14:43:13 INFO [auto_gptq.quantization.gptq] avg loss: 3740.1640625\n",
      "INFO - Quantizing self_attn.o_proj in layer 37/52...\n",
      "2024-11-12 14:43:49 INFO [auto_gptq.quantization.gptq] duration: 0.5040068626403809\n",
      "2024-11-12 14:43:49 INFO [auto_gptq.quantization.gptq] avg loss: 1726.16455078125\n",
      "INFO - Quantizing mlp.up_proj in layer 37/52...\n",
      "2024-11-12 14:44:26 INFO [auto_gptq.quantization.gptq] duration: 0.5136299133300781\n",
      "2024-11-12 14:44:26 INFO [auto_gptq.quantization.gptq] avg loss: 4312.77783203125\n",
      "INFO - Quantizing mlp.gate_proj in layer 37/52...\n",
      "2024-11-12 14:44:26 INFO [auto_gptq.quantization.gptq] duration: 0.2336428165435791\n",
      "2024-11-12 14:44:26 INFO [auto_gptq.quantization.gptq] avg loss: 3989.0546875\n",
      "INFO - Quantizing mlp.down_proj in layer 37/52...\n",
      "2024-11-12 14:45:09 INFO [auto_gptq.quantization.gptq] duration: 0.888831377029419\n",
      "2024-11-12 14:45:09 INFO [auto_gptq.quantization.gptq] avg loss: 4104.28759765625\n",
      "INFO - Start quantizing layer 38/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 38/52...\n",
      "2024-11-12 14:46:22 INFO [auto_gptq.quantization.gptq] duration: 0.5269308090209961\n",
      "2024-11-12 14:46:22 INFO [auto_gptq.quantization.gptq] avg loss: 1468.7047119140625\n",
      "INFO - Quantizing self_attn.v_proj in layer 38/52...\n",
      "2024-11-12 14:46:23 INFO [auto_gptq.quantization.gptq] duration: 0.22568011283874512\n",
      "2024-11-12 14:46:23 INFO [auto_gptq.quantization.gptq] avg loss: 2212.193115234375\n",
      "INFO - Quantizing self_attn.q_proj in layer 38/52...\n",
      "2024-11-12 14:46:23 INFO [auto_gptq.quantization.gptq] duration: 0.22715163230895996\n",
      "2024-11-12 14:46:23 INFO [auto_gptq.quantization.gptq] avg loss: 4280.14453125\n",
      "INFO - Quantizing self_attn.o_proj in layer 38/52...\n",
      "2024-11-12 14:46:59 INFO [auto_gptq.quantization.gptq] duration: 0.50486159324646\n",
      "2024-11-12 14:46:59 INFO [auto_gptq.quantization.gptq] avg loss: 1779.833740234375\n",
      "INFO - Quantizing mlp.up_proj in layer 38/52...\n",
      "2024-11-12 14:47:37 INFO [auto_gptq.quantization.gptq] duration: 0.5194694995880127\n",
      "2024-11-12 14:47:37 INFO [auto_gptq.quantization.gptq] avg loss: 4672.23681640625\n",
      "INFO - Quantizing mlp.gate_proj in layer 38/52...\n",
      "2024-11-12 14:47:37 INFO [auto_gptq.quantization.gptq] duration: 0.22883868217468262\n",
      "2024-11-12 14:47:37 INFO [auto_gptq.quantization.gptq] avg loss: 4407.54052734375\n",
      "INFO - Quantizing mlp.down_proj in layer 38/52...\n",
      "2024-11-12 14:48:19 INFO [auto_gptq.quantization.gptq] duration: 0.893714189529419\n",
      "2024-11-12 14:48:19 INFO [auto_gptq.quantization.gptq] avg loss: 5381.6337890625\n",
      "INFO - Start quantizing layer 39/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 39/52...\n",
      "2024-11-12 14:49:33 INFO [auto_gptq.quantization.gptq] duration: 0.5212016105651855\n",
      "2024-11-12 14:49:33 INFO [auto_gptq.quantization.gptq] avg loss: 970.5779418945312\n",
      "INFO - Quantizing self_attn.v_proj in layer 39/52...\n",
      "2024-11-12 14:49:33 INFO [auto_gptq.quantization.gptq] duration: 0.37805962562561035\n",
      "2024-11-12 14:49:33 INFO [auto_gptq.quantization.gptq] avg loss: 1263.64599609375\n",
      "INFO - Quantizing self_attn.q_proj in layer 39/52...\n",
      "2024-11-12 14:49:34 INFO [auto_gptq.quantization.gptq] duration: 0.37312746047973633\n",
      "2024-11-12 14:49:34 INFO [auto_gptq.quantization.gptq] avg loss: 3288.15576171875\n",
      "INFO - Quantizing self_attn.o_proj in layer 39/52...\n",
      "2024-11-12 14:50:10 INFO [auto_gptq.quantization.gptq] duration: 0.5045764446258545\n",
      "2024-11-12 14:50:10 INFO [auto_gptq.quantization.gptq] avg loss: 1948.6298828125\n",
      "INFO - Quantizing mlp.up_proj in layer 39/52...\n",
      "2024-11-12 14:50:47 INFO [auto_gptq.quantization.gptq] duration: 0.520033597946167\n",
      "2024-11-12 14:50:47 INFO [auto_gptq.quantization.gptq] avg loss: 4816.7978515625\n",
      "INFO - Quantizing mlp.gate_proj in layer 39/52...\n",
      "2024-11-12 14:50:47 INFO [auto_gptq.quantization.gptq] duration: 0.23590326309204102\n",
      "2024-11-12 14:50:47 INFO [auto_gptq.quantization.gptq] avg loss: 4504.20263671875\n",
      "INFO - Quantizing mlp.down_proj in layer 39/52...\n",
      "2024-11-12 14:51:30 INFO [auto_gptq.quantization.gptq] duration: 1.2505853176116943\n",
      "2024-11-12 14:51:30 INFO [auto_gptq.quantization.gptq] avg loss: 6398.5146484375\n",
      "INFO - Start quantizing layer 40/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 40/52...\n",
      "2024-11-12 14:52:43 INFO [auto_gptq.quantization.gptq] duration: 0.5200860500335693\n",
      "2024-11-12 14:52:43 INFO [auto_gptq.quantization.gptq] avg loss: 848.3948974609375\n",
      "INFO - Quantizing self_attn.v_proj in layer 40/52...\n",
      "2024-11-12 14:52:44 INFO [auto_gptq.quantization.gptq] duration: 0.22376108169555664\n",
      "2024-11-12 14:52:44 INFO [auto_gptq.quantization.gptq] avg loss: 1522.405029296875\n",
      "INFO - Quantizing self_attn.q_proj in layer 40/52...\n",
      "2024-11-12 14:52:44 INFO [auto_gptq.quantization.gptq] duration: 0.22658514976501465\n",
      "2024-11-12 14:52:44 INFO [auto_gptq.quantization.gptq] avg loss: 2737.9775390625\n",
      "INFO - Quantizing self_attn.o_proj in layer 40/52...\n",
      "2024-11-12 14:53:20 INFO [auto_gptq.quantization.gptq] duration: 0.5053582191467285\n",
      "2024-11-12 14:53:20 INFO [auto_gptq.quantization.gptq] avg loss: 2503.681640625\n",
      "INFO - Quantizing mlp.up_proj in layer 40/52...\n",
      "2024-11-12 14:53:57 INFO [auto_gptq.quantization.gptq] duration: 0.5208644866943359\n",
      "2024-11-12 14:53:57 INFO [auto_gptq.quantization.gptq] avg loss: 5458.12060546875\n",
      "INFO - Quantizing mlp.gate_proj in layer 40/52...\n",
      "2024-11-12 14:53:58 INFO [auto_gptq.quantization.gptq] duration: 0.23458003997802734\n",
      "2024-11-12 14:53:58 INFO [auto_gptq.quantization.gptq] avg loss: 5104.9248046875\n",
      "INFO - Quantizing mlp.down_proj in layer 40/52...\n",
      "2024-11-12 14:54:40 INFO [auto_gptq.quantization.gptq] duration: 0.8950014114379883\n",
      "2024-11-12 14:54:40 INFO [auto_gptq.quantization.gptq] avg loss: 7557.81787109375\n",
      "INFO - Start quantizing layer 41/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 41/52...\n",
      "2024-11-12 14:55:53 INFO [auto_gptq.quantization.gptq] duration: 0.522284746170044\n",
      "2024-11-12 14:55:53 INFO [auto_gptq.quantization.gptq] avg loss: 1129.08349609375\n",
      "INFO - Quantizing self_attn.v_proj in layer 41/52...\n",
      "2024-11-12 14:55:53 INFO [auto_gptq.quantization.gptq] duration: 0.22438764572143555\n",
      "2024-11-12 14:55:53 INFO [auto_gptq.quantization.gptq] avg loss: 1634.400146484375\n",
      "INFO - Quantizing self_attn.q_proj in layer 41/52...\n",
      "2024-11-12 14:55:54 INFO [auto_gptq.quantization.gptq] duration: 0.22285819053649902\n",
      "2024-11-12 14:55:54 INFO [auto_gptq.quantization.gptq] avg loss: 4172.96240234375\n",
      "INFO - Quantizing self_attn.o_proj in layer 41/52...\n",
      "2024-11-12 14:56:30 INFO [auto_gptq.quantization.gptq] duration: 0.5009174346923828\n",
      "2024-11-12 14:56:30 INFO [auto_gptq.quantization.gptq] avg loss: 1959.760009765625\n",
      "INFO - Quantizing mlp.up_proj in layer 41/52...\n",
      "2024-11-12 14:57:07 INFO [auto_gptq.quantization.gptq] duration: 0.5187726020812988\n",
      "2024-11-12 14:57:07 INFO [auto_gptq.quantization.gptq] avg loss: 6705.46484375\n",
      "INFO - Quantizing mlp.gate_proj in layer 41/52...\n",
      "2024-11-12 14:57:08 INFO [auto_gptq.quantization.gptq] duration: 0.2375493049621582\n",
      "2024-11-12 14:57:08 INFO [auto_gptq.quantization.gptq] avg loss: 6239.1533203125\n",
      "INFO - Quantizing mlp.down_proj in layer 41/52...\n",
      "2024-11-12 14:57:50 INFO [auto_gptq.quantization.gptq] duration: 0.8964650630950928\n",
      "2024-11-12 14:57:50 INFO [auto_gptq.quantization.gptq] avg loss: 11316.3623046875\n",
      "INFO - Start quantizing layer 42/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 42/52...\n",
      "2024-11-12 14:59:03 INFO [auto_gptq.quantization.gptq] duration: 0.5244119167327881\n",
      "2024-11-12 14:59:03 INFO [auto_gptq.quantization.gptq] avg loss: 1061.4534912109375\n",
      "INFO - Quantizing self_attn.v_proj in layer 42/52...\n",
      "2024-11-12 14:59:04 INFO [auto_gptq.quantization.gptq] duration: 0.3007180690765381\n",
      "2024-11-12 14:59:04 INFO [auto_gptq.quantization.gptq] avg loss: 2835.131103515625\n",
      "INFO - Quantizing self_attn.q_proj in layer 42/52...\n",
      "2024-11-12 14:59:04 INFO [auto_gptq.quantization.gptq] duration: 0.36908531188964844\n",
      "2024-11-12 14:59:04 INFO [auto_gptq.quantization.gptq] avg loss: 3622.11669921875\n",
      "INFO - Quantizing self_attn.o_proj in layer 42/52...\n",
      "2024-11-12 14:59:40 INFO [auto_gptq.quantization.gptq] duration: 0.5054061412811279\n",
      "2024-11-12 14:59:40 INFO [auto_gptq.quantization.gptq] avg loss: 2214.8828125\n",
      "INFO - Quantizing mlp.up_proj in layer 42/52...\n",
      "2024-11-12 15:00:18 INFO [auto_gptq.quantization.gptq] duration: 0.5173399448394775\n",
      "2024-11-12 15:00:18 INFO [auto_gptq.quantization.gptq] avg loss: 7944.5625\n",
      "INFO - Quantizing mlp.gate_proj in layer 42/52...\n",
      "2024-11-12 15:00:18 INFO [auto_gptq.quantization.gptq] duration: 0.23607540130615234\n",
      "2024-11-12 15:00:18 INFO [auto_gptq.quantization.gptq] avg loss: 7063.0947265625\n",
      "INFO - Quantizing mlp.down_proj in layer 42/52...\n",
      "2024-11-12 15:01:00 INFO [auto_gptq.quantization.gptq] duration: 0.8897228240966797\n",
      "2024-11-12 15:01:00 INFO [auto_gptq.quantization.gptq] avg loss: 15240.583984375\n",
      "INFO - Start quantizing layer 43/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 43/52...\n",
      "2024-11-12 15:02:14 INFO [auto_gptq.quantization.gptq] duration: 0.5217599868774414\n",
      "2024-11-12 15:02:14 INFO [auto_gptq.quantization.gptq] avg loss: 998.1783447265625\n",
      "INFO - Quantizing self_attn.v_proj in layer 43/52...\n",
      "2024-11-12 15:02:14 INFO [auto_gptq.quantization.gptq] duration: 0.22384858131408691\n",
      "2024-11-12 15:02:14 INFO [auto_gptq.quantization.gptq] avg loss: 2206.25390625\n",
      "INFO - Quantizing self_attn.q_proj in layer 43/52...\n",
      "2024-11-12 15:02:14 INFO [auto_gptq.quantization.gptq] duration: 0.22367119789123535\n",
      "2024-11-12 15:02:14 INFO [auto_gptq.quantization.gptq] avg loss: 3668.439208984375\n",
      "INFO - Quantizing self_attn.o_proj in layer 43/52...\n",
      "2024-11-12 15:02:50 INFO [auto_gptq.quantization.gptq] duration: 0.5025167465209961\n",
      "2024-11-12 15:02:50 INFO [auto_gptq.quantization.gptq] avg loss: 3305.0390625\n",
      "INFO - Quantizing mlp.up_proj in layer 43/52...\n",
      "2024-11-12 15:03:28 INFO [auto_gptq.quantization.gptq] duration: 0.5150964260101318\n",
      "2024-11-12 15:03:28 INFO [auto_gptq.quantization.gptq] avg loss: 9213.2373046875\n",
      "INFO - Quantizing mlp.gate_proj in layer 43/52...\n",
      "2024-11-12 15:03:28 INFO [auto_gptq.quantization.gptq] duration: 0.23430371284484863\n",
      "2024-11-12 15:03:28 INFO [auto_gptq.quantization.gptq] avg loss: 8203.150390625\n",
      "INFO - Quantizing mlp.down_proj in layer 43/52...\n",
      "2024-11-12 15:04:10 INFO [auto_gptq.quantization.gptq] duration: 0.8884584903717041\n",
      "2024-11-12 15:04:10 INFO [auto_gptq.quantization.gptq] avg loss: 19455.025390625\n",
      "INFO - Start quantizing layer 44/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 44/52...\n",
      "2024-11-12 15:05:24 INFO [auto_gptq.quantization.gptq] duration: 0.5258722305297852\n",
      "2024-11-12 15:05:24 INFO [auto_gptq.quantization.gptq] avg loss: 1082.577880859375\n",
      "INFO - Quantizing self_attn.v_proj in layer 44/52...\n",
      "2024-11-12 15:05:24 INFO [auto_gptq.quantization.gptq] duration: 0.22341513633728027\n",
      "2024-11-12 15:05:24 INFO [auto_gptq.quantization.gptq] avg loss: 4661.3798828125\n",
      "INFO - Quantizing self_attn.q_proj in layer 44/52...\n",
      "2024-11-12 15:05:25 INFO [auto_gptq.quantization.gptq] duration: 0.22455930709838867\n",
      "2024-11-12 15:05:25 INFO [auto_gptq.quantization.gptq] avg loss: 4283.5546875\n",
      "INFO - Quantizing self_attn.o_proj in layer 44/52...\n",
      "2024-11-12 15:06:01 INFO [auto_gptq.quantization.gptq] duration: 0.5034523010253906\n",
      "2024-11-12 15:06:01 INFO [auto_gptq.quantization.gptq] avg loss: 4183.896484375\n",
      "INFO - Quantizing mlp.up_proj in layer 44/52...\n",
      "2024-11-12 15:06:38 INFO [auto_gptq.quantization.gptq] duration: 0.5229263305664062\n",
      "2024-11-12 15:06:38 INFO [auto_gptq.quantization.gptq] avg loss: 10386.583984375\n",
      "INFO - Quantizing mlp.gate_proj in layer 44/52...\n",
      "2024-11-12 15:06:39 INFO [auto_gptq.quantization.gptq] duration: 0.3760552406311035\n",
      "2024-11-12 15:06:39 INFO [auto_gptq.quantization.gptq] avg loss: 9060.71484375\n",
      "INFO - Quantizing mlp.down_proj in layer 44/52...\n",
      "2024-11-12 15:07:21 INFO [auto_gptq.quantization.gptq] duration: 0.8960545063018799\n",
      "2024-11-12 15:07:21 INFO [auto_gptq.quantization.gptq] avg loss: 35781.5\n",
      "INFO - Start quantizing layer 45/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 45/52...\n",
      "2024-11-12 15:08:35 INFO [auto_gptq.quantization.gptq] duration: 0.5244755744934082\n",
      "2024-11-12 15:08:35 INFO [auto_gptq.quantization.gptq] avg loss: 1320.981689453125\n",
      "INFO - Quantizing self_attn.v_proj in layer 45/52...\n",
      "2024-11-12 15:08:35 INFO [auto_gptq.quantization.gptq] duration: 0.22611260414123535\n",
      "2024-11-12 15:08:35 INFO [auto_gptq.quantization.gptq] avg loss: 4857.05712890625\n",
      "INFO - Quantizing self_attn.q_proj in layer 45/52...\n",
      "2024-11-12 15:08:35 INFO [auto_gptq.quantization.gptq] duration: 0.22599053382873535\n",
      "2024-11-12 15:08:35 INFO [auto_gptq.quantization.gptq] avg loss: 4813.4052734375\n",
      "INFO - Quantizing self_attn.o_proj in layer 45/52...\n",
      "2024-11-12 15:09:12 INFO [auto_gptq.quantization.gptq] duration: 0.5024886131286621\n",
      "2024-11-12 15:09:12 INFO [auto_gptq.quantization.gptq] avg loss: 4049.705810546875\n",
      "INFO - Quantizing mlp.up_proj in layer 45/52...\n",
      "2024-11-12 15:09:49 INFO [auto_gptq.quantization.gptq] duration: 0.5198659896850586\n",
      "2024-11-12 15:09:49 INFO [auto_gptq.quantization.gptq] avg loss: 11340.591796875\n",
      "INFO - Quantizing mlp.gate_proj in layer 45/52...\n",
      "2024-11-12 15:09:49 INFO [auto_gptq.quantization.gptq] duration: 0.4236791133880615\n",
      "2024-11-12 15:09:49 INFO [auto_gptq.quantization.gptq] avg loss: 9832.185546875\n",
      "INFO - Quantizing mlp.down_proj in layer 45/52...\n",
      "2024-11-12 15:10:32 INFO [auto_gptq.quantization.gptq] duration: 0.8921983242034912\n",
      "2024-11-12 15:10:32 INFO [auto_gptq.quantization.gptq] avg loss: 37766.5546875\n",
      "INFO - Start quantizing layer 46/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 46/52...\n",
      "2024-11-12 15:11:45 INFO [auto_gptq.quantization.gptq] duration: 0.5267839431762695\n",
      "2024-11-12 15:11:46 INFO [auto_gptq.quantization.gptq] avg loss: 1469.96923828125\n",
      "INFO - Quantizing self_attn.v_proj in layer 46/52...\n",
      "2024-11-12 15:11:46 INFO [auto_gptq.quantization.gptq] duration: 0.2236185073852539\n",
      "2024-11-12 15:11:46 INFO [auto_gptq.quantization.gptq] avg loss: 6015.72998046875\n",
      "INFO - Quantizing self_attn.q_proj in layer 46/52...\n",
      "2024-11-12 15:11:46 INFO [auto_gptq.quantization.gptq] duration: 0.2242293357849121\n",
      "2024-11-12 15:11:46 INFO [auto_gptq.quantization.gptq] avg loss: 5300.4873046875\n",
      "INFO - Quantizing self_attn.o_proj in layer 46/52...\n",
      "2024-11-12 15:12:22 INFO [auto_gptq.quantization.gptq] duration: 0.5035929679870605\n",
      "2024-11-12 15:12:22 INFO [auto_gptq.quantization.gptq] avg loss: 6272.9599609375\n",
      "INFO - Quantizing mlp.up_proj in layer 46/52...\n",
      "2024-11-12 15:13:00 INFO [auto_gptq.quantization.gptq] duration: 0.520026445388794\n",
      "2024-11-12 15:13:00 INFO [auto_gptq.quantization.gptq] avg loss: 12017.2607421875\n",
      "INFO - Quantizing mlp.gate_proj in layer 46/52...\n",
      "2024-11-12 15:13:00 INFO [auto_gptq.quantization.gptq] duration: 0.23496198654174805\n",
      "2024-11-12 15:13:00 INFO [auto_gptq.quantization.gptq] avg loss: 10064.353515625\n",
      "INFO - Quantizing mlp.down_proj in layer 46/52...\n",
      "2024-11-12 15:13:42 INFO [auto_gptq.quantization.gptq] duration: 0.8920629024505615\n",
      "2024-11-12 15:13:42 INFO [auto_gptq.quantization.gptq] avg loss: 43762.94140625\n",
      "INFO - Start quantizing layer 47/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 47/52...\n",
      "2024-11-12 15:14:56 INFO [auto_gptq.quantization.gptq] duration: 0.5239510536193848\n",
      "2024-11-12 15:14:56 INFO [auto_gptq.quantization.gptq] avg loss: 1756.37548828125\n",
      "INFO - Quantizing self_attn.v_proj in layer 47/52...\n",
      "2024-11-12 15:14:56 INFO [auto_gptq.quantization.gptq] duration: 0.22335147857666016\n",
      "2024-11-12 15:14:56 INFO [auto_gptq.quantization.gptq] avg loss: 6408.384765625\n",
      "INFO - Quantizing self_attn.q_proj in layer 47/52...\n",
      "2024-11-12 15:14:56 INFO [auto_gptq.quantization.gptq] duration: 0.22607827186584473\n",
      "2024-11-12 15:14:56 INFO [auto_gptq.quantization.gptq] avg loss: 5829.78955078125\n",
      "INFO - Quantizing self_attn.o_proj in layer 47/52...\n",
      "2024-11-12 15:15:33 INFO [auto_gptq.quantization.gptq] duration: 0.5045833587646484\n",
      "2024-11-12 15:15:33 INFO [auto_gptq.quantization.gptq] avg loss: 7935.4638671875\n",
      "INFO - Quantizing mlp.up_proj in layer 47/52...\n",
      "2024-11-12 15:16:10 INFO [auto_gptq.quantization.gptq] duration: 0.5229969024658203\n",
      "2024-11-12 15:16:10 INFO [auto_gptq.quantization.gptq] avg loss: 12251.8134765625\n",
      "INFO - Quantizing mlp.gate_proj in layer 47/52...\n",
      "2024-11-12 15:16:10 INFO [auto_gptq.quantization.gptq] duration: 0.2367234230041504\n",
      "2024-11-12 15:16:10 INFO [auto_gptq.quantization.gptq] avg loss: 9838.7724609375\n",
      "INFO - Quantizing mlp.down_proj in layer 47/52...\n",
      "2024-11-12 15:16:53 INFO [auto_gptq.quantization.gptq] duration: 0.8901917934417725\n",
      "2024-11-12 15:16:53 INFO [auto_gptq.quantization.gptq] avg loss: 51235.83203125\n",
      "INFO - Start quantizing layer 48/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 48/52...\n",
      "2024-11-12 15:18:06 INFO [auto_gptq.quantization.gptq] duration: 0.5247244834899902\n",
      "2024-11-12 15:18:06 INFO [auto_gptq.quantization.gptq] avg loss: 1574.6962890625\n",
      "INFO - Quantizing self_attn.v_proj in layer 48/52...\n",
      "2024-11-12 15:18:07 INFO [auto_gptq.quantization.gptq] duration: 0.2235887050628662\n",
      "2024-11-12 15:18:07 INFO [auto_gptq.quantization.gptq] avg loss: 5593.498046875\n",
      "INFO - Quantizing self_attn.q_proj in layer 48/52...\n",
      "2024-11-12 15:18:07 INFO [auto_gptq.quantization.gptq] duration: 0.22400474548339844\n",
      "2024-11-12 15:18:07 INFO [auto_gptq.quantization.gptq] avg loss: 5323.296875\n",
      "INFO - Quantizing self_attn.o_proj in layer 48/52...\n",
      "2024-11-12 15:18:43 INFO [auto_gptq.quantization.gptq] duration: 0.5021660327911377\n",
      "2024-11-12 15:18:43 INFO [auto_gptq.quantization.gptq] avg loss: 6810.44189453125\n",
      "INFO - Quantizing mlp.up_proj in layer 48/52...\n",
      "2024-11-12 15:19:21 INFO [auto_gptq.quantization.gptq] duration: 0.5230913162231445\n",
      "2024-11-12 15:19:21 INFO [auto_gptq.quantization.gptq] avg loss: 12748.369140625\n",
      "INFO - Quantizing mlp.gate_proj in layer 48/52...\n",
      "2024-11-12 15:19:21 INFO [auto_gptq.quantization.gptq] duration: 0.2324833869934082\n",
      "2024-11-12 15:19:21 INFO [auto_gptq.quantization.gptq] avg loss: 10539.2021484375\n",
      "INFO - Quantizing mlp.down_proj in layer 48/52...\n",
      "2024-11-12 15:20:03 INFO [auto_gptq.quantization.gptq] duration: 0.8894054889678955\n",
      "2024-11-12 15:20:03 INFO [auto_gptq.quantization.gptq] avg loss: 50370.25390625\n",
      "INFO - Start quantizing layer 49/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 49/52...\n",
      "2024-11-12 15:21:17 INFO [auto_gptq.quantization.gptq] duration: 0.5275571346282959\n",
      "2024-11-12 15:21:17 INFO [auto_gptq.quantization.gptq] avg loss: 1480.852294921875\n",
      "INFO - Quantizing self_attn.v_proj in layer 49/52...\n",
      "2024-11-12 15:21:17 INFO [auto_gptq.quantization.gptq] duration: 0.22608184814453125\n",
      "2024-11-12 15:21:17 INFO [auto_gptq.quantization.gptq] avg loss: 5368.4697265625\n",
      "INFO - Quantizing self_attn.q_proj in layer 49/52...\n",
      "2024-11-12 15:21:17 INFO [auto_gptq.quantization.gptq] duration: 0.22552275657653809\n",
      "2024-11-12 15:21:17 INFO [auto_gptq.quantization.gptq] avg loss: 4950.66845703125\n",
      "INFO - Quantizing self_attn.o_proj in layer 49/52...\n",
      "2024-11-12 15:21:54 INFO [auto_gptq.quantization.gptq] duration: 0.5054223537445068\n",
      "2024-11-12 15:21:54 INFO [auto_gptq.quantization.gptq] avg loss: 6033.587890625\n",
      "INFO - Quantizing mlp.up_proj in layer 49/52...\n",
      "2024-11-12 15:22:31 INFO [auto_gptq.quantization.gptq] duration: 0.5227415561676025\n",
      "2024-11-12 15:22:31 INFO [auto_gptq.quantization.gptq] avg loss: 13469.6787109375\n",
      "INFO - Quantizing mlp.gate_proj in layer 49/52...\n",
      "2024-11-12 15:22:31 INFO [auto_gptq.quantization.gptq] duration: 0.2343454360961914\n",
      "2024-11-12 15:22:31 INFO [auto_gptq.quantization.gptq] avg loss: 10903.7978515625\n",
      "INFO - Quantizing mlp.down_proj in layer 49/52...\n",
      "2024-11-12 15:23:14 INFO [auto_gptq.quantization.gptq] duration: 0.8955297470092773\n",
      "2024-11-12 15:23:14 INFO [auto_gptq.quantization.gptq] avg loss: 58175.0859375\n",
      "INFO - Start quantizing layer 50/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 50/52...\n",
      "2024-11-12 15:24:27 INFO [auto_gptq.quantization.gptq] duration: 0.525688886642456\n",
      "2024-11-12 15:24:27 INFO [auto_gptq.quantization.gptq] avg loss: 1607.634033203125\n",
      "INFO - Quantizing self_attn.v_proj in layer 50/52...\n",
      "2024-11-12 15:24:28 INFO [auto_gptq.quantization.gptq] duration: 0.2264101505279541\n",
      "2024-11-12 15:24:28 INFO [auto_gptq.quantization.gptq] avg loss: 6302.9697265625\n",
      "INFO - Quantizing self_attn.q_proj in layer 50/52...\n",
      "2024-11-12 15:24:28 INFO [auto_gptq.quantization.gptq] duration: 0.22476911544799805\n",
      "2024-11-12 15:24:28 INFO [auto_gptq.quantization.gptq] avg loss: 5318.7080078125\n",
      "INFO - Quantizing self_attn.o_proj in layer 50/52...\n",
      "2024-11-12 15:25:04 INFO [auto_gptq.quantization.gptq] duration: 0.5054588317871094\n",
      "2024-11-12 15:25:04 INFO [auto_gptq.quantization.gptq] avg loss: 14360.5859375\n",
      "INFO - Quantizing mlp.up_proj in layer 50/52...\n",
      "2024-11-12 15:25:42 INFO [auto_gptq.quantization.gptq] duration: 0.5229790210723877\n",
      "2024-11-12 15:25:42 INFO [auto_gptq.quantization.gptq] avg loss: 14102.744140625\n",
      "INFO - Quantizing mlp.gate_proj in layer 50/52...\n",
      "2024-11-12 15:25:42 INFO [auto_gptq.quantization.gptq] duration: 0.23537492752075195\n",
      "2024-11-12 15:25:42 INFO [auto_gptq.quantization.gptq] avg loss: 11249.802734375\n",
      "INFO - Quantizing mlp.down_proj in layer 50/52...\n",
      "2024-11-12 15:26:24 INFO [auto_gptq.quantization.gptq] duration: 0.8945608139038086\n",
      "2024-11-12 15:26:24 INFO [auto_gptq.quantization.gptq] avg loss: 68934.46875\n",
      "INFO - Start quantizing layer 51/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 51/52...\n",
      "2024-11-12 15:27:38 INFO [auto_gptq.quantization.gptq] duration: 0.5273230075836182\n",
      "2024-11-12 15:27:38 INFO [auto_gptq.quantization.gptq] avg loss: 1185.7147216796875\n",
      "INFO - Quantizing self_attn.v_proj in layer 51/52...\n",
      "2024-11-12 15:27:38 INFO [auto_gptq.quantization.gptq] duration: 0.22584939002990723\n",
      "2024-11-12 15:27:38 INFO [auto_gptq.quantization.gptq] avg loss: 4048.994140625\n",
      "INFO - Quantizing self_attn.q_proj in layer 51/52...\n",
      "2024-11-12 15:27:38 INFO [auto_gptq.quantization.gptq] duration: 0.2258586883544922\n",
      "2024-11-12 15:27:38 INFO [auto_gptq.quantization.gptq] avg loss: 4203.2783203125\n",
      "INFO - Quantizing self_attn.o_proj in layer 51/52...\n",
      "2024-11-12 15:28:15 INFO [auto_gptq.quantization.gptq] duration: 0.5044846534729004\n",
      "2024-11-12 15:28:15 INFO [auto_gptq.quantization.gptq] avg loss: 6003.7900390625\n",
      "INFO - Quantizing mlp.up_proj in layer 51/52...\n",
      "2024-11-12 15:28:52 INFO [auto_gptq.quantization.gptq] duration: 0.5162022113800049\n",
      "2024-11-12 15:28:52 INFO [auto_gptq.quantization.gptq] avg loss: 14700.240234375\n",
      "INFO - Quantizing mlp.gate_proj in layer 51/52...\n",
      "2024-11-12 15:28:52 INFO [auto_gptq.quantization.gptq] duration: 0.23588156700134277\n",
      "2024-11-12 15:28:52 INFO [auto_gptq.quantization.gptq] avg loss: 12322.724609375\n",
      "INFO - Quantizing mlp.down_proj in layer 51/52...\n",
      "2024-11-12 15:29:35 INFO [auto_gptq.quantization.gptq] duration: 0.8987081050872803\n",
      "2024-11-12 15:29:35 INFO [auto_gptq.quantization.gptq] avg loss: 87054.109375\n",
      "INFO - Start quantizing layer 52/52\n",
      "INFO - Quantizing self_attn.k_proj in layer 52/52...\n",
      "2024-11-12 15:30:48 INFO [auto_gptq.quantization.gptq] duration: 0.523451566696167\n",
      "2024-11-12 15:30:48 INFO [auto_gptq.quantization.gptq] avg loss: 896.3395385742188\n",
      "INFO - Quantizing self_attn.v_proj in layer 52/52...\n",
      "2024-11-12 15:30:49 INFO [auto_gptq.quantization.gptq] duration: 0.3757193088531494\n",
      "2024-11-12 15:30:49 INFO [auto_gptq.quantization.gptq] avg loss: 2998.083740234375\n",
      "INFO - Quantizing self_attn.q_proj in layer 52/52...\n",
      "2024-11-12 15:30:49 INFO [auto_gptq.quantization.gptq] duration: 0.366199254989624\n",
      "2024-11-12 15:30:49 INFO [auto_gptq.quantization.gptq] avg loss: 3393.86669921875\n",
      "INFO - Quantizing self_attn.o_proj in layer 52/52...\n",
      "2024-11-12 15:31:25 INFO [auto_gptq.quantization.gptq] duration: 0.5011265277862549\n",
      "2024-11-12 15:31:25 INFO [auto_gptq.quantization.gptq] avg loss: 6337.02490234375\n",
      "INFO - Quantizing mlp.up_proj in layer 52/52...\n",
      "2024-11-12 15:32:03 INFO [auto_gptq.quantization.gptq] duration: 0.5175418853759766\n",
      "2024-11-12 15:32:03 INFO [auto_gptq.quantization.gptq] avg loss: 18912.5703125\n",
      "INFO - Quantizing mlp.gate_proj in layer 52/52...\n",
      "2024-11-12 15:32:03 INFO [auto_gptq.quantization.gptq] duration: 0.23381733894348145\n",
      "2024-11-12 15:32:03 INFO [auto_gptq.quantization.gptq] avg loss: 17375.61328125\n",
      "INFO - Quantizing mlp.down_proj in layer 52/52...\n",
      "2024-11-12 15:32:45 INFO [auto_gptq.quantization.gptq] duration: 0.8926010131835938\n",
      "2024-11-12 15:32:45 INFO [auto_gptq.quantization.gptq] avg loss: 201516.921875\n",
      "INFO - Packing model...\n",
      "2024-11-12 15:33:20 INFO [auto_gptq.modeling._utils] Packing model...\n",
      "Packing llm.model.layers.51.mlp.down_proj...: 100%|██████████| 364/364 [00:28<00:00, 12.75it/s]   \n",
      "INFO - Model packed.\n",
      "2024-11-12 15:33:52 INFO [auto_gptq.modeling._utils] Model packed.\n"
     ]
    }
   ],
   "source": [
    "model.quantize(traindataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:33:59 INFO [auto_gptq.modeling.minicpm.configuration_minicpm] vision_config is None, using default vision config\n"
     ]
    }
   ],
   "source": [
    "# save quantized model\n",
    "model.save_quantized(quantized_model_dir)\n",
    "# model.save_quantized(quantized_model_dir, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalMLM, BaseQuantizeConfig\n",
    "torch.manual_seed(0)\n",
    "def llm_load(path = '/data1/liyx/Models/MiniCPM3-1B-sft-bf16/'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "    # model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.float16, device_map='cuda', trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.float32, device_map='cuda', trust_remote_code=True)\n",
    "    prompt = \"'y = 2x - 7, 3x^2 + y = 8'是二元一次方程吗？\"\n",
    "    responds, history = model.chat(\n",
    "            tokenizer, \n",
    "            prompt, \n",
    "            max_new_tokens=512,\n",
    "            do_sample=False, \n",
    "            num_beams=1,\n",
    "            repetition_penalty=1,\n",
    "            top_p=None, \n",
    "            temperature=None,\n",
    "    )\n",
    "    print(f\"[Prompt]\\n{prompt}\\n\")\n",
    "    print(f\"[Respond]\\n{responds}\\n\")\n",
    "    return model, tokenizer\n",
    "def mllm_load(path = '/data/zyq/minicpm-3o-1b-sft-v1'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "    model = AutoModel.from_pretrained(path, torch_dtype=torch.float16, device_map='cuda', trust_remote_code=True)\n",
    "    # for block in model.llm.model.layers:\n",
    "        # block.prepare_layernorm()\n",
    "        ## block.self_attn.prepare_conv()\n",
    "        # block.self_attn.prepare_sha()\n",
    "        ## block.mlp.prepare_conv()\n",
    "    model.config.use_cache = False\n",
    "    return model, tokenizer\n",
    "def gptq_mllm_load(path = '/data/zyq/8295/checkpoints/minicpm-3o-sft-v1-gptq-1107'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "    model = AutoGPTQForCausalMLM.from_quantized(path, torch_dtype=torch.float16, device=\"cuda:0\", trust_remote_code=True, disable_exllama=True)\n",
    "    # for block in model.model.llm.model.layers:\n",
    "        ## block.prepare_layernorm()\n",
    "        ## block.self_attn.prepare_conv()\n",
    "        # block.self_attn.prepare_sha()\n",
    "        ## block.mlp.prepare_conv()\n",
    "    model.config.use_cache = False\n",
    "    return model, tokenizer\n",
    "def llm_test(prompt, model, tokenizer):\n",
    "    res, _ = model.chat(\n",
    "            tokenizer, \n",
    "            prompt, \n",
    "            max_new_tokens=256,\n",
    "            do_sample=False, \n",
    "            num_beams=1,\n",
    "            repetition_penalty=1,\n",
    "            top_p=None, \n",
    "            temperature=None,\n",
    "    )\n",
    "    print(f\"[Prompt]\\n{prompt}\\n\")\n",
    "    print(f\"[Respond]\\n{res}\\n\")\n",
    "def mllm_test(imagepath, prompt, model, tokenizer, max_new_tokens=256):\n",
    "    # msgs = [{'role': 'user', 'content': [None, prompt]}]\n",
    "    image = Image.open(imagepath).convert('RGB')\n",
    "    msgs = [{'role': 'user', 'content': [image, prompt]}]\n",
    "    res = model.chat(\n",
    "            image=None,\n",
    "            msgs=msgs,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            sampling=False,\n",
    "            num_beams=1,\n",
    "            repetition_penalty=1,\n",
    "    )\n",
    "    print(f\"[Image]\\n{imagepath}\\n\")\n",
    "    print(f\"[Prompt]\\n{prompt}\\n\")\n",
    "    print(f\"[Respond]\\n{res}\\n\")\n",
    "def mllm_test_txt(prompt, model, tokenizer, max_new_tokens=256):\n",
    "    # msgs = [{'role': 'user', 'content': [None, prompt]}]\n",
    "    msgs = [{'role': 'user', 'content': [None, prompt]}]\n",
    "    res = model.chat(\n",
    "            image=None,\n",
    "            msgs=msgs,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            sampling=False,\n",
    "            num_beams=1,\n",
    "            repetition_penalty=1,\n",
    "    )\n",
    "    print(f\"[Prompt]\\n{prompt}\\n\")\n",
    "    print(f\"[Respond]\\n{res}\\n\")\n",
    "def gptq_mllm_test(imagepath, prompt, model, tokenizer, max_new_tokens=256):\n",
    "    # msgs = [{'role': 'user', 'content': [None, prompt]}]\n",
    "    image = Image.open(imagepath).convert('RGB')\n",
    "    msgs = [{'role': 'user', 'content': [image, prompt]}]\n",
    "    res = model.chat(\n",
    "            image=None,\n",
    "            msgs=msgs,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            sampling=False,\n",
    "            num_beams=1,\n",
    "            repetition_penalty=1,\n",
    "    )\n",
    "    print(f\"[Image]\\n{imagepath}\\n\")\n",
    "    print(f\"[Prompt]\\n{prompt}\\n\")\n",
    "    print(f\"[Respond]\\n{res}\\n\")\n",
    "def gptq_mllm_test_txt(prompt, model, tokenizer, max_new_tokens=256):\n",
    "    # msgs = [{'role': 'user', 'content': [None, prompt]}]\n",
    "    msgs = [{'role': 'user', 'content': [None, prompt]}]\n",
    "    res = model.chat(\n",
    "            image=None,\n",
    "            msgs=msgs,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            sampling=False,\n",
    "            num_beams=1,\n",
    "            repetition_penalty=1,\n",
    "    )\n",
    "    print(f\"[Prompt]\\n{prompt}\\n\")\n",
    "    print(f\"[Respond]\\n{res}\\n\")\n",
    "def mllm_stream_test(imagepath, prompt, model, tokenizer, max_new_tokens=256):\n",
    "    image = Image.open(imagepath).convert('RGB')\n",
    "    msgs = [{'role': 'user', 'content': [image, prompt]}]\n",
    "    res = model.chat(\n",
    "            image=None,\n",
    "            msgs=msgs,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stream=True,\n",
    "    )\n",
    "    print(f\"[Image]\\n{imagepath}\\n\")\n",
    "    print(f\"[Prompt]\\n{prompt}\\n\")\n",
    "    print(f\"[Respond]\\n\")\n",
    "    generated_text = \"\"\n",
    "    for new_text in res:\n",
    "        generated_text += new_text\n",
    "        print(new_text, flush=True, end='')\n",
    "    print(\"\\n\")\n",
    "def mllm_multiturn_test(imagepath, prompt, model, tokenizer, max_new_tokens=256):\n",
    "    image = Image.open(imagepath).convert('RGB')\n",
    "    msgs = [{'role': 'user', 'content': [image, prompt]}]\n",
    "    res = model.chat(\n",
    "            image=None,\n",
    "            msgs=msgs,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            sampling=False,\n",
    "            num_beams=1,\n",
    "            repetition_penalty=1,\n",
    "    )\n",
    "    print(f\"[Image]\\n{imagepath}\\n\")\n",
    "    print(f\"[Prompt]\\n{prompt}\\n\")\n",
    "    print(f\"[Respond]\\n{res}\\n\")\n",
    "    prompt = \"茶位费多少钱\"\n",
    "    msgs.append({\"role\": \"assistant\", \"content\": [res]})\n",
    "    msgs.append({\"role\": \"user\", \"content\": [prompt]})\n",
    "    res = model.chat(\n",
    "            image=None,\n",
    "            msgs=msgs,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            sampling=False,\n",
    "            num_beams=1,\n",
    "            repetition_penalty=1,\n",
    "    )\n",
    "    print(f\"[Prompt]\\n{prompt}\\n\")\n",
    "    print(f\"[Respond]\\n{res}\\n\")\n",
    "    prompt = \"红糖核桃包的价格\"\n",
    "    msgs.append({\"role\": \"assistant\", \"content\": [res]})\n",
    "    msgs.append({\"role\": \"user\", \"content\": [prompt]})\n",
    "    res = model.chat(\n",
    "            image=None,\n",
    "            msgs=msgs,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            sampling=False,\n",
    "            num_beams=1,\n",
    "            repetition_penalty=1,\n",
    "    )\n",
    "    print(f\"[Prompt]\\n{prompt}\\n\")\n",
    "    print(f\"[Respond]\\n{res}\\n\")\n",
    "# mllm_path = \"/data/zyq/8295/checkpoints/minicpm-3o-sft-v1-gptq-llm-1107-fakequant\"\n",
    "# mllm, mllm_tokenizer = mllm_load(mllm_path)\n",
    "# mllm_test(\"/data/zyq/8295/tools/pics/airplane.jpeg\", \"这是什么东西\", mllm, mllm_tokenizer, max_new_tokens=512)\n",
    "# mllm_test_txt(\"求解方程x - y = 4, 2x + y = 9\", mllm, mllm_tokenizer, max_new_tokens=512)\n",
    "gptq_mllm, gptq_mllm_tokenizer = gptq_mllm_load('/data/zyq/8295/checkpoints/minicpm-3o-sft-v1-gptq-1112')\n",
    "print(gptq_mllm)\n",
    "# print(gptq_mllm.model.llm.model.layers[1].self_attn.q_proj.qweight)\n",
    "mllm_test(\"/data/zyq/8295/tools/pics/airplane.jpeg\", \"这是什么东西\", gptq_mllm, gptq_mllm_tokenizer, max_new_tokens=512)\n",
    "gptq_mllm_test_txt(\"求解方程x - y = 4, 2x + y = 9\", gptq_mllm, gptq_mllm_tokenizer, max_new_tokens=512)\n",
    "llm = gptq_mllm.model.llm\n",
    "llm_test(\"求解方程x - y = 4, 2x + y = 9\", llm, gptq_mllm_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import safe_open\n",
    "s = 0\n",
    "# 打开 safetensors 文件\n",
    "with safe_open(\"/home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256/model.safetensors\", framework=\"pt\") as f:\n",
    "    # 手动指定 metadata\n",
    "    # metadata = {\"format\": \"pt\"}  # 手动添加格式信息\n",
    "\n",
    "    # # 读取模型的权重\n",
    "    # tensors = {key: f.get_tensor(key) for key in f.keys()}\n",
    "    s = f.metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gptq_group_size': '-1',\n",
       " 'gptq_damp_percent': '0.01',\n",
       " 'auto_gptq_version': '0.8.0.dev0',\n",
       " 'gptq_quant_method': 'gptq',\n",
       " 'gptq_desc_act': 'True',\n",
       " 'gptq_bits': '8',\n",
       " 'gptq_checkpoint_format': 'gptq',\n",
       " 'format': 'pt'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file, save_file\n",
    "\n",
    "# 加载现有的 safetensors 文件\n",
    "file_path = \"/home/workspace/model/minicpm-3o-sft-v1-gptq-1112/model.safetensors\"\n",
    "tensors = load_file(file_path)\n",
    "\n",
    "# 修改或添加 metadata\n",
    "metadata = s  # 你可以添加你想要的 metadata\n",
    "\n",
    "# 重新保存文件，带上新的 metadata\n",
    "save_file(tensors, file_path, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lmquant/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "You are using a model of type minicpmv to instantiate a model of type minicpm. This is not supported for all configurations of models and can yield errors.\n",
      "INFO - Ignoring unknown parameter in the quantization configuration: vit_bits.\n",
      "INFO - Ignoring unknown parameter in the quantization configuration: llm_bits.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "/opt/conda/envs/lmquant/lib/python3.11/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256-llm-w4-pc were not used when initializing MiniCPMV: ['llm.model.layers.0.mlp.down_proj.g_idx', 'llm.model.layers.0.mlp.down_proj.qweight', 'llm.model.layers.0.mlp.down_proj.qzeros', 'llm.model.layers.0.mlp.down_proj.scales', 'llm.model.layers.0.mlp.gate_proj.g_idx', 'llm.model.layers.0.mlp.gate_proj.qweight', 'llm.model.layers.0.mlp.gate_proj.qzeros', 'llm.model.layers.0.mlp.gate_proj.scales', 'llm.model.layers.0.mlp.up_proj.g_idx', 'llm.model.layers.0.mlp.up_proj.qweight', 'llm.model.layers.0.mlp.up_proj.qzeros', 'llm.model.layers.0.mlp.up_proj.scales', 'llm.model.layers.0.self_attn.k_proj.g_idx', 'llm.model.layers.0.self_attn.k_proj.qweight', 'llm.model.layers.0.self_attn.k_proj.qzeros', 'llm.model.layers.0.self_attn.k_proj.scales', 'llm.model.layers.0.self_attn.o_proj.g_idx', 'llm.model.layers.0.self_attn.o_proj.qweight', 'llm.model.layers.0.self_attn.o_proj.qzeros', 'llm.model.layers.0.self_attn.o_proj.scales', 'llm.model.layers.0.self_attn.q_proj.g_idx', 'llm.model.layers.0.self_attn.q_proj.qweight', 'llm.model.layers.0.self_attn.q_proj.qzeros', 'llm.model.layers.0.self_attn.q_proj.scales', 'llm.model.layers.0.self_attn.v_proj.g_idx', 'llm.model.layers.0.self_attn.v_proj.qweight', 'llm.model.layers.0.self_attn.v_proj.qzeros', 'llm.model.layers.0.self_attn.v_proj.scales', 'llm.model.layers.1.mlp.down_proj.g_idx', 'llm.model.layers.1.mlp.down_proj.qweight', 'llm.model.layers.1.mlp.down_proj.qzeros', 'llm.model.layers.1.mlp.down_proj.scales', 'llm.model.layers.1.mlp.gate_proj.g_idx', 'llm.model.layers.1.mlp.gate_proj.qweight', 'llm.model.layers.1.mlp.gate_proj.qzeros', 'llm.model.layers.1.mlp.gate_proj.scales', 'llm.model.layers.1.mlp.up_proj.g_idx', 'llm.model.layers.1.mlp.up_proj.qweight', 'llm.model.layers.1.mlp.up_proj.qzeros', 'llm.model.layers.1.mlp.up_proj.scales', 'llm.model.layers.1.self_attn.k_proj.g_idx', 'llm.model.layers.1.self_attn.k_proj.qweight', 'llm.model.layers.1.self_attn.k_proj.qzeros', 'llm.model.layers.1.self_attn.k_proj.scales', 'llm.model.layers.1.self_attn.o_proj.g_idx', 'llm.model.layers.1.self_attn.o_proj.qweight', 'llm.model.layers.1.self_attn.o_proj.qzeros', 'llm.model.layers.1.self_attn.o_proj.scales', 'llm.model.layers.1.self_attn.q_proj.g_idx', 'llm.model.layers.1.self_attn.q_proj.qweight', 'llm.model.layers.1.self_attn.q_proj.qzeros', 'llm.model.layers.1.self_attn.q_proj.scales', 'llm.model.layers.1.self_attn.v_proj.g_idx', 'llm.model.layers.1.self_attn.v_proj.qweight', 'llm.model.layers.1.self_attn.v_proj.qzeros', 'llm.model.layers.1.self_attn.v_proj.scales', 'llm.model.layers.10.mlp.down_proj.g_idx', 'llm.model.layers.10.mlp.down_proj.qweight', 'llm.model.layers.10.mlp.down_proj.qzeros', 'llm.model.layers.10.mlp.down_proj.scales', 'llm.model.layers.10.mlp.gate_proj.g_idx', 'llm.model.layers.10.mlp.gate_proj.qweight', 'llm.model.layers.10.mlp.gate_proj.qzeros', 'llm.model.layers.10.mlp.gate_proj.scales', 'llm.model.layers.10.mlp.up_proj.g_idx', 'llm.model.layers.10.mlp.up_proj.qweight', 'llm.model.layers.10.mlp.up_proj.qzeros', 'llm.model.layers.10.mlp.up_proj.scales', 'llm.model.layers.10.self_attn.k_proj.g_idx', 'llm.model.layers.10.self_attn.k_proj.qweight', 'llm.model.layers.10.self_attn.k_proj.qzeros', 'llm.model.layers.10.self_attn.k_proj.scales', 'llm.model.layers.10.self_attn.o_proj.g_idx', 'llm.model.layers.10.self_attn.o_proj.qweight', 'llm.model.layers.10.self_attn.o_proj.qzeros', 'llm.model.layers.10.self_attn.o_proj.scales', 'llm.model.layers.10.self_attn.q_proj.g_idx', 'llm.model.layers.10.self_attn.q_proj.qweight', 'llm.model.layers.10.self_attn.q_proj.qzeros', 'llm.model.layers.10.self_attn.q_proj.scales', 'llm.model.layers.10.self_attn.v_proj.g_idx', 'llm.model.layers.10.self_attn.v_proj.qweight', 'llm.model.layers.10.self_attn.v_proj.qzeros', 'llm.model.layers.10.self_attn.v_proj.scales', 'llm.model.layers.11.mlp.down_proj.g_idx', 'llm.model.layers.11.mlp.down_proj.qweight', 'llm.model.layers.11.mlp.down_proj.qzeros', 'llm.model.layers.11.mlp.down_proj.scales', 'llm.model.layers.11.mlp.gate_proj.g_idx', 'llm.model.layers.11.mlp.gate_proj.qweight', 'llm.model.layers.11.mlp.gate_proj.qzeros', 'llm.model.layers.11.mlp.gate_proj.scales', 'llm.model.layers.11.mlp.up_proj.g_idx', 'llm.model.layers.11.mlp.up_proj.qweight', 'llm.model.layers.11.mlp.up_proj.qzeros', 'llm.model.layers.11.mlp.up_proj.scales', 'llm.model.layers.11.self_attn.k_proj.g_idx', 'llm.model.layers.11.self_attn.k_proj.qweight', 'llm.model.layers.11.self_attn.k_proj.qzeros', 'llm.model.layers.11.self_attn.k_proj.scales', 'llm.model.layers.11.self_attn.o_proj.g_idx', 'llm.model.layers.11.self_attn.o_proj.qweight', 'llm.model.layers.11.self_attn.o_proj.qzeros', 'llm.model.layers.11.self_attn.o_proj.scales', 'llm.model.layers.11.self_attn.q_proj.g_idx', 'llm.model.layers.11.self_attn.q_proj.qweight', 'llm.model.layers.11.self_attn.q_proj.qzeros', 'llm.model.layers.11.self_attn.q_proj.scales', 'llm.model.layers.11.self_attn.v_proj.g_idx', 'llm.model.layers.11.self_attn.v_proj.qweight', 'llm.model.layers.11.self_attn.v_proj.qzeros', 'llm.model.layers.11.self_attn.v_proj.scales', 'llm.model.layers.12.mlp.down_proj.g_idx', 'llm.model.layers.12.mlp.down_proj.qweight', 'llm.model.layers.12.mlp.down_proj.qzeros', 'llm.model.layers.12.mlp.down_proj.scales', 'llm.model.layers.12.mlp.gate_proj.g_idx', 'llm.model.layers.12.mlp.gate_proj.qweight', 'llm.model.layers.12.mlp.gate_proj.qzeros', 'llm.model.layers.12.mlp.gate_proj.scales', 'llm.model.layers.12.mlp.up_proj.g_idx', 'llm.model.layers.12.mlp.up_proj.qweight', 'llm.model.layers.12.mlp.up_proj.qzeros', 'llm.model.layers.12.mlp.up_proj.scales', 'llm.model.layers.12.self_attn.k_proj.g_idx', 'llm.model.layers.12.self_attn.k_proj.qweight', 'llm.model.layers.12.self_attn.k_proj.qzeros', 'llm.model.layers.12.self_attn.k_proj.scales', 'llm.model.layers.12.self_attn.o_proj.g_idx', 'llm.model.layers.12.self_attn.o_proj.qweight', 'llm.model.layers.12.self_attn.o_proj.qzeros', 'llm.model.layers.12.self_attn.o_proj.scales', 'llm.model.layers.12.self_attn.q_proj.g_idx', 'llm.model.layers.12.self_attn.q_proj.qweight', 'llm.model.layers.12.self_attn.q_proj.qzeros', 'llm.model.layers.12.self_attn.q_proj.scales', 'llm.model.layers.12.self_attn.v_proj.g_idx', 'llm.model.layers.12.self_attn.v_proj.qweight', 'llm.model.layers.12.self_attn.v_proj.qzeros', 'llm.model.layers.12.self_attn.v_proj.scales', 'llm.model.layers.13.mlp.down_proj.g_idx', 'llm.model.layers.13.mlp.down_proj.qweight', 'llm.model.layers.13.mlp.down_proj.qzeros', 'llm.model.layers.13.mlp.down_proj.scales', 'llm.model.layers.13.mlp.gate_proj.g_idx', 'llm.model.layers.13.mlp.gate_proj.qweight', 'llm.model.layers.13.mlp.gate_proj.qzeros', 'llm.model.layers.13.mlp.gate_proj.scales', 'llm.model.layers.13.mlp.up_proj.g_idx', 'llm.model.layers.13.mlp.up_proj.qweight', 'llm.model.layers.13.mlp.up_proj.qzeros', 'llm.model.layers.13.mlp.up_proj.scales', 'llm.model.layers.13.self_attn.k_proj.g_idx', 'llm.model.layers.13.self_attn.k_proj.qweight', 'llm.model.layers.13.self_attn.k_proj.qzeros', 'llm.model.layers.13.self_attn.k_proj.scales', 'llm.model.layers.13.self_attn.o_proj.g_idx', 'llm.model.layers.13.self_attn.o_proj.qweight', 'llm.model.layers.13.self_attn.o_proj.qzeros', 'llm.model.layers.13.self_attn.o_proj.scales', 'llm.model.layers.13.self_attn.q_proj.g_idx', 'llm.model.layers.13.self_attn.q_proj.qweight', 'llm.model.layers.13.self_attn.q_proj.qzeros', 'llm.model.layers.13.self_attn.q_proj.scales', 'llm.model.layers.13.self_attn.v_proj.g_idx', 'llm.model.layers.13.self_attn.v_proj.qweight', 'llm.model.layers.13.self_attn.v_proj.qzeros', 'llm.model.layers.13.self_attn.v_proj.scales', 'llm.model.layers.14.mlp.down_proj.g_idx', 'llm.model.layers.14.mlp.down_proj.qweight', 'llm.model.layers.14.mlp.down_proj.qzeros', 'llm.model.layers.14.mlp.down_proj.scales', 'llm.model.layers.14.mlp.gate_proj.g_idx', 'llm.model.layers.14.mlp.gate_proj.qweight', 'llm.model.layers.14.mlp.gate_proj.qzeros', 'llm.model.layers.14.mlp.gate_proj.scales', 'llm.model.layers.14.mlp.up_proj.g_idx', 'llm.model.layers.14.mlp.up_proj.qweight', 'llm.model.layers.14.mlp.up_proj.qzeros', 'llm.model.layers.14.mlp.up_proj.scales', 'llm.model.layers.14.self_attn.k_proj.g_idx', 'llm.model.layers.14.self_attn.k_proj.qweight', 'llm.model.layers.14.self_attn.k_proj.qzeros', 'llm.model.layers.14.self_attn.k_proj.scales', 'llm.model.layers.14.self_attn.o_proj.g_idx', 'llm.model.layers.14.self_attn.o_proj.qweight', 'llm.model.layers.14.self_attn.o_proj.qzeros', 'llm.model.layers.14.self_attn.o_proj.scales', 'llm.model.layers.14.self_attn.q_proj.g_idx', 'llm.model.layers.14.self_attn.q_proj.qweight', 'llm.model.layers.14.self_attn.q_proj.qzeros', 'llm.model.layers.14.self_attn.q_proj.scales', 'llm.model.layers.14.self_attn.v_proj.g_idx', 'llm.model.layers.14.self_attn.v_proj.qweight', 'llm.model.layers.14.self_attn.v_proj.qzeros', 'llm.model.layers.14.self_attn.v_proj.scales', 'llm.model.layers.15.mlp.down_proj.g_idx', 'llm.model.layers.15.mlp.down_proj.qweight', 'llm.model.layers.15.mlp.down_proj.qzeros', 'llm.model.layers.15.mlp.down_proj.scales', 'llm.model.layers.15.mlp.gate_proj.g_idx', 'llm.model.layers.15.mlp.gate_proj.qweight', 'llm.model.layers.15.mlp.gate_proj.qzeros', 'llm.model.layers.15.mlp.gate_proj.scales', 'llm.model.layers.15.mlp.up_proj.g_idx', 'llm.model.layers.15.mlp.up_proj.qweight', 'llm.model.layers.15.mlp.up_proj.qzeros', 'llm.model.layers.15.mlp.up_proj.scales', 'llm.model.layers.15.self_attn.k_proj.g_idx', 'llm.model.layers.15.self_attn.k_proj.qweight', 'llm.model.layers.15.self_attn.k_proj.qzeros', 'llm.model.layers.15.self_attn.k_proj.scales', 'llm.model.layers.15.self_attn.o_proj.g_idx', 'llm.model.layers.15.self_attn.o_proj.qweight', 'llm.model.layers.15.self_attn.o_proj.qzeros', 'llm.model.layers.15.self_attn.o_proj.scales', 'llm.model.layers.15.self_attn.q_proj.g_idx', 'llm.model.layers.15.self_attn.q_proj.qweight', 'llm.model.layers.15.self_attn.q_proj.qzeros', 'llm.model.layers.15.self_attn.q_proj.scales', 'llm.model.layers.15.self_attn.v_proj.g_idx', 'llm.model.layers.15.self_attn.v_proj.qweight', 'llm.model.layers.15.self_attn.v_proj.qzeros', 'llm.model.layers.15.self_attn.v_proj.scales', 'llm.model.layers.16.mlp.down_proj.g_idx', 'llm.model.layers.16.mlp.down_proj.qweight', 'llm.model.layers.16.mlp.down_proj.qzeros', 'llm.model.layers.16.mlp.down_proj.scales', 'llm.model.layers.16.mlp.gate_proj.g_idx', 'llm.model.layers.16.mlp.gate_proj.qweight', 'llm.model.layers.16.mlp.gate_proj.qzeros', 'llm.model.layers.16.mlp.gate_proj.scales', 'llm.model.layers.16.mlp.up_proj.g_idx', 'llm.model.layers.16.mlp.up_proj.qweight', 'llm.model.layers.16.mlp.up_proj.qzeros', 'llm.model.layers.16.mlp.up_proj.scales', 'llm.model.layers.16.self_attn.k_proj.g_idx', 'llm.model.layers.16.self_attn.k_proj.qweight', 'llm.model.layers.16.self_attn.k_proj.qzeros', 'llm.model.layers.16.self_attn.k_proj.scales', 'llm.model.layers.16.self_attn.o_proj.g_idx', 'llm.model.layers.16.self_attn.o_proj.qweight', 'llm.model.layers.16.self_attn.o_proj.qzeros', 'llm.model.layers.16.self_attn.o_proj.scales', 'llm.model.layers.16.self_attn.q_proj.g_idx', 'llm.model.layers.16.self_attn.q_proj.qweight', 'llm.model.layers.16.self_attn.q_proj.qzeros', 'llm.model.layers.16.self_attn.q_proj.scales', 'llm.model.layers.16.self_attn.v_proj.g_idx', 'llm.model.layers.16.self_attn.v_proj.qweight', 'llm.model.layers.16.self_attn.v_proj.qzeros', 'llm.model.layers.16.self_attn.v_proj.scales', 'llm.model.layers.17.mlp.down_proj.g_idx', 'llm.model.layers.17.mlp.down_proj.qweight', 'llm.model.layers.17.mlp.down_proj.qzeros', 'llm.model.layers.17.mlp.down_proj.scales', 'llm.model.layers.17.mlp.gate_proj.g_idx', 'llm.model.layers.17.mlp.gate_proj.qweight', 'llm.model.layers.17.mlp.gate_proj.qzeros', 'llm.model.layers.17.mlp.gate_proj.scales', 'llm.model.layers.17.mlp.up_proj.g_idx', 'llm.model.layers.17.mlp.up_proj.qweight', 'llm.model.layers.17.mlp.up_proj.qzeros', 'llm.model.layers.17.mlp.up_proj.scales', 'llm.model.layers.17.self_attn.k_proj.g_idx', 'llm.model.layers.17.self_attn.k_proj.qweight', 'llm.model.layers.17.self_attn.k_proj.qzeros', 'llm.model.layers.17.self_attn.k_proj.scales', 'llm.model.layers.17.self_attn.o_proj.g_idx', 'llm.model.layers.17.self_attn.o_proj.qweight', 'llm.model.layers.17.self_attn.o_proj.qzeros', 'llm.model.layers.17.self_attn.o_proj.scales', 'llm.model.layers.17.self_attn.q_proj.g_idx', 'llm.model.layers.17.self_attn.q_proj.qweight', 'llm.model.layers.17.self_attn.q_proj.qzeros', 'llm.model.layers.17.self_attn.q_proj.scales', 'llm.model.layers.17.self_attn.v_proj.g_idx', 'llm.model.layers.17.self_attn.v_proj.qweight', 'llm.model.layers.17.self_attn.v_proj.qzeros', 'llm.model.layers.17.self_attn.v_proj.scales', 'llm.model.layers.18.mlp.down_proj.g_idx', 'llm.model.layers.18.mlp.down_proj.qweight', 'llm.model.layers.18.mlp.down_proj.qzeros', 'llm.model.layers.18.mlp.down_proj.scales', 'llm.model.layers.18.mlp.gate_proj.g_idx', 'llm.model.layers.18.mlp.gate_proj.qweight', 'llm.model.layers.18.mlp.gate_proj.qzeros', 'llm.model.layers.18.mlp.gate_proj.scales', 'llm.model.layers.18.mlp.up_proj.g_idx', 'llm.model.layers.18.mlp.up_proj.qweight', 'llm.model.layers.18.mlp.up_proj.qzeros', 'llm.model.layers.18.mlp.up_proj.scales', 'llm.model.layers.18.self_attn.k_proj.g_idx', 'llm.model.layers.18.self_attn.k_proj.qweight', 'llm.model.layers.18.self_attn.k_proj.qzeros', 'llm.model.layers.18.self_attn.k_proj.scales', 'llm.model.layers.18.self_attn.o_proj.g_idx', 'llm.model.layers.18.self_attn.o_proj.qweight', 'llm.model.layers.18.self_attn.o_proj.qzeros', 'llm.model.layers.18.self_attn.o_proj.scales', 'llm.model.layers.18.self_attn.q_proj.g_idx', 'llm.model.layers.18.self_attn.q_proj.qweight', 'llm.model.layers.18.self_attn.q_proj.qzeros', 'llm.model.layers.18.self_attn.q_proj.scales', 'llm.model.layers.18.self_attn.v_proj.g_idx', 'llm.model.layers.18.self_attn.v_proj.qweight', 'llm.model.layers.18.self_attn.v_proj.qzeros', 'llm.model.layers.18.self_attn.v_proj.scales', 'llm.model.layers.19.mlp.down_proj.g_idx', 'llm.model.layers.19.mlp.down_proj.qweight', 'llm.model.layers.19.mlp.down_proj.qzeros', 'llm.model.layers.19.mlp.down_proj.scales', 'llm.model.layers.19.mlp.gate_proj.g_idx', 'llm.model.layers.19.mlp.gate_proj.qweight', 'llm.model.layers.19.mlp.gate_proj.qzeros', 'llm.model.layers.19.mlp.gate_proj.scales', 'llm.model.layers.19.mlp.up_proj.g_idx', 'llm.model.layers.19.mlp.up_proj.qweight', 'llm.model.layers.19.mlp.up_proj.qzeros', 'llm.model.layers.19.mlp.up_proj.scales', 'llm.model.layers.19.self_attn.k_proj.g_idx', 'llm.model.layers.19.self_attn.k_proj.qweight', 'llm.model.layers.19.self_attn.k_proj.qzeros', 'llm.model.layers.19.self_attn.k_proj.scales', 'llm.model.layers.19.self_attn.o_proj.g_idx', 'llm.model.layers.19.self_attn.o_proj.qweight', 'llm.model.layers.19.self_attn.o_proj.qzeros', 'llm.model.layers.19.self_attn.o_proj.scales', 'llm.model.layers.19.self_attn.q_proj.g_idx', 'llm.model.layers.19.self_attn.q_proj.qweight', 'llm.model.layers.19.self_attn.q_proj.qzeros', 'llm.model.layers.19.self_attn.q_proj.scales', 'llm.model.layers.19.self_attn.v_proj.g_idx', 'llm.model.layers.19.self_attn.v_proj.qweight', 'llm.model.layers.19.self_attn.v_proj.qzeros', 'llm.model.layers.19.self_attn.v_proj.scales', 'llm.model.layers.2.mlp.down_proj.g_idx', 'llm.model.layers.2.mlp.down_proj.qweight', 'llm.model.layers.2.mlp.down_proj.qzeros', 'llm.model.layers.2.mlp.down_proj.scales', 'llm.model.layers.2.mlp.gate_proj.g_idx', 'llm.model.layers.2.mlp.gate_proj.qweight', 'llm.model.layers.2.mlp.gate_proj.qzeros', 'llm.model.layers.2.mlp.gate_proj.scales', 'llm.model.layers.2.mlp.up_proj.g_idx', 'llm.model.layers.2.mlp.up_proj.qweight', 'llm.model.layers.2.mlp.up_proj.qzeros', 'llm.model.layers.2.mlp.up_proj.scales', 'llm.model.layers.2.self_attn.k_proj.g_idx', 'llm.model.layers.2.self_attn.k_proj.qweight', 'llm.model.layers.2.self_attn.k_proj.qzeros', 'llm.model.layers.2.self_attn.k_proj.scales', 'llm.model.layers.2.self_attn.o_proj.g_idx', 'llm.model.layers.2.self_attn.o_proj.qweight', 'llm.model.layers.2.self_attn.o_proj.qzeros', 'llm.model.layers.2.self_attn.o_proj.scales', 'llm.model.layers.2.self_attn.q_proj.g_idx', 'llm.model.layers.2.self_attn.q_proj.qweight', 'llm.model.layers.2.self_attn.q_proj.qzeros', 'llm.model.layers.2.self_attn.q_proj.scales', 'llm.model.layers.2.self_attn.v_proj.g_idx', 'llm.model.layers.2.self_attn.v_proj.qweight', 'llm.model.layers.2.self_attn.v_proj.qzeros', 'llm.model.layers.2.self_attn.v_proj.scales', 'llm.model.layers.20.mlp.down_proj.g_idx', 'llm.model.layers.20.mlp.down_proj.qweight', 'llm.model.layers.20.mlp.down_proj.qzeros', 'llm.model.layers.20.mlp.down_proj.scales', 'llm.model.layers.20.mlp.gate_proj.g_idx', 'llm.model.layers.20.mlp.gate_proj.qweight', 'llm.model.layers.20.mlp.gate_proj.qzeros', 'llm.model.layers.20.mlp.gate_proj.scales', 'llm.model.layers.20.mlp.up_proj.g_idx', 'llm.model.layers.20.mlp.up_proj.qweight', 'llm.model.layers.20.mlp.up_proj.qzeros', 'llm.model.layers.20.mlp.up_proj.scales', 'llm.model.layers.20.self_attn.k_proj.g_idx', 'llm.model.layers.20.self_attn.k_proj.qweight', 'llm.model.layers.20.self_attn.k_proj.qzeros', 'llm.model.layers.20.self_attn.k_proj.scales', 'llm.model.layers.20.self_attn.o_proj.g_idx', 'llm.model.layers.20.self_attn.o_proj.qweight', 'llm.model.layers.20.self_attn.o_proj.qzeros', 'llm.model.layers.20.self_attn.o_proj.scales', 'llm.model.layers.20.self_attn.q_proj.g_idx', 'llm.model.layers.20.self_attn.q_proj.qweight', 'llm.model.layers.20.self_attn.q_proj.qzeros', 'llm.model.layers.20.self_attn.q_proj.scales', 'llm.model.layers.20.self_attn.v_proj.g_idx', 'llm.model.layers.20.self_attn.v_proj.qweight', 'llm.model.layers.20.self_attn.v_proj.qzeros', 'llm.model.layers.20.self_attn.v_proj.scales', 'llm.model.layers.21.mlp.down_proj.g_idx', 'llm.model.layers.21.mlp.down_proj.qweight', 'llm.model.layers.21.mlp.down_proj.qzeros', 'llm.model.layers.21.mlp.down_proj.scales', 'llm.model.layers.21.mlp.gate_proj.g_idx', 'llm.model.layers.21.mlp.gate_proj.qweight', 'llm.model.layers.21.mlp.gate_proj.qzeros', 'llm.model.layers.21.mlp.gate_proj.scales', 'llm.model.layers.21.mlp.up_proj.g_idx', 'llm.model.layers.21.mlp.up_proj.qweight', 'llm.model.layers.21.mlp.up_proj.qzeros', 'llm.model.layers.21.mlp.up_proj.scales', 'llm.model.layers.21.self_attn.k_proj.g_idx', 'llm.model.layers.21.self_attn.k_proj.qweight', 'llm.model.layers.21.self_attn.k_proj.qzeros', 'llm.model.layers.21.self_attn.k_proj.scales', 'llm.model.layers.21.self_attn.o_proj.g_idx', 'llm.model.layers.21.self_attn.o_proj.qweight', 'llm.model.layers.21.self_attn.o_proj.qzeros', 'llm.model.layers.21.self_attn.o_proj.scales', 'llm.model.layers.21.self_attn.q_proj.g_idx', 'llm.model.layers.21.self_attn.q_proj.qweight', 'llm.model.layers.21.self_attn.q_proj.qzeros', 'llm.model.layers.21.self_attn.q_proj.scales', 'llm.model.layers.21.self_attn.v_proj.g_idx', 'llm.model.layers.21.self_attn.v_proj.qweight', 'llm.model.layers.21.self_attn.v_proj.qzeros', 'llm.model.layers.21.self_attn.v_proj.scales', 'llm.model.layers.22.mlp.down_proj.g_idx', 'llm.model.layers.22.mlp.down_proj.qweight', 'llm.model.layers.22.mlp.down_proj.qzeros', 'llm.model.layers.22.mlp.down_proj.scales', 'llm.model.layers.22.mlp.gate_proj.g_idx', 'llm.model.layers.22.mlp.gate_proj.qweight', 'llm.model.layers.22.mlp.gate_proj.qzeros', 'llm.model.layers.22.mlp.gate_proj.scales', 'llm.model.layers.22.mlp.up_proj.g_idx', 'llm.model.layers.22.mlp.up_proj.qweight', 'llm.model.layers.22.mlp.up_proj.qzeros', 'llm.model.layers.22.mlp.up_proj.scales', 'llm.model.layers.22.self_attn.k_proj.g_idx', 'llm.model.layers.22.self_attn.k_proj.qweight', 'llm.model.layers.22.self_attn.k_proj.qzeros', 'llm.model.layers.22.self_attn.k_proj.scales', 'llm.model.layers.22.self_attn.o_proj.g_idx', 'llm.model.layers.22.self_attn.o_proj.qweight', 'llm.model.layers.22.self_attn.o_proj.qzeros', 'llm.model.layers.22.self_attn.o_proj.scales', 'llm.model.layers.22.self_attn.q_proj.g_idx', 'llm.model.layers.22.self_attn.q_proj.qweight', 'llm.model.layers.22.self_attn.q_proj.qzeros', 'llm.model.layers.22.self_attn.q_proj.scales', 'llm.model.layers.22.self_attn.v_proj.g_idx', 'llm.model.layers.22.self_attn.v_proj.qweight', 'llm.model.layers.22.self_attn.v_proj.qzeros', 'llm.model.layers.22.self_attn.v_proj.scales', 'llm.model.layers.23.mlp.down_proj.g_idx', 'llm.model.layers.23.mlp.down_proj.qweight', 'llm.model.layers.23.mlp.down_proj.qzeros', 'llm.model.layers.23.mlp.down_proj.scales', 'llm.model.layers.23.mlp.gate_proj.g_idx', 'llm.model.layers.23.mlp.gate_proj.qweight', 'llm.model.layers.23.mlp.gate_proj.qzeros', 'llm.model.layers.23.mlp.gate_proj.scales', 'llm.model.layers.23.mlp.up_proj.g_idx', 'llm.model.layers.23.mlp.up_proj.qweight', 'llm.model.layers.23.mlp.up_proj.qzeros', 'llm.model.layers.23.mlp.up_proj.scales', 'llm.model.layers.23.self_attn.k_proj.g_idx', 'llm.model.layers.23.self_attn.k_proj.qweight', 'llm.model.layers.23.self_attn.k_proj.qzeros', 'llm.model.layers.23.self_attn.k_proj.scales', 'llm.model.layers.23.self_attn.o_proj.g_idx', 'llm.model.layers.23.self_attn.o_proj.qweight', 'llm.model.layers.23.self_attn.o_proj.qzeros', 'llm.model.layers.23.self_attn.o_proj.scales', 'llm.model.layers.23.self_attn.q_proj.g_idx', 'llm.model.layers.23.self_attn.q_proj.qweight', 'llm.model.layers.23.self_attn.q_proj.qzeros', 'llm.model.layers.23.self_attn.q_proj.scales', 'llm.model.layers.23.self_attn.v_proj.g_idx', 'llm.model.layers.23.self_attn.v_proj.qweight', 'llm.model.layers.23.self_attn.v_proj.qzeros', 'llm.model.layers.23.self_attn.v_proj.scales', 'llm.model.layers.24.mlp.down_proj.g_idx', 'llm.model.layers.24.mlp.down_proj.qweight', 'llm.model.layers.24.mlp.down_proj.qzeros', 'llm.model.layers.24.mlp.down_proj.scales', 'llm.model.layers.24.mlp.gate_proj.g_idx', 'llm.model.layers.24.mlp.gate_proj.qweight', 'llm.model.layers.24.mlp.gate_proj.qzeros', 'llm.model.layers.24.mlp.gate_proj.scales', 'llm.model.layers.24.mlp.up_proj.g_idx', 'llm.model.layers.24.mlp.up_proj.qweight', 'llm.model.layers.24.mlp.up_proj.qzeros', 'llm.model.layers.24.mlp.up_proj.scales', 'llm.model.layers.24.self_attn.k_proj.g_idx', 'llm.model.layers.24.self_attn.k_proj.qweight', 'llm.model.layers.24.self_attn.k_proj.qzeros', 'llm.model.layers.24.self_attn.k_proj.scales', 'llm.model.layers.24.self_attn.o_proj.g_idx', 'llm.model.layers.24.self_attn.o_proj.qweight', 'llm.model.layers.24.self_attn.o_proj.qzeros', 'llm.model.layers.24.self_attn.o_proj.scales', 'llm.model.layers.24.self_attn.q_proj.g_idx', 'llm.model.layers.24.self_attn.q_proj.qweight', 'llm.model.layers.24.self_attn.q_proj.qzeros', 'llm.model.layers.24.self_attn.q_proj.scales', 'llm.model.layers.24.self_attn.v_proj.g_idx', 'llm.model.layers.24.self_attn.v_proj.qweight', 'llm.model.layers.24.self_attn.v_proj.qzeros', 'llm.model.layers.24.self_attn.v_proj.scales', 'llm.model.layers.25.mlp.down_proj.g_idx', 'llm.model.layers.25.mlp.down_proj.qweight', 'llm.model.layers.25.mlp.down_proj.qzeros', 'llm.model.layers.25.mlp.down_proj.scales', 'llm.model.layers.25.mlp.gate_proj.g_idx', 'llm.model.layers.25.mlp.gate_proj.qweight', 'llm.model.layers.25.mlp.gate_proj.qzeros', 'llm.model.layers.25.mlp.gate_proj.scales', 'llm.model.layers.25.mlp.up_proj.g_idx', 'llm.model.layers.25.mlp.up_proj.qweight', 'llm.model.layers.25.mlp.up_proj.qzeros', 'llm.model.layers.25.mlp.up_proj.scales', 'llm.model.layers.25.self_attn.k_proj.g_idx', 'llm.model.layers.25.self_attn.k_proj.qweight', 'llm.model.layers.25.self_attn.k_proj.qzeros', 'llm.model.layers.25.self_attn.k_proj.scales', 'llm.model.layers.25.self_attn.o_proj.g_idx', 'llm.model.layers.25.self_attn.o_proj.qweight', 'llm.model.layers.25.self_attn.o_proj.qzeros', 'llm.model.layers.25.self_attn.o_proj.scales', 'llm.model.layers.25.self_attn.q_proj.g_idx', 'llm.model.layers.25.self_attn.q_proj.qweight', 'llm.model.layers.25.self_attn.q_proj.qzeros', 'llm.model.layers.25.self_attn.q_proj.scales', 'llm.model.layers.25.self_attn.v_proj.g_idx', 'llm.model.layers.25.self_attn.v_proj.qweight', 'llm.model.layers.25.self_attn.v_proj.qzeros', 'llm.model.layers.25.self_attn.v_proj.scales', 'llm.model.layers.26.mlp.down_proj.g_idx', 'llm.model.layers.26.mlp.down_proj.qweight', 'llm.model.layers.26.mlp.down_proj.qzeros', 'llm.model.layers.26.mlp.down_proj.scales', 'llm.model.layers.26.mlp.gate_proj.g_idx', 'llm.model.layers.26.mlp.gate_proj.qweight', 'llm.model.layers.26.mlp.gate_proj.qzeros', 'llm.model.layers.26.mlp.gate_proj.scales', 'llm.model.layers.26.mlp.up_proj.g_idx', 'llm.model.layers.26.mlp.up_proj.qweight', 'llm.model.layers.26.mlp.up_proj.qzeros', 'llm.model.layers.26.mlp.up_proj.scales', 'llm.model.layers.26.self_attn.k_proj.g_idx', 'llm.model.layers.26.self_attn.k_proj.qweight', 'llm.model.layers.26.self_attn.k_proj.qzeros', 'llm.model.layers.26.self_attn.k_proj.scales', 'llm.model.layers.26.self_attn.o_proj.g_idx', 'llm.model.layers.26.self_attn.o_proj.qweight', 'llm.model.layers.26.self_attn.o_proj.qzeros', 'llm.model.layers.26.self_attn.o_proj.scales', 'llm.model.layers.26.self_attn.q_proj.g_idx', 'llm.model.layers.26.self_attn.q_proj.qweight', 'llm.model.layers.26.self_attn.q_proj.qzeros', 'llm.model.layers.26.self_attn.q_proj.scales', 'llm.model.layers.26.self_attn.v_proj.g_idx', 'llm.model.layers.26.self_attn.v_proj.qweight', 'llm.model.layers.26.self_attn.v_proj.qzeros', 'llm.model.layers.26.self_attn.v_proj.scales', 'llm.model.layers.27.mlp.down_proj.g_idx', 'llm.model.layers.27.mlp.down_proj.qweight', 'llm.model.layers.27.mlp.down_proj.qzeros', 'llm.model.layers.27.mlp.down_proj.scales', 'llm.model.layers.27.mlp.gate_proj.g_idx', 'llm.model.layers.27.mlp.gate_proj.qweight', 'llm.model.layers.27.mlp.gate_proj.qzeros', 'llm.model.layers.27.mlp.gate_proj.scales', 'llm.model.layers.27.mlp.up_proj.g_idx', 'llm.model.layers.27.mlp.up_proj.qweight', 'llm.model.layers.27.mlp.up_proj.qzeros', 'llm.model.layers.27.mlp.up_proj.scales', 'llm.model.layers.27.self_attn.k_proj.g_idx', 'llm.model.layers.27.self_attn.k_proj.qweight', 'llm.model.layers.27.self_attn.k_proj.qzeros', 'llm.model.layers.27.self_attn.k_proj.scales', 'llm.model.layers.27.self_attn.o_proj.g_idx', 'llm.model.layers.27.self_attn.o_proj.qweight', 'llm.model.layers.27.self_attn.o_proj.qzeros', 'llm.model.layers.27.self_attn.o_proj.scales', 'llm.model.layers.27.self_attn.q_proj.g_idx', 'llm.model.layers.27.self_attn.q_proj.qweight', 'llm.model.layers.27.self_attn.q_proj.qzeros', 'llm.model.layers.27.self_attn.q_proj.scales', 'llm.model.layers.27.self_attn.v_proj.g_idx', 'llm.model.layers.27.self_attn.v_proj.qweight', 'llm.model.layers.27.self_attn.v_proj.qzeros', 'llm.model.layers.27.self_attn.v_proj.scales', 'llm.model.layers.28.mlp.down_proj.g_idx', 'llm.model.layers.28.mlp.down_proj.qweight', 'llm.model.layers.28.mlp.down_proj.qzeros', 'llm.model.layers.28.mlp.down_proj.scales', 'llm.model.layers.28.mlp.gate_proj.g_idx', 'llm.model.layers.28.mlp.gate_proj.qweight', 'llm.model.layers.28.mlp.gate_proj.qzeros', 'llm.model.layers.28.mlp.gate_proj.scales', 'llm.model.layers.28.mlp.up_proj.g_idx', 'llm.model.layers.28.mlp.up_proj.qweight', 'llm.model.layers.28.mlp.up_proj.qzeros', 'llm.model.layers.28.mlp.up_proj.scales', 'llm.model.layers.28.self_attn.k_proj.g_idx', 'llm.model.layers.28.self_attn.k_proj.qweight', 'llm.model.layers.28.self_attn.k_proj.qzeros', 'llm.model.layers.28.self_attn.k_proj.scales', 'llm.model.layers.28.self_attn.o_proj.g_idx', 'llm.model.layers.28.self_attn.o_proj.qweight', 'llm.model.layers.28.self_attn.o_proj.qzeros', 'llm.model.layers.28.self_attn.o_proj.scales', 'llm.model.layers.28.self_attn.q_proj.g_idx', 'llm.model.layers.28.self_attn.q_proj.qweight', 'llm.model.layers.28.self_attn.q_proj.qzeros', 'llm.model.layers.28.self_attn.q_proj.scales', 'llm.model.layers.28.self_attn.v_proj.g_idx', 'llm.model.layers.28.self_attn.v_proj.qweight', 'llm.model.layers.28.self_attn.v_proj.qzeros', 'llm.model.layers.28.self_attn.v_proj.scales', 'llm.model.layers.29.mlp.down_proj.g_idx', 'llm.model.layers.29.mlp.down_proj.qweight', 'llm.model.layers.29.mlp.down_proj.qzeros', 'llm.model.layers.29.mlp.down_proj.scales', 'llm.model.layers.29.mlp.gate_proj.g_idx', 'llm.model.layers.29.mlp.gate_proj.qweight', 'llm.model.layers.29.mlp.gate_proj.qzeros', 'llm.model.layers.29.mlp.gate_proj.scales', 'llm.model.layers.29.mlp.up_proj.g_idx', 'llm.model.layers.29.mlp.up_proj.qweight', 'llm.model.layers.29.mlp.up_proj.qzeros', 'llm.model.layers.29.mlp.up_proj.scales', 'llm.model.layers.29.self_attn.k_proj.g_idx', 'llm.model.layers.29.self_attn.k_proj.qweight', 'llm.model.layers.29.self_attn.k_proj.qzeros', 'llm.model.layers.29.self_attn.k_proj.scales', 'llm.model.layers.29.self_attn.o_proj.g_idx', 'llm.model.layers.29.self_attn.o_proj.qweight', 'llm.model.layers.29.self_attn.o_proj.qzeros', 'llm.model.layers.29.self_attn.o_proj.scales', 'llm.model.layers.29.self_attn.q_proj.g_idx', 'llm.model.layers.29.self_attn.q_proj.qweight', 'llm.model.layers.29.self_attn.q_proj.qzeros', 'llm.model.layers.29.self_attn.q_proj.scales', 'llm.model.layers.29.self_attn.v_proj.g_idx', 'llm.model.layers.29.self_attn.v_proj.qweight', 'llm.model.layers.29.self_attn.v_proj.qzeros', 'llm.model.layers.29.self_attn.v_proj.scales', 'llm.model.layers.3.mlp.down_proj.g_idx', 'llm.model.layers.3.mlp.down_proj.qweight', 'llm.model.layers.3.mlp.down_proj.qzeros', 'llm.model.layers.3.mlp.down_proj.scales', 'llm.model.layers.3.mlp.gate_proj.g_idx', 'llm.model.layers.3.mlp.gate_proj.qweight', 'llm.model.layers.3.mlp.gate_proj.qzeros', 'llm.model.layers.3.mlp.gate_proj.scales', 'llm.model.layers.3.mlp.up_proj.g_idx', 'llm.model.layers.3.mlp.up_proj.qweight', 'llm.model.layers.3.mlp.up_proj.qzeros', 'llm.model.layers.3.mlp.up_proj.scales', 'llm.model.layers.3.self_attn.k_proj.g_idx', 'llm.model.layers.3.self_attn.k_proj.qweight', 'llm.model.layers.3.self_attn.k_proj.qzeros', 'llm.model.layers.3.self_attn.k_proj.scales', 'llm.model.layers.3.self_attn.o_proj.g_idx', 'llm.model.layers.3.self_attn.o_proj.qweight', 'llm.model.layers.3.self_attn.o_proj.qzeros', 'llm.model.layers.3.self_attn.o_proj.scales', 'llm.model.layers.3.self_attn.q_proj.g_idx', 'llm.model.layers.3.self_attn.q_proj.qweight', 'llm.model.layers.3.self_attn.q_proj.qzeros', 'llm.model.layers.3.self_attn.q_proj.scales', 'llm.model.layers.3.self_attn.v_proj.g_idx', 'llm.model.layers.3.self_attn.v_proj.qweight', 'llm.model.layers.3.self_attn.v_proj.qzeros', 'llm.model.layers.3.self_attn.v_proj.scales', 'llm.model.layers.30.mlp.down_proj.g_idx', 'llm.model.layers.30.mlp.down_proj.qweight', 'llm.model.layers.30.mlp.down_proj.qzeros', 'llm.model.layers.30.mlp.down_proj.scales', 'llm.model.layers.30.mlp.gate_proj.g_idx', 'llm.model.layers.30.mlp.gate_proj.qweight', 'llm.model.layers.30.mlp.gate_proj.qzeros', 'llm.model.layers.30.mlp.gate_proj.scales', 'llm.model.layers.30.mlp.up_proj.g_idx', 'llm.model.layers.30.mlp.up_proj.qweight', 'llm.model.layers.30.mlp.up_proj.qzeros', 'llm.model.layers.30.mlp.up_proj.scales', 'llm.model.layers.30.self_attn.k_proj.g_idx', 'llm.model.layers.30.self_attn.k_proj.qweight', 'llm.model.layers.30.self_attn.k_proj.qzeros', 'llm.model.layers.30.self_attn.k_proj.scales', 'llm.model.layers.30.self_attn.o_proj.g_idx', 'llm.model.layers.30.self_attn.o_proj.qweight', 'llm.model.layers.30.self_attn.o_proj.qzeros', 'llm.model.layers.30.self_attn.o_proj.scales', 'llm.model.layers.30.self_attn.q_proj.g_idx', 'llm.model.layers.30.self_attn.q_proj.qweight', 'llm.model.layers.30.self_attn.q_proj.qzeros', 'llm.model.layers.30.self_attn.q_proj.scales', 'llm.model.layers.30.self_attn.v_proj.g_idx', 'llm.model.layers.30.self_attn.v_proj.qweight', 'llm.model.layers.30.self_attn.v_proj.qzeros', 'llm.model.layers.30.self_attn.v_proj.scales', 'llm.model.layers.31.mlp.down_proj.g_idx', 'llm.model.layers.31.mlp.down_proj.qweight', 'llm.model.layers.31.mlp.down_proj.qzeros', 'llm.model.layers.31.mlp.down_proj.scales', 'llm.model.layers.31.mlp.gate_proj.g_idx', 'llm.model.layers.31.mlp.gate_proj.qweight', 'llm.model.layers.31.mlp.gate_proj.qzeros', 'llm.model.layers.31.mlp.gate_proj.scales', 'llm.model.layers.31.mlp.up_proj.g_idx', 'llm.model.layers.31.mlp.up_proj.qweight', 'llm.model.layers.31.mlp.up_proj.qzeros', 'llm.model.layers.31.mlp.up_proj.scales', 'llm.model.layers.31.self_attn.k_proj.g_idx', 'llm.model.layers.31.self_attn.k_proj.qweight', 'llm.model.layers.31.self_attn.k_proj.qzeros', 'llm.model.layers.31.self_attn.k_proj.scales', 'llm.model.layers.31.self_attn.o_proj.g_idx', 'llm.model.layers.31.self_attn.o_proj.qweight', 'llm.model.layers.31.self_attn.o_proj.qzeros', 'llm.model.layers.31.self_attn.o_proj.scales', 'llm.model.layers.31.self_attn.q_proj.g_idx', 'llm.model.layers.31.self_attn.q_proj.qweight', 'llm.model.layers.31.self_attn.q_proj.qzeros', 'llm.model.layers.31.self_attn.q_proj.scales', 'llm.model.layers.31.self_attn.v_proj.g_idx', 'llm.model.layers.31.self_attn.v_proj.qweight', 'llm.model.layers.31.self_attn.v_proj.qzeros', 'llm.model.layers.31.self_attn.v_proj.scales', 'llm.model.layers.32.mlp.down_proj.g_idx', 'llm.model.layers.32.mlp.down_proj.qweight', 'llm.model.layers.32.mlp.down_proj.qzeros', 'llm.model.layers.32.mlp.down_proj.scales', 'llm.model.layers.32.mlp.gate_proj.g_idx', 'llm.model.layers.32.mlp.gate_proj.qweight', 'llm.model.layers.32.mlp.gate_proj.qzeros', 'llm.model.layers.32.mlp.gate_proj.scales', 'llm.model.layers.32.mlp.up_proj.g_idx', 'llm.model.layers.32.mlp.up_proj.qweight', 'llm.model.layers.32.mlp.up_proj.qzeros', 'llm.model.layers.32.mlp.up_proj.scales', 'llm.model.layers.32.self_attn.k_proj.g_idx', 'llm.model.layers.32.self_attn.k_proj.qweight', 'llm.model.layers.32.self_attn.k_proj.qzeros', 'llm.model.layers.32.self_attn.k_proj.scales', 'llm.model.layers.32.self_attn.o_proj.g_idx', 'llm.model.layers.32.self_attn.o_proj.qweight', 'llm.model.layers.32.self_attn.o_proj.qzeros', 'llm.model.layers.32.self_attn.o_proj.scales', 'llm.model.layers.32.self_attn.q_proj.g_idx', 'llm.model.layers.32.self_attn.q_proj.qweight', 'llm.model.layers.32.self_attn.q_proj.qzeros', 'llm.model.layers.32.self_attn.q_proj.scales', 'llm.model.layers.32.self_attn.v_proj.g_idx', 'llm.model.layers.32.self_attn.v_proj.qweight', 'llm.model.layers.32.self_attn.v_proj.qzeros', 'llm.model.layers.32.self_attn.v_proj.scales', 'llm.model.layers.33.mlp.down_proj.g_idx', 'llm.model.layers.33.mlp.down_proj.qweight', 'llm.model.layers.33.mlp.down_proj.qzeros', 'llm.model.layers.33.mlp.down_proj.scales', 'llm.model.layers.33.mlp.gate_proj.g_idx', 'llm.model.layers.33.mlp.gate_proj.qweight', 'llm.model.layers.33.mlp.gate_proj.qzeros', 'llm.model.layers.33.mlp.gate_proj.scales', 'llm.model.layers.33.mlp.up_proj.g_idx', 'llm.model.layers.33.mlp.up_proj.qweight', 'llm.model.layers.33.mlp.up_proj.qzeros', 'llm.model.layers.33.mlp.up_proj.scales', 'llm.model.layers.33.self_attn.k_proj.g_idx', 'llm.model.layers.33.self_attn.k_proj.qweight', 'llm.model.layers.33.self_attn.k_proj.qzeros', 'llm.model.layers.33.self_attn.k_proj.scales', 'llm.model.layers.33.self_attn.o_proj.g_idx', 'llm.model.layers.33.self_attn.o_proj.qweight', 'llm.model.layers.33.self_attn.o_proj.qzeros', 'llm.model.layers.33.self_attn.o_proj.scales', 'llm.model.layers.33.self_attn.q_proj.g_idx', 'llm.model.layers.33.self_attn.q_proj.qweight', 'llm.model.layers.33.self_attn.q_proj.qzeros', 'llm.model.layers.33.self_attn.q_proj.scales', 'llm.model.layers.33.self_attn.v_proj.g_idx', 'llm.model.layers.33.self_attn.v_proj.qweight', 'llm.model.layers.33.self_attn.v_proj.qzeros', 'llm.model.layers.33.self_attn.v_proj.scales', 'llm.model.layers.34.mlp.down_proj.g_idx', 'llm.model.layers.34.mlp.down_proj.qweight', 'llm.model.layers.34.mlp.down_proj.qzeros', 'llm.model.layers.34.mlp.down_proj.scales', 'llm.model.layers.34.mlp.gate_proj.g_idx', 'llm.model.layers.34.mlp.gate_proj.qweight', 'llm.model.layers.34.mlp.gate_proj.qzeros', 'llm.model.layers.34.mlp.gate_proj.scales', 'llm.model.layers.34.mlp.up_proj.g_idx', 'llm.model.layers.34.mlp.up_proj.qweight', 'llm.model.layers.34.mlp.up_proj.qzeros', 'llm.model.layers.34.mlp.up_proj.scales', 'llm.model.layers.34.self_attn.k_proj.g_idx', 'llm.model.layers.34.self_attn.k_proj.qweight', 'llm.model.layers.34.self_attn.k_proj.qzeros', 'llm.model.layers.34.self_attn.k_proj.scales', 'llm.model.layers.34.self_attn.o_proj.g_idx', 'llm.model.layers.34.self_attn.o_proj.qweight', 'llm.model.layers.34.self_attn.o_proj.qzeros', 'llm.model.layers.34.self_attn.o_proj.scales', 'llm.model.layers.34.self_attn.q_proj.g_idx', 'llm.model.layers.34.self_attn.q_proj.qweight', 'llm.model.layers.34.self_attn.q_proj.qzeros', 'llm.model.layers.34.self_attn.q_proj.scales', 'llm.model.layers.34.self_attn.v_proj.g_idx', 'llm.model.layers.34.self_attn.v_proj.qweight', 'llm.model.layers.34.self_attn.v_proj.qzeros', 'llm.model.layers.34.self_attn.v_proj.scales', 'llm.model.layers.35.mlp.down_proj.g_idx', 'llm.model.layers.35.mlp.down_proj.qweight', 'llm.model.layers.35.mlp.down_proj.qzeros', 'llm.model.layers.35.mlp.down_proj.scales', 'llm.model.layers.35.mlp.gate_proj.g_idx', 'llm.model.layers.35.mlp.gate_proj.qweight', 'llm.model.layers.35.mlp.gate_proj.qzeros', 'llm.model.layers.35.mlp.gate_proj.scales', 'llm.model.layers.35.mlp.up_proj.g_idx', 'llm.model.layers.35.mlp.up_proj.qweight', 'llm.model.layers.35.mlp.up_proj.qzeros', 'llm.model.layers.35.mlp.up_proj.scales', 'llm.model.layers.35.self_attn.k_proj.g_idx', 'llm.model.layers.35.self_attn.k_proj.qweight', 'llm.model.layers.35.self_attn.k_proj.qzeros', 'llm.model.layers.35.self_attn.k_proj.scales', 'llm.model.layers.35.self_attn.o_proj.g_idx', 'llm.model.layers.35.self_attn.o_proj.qweight', 'llm.model.layers.35.self_attn.o_proj.qzeros', 'llm.model.layers.35.self_attn.o_proj.scales', 'llm.model.layers.35.self_attn.q_proj.g_idx', 'llm.model.layers.35.self_attn.q_proj.qweight', 'llm.model.layers.35.self_attn.q_proj.qzeros', 'llm.model.layers.35.self_attn.q_proj.scales', 'llm.model.layers.35.self_attn.v_proj.g_idx', 'llm.model.layers.35.self_attn.v_proj.qweight', 'llm.model.layers.35.self_attn.v_proj.qzeros', 'llm.model.layers.35.self_attn.v_proj.scales', 'llm.model.layers.36.mlp.down_proj.g_idx', 'llm.model.layers.36.mlp.down_proj.qweight', 'llm.model.layers.36.mlp.down_proj.qzeros', 'llm.model.layers.36.mlp.down_proj.scales', 'llm.model.layers.36.mlp.gate_proj.g_idx', 'llm.model.layers.36.mlp.gate_proj.qweight', 'llm.model.layers.36.mlp.gate_proj.qzeros', 'llm.model.layers.36.mlp.gate_proj.scales', 'llm.model.layers.36.mlp.up_proj.g_idx', 'llm.model.layers.36.mlp.up_proj.qweight', 'llm.model.layers.36.mlp.up_proj.qzeros', 'llm.model.layers.36.mlp.up_proj.scales', 'llm.model.layers.36.self_attn.k_proj.g_idx', 'llm.model.layers.36.self_attn.k_proj.qweight', 'llm.model.layers.36.self_attn.k_proj.qzeros', 'llm.model.layers.36.self_attn.k_proj.scales', 'llm.model.layers.36.self_attn.o_proj.g_idx', 'llm.model.layers.36.self_attn.o_proj.qweight', 'llm.model.layers.36.self_attn.o_proj.qzeros', 'llm.model.layers.36.self_attn.o_proj.scales', 'llm.model.layers.36.self_attn.q_proj.g_idx', 'llm.model.layers.36.self_attn.q_proj.qweight', 'llm.model.layers.36.self_attn.q_proj.qzeros', 'llm.model.layers.36.self_attn.q_proj.scales', 'llm.model.layers.36.self_attn.v_proj.g_idx', 'llm.model.layers.36.self_attn.v_proj.qweight', 'llm.model.layers.36.self_attn.v_proj.qzeros', 'llm.model.layers.36.self_attn.v_proj.scales', 'llm.model.layers.37.mlp.down_proj.g_idx', 'llm.model.layers.37.mlp.down_proj.qweight', 'llm.model.layers.37.mlp.down_proj.qzeros', 'llm.model.layers.37.mlp.down_proj.scales', 'llm.model.layers.37.mlp.gate_proj.g_idx', 'llm.model.layers.37.mlp.gate_proj.qweight', 'llm.model.layers.37.mlp.gate_proj.qzeros', 'llm.model.layers.37.mlp.gate_proj.scales', 'llm.model.layers.37.mlp.up_proj.g_idx', 'llm.model.layers.37.mlp.up_proj.qweight', 'llm.model.layers.37.mlp.up_proj.qzeros', 'llm.model.layers.37.mlp.up_proj.scales', 'llm.model.layers.37.self_attn.k_proj.g_idx', 'llm.model.layers.37.self_attn.k_proj.qweight', 'llm.model.layers.37.self_attn.k_proj.qzeros', 'llm.model.layers.37.self_attn.k_proj.scales', 'llm.model.layers.37.self_attn.o_proj.g_idx', 'llm.model.layers.37.self_attn.o_proj.qweight', 'llm.model.layers.37.self_attn.o_proj.qzeros', 'llm.model.layers.37.self_attn.o_proj.scales', 'llm.model.layers.37.self_attn.q_proj.g_idx', 'llm.model.layers.37.self_attn.q_proj.qweight', 'llm.model.layers.37.self_attn.q_proj.qzeros', 'llm.model.layers.37.self_attn.q_proj.scales', 'llm.model.layers.37.self_attn.v_proj.g_idx', 'llm.model.layers.37.self_attn.v_proj.qweight', 'llm.model.layers.37.self_attn.v_proj.qzeros', 'llm.model.layers.37.self_attn.v_proj.scales', 'llm.model.layers.38.mlp.down_proj.g_idx', 'llm.model.layers.38.mlp.down_proj.qweight', 'llm.model.layers.38.mlp.down_proj.qzeros', 'llm.model.layers.38.mlp.down_proj.scales', 'llm.model.layers.38.mlp.gate_proj.g_idx', 'llm.model.layers.38.mlp.gate_proj.qweight', 'llm.model.layers.38.mlp.gate_proj.qzeros', 'llm.model.layers.38.mlp.gate_proj.scales', 'llm.model.layers.38.mlp.up_proj.g_idx', 'llm.model.layers.38.mlp.up_proj.qweight', 'llm.model.layers.38.mlp.up_proj.qzeros', 'llm.model.layers.38.mlp.up_proj.scales', 'llm.model.layers.38.self_attn.k_proj.g_idx', 'llm.model.layers.38.self_attn.k_proj.qweight', 'llm.model.layers.38.self_attn.k_proj.qzeros', 'llm.model.layers.38.self_attn.k_proj.scales', 'llm.model.layers.38.self_attn.o_proj.g_idx', 'llm.model.layers.38.self_attn.o_proj.qweight', 'llm.model.layers.38.self_attn.o_proj.qzeros', 'llm.model.layers.38.self_attn.o_proj.scales', 'llm.model.layers.38.self_attn.q_proj.g_idx', 'llm.model.layers.38.self_attn.q_proj.qweight', 'llm.model.layers.38.self_attn.q_proj.qzeros', 'llm.model.layers.38.self_attn.q_proj.scales', 'llm.model.layers.38.self_attn.v_proj.g_idx', 'llm.model.layers.38.self_attn.v_proj.qweight', 'llm.model.layers.38.self_attn.v_proj.qzeros', 'llm.model.layers.38.self_attn.v_proj.scales', 'llm.model.layers.39.mlp.down_proj.g_idx', 'llm.model.layers.39.mlp.down_proj.qweight', 'llm.model.layers.39.mlp.down_proj.qzeros', 'llm.model.layers.39.mlp.down_proj.scales', 'llm.model.layers.39.mlp.gate_proj.g_idx', 'llm.model.layers.39.mlp.gate_proj.qweight', 'llm.model.layers.39.mlp.gate_proj.qzeros', 'llm.model.layers.39.mlp.gate_proj.scales', 'llm.model.layers.39.mlp.up_proj.g_idx', 'llm.model.layers.39.mlp.up_proj.qweight', 'llm.model.layers.39.mlp.up_proj.qzeros', 'llm.model.layers.39.mlp.up_proj.scales', 'llm.model.layers.39.self_attn.k_proj.g_idx', 'llm.model.layers.39.self_attn.k_proj.qweight', 'llm.model.layers.39.self_attn.k_proj.qzeros', 'llm.model.layers.39.self_attn.k_proj.scales', 'llm.model.layers.39.self_attn.o_proj.g_idx', 'llm.model.layers.39.self_attn.o_proj.qweight', 'llm.model.layers.39.self_attn.o_proj.qzeros', 'llm.model.layers.39.self_attn.o_proj.scales', 'llm.model.layers.39.self_attn.q_proj.g_idx', 'llm.model.layers.39.self_attn.q_proj.qweight', 'llm.model.layers.39.self_attn.q_proj.qzeros', 'llm.model.layers.39.self_attn.q_proj.scales', 'llm.model.layers.39.self_attn.v_proj.g_idx', 'llm.model.layers.39.self_attn.v_proj.qweight', 'llm.model.layers.39.self_attn.v_proj.qzeros', 'llm.model.layers.39.self_attn.v_proj.scales', 'llm.model.layers.4.mlp.down_proj.g_idx', 'llm.model.layers.4.mlp.down_proj.qweight', 'llm.model.layers.4.mlp.down_proj.qzeros', 'llm.model.layers.4.mlp.down_proj.scales', 'llm.model.layers.4.mlp.gate_proj.g_idx', 'llm.model.layers.4.mlp.gate_proj.qweight', 'llm.model.layers.4.mlp.gate_proj.qzeros', 'llm.model.layers.4.mlp.gate_proj.scales', 'llm.model.layers.4.mlp.up_proj.g_idx', 'llm.model.layers.4.mlp.up_proj.qweight', 'llm.model.layers.4.mlp.up_proj.qzeros', 'llm.model.layers.4.mlp.up_proj.scales', 'llm.model.layers.4.self_attn.k_proj.g_idx', 'llm.model.layers.4.self_attn.k_proj.qweight', 'llm.model.layers.4.self_attn.k_proj.qzeros', 'llm.model.layers.4.self_attn.k_proj.scales', 'llm.model.layers.4.self_attn.o_proj.g_idx', 'llm.model.layers.4.self_attn.o_proj.qweight', 'llm.model.layers.4.self_attn.o_proj.qzeros', 'llm.model.layers.4.self_attn.o_proj.scales', 'llm.model.layers.4.self_attn.q_proj.g_idx', 'llm.model.layers.4.self_attn.q_proj.qweight', 'llm.model.layers.4.self_attn.q_proj.qzeros', 'llm.model.layers.4.self_attn.q_proj.scales', 'llm.model.layers.4.self_attn.v_proj.g_idx', 'llm.model.layers.4.self_attn.v_proj.qweight', 'llm.model.layers.4.self_attn.v_proj.qzeros', 'llm.model.layers.4.self_attn.v_proj.scales', 'llm.model.layers.40.mlp.down_proj.g_idx', 'llm.model.layers.40.mlp.down_proj.qweight', 'llm.model.layers.40.mlp.down_proj.qzeros', 'llm.model.layers.40.mlp.down_proj.scales', 'llm.model.layers.40.mlp.gate_proj.g_idx', 'llm.model.layers.40.mlp.gate_proj.qweight', 'llm.model.layers.40.mlp.gate_proj.qzeros', 'llm.model.layers.40.mlp.gate_proj.scales', 'llm.model.layers.40.mlp.up_proj.g_idx', 'llm.model.layers.40.mlp.up_proj.qweight', 'llm.model.layers.40.mlp.up_proj.qzeros', 'llm.model.layers.40.mlp.up_proj.scales', 'llm.model.layers.40.self_attn.k_proj.g_idx', 'llm.model.layers.40.self_attn.k_proj.qweight', 'llm.model.layers.40.self_attn.k_proj.qzeros', 'llm.model.layers.40.self_attn.k_proj.scales', 'llm.model.layers.40.self_attn.o_proj.g_idx', 'llm.model.layers.40.self_attn.o_proj.qweight', 'llm.model.layers.40.self_attn.o_proj.qzeros', 'llm.model.layers.40.self_attn.o_proj.scales', 'llm.model.layers.40.self_attn.q_proj.g_idx', 'llm.model.layers.40.self_attn.q_proj.qweight', 'llm.model.layers.40.self_attn.q_proj.qzeros', 'llm.model.layers.40.self_attn.q_proj.scales', 'llm.model.layers.40.self_attn.v_proj.g_idx', 'llm.model.layers.40.self_attn.v_proj.qweight', 'llm.model.layers.40.self_attn.v_proj.qzeros', 'llm.model.layers.40.self_attn.v_proj.scales', 'llm.model.layers.41.mlp.down_proj.g_idx', 'llm.model.layers.41.mlp.down_proj.qweight', 'llm.model.layers.41.mlp.down_proj.qzeros', 'llm.model.layers.41.mlp.down_proj.scales', 'llm.model.layers.41.mlp.gate_proj.g_idx', 'llm.model.layers.41.mlp.gate_proj.qweight', 'llm.model.layers.41.mlp.gate_proj.qzeros', 'llm.model.layers.41.mlp.gate_proj.scales', 'llm.model.layers.41.mlp.up_proj.g_idx', 'llm.model.layers.41.mlp.up_proj.qweight', 'llm.model.layers.41.mlp.up_proj.qzeros', 'llm.model.layers.41.mlp.up_proj.scales', 'llm.model.layers.41.self_attn.k_proj.g_idx', 'llm.model.layers.41.self_attn.k_proj.qweight', 'llm.model.layers.41.self_attn.k_proj.qzeros', 'llm.model.layers.41.self_attn.k_proj.scales', 'llm.model.layers.41.self_attn.o_proj.g_idx', 'llm.model.layers.41.self_attn.o_proj.qweight', 'llm.model.layers.41.self_attn.o_proj.qzeros', 'llm.model.layers.41.self_attn.o_proj.scales', 'llm.model.layers.41.self_attn.q_proj.g_idx', 'llm.model.layers.41.self_attn.q_proj.qweight', 'llm.model.layers.41.self_attn.q_proj.qzeros', 'llm.model.layers.41.self_attn.q_proj.scales', 'llm.model.layers.41.self_attn.v_proj.g_idx', 'llm.model.layers.41.self_attn.v_proj.qweight', 'llm.model.layers.41.self_attn.v_proj.qzeros', 'llm.model.layers.41.self_attn.v_proj.scales', 'llm.model.layers.42.mlp.down_proj.g_idx', 'llm.model.layers.42.mlp.down_proj.qweight', 'llm.model.layers.42.mlp.down_proj.qzeros', 'llm.model.layers.42.mlp.down_proj.scales', 'llm.model.layers.42.mlp.gate_proj.g_idx', 'llm.model.layers.42.mlp.gate_proj.qweight', 'llm.model.layers.42.mlp.gate_proj.qzeros', 'llm.model.layers.42.mlp.gate_proj.scales', 'llm.model.layers.42.mlp.up_proj.g_idx', 'llm.model.layers.42.mlp.up_proj.qweight', 'llm.model.layers.42.mlp.up_proj.qzeros', 'llm.model.layers.42.mlp.up_proj.scales', 'llm.model.layers.42.self_attn.k_proj.g_idx', 'llm.model.layers.42.self_attn.k_proj.qweight', 'llm.model.layers.42.self_attn.k_proj.qzeros', 'llm.model.layers.42.self_attn.k_proj.scales', 'llm.model.layers.42.self_attn.o_proj.g_idx', 'llm.model.layers.42.self_attn.o_proj.qweight', 'llm.model.layers.42.self_attn.o_proj.qzeros', 'llm.model.layers.42.self_attn.o_proj.scales', 'llm.model.layers.42.self_attn.q_proj.g_idx', 'llm.model.layers.42.self_attn.q_proj.qweight', 'llm.model.layers.42.self_attn.q_proj.qzeros', 'llm.model.layers.42.self_attn.q_proj.scales', 'llm.model.layers.42.self_attn.v_proj.g_idx', 'llm.model.layers.42.self_attn.v_proj.qweight', 'llm.model.layers.42.self_attn.v_proj.qzeros', 'llm.model.layers.42.self_attn.v_proj.scales', 'llm.model.layers.43.mlp.down_proj.g_idx', 'llm.model.layers.43.mlp.down_proj.qweight', 'llm.model.layers.43.mlp.down_proj.qzeros', 'llm.model.layers.43.mlp.down_proj.scales', 'llm.model.layers.43.mlp.gate_proj.g_idx', 'llm.model.layers.43.mlp.gate_proj.qweight', 'llm.model.layers.43.mlp.gate_proj.qzeros', 'llm.model.layers.43.mlp.gate_proj.scales', 'llm.model.layers.43.mlp.up_proj.g_idx', 'llm.model.layers.43.mlp.up_proj.qweight', 'llm.model.layers.43.mlp.up_proj.qzeros', 'llm.model.layers.43.mlp.up_proj.scales', 'llm.model.layers.43.self_attn.k_proj.g_idx', 'llm.model.layers.43.self_attn.k_proj.qweight', 'llm.model.layers.43.self_attn.k_proj.qzeros', 'llm.model.layers.43.self_attn.k_proj.scales', 'llm.model.layers.43.self_attn.o_proj.g_idx', 'llm.model.layers.43.self_attn.o_proj.qweight', 'llm.model.layers.43.self_attn.o_proj.qzeros', 'llm.model.layers.43.self_attn.o_proj.scales', 'llm.model.layers.43.self_attn.q_proj.g_idx', 'llm.model.layers.43.self_attn.q_proj.qweight', 'llm.model.layers.43.self_attn.q_proj.qzeros', 'llm.model.layers.43.self_attn.q_proj.scales', 'llm.model.layers.43.self_attn.v_proj.g_idx', 'llm.model.layers.43.self_attn.v_proj.qweight', 'llm.model.layers.43.self_attn.v_proj.qzeros', 'llm.model.layers.43.self_attn.v_proj.scales', 'llm.model.layers.44.mlp.down_proj.g_idx', 'llm.model.layers.44.mlp.down_proj.qweight', 'llm.model.layers.44.mlp.down_proj.qzeros', 'llm.model.layers.44.mlp.down_proj.scales', 'llm.model.layers.44.mlp.gate_proj.g_idx', 'llm.model.layers.44.mlp.gate_proj.qweight', 'llm.model.layers.44.mlp.gate_proj.qzeros', 'llm.model.layers.44.mlp.gate_proj.scales', 'llm.model.layers.44.mlp.up_proj.g_idx', 'llm.model.layers.44.mlp.up_proj.qweight', 'llm.model.layers.44.mlp.up_proj.qzeros', 'llm.model.layers.44.mlp.up_proj.scales', 'llm.model.layers.44.self_attn.k_proj.g_idx', 'llm.model.layers.44.self_attn.k_proj.qweight', 'llm.model.layers.44.self_attn.k_proj.qzeros', 'llm.model.layers.44.self_attn.k_proj.scales', 'llm.model.layers.44.self_attn.o_proj.g_idx', 'llm.model.layers.44.self_attn.o_proj.qweight', 'llm.model.layers.44.self_attn.o_proj.qzeros', 'llm.model.layers.44.self_attn.o_proj.scales', 'llm.model.layers.44.self_attn.q_proj.g_idx', 'llm.model.layers.44.self_attn.q_proj.qweight', 'llm.model.layers.44.self_attn.q_proj.qzeros', 'llm.model.layers.44.self_attn.q_proj.scales', 'llm.model.layers.44.self_attn.v_proj.g_idx', 'llm.model.layers.44.self_attn.v_proj.qweight', 'llm.model.layers.44.self_attn.v_proj.qzeros', 'llm.model.layers.44.self_attn.v_proj.scales', 'llm.model.layers.45.mlp.down_proj.g_idx', 'llm.model.layers.45.mlp.down_proj.qweight', 'llm.model.layers.45.mlp.down_proj.qzeros', 'llm.model.layers.45.mlp.down_proj.scales', 'llm.model.layers.45.mlp.gate_proj.g_idx', 'llm.model.layers.45.mlp.gate_proj.qweight', 'llm.model.layers.45.mlp.gate_proj.qzeros', 'llm.model.layers.45.mlp.gate_proj.scales', 'llm.model.layers.45.mlp.up_proj.g_idx', 'llm.model.layers.45.mlp.up_proj.qweight', 'llm.model.layers.45.mlp.up_proj.qzeros', 'llm.model.layers.45.mlp.up_proj.scales', 'llm.model.layers.45.self_attn.k_proj.g_idx', 'llm.model.layers.45.self_attn.k_proj.qweight', 'llm.model.layers.45.self_attn.k_proj.qzeros', 'llm.model.layers.45.self_attn.k_proj.scales', 'llm.model.layers.45.self_attn.o_proj.g_idx', 'llm.model.layers.45.self_attn.o_proj.qweight', 'llm.model.layers.45.self_attn.o_proj.qzeros', 'llm.model.layers.45.self_attn.o_proj.scales', 'llm.model.layers.45.self_attn.q_proj.g_idx', 'llm.model.layers.45.self_attn.q_proj.qweight', 'llm.model.layers.45.self_attn.q_proj.qzeros', 'llm.model.layers.45.self_attn.q_proj.scales', 'llm.model.layers.45.self_attn.v_proj.g_idx', 'llm.model.layers.45.self_attn.v_proj.qweight', 'llm.model.layers.45.self_attn.v_proj.qzeros', 'llm.model.layers.45.self_attn.v_proj.scales', 'llm.model.layers.46.mlp.down_proj.g_idx', 'llm.model.layers.46.mlp.down_proj.qweight', 'llm.model.layers.46.mlp.down_proj.qzeros', 'llm.model.layers.46.mlp.down_proj.scales', 'llm.model.layers.46.mlp.gate_proj.g_idx', 'llm.model.layers.46.mlp.gate_proj.qweight', 'llm.model.layers.46.mlp.gate_proj.qzeros', 'llm.model.layers.46.mlp.gate_proj.scales', 'llm.model.layers.46.mlp.up_proj.g_idx', 'llm.model.layers.46.mlp.up_proj.qweight', 'llm.model.layers.46.mlp.up_proj.qzeros', 'llm.model.layers.46.mlp.up_proj.scales', 'llm.model.layers.46.self_attn.k_proj.g_idx', 'llm.model.layers.46.self_attn.k_proj.qweight', 'llm.model.layers.46.self_attn.k_proj.qzeros', 'llm.model.layers.46.self_attn.k_proj.scales', 'llm.model.layers.46.self_attn.o_proj.g_idx', 'llm.model.layers.46.self_attn.o_proj.qweight', 'llm.model.layers.46.self_attn.o_proj.qzeros', 'llm.model.layers.46.self_attn.o_proj.scales', 'llm.model.layers.46.self_attn.q_proj.g_idx', 'llm.model.layers.46.self_attn.q_proj.qweight', 'llm.model.layers.46.self_attn.q_proj.qzeros', 'llm.model.layers.46.self_attn.q_proj.scales', 'llm.model.layers.46.self_attn.v_proj.g_idx', 'llm.model.layers.46.self_attn.v_proj.qweight', 'llm.model.layers.46.self_attn.v_proj.qzeros', 'llm.model.layers.46.self_attn.v_proj.scales', 'llm.model.layers.47.mlp.down_proj.g_idx', 'llm.model.layers.47.mlp.down_proj.qweight', 'llm.model.layers.47.mlp.down_proj.qzeros', 'llm.model.layers.47.mlp.down_proj.scales', 'llm.model.layers.47.mlp.gate_proj.g_idx', 'llm.model.layers.47.mlp.gate_proj.qweight', 'llm.model.layers.47.mlp.gate_proj.qzeros', 'llm.model.layers.47.mlp.gate_proj.scales', 'llm.model.layers.47.mlp.up_proj.g_idx', 'llm.model.layers.47.mlp.up_proj.qweight', 'llm.model.layers.47.mlp.up_proj.qzeros', 'llm.model.layers.47.mlp.up_proj.scales', 'llm.model.layers.47.self_attn.k_proj.g_idx', 'llm.model.layers.47.self_attn.k_proj.qweight', 'llm.model.layers.47.self_attn.k_proj.qzeros', 'llm.model.layers.47.self_attn.k_proj.scales', 'llm.model.layers.47.self_attn.o_proj.g_idx', 'llm.model.layers.47.self_attn.o_proj.qweight', 'llm.model.layers.47.self_attn.o_proj.qzeros', 'llm.model.layers.47.self_attn.o_proj.scales', 'llm.model.layers.47.self_attn.q_proj.g_idx', 'llm.model.layers.47.self_attn.q_proj.qweight', 'llm.model.layers.47.self_attn.q_proj.qzeros', 'llm.model.layers.47.self_attn.q_proj.scales', 'llm.model.layers.47.self_attn.v_proj.g_idx', 'llm.model.layers.47.self_attn.v_proj.qweight', 'llm.model.layers.47.self_attn.v_proj.qzeros', 'llm.model.layers.47.self_attn.v_proj.scales', 'llm.model.layers.48.mlp.down_proj.g_idx', 'llm.model.layers.48.mlp.down_proj.qweight', 'llm.model.layers.48.mlp.down_proj.qzeros', 'llm.model.layers.48.mlp.down_proj.scales', 'llm.model.layers.48.mlp.gate_proj.g_idx', 'llm.model.layers.48.mlp.gate_proj.qweight', 'llm.model.layers.48.mlp.gate_proj.qzeros', 'llm.model.layers.48.mlp.gate_proj.scales', 'llm.model.layers.48.mlp.up_proj.g_idx', 'llm.model.layers.48.mlp.up_proj.qweight', 'llm.model.layers.48.mlp.up_proj.qzeros', 'llm.model.layers.48.mlp.up_proj.scales', 'llm.model.layers.48.self_attn.k_proj.g_idx', 'llm.model.layers.48.self_attn.k_proj.qweight', 'llm.model.layers.48.self_attn.k_proj.qzeros', 'llm.model.layers.48.self_attn.k_proj.scales', 'llm.model.layers.48.self_attn.o_proj.g_idx', 'llm.model.layers.48.self_attn.o_proj.qweight', 'llm.model.layers.48.self_attn.o_proj.qzeros', 'llm.model.layers.48.self_attn.o_proj.scales', 'llm.model.layers.48.self_attn.q_proj.g_idx', 'llm.model.layers.48.self_attn.q_proj.qweight', 'llm.model.layers.48.self_attn.q_proj.qzeros', 'llm.model.layers.48.self_attn.q_proj.scales', 'llm.model.layers.48.self_attn.v_proj.g_idx', 'llm.model.layers.48.self_attn.v_proj.qweight', 'llm.model.layers.48.self_attn.v_proj.qzeros', 'llm.model.layers.48.self_attn.v_proj.scales', 'llm.model.layers.49.mlp.down_proj.g_idx', 'llm.model.layers.49.mlp.down_proj.qweight', 'llm.model.layers.49.mlp.down_proj.qzeros', 'llm.model.layers.49.mlp.down_proj.scales', 'llm.model.layers.49.mlp.gate_proj.g_idx', 'llm.model.layers.49.mlp.gate_proj.qweight', 'llm.model.layers.49.mlp.gate_proj.qzeros', 'llm.model.layers.49.mlp.gate_proj.scales', 'llm.model.layers.49.mlp.up_proj.g_idx', 'llm.model.layers.49.mlp.up_proj.qweight', 'llm.model.layers.49.mlp.up_proj.qzeros', 'llm.model.layers.49.mlp.up_proj.scales', 'llm.model.layers.49.self_attn.k_proj.g_idx', 'llm.model.layers.49.self_attn.k_proj.qweight', 'llm.model.layers.49.self_attn.k_proj.qzeros', 'llm.model.layers.49.self_attn.k_proj.scales', 'llm.model.layers.49.self_attn.o_proj.g_idx', 'llm.model.layers.49.self_attn.o_proj.qweight', 'llm.model.layers.49.self_attn.o_proj.qzeros', 'llm.model.layers.49.self_attn.o_proj.scales', 'llm.model.layers.49.self_attn.q_proj.g_idx', 'llm.model.layers.49.self_attn.q_proj.qweight', 'llm.model.layers.49.self_attn.q_proj.qzeros', 'llm.model.layers.49.self_attn.q_proj.scales', 'llm.model.layers.49.self_attn.v_proj.g_idx', 'llm.model.layers.49.self_attn.v_proj.qweight', 'llm.model.layers.49.self_attn.v_proj.qzeros', 'llm.model.layers.49.self_attn.v_proj.scales', 'llm.model.layers.5.mlp.down_proj.g_idx', 'llm.model.layers.5.mlp.down_proj.qweight', 'llm.model.layers.5.mlp.down_proj.qzeros', 'llm.model.layers.5.mlp.down_proj.scales', 'llm.model.layers.5.mlp.gate_proj.g_idx', 'llm.model.layers.5.mlp.gate_proj.qweight', 'llm.model.layers.5.mlp.gate_proj.qzeros', 'llm.model.layers.5.mlp.gate_proj.scales', 'llm.model.layers.5.mlp.up_proj.g_idx', 'llm.model.layers.5.mlp.up_proj.qweight', 'llm.model.layers.5.mlp.up_proj.qzeros', 'llm.model.layers.5.mlp.up_proj.scales', 'llm.model.layers.5.self_attn.k_proj.g_idx', 'llm.model.layers.5.self_attn.k_proj.qweight', 'llm.model.layers.5.self_attn.k_proj.qzeros', 'llm.model.layers.5.self_attn.k_proj.scales', 'llm.model.layers.5.self_attn.o_proj.g_idx', 'llm.model.layers.5.self_attn.o_proj.qweight', 'llm.model.layers.5.self_attn.o_proj.qzeros', 'llm.model.layers.5.self_attn.o_proj.scales', 'llm.model.layers.5.self_attn.q_proj.g_idx', 'llm.model.layers.5.self_attn.q_proj.qweight', 'llm.model.layers.5.self_attn.q_proj.qzeros', 'llm.model.layers.5.self_attn.q_proj.scales', 'llm.model.layers.5.self_attn.v_proj.g_idx', 'llm.model.layers.5.self_attn.v_proj.qweight', 'llm.model.layers.5.self_attn.v_proj.qzeros', 'llm.model.layers.5.self_attn.v_proj.scales', 'llm.model.layers.50.mlp.down_proj.g_idx', 'llm.model.layers.50.mlp.down_proj.qweight', 'llm.model.layers.50.mlp.down_proj.qzeros', 'llm.model.layers.50.mlp.down_proj.scales', 'llm.model.layers.50.mlp.gate_proj.g_idx', 'llm.model.layers.50.mlp.gate_proj.qweight', 'llm.model.layers.50.mlp.gate_proj.qzeros', 'llm.model.layers.50.mlp.gate_proj.scales', 'llm.model.layers.50.mlp.up_proj.g_idx', 'llm.model.layers.50.mlp.up_proj.qweight', 'llm.model.layers.50.mlp.up_proj.qzeros', 'llm.model.layers.50.mlp.up_proj.scales', 'llm.model.layers.50.self_attn.k_proj.g_idx', 'llm.model.layers.50.self_attn.k_proj.qweight', 'llm.model.layers.50.self_attn.k_proj.qzeros', 'llm.model.layers.50.self_attn.k_proj.scales', 'llm.model.layers.50.self_attn.o_proj.g_idx', 'llm.model.layers.50.self_attn.o_proj.qweight', 'llm.model.layers.50.self_attn.o_proj.qzeros', 'llm.model.layers.50.self_attn.o_proj.scales', 'llm.model.layers.50.self_attn.q_proj.g_idx', 'llm.model.layers.50.self_attn.q_proj.qweight', 'llm.model.layers.50.self_attn.q_proj.qzeros', 'llm.model.layers.50.self_attn.q_proj.scales', 'llm.model.layers.50.self_attn.v_proj.g_idx', 'llm.model.layers.50.self_attn.v_proj.qweight', 'llm.model.layers.50.self_attn.v_proj.qzeros', 'llm.model.layers.50.self_attn.v_proj.scales', 'llm.model.layers.51.mlp.down_proj.g_idx', 'llm.model.layers.51.mlp.down_proj.qweight', 'llm.model.layers.51.mlp.down_proj.qzeros', 'llm.model.layers.51.mlp.down_proj.scales', 'llm.model.layers.51.mlp.gate_proj.g_idx', 'llm.model.layers.51.mlp.gate_proj.qweight', 'llm.model.layers.51.mlp.gate_proj.qzeros', 'llm.model.layers.51.mlp.gate_proj.scales', 'llm.model.layers.51.mlp.up_proj.g_idx', 'llm.model.layers.51.mlp.up_proj.qweight', 'llm.model.layers.51.mlp.up_proj.qzeros', 'llm.model.layers.51.mlp.up_proj.scales', 'llm.model.layers.51.self_attn.k_proj.g_idx', 'llm.model.layers.51.self_attn.k_proj.qweight', 'llm.model.layers.51.self_attn.k_proj.qzeros', 'llm.model.layers.51.self_attn.k_proj.scales', 'llm.model.layers.51.self_attn.o_proj.g_idx', 'llm.model.layers.51.self_attn.o_proj.qweight', 'llm.model.layers.51.self_attn.o_proj.qzeros', 'llm.model.layers.51.self_attn.o_proj.scales', 'llm.model.layers.51.self_attn.q_proj.g_idx', 'llm.model.layers.51.self_attn.q_proj.qweight', 'llm.model.layers.51.self_attn.q_proj.qzeros', 'llm.model.layers.51.self_attn.q_proj.scales', 'llm.model.layers.51.self_attn.v_proj.g_idx', 'llm.model.layers.51.self_attn.v_proj.qweight', 'llm.model.layers.51.self_attn.v_proj.qzeros', 'llm.model.layers.51.self_attn.v_proj.scales', 'llm.model.layers.6.mlp.down_proj.g_idx', 'llm.model.layers.6.mlp.down_proj.qweight', 'llm.model.layers.6.mlp.down_proj.qzeros', 'llm.model.layers.6.mlp.down_proj.scales', 'llm.model.layers.6.mlp.gate_proj.g_idx', 'llm.model.layers.6.mlp.gate_proj.qweight', 'llm.model.layers.6.mlp.gate_proj.qzeros', 'llm.model.layers.6.mlp.gate_proj.scales', 'llm.model.layers.6.mlp.up_proj.g_idx', 'llm.model.layers.6.mlp.up_proj.qweight', 'llm.model.layers.6.mlp.up_proj.qzeros', 'llm.model.layers.6.mlp.up_proj.scales', 'llm.model.layers.6.self_attn.k_proj.g_idx', 'llm.model.layers.6.self_attn.k_proj.qweight', 'llm.model.layers.6.self_attn.k_proj.qzeros', 'llm.model.layers.6.self_attn.k_proj.scales', 'llm.model.layers.6.self_attn.o_proj.g_idx', 'llm.model.layers.6.self_attn.o_proj.qweight', 'llm.model.layers.6.self_attn.o_proj.qzeros', 'llm.model.layers.6.self_attn.o_proj.scales', 'llm.model.layers.6.self_attn.q_proj.g_idx', 'llm.model.layers.6.self_attn.q_proj.qweight', 'llm.model.layers.6.self_attn.q_proj.qzeros', 'llm.model.layers.6.self_attn.q_proj.scales', 'llm.model.layers.6.self_attn.v_proj.g_idx', 'llm.model.layers.6.self_attn.v_proj.qweight', 'llm.model.layers.6.self_attn.v_proj.qzeros', 'llm.model.layers.6.self_attn.v_proj.scales', 'llm.model.layers.7.mlp.down_proj.g_idx', 'llm.model.layers.7.mlp.down_proj.qweight', 'llm.model.layers.7.mlp.down_proj.qzeros', 'llm.model.layers.7.mlp.down_proj.scales', 'llm.model.layers.7.mlp.gate_proj.g_idx', 'llm.model.layers.7.mlp.gate_proj.qweight', 'llm.model.layers.7.mlp.gate_proj.qzeros', 'llm.model.layers.7.mlp.gate_proj.scales', 'llm.model.layers.7.mlp.up_proj.g_idx', 'llm.model.layers.7.mlp.up_proj.qweight', 'llm.model.layers.7.mlp.up_proj.qzeros', 'llm.model.layers.7.mlp.up_proj.scales', 'llm.model.layers.7.self_attn.k_proj.g_idx', 'llm.model.layers.7.self_attn.k_proj.qweight', 'llm.model.layers.7.self_attn.k_proj.qzeros', 'llm.model.layers.7.self_attn.k_proj.scales', 'llm.model.layers.7.self_attn.o_proj.g_idx', 'llm.model.layers.7.self_attn.o_proj.qweight', 'llm.model.layers.7.self_attn.o_proj.qzeros', 'llm.model.layers.7.self_attn.o_proj.scales', 'llm.model.layers.7.self_attn.q_proj.g_idx', 'llm.model.layers.7.self_attn.q_proj.qweight', 'llm.model.layers.7.self_attn.q_proj.qzeros', 'llm.model.layers.7.self_attn.q_proj.scales', 'llm.model.layers.7.self_attn.v_proj.g_idx', 'llm.model.layers.7.self_attn.v_proj.qweight', 'llm.model.layers.7.self_attn.v_proj.qzeros', 'llm.model.layers.7.self_attn.v_proj.scales', 'llm.model.layers.8.mlp.down_proj.g_idx', 'llm.model.layers.8.mlp.down_proj.qweight', 'llm.model.layers.8.mlp.down_proj.qzeros', 'llm.model.layers.8.mlp.down_proj.scales', 'llm.model.layers.8.mlp.gate_proj.g_idx', 'llm.model.layers.8.mlp.gate_proj.qweight', 'llm.model.layers.8.mlp.gate_proj.qzeros', 'llm.model.layers.8.mlp.gate_proj.scales', 'llm.model.layers.8.mlp.up_proj.g_idx', 'llm.model.layers.8.mlp.up_proj.qweight', 'llm.model.layers.8.mlp.up_proj.qzeros', 'llm.model.layers.8.mlp.up_proj.scales', 'llm.model.layers.8.self_attn.k_proj.g_idx', 'llm.model.layers.8.self_attn.k_proj.qweight', 'llm.model.layers.8.self_attn.k_proj.qzeros', 'llm.model.layers.8.self_attn.k_proj.scales', 'llm.model.layers.8.self_attn.o_proj.g_idx', 'llm.model.layers.8.self_attn.o_proj.qweight', 'llm.model.layers.8.self_attn.o_proj.qzeros', 'llm.model.layers.8.self_attn.o_proj.scales', 'llm.model.layers.8.self_attn.q_proj.g_idx', 'llm.model.layers.8.self_attn.q_proj.qweight', 'llm.model.layers.8.self_attn.q_proj.qzeros', 'llm.model.layers.8.self_attn.q_proj.scales', 'llm.model.layers.8.self_attn.v_proj.g_idx', 'llm.model.layers.8.self_attn.v_proj.qweight', 'llm.model.layers.8.self_attn.v_proj.qzeros', 'llm.model.layers.8.self_attn.v_proj.scales', 'llm.model.layers.9.mlp.down_proj.g_idx', 'llm.model.layers.9.mlp.down_proj.qweight', 'llm.model.layers.9.mlp.down_proj.qzeros', 'llm.model.layers.9.mlp.down_proj.scales', 'llm.model.layers.9.mlp.gate_proj.g_idx', 'llm.model.layers.9.mlp.gate_proj.qweight', 'llm.model.layers.9.mlp.gate_proj.qzeros', 'llm.model.layers.9.mlp.gate_proj.scales', 'llm.model.layers.9.mlp.up_proj.g_idx', 'llm.model.layers.9.mlp.up_proj.qweight', 'llm.model.layers.9.mlp.up_proj.qzeros', 'llm.model.layers.9.mlp.up_proj.scales', 'llm.model.layers.9.self_attn.k_proj.g_idx', 'llm.model.layers.9.self_attn.k_proj.qweight', 'llm.model.layers.9.self_attn.k_proj.qzeros', 'llm.model.layers.9.self_attn.k_proj.scales', 'llm.model.layers.9.self_attn.o_proj.g_idx', 'llm.model.layers.9.self_attn.o_proj.qweight', 'llm.model.layers.9.self_attn.o_proj.qzeros', 'llm.model.layers.9.self_attn.o_proj.scales', 'llm.model.layers.9.self_attn.q_proj.g_idx', 'llm.model.layers.9.self_attn.q_proj.qweight', 'llm.model.layers.9.self_attn.q_proj.qzeros', 'llm.model.layers.9.self_attn.q_proj.scales', 'llm.model.layers.9.self_attn.v_proj.g_idx', 'llm.model.layers.9.self_attn.v_proj.qweight', 'llm.model.layers.9.self_attn.v_proj.qzeros', 'llm.model.layers.9.self_attn.v_proj.scales', 'vpm.encoder.layers.0.mlp.fc1.g_idx', 'vpm.encoder.layers.0.mlp.fc1.qweight', 'vpm.encoder.layers.0.mlp.fc1.qzeros', 'vpm.encoder.layers.0.mlp.fc1.scales', 'vpm.encoder.layers.0.mlp.fc2.g_idx', 'vpm.encoder.layers.0.mlp.fc2.qweight', 'vpm.encoder.layers.0.mlp.fc2.qzeros', 'vpm.encoder.layers.0.mlp.fc2.scales', 'vpm.encoder.layers.0.self_attn.k_proj.g_idx', 'vpm.encoder.layers.0.self_attn.k_proj.qweight', 'vpm.encoder.layers.0.self_attn.k_proj.qzeros', 'vpm.encoder.layers.0.self_attn.k_proj.scales', 'vpm.encoder.layers.0.self_attn.out_proj.g_idx', 'vpm.encoder.layers.0.self_attn.out_proj.qweight', 'vpm.encoder.layers.0.self_attn.out_proj.qzeros', 'vpm.encoder.layers.0.self_attn.out_proj.scales', 'vpm.encoder.layers.0.self_attn.q_proj.g_idx', 'vpm.encoder.layers.0.self_attn.q_proj.qweight', 'vpm.encoder.layers.0.self_attn.q_proj.qzeros', 'vpm.encoder.layers.0.self_attn.q_proj.scales', 'vpm.encoder.layers.0.self_attn.v_proj.g_idx', 'vpm.encoder.layers.0.self_attn.v_proj.qweight', 'vpm.encoder.layers.0.self_attn.v_proj.qzeros', 'vpm.encoder.layers.0.self_attn.v_proj.scales', 'vpm.encoder.layers.1.mlp.fc1.g_idx', 'vpm.encoder.layers.1.mlp.fc1.qweight', 'vpm.encoder.layers.1.mlp.fc1.qzeros', 'vpm.encoder.layers.1.mlp.fc1.scales', 'vpm.encoder.layers.1.mlp.fc2.g_idx', 'vpm.encoder.layers.1.mlp.fc2.qweight', 'vpm.encoder.layers.1.mlp.fc2.qzeros', 'vpm.encoder.layers.1.mlp.fc2.scales', 'vpm.encoder.layers.1.self_attn.k_proj.g_idx', 'vpm.encoder.layers.1.self_attn.k_proj.qweight', 'vpm.encoder.layers.1.self_attn.k_proj.qzeros', 'vpm.encoder.layers.1.self_attn.k_proj.scales', 'vpm.encoder.layers.1.self_attn.out_proj.g_idx', 'vpm.encoder.layers.1.self_attn.out_proj.qweight', 'vpm.encoder.layers.1.self_attn.out_proj.qzeros', 'vpm.encoder.layers.1.self_attn.out_proj.scales', 'vpm.encoder.layers.1.self_attn.q_proj.g_idx', 'vpm.encoder.layers.1.self_attn.q_proj.qweight', 'vpm.encoder.layers.1.self_attn.q_proj.qzeros', 'vpm.encoder.layers.1.self_attn.q_proj.scales', 'vpm.encoder.layers.1.self_attn.v_proj.g_idx', 'vpm.encoder.layers.1.self_attn.v_proj.qweight', 'vpm.encoder.layers.1.self_attn.v_proj.qzeros', 'vpm.encoder.layers.1.self_attn.v_proj.scales', 'vpm.encoder.layers.10.mlp.fc1.g_idx', 'vpm.encoder.layers.10.mlp.fc1.qweight', 'vpm.encoder.layers.10.mlp.fc1.qzeros', 'vpm.encoder.layers.10.mlp.fc1.scales', 'vpm.encoder.layers.10.mlp.fc2.g_idx', 'vpm.encoder.layers.10.mlp.fc2.qweight', 'vpm.encoder.layers.10.mlp.fc2.qzeros', 'vpm.encoder.layers.10.mlp.fc2.scales', 'vpm.encoder.layers.10.self_attn.k_proj.g_idx', 'vpm.encoder.layers.10.self_attn.k_proj.qweight', 'vpm.encoder.layers.10.self_attn.k_proj.qzeros', 'vpm.encoder.layers.10.self_attn.k_proj.scales', 'vpm.encoder.layers.10.self_attn.out_proj.g_idx', 'vpm.encoder.layers.10.self_attn.out_proj.qweight', 'vpm.encoder.layers.10.self_attn.out_proj.qzeros', 'vpm.encoder.layers.10.self_attn.out_proj.scales', 'vpm.encoder.layers.10.self_attn.q_proj.g_idx', 'vpm.encoder.layers.10.self_attn.q_proj.qweight', 'vpm.encoder.layers.10.self_attn.q_proj.qzeros', 'vpm.encoder.layers.10.self_attn.q_proj.scales', 'vpm.encoder.layers.10.self_attn.v_proj.g_idx', 'vpm.encoder.layers.10.self_attn.v_proj.qweight', 'vpm.encoder.layers.10.self_attn.v_proj.qzeros', 'vpm.encoder.layers.10.self_attn.v_proj.scales', 'vpm.encoder.layers.11.mlp.fc1.g_idx', 'vpm.encoder.layers.11.mlp.fc1.qweight', 'vpm.encoder.layers.11.mlp.fc1.qzeros', 'vpm.encoder.layers.11.mlp.fc1.scales', 'vpm.encoder.layers.11.mlp.fc2.g_idx', 'vpm.encoder.layers.11.mlp.fc2.qweight', 'vpm.encoder.layers.11.mlp.fc2.qzeros', 'vpm.encoder.layers.11.mlp.fc2.scales', 'vpm.encoder.layers.11.self_attn.k_proj.g_idx', 'vpm.encoder.layers.11.self_attn.k_proj.qweight', 'vpm.encoder.layers.11.self_attn.k_proj.qzeros', 'vpm.encoder.layers.11.self_attn.k_proj.scales', 'vpm.encoder.layers.11.self_attn.out_proj.g_idx', 'vpm.encoder.layers.11.self_attn.out_proj.qweight', 'vpm.encoder.layers.11.self_attn.out_proj.qzeros', 'vpm.encoder.layers.11.self_attn.out_proj.scales', 'vpm.encoder.layers.11.self_attn.q_proj.g_idx', 'vpm.encoder.layers.11.self_attn.q_proj.qweight', 'vpm.encoder.layers.11.self_attn.q_proj.qzeros', 'vpm.encoder.layers.11.self_attn.q_proj.scales', 'vpm.encoder.layers.11.self_attn.v_proj.g_idx', 'vpm.encoder.layers.11.self_attn.v_proj.qweight', 'vpm.encoder.layers.11.self_attn.v_proj.qzeros', 'vpm.encoder.layers.11.self_attn.v_proj.scales', 'vpm.encoder.layers.12.mlp.fc1.g_idx', 'vpm.encoder.layers.12.mlp.fc1.qweight', 'vpm.encoder.layers.12.mlp.fc1.qzeros', 'vpm.encoder.layers.12.mlp.fc1.scales', 'vpm.encoder.layers.12.mlp.fc2.g_idx', 'vpm.encoder.layers.12.mlp.fc2.qweight', 'vpm.encoder.layers.12.mlp.fc2.qzeros', 'vpm.encoder.layers.12.mlp.fc2.scales', 'vpm.encoder.layers.12.self_attn.k_proj.g_idx', 'vpm.encoder.layers.12.self_attn.k_proj.qweight', 'vpm.encoder.layers.12.self_attn.k_proj.qzeros', 'vpm.encoder.layers.12.self_attn.k_proj.scales', 'vpm.encoder.layers.12.self_attn.out_proj.g_idx', 'vpm.encoder.layers.12.self_attn.out_proj.qweight', 'vpm.encoder.layers.12.self_attn.out_proj.qzeros', 'vpm.encoder.layers.12.self_attn.out_proj.scales', 'vpm.encoder.layers.12.self_attn.q_proj.g_idx', 'vpm.encoder.layers.12.self_attn.q_proj.qweight', 'vpm.encoder.layers.12.self_attn.q_proj.qzeros', 'vpm.encoder.layers.12.self_attn.q_proj.scales', 'vpm.encoder.layers.12.self_attn.v_proj.g_idx', 'vpm.encoder.layers.12.self_attn.v_proj.qweight', 'vpm.encoder.layers.12.self_attn.v_proj.qzeros', 'vpm.encoder.layers.12.self_attn.v_proj.scales', 'vpm.encoder.layers.13.mlp.fc1.g_idx', 'vpm.encoder.layers.13.mlp.fc1.qweight', 'vpm.encoder.layers.13.mlp.fc1.qzeros', 'vpm.encoder.layers.13.mlp.fc1.scales', 'vpm.encoder.layers.13.mlp.fc2.g_idx', 'vpm.encoder.layers.13.mlp.fc2.qweight', 'vpm.encoder.layers.13.mlp.fc2.qzeros', 'vpm.encoder.layers.13.mlp.fc2.scales', 'vpm.encoder.layers.13.self_attn.k_proj.g_idx', 'vpm.encoder.layers.13.self_attn.k_proj.qweight', 'vpm.encoder.layers.13.self_attn.k_proj.qzeros', 'vpm.encoder.layers.13.self_attn.k_proj.scales', 'vpm.encoder.layers.13.self_attn.out_proj.g_idx', 'vpm.encoder.layers.13.self_attn.out_proj.qweight', 'vpm.encoder.layers.13.self_attn.out_proj.qzeros', 'vpm.encoder.layers.13.self_attn.out_proj.scales', 'vpm.encoder.layers.13.self_attn.q_proj.g_idx', 'vpm.encoder.layers.13.self_attn.q_proj.qweight', 'vpm.encoder.layers.13.self_attn.q_proj.qzeros', 'vpm.encoder.layers.13.self_attn.q_proj.scales', 'vpm.encoder.layers.13.self_attn.v_proj.g_idx', 'vpm.encoder.layers.13.self_attn.v_proj.qweight', 'vpm.encoder.layers.13.self_attn.v_proj.qzeros', 'vpm.encoder.layers.13.self_attn.v_proj.scales', 'vpm.encoder.layers.14.mlp.fc1.g_idx', 'vpm.encoder.layers.14.mlp.fc1.qweight', 'vpm.encoder.layers.14.mlp.fc1.qzeros', 'vpm.encoder.layers.14.mlp.fc1.scales', 'vpm.encoder.layers.14.mlp.fc2.g_idx', 'vpm.encoder.layers.14.mlp.fc2.qweight', 'vpm.encoder.layers.14.mlp.fc2.qzeros', 'vpm.encoder.layers.14.mlp.fc2.scales', 'vpm.encoder.layers.14.self_attn.k_proj.g_idx', 'vpm.encoder.layers.14.self_attn.k_proj.qweight', 'vpm.encoder.layers.14.self_attn.k_proj.qzeros', 'vpm.encoder.layers.14.self_attn.k_proj.scales', 'vpm.encoder.layers.14.self_attn.out_proj.g_idx', 'vpm.encoder.layers.14.self_attn.out_proj.qweight', 'vpm.encoder.layers.14.self_attn.out_proj.qzeros', 'vpm.encoder.layers.14.self_attn.out_proj.scales', 'vpm.encoder.layers.14.self_attn.q_proj.g_idx', 'vpm.encoder.layers.14.self_attn.q_proj.qweight', 'vpm.encoder.layers.14.self_attn.q_proj.qzeros', 'vpm.encoder.layers.14.self_attn.q_proj.scales', 'vpm.encoder.layers.14.self_attn.v_proj.g_idx', 'vpm.encoder.layers.14.self_attn.v_proj.qweight', 'vpm.encoder.layers.14.self_attn.v_proj.qzeros', 'vpm.encoder.layers.14.self_attn.v_proj.scales', 'vpm.encoder.layers.15.mlp.fc1.g_idx', 'vpm.encoder.layers.15.mlp.fc1.qweight', 'vpm.encoder.layers.15.mlp.fc1.qzeros', 'vpm.encoder.layers.15.mlp.fc1.scales', 'vpm.encoder.layers.15.mlp.fc2.g_idx', 'vpm.encoder.layers.15.mlp.fc2.qweight', 'vpm.encoder.layers.15.mlp.fc2.qzeros', 'vpm.encoder.layers.15.mlp.fc2.scales', 'vpm.encoder.layers.15.self_attn.k_proj.g_idx', 'vpm.encoder.layers.15.self_attn.k_proj.qweight', 'vpm.encoder.layers.15.self_attn.k_proj.qzeros', 'vpm.encoder.layers.15.self_attn.k_proj.scales', 'vpm.encoder.layers.15.self_attn.out_proj.g_idx', 'vpm.encoder.layers.15.self_attn.out_proj.qweight', 'vpm.encoder.layers.15.self_attn.out_proj.qzeros', 'vpm.encoder.layers.15.self_attn.out_proj.scales', 'vpm.encoder.layers.15.self_attn.q_proj.g_idx', 'vpm.encoder.layers.15.self_attn.q_proj.qweight', 'vpm.encoder.layers.15.self_attn.q_proj.qzeros', 'vpm.encoder.layers.15.self_attn.q_proj.scales', 'vpm.encoder.layers.15.self_attn.v_proj.g_idx', 'vpm.encoder.layers.15.self_attn.v_proj.qweight', 'vpm.encoder.layers.15.self_attn.v_proj.qzeros', 'vpm.encoder.layers.15.self_attn.v_proj.scales', 'vpm.encoder.layers.16.mlp.fc1.g_idx', 'vpm.encoder.layers.16.mlp.fc1.qweight', 'vpm.encoder.layers.16.mlp.fc1.qzeros', 'vpm.encoder.layers.16.mlp.fc1.scales', 'vpm.encoder.layers.16.mlp.fc2.g_idx', 'vpm.encoder.layers.16.mlp.fc2.qweight', 'vpm.encoder.layers.16.mlp.fc2.qzeros', 'vpm.encoder.layers.16.mlp.fc2.scales', 'vpm.encoder.layers.16.self_attn.k_proj.g_idx', 'vpm.encoder.layers.16.self_attn.k_proj.qweight', 'vpm.encoder.layers.16.self_attn.k_proj.qzeros', 'vpm.encoder.layers.16.self_attn.k_proj.scales', 'vpm.encoder.layers.16.self_attn.out_proj.g_idx', 'vpm.encoder.layers.16.self_attn.out_proj.qweight', 'vpm.encoder.layers.16.self_attn.out_proj.qzeros', 'vpm.encoder.layers.16.self_attn.out_proj.scales', 'vpm.encoder.layers.16.self_attn.q_proj.g_idx', 'vpm.encoder.layers.16.self_attn.q_proj.qweight', 'vpm.encoder.layers.16.self_attn.q_proj.qzeros', 'vpm.encoder.layers.16.self_attn.q_proj.scales', 'vpm.encoder.layers.16.self_attn.v_proj.g_idx', 'vpm.encoder.layers.16.self_attn.v_proj.qweight', 'vpm.encoder.layers.16.self_attn.v_proj.qzeros', 'vpm.encoder.layers.16.self_attn.v_proj.scales', 'vpm.encoder.layers.17.mlp.fc1.g_idx', 'vpm.encoder.layers.17.mlp.fc1.qweight', 'vpm.encoder.layers.17.mlp.fc1.qzeros', 'vpm.encoder.layers.17.mlp.fc1.scales', 'vpm.encoder.layers.17.mlp.fc2.g_idx', 'vpm.encoder.layers.17.mlp.fc2.qweight', 'vpm.encoder.layers.17.mlp.fc2.qzeros', 'vpm.encoder.layers.17.mlp.fc2.scales', 'vpm.encoder.layers.17.self_attn.k_proj.g_idx', 'vpm.encoder.layers.17.self_attn.k_proj.qweight', 'vpm.encoder.layers.17.self_attn.k_proj.qzeros', 'vpm.encoder.layers.17.self_attn.k_proj.scales', 'vpm.encoder.layers.17.self_attn.out_proj.g_idx', 'vpm.encoder.layers.17.self_attn.out_proj.qweight', 'vpm.encoder.layers.17.self_attn.out_proj.qzeros', 'vpm.encoder.layers.17.self_attn.out_proj.scales', 'vpm.encoder.layers.17.self_attn.q_proj.g_idx', 'vpm.encoder.layers.17.self_attn.q_proj.qweight', 'vpm.encoder.layers.17.self_attn.q_proj.qzeros', 'vpm.encoder.layers.17.self_attn.q_proj.scales', 'vpm.encoder.layers.17.self_attn.v_proj.g_idx', 'vpm.encoder.layers.17.self_attn.v_proj.qweight', 'vpm.encoder.layers.17.self_attn.v_proj.qzeros', 'vpm.encoder.layers.17.self_attn.v_proj.scales', 'vpm.encoder.layers.18.mlp.fc1.g_idx', 'vpm.encoder.layers.18.mlp.fc1.qweight', 'vpm.encoder.layers.18.mlp.fc1.qzeros', 'vpm.encoder.layers.18.mlp.fc1.scales', 'vpm.encoder.layers.18.mlp.fc2.g_idx', 'vpm.encoder.layers.18.mlp.fc2.qweight', 'vpm.encoder.layers.18.mlp.fc2.qzeros', 'vpm.encoder.layers.18.mlp.fc2.scales', 'vpm.encoder.layers.18.self_attn.k_proj.g_idx', 'vpm.encoder.layers.18.self_attn.k_proj.qweight', 'vpm.encoder.layers.18.self_attn.k_proj.qzeros', 'vpm.encoder.layers.18.self_attn.k_proj.scales', 'vpm.encoder.layers.18.self_attn.out_proj.g_idx', 'vpm.encoder.layers.18.self_attn.out_proj.qweight', 'vpm.encoder.layers.18.self_attn.out_proj.qzeros', 'vpm.encoder.layers.18.self_attn.out_proj.scales', 'vpm.encoder.layers.18.self_attn.q_proj.g_idx', 'vpm.encoder.layers.18.self_attn.q_proj.qweight', 'vpm.encoder.layers.18.self_attn.q_proj.qzeros', 'vpm.encoder.layers.18.self_attn.q_proj.scales', 'vpm.encoder.layers.18.self_attn.v_proj.g_idx', 'vpm.encoder.layers.18.self_attn.v_proj.qweight', 'vpm.encoder.layers.18.self_attn.v_proj.qzeros', 'vpm.encoder.layers.18.self_attn.v_proj.scales', 'vpm.encoder.layers.19.mlp.fc1.g_idx', 'vpm.encoder.layers.19.mlp.fc1.qweight', 'vpm.encoder.layers.19.mlp.fc1.qzeros', 'vpm.encoder.layers.19.mlp.fc1.scales', 'vpm.encoder.layers.19.mlp.fc2.g_idx', 'vpm.encoder.layers.19.mlp.fc2.qweight', 'vpm.encoder.layers.19.mlp.fc2.qzeros', 'vpm.encoder.layers.19.mlp.fc2.scales', 'vpm.encoder.layers.19.self_attn.k_proj.g_idx', 'vpm.encoder.layers.19.self_attn.k_proj.qweight', 'vpm.encoder.layers.19.self_attn.k_proj.qzeros', 'vpm.encoder.layers.19.self_attn.k_proj.scales', 'vpm.encoder.layers.19.self_attn.out_proj.g_idx', 'vpm.encoder.layers.19.self_attn.out_proj.qweight', 'vpm.encoder.layers.19.self_attn.out_proj.qzeros', 'vpm.encoder.layers.19.self_attn.out_proj.scales', 'vpm.encoder.layers.19.self_attn.q_proj.g_idx', 'vpm.encoder.layers.19.self_attn.q_proj.qweight', 'vpm.encoder.layers.19.self_attn.q_proj.qzeros', 'vpm.encoder.layers.19.self_attn.q_proj.scales', 'vpm.encoder.layers.19.self_attn.v_proj.g_idx', 'vpm.encoder.layers.19.self_attn.v_proj.qweight', 'vpm.encoder.layers.19.self_attn.v_proj.qzeros', 'vpm.encoder.layers.19.self_attn.v_proj.scales', 'vpm.encoder.layers.2.mlp.fc1.g_idx', 'vpm.encoder.layers.2.mlp.fc1.qweight', 'vpm.encoder.layers.2.mlp.fc1.qzeros', 'vpm.encoder.layers.2.mlp.fc1.scales', 'vpm.encoder.layers.2.mlp.fc2.g_idx', 'vpm.encoder.layers.2.mlp.fc2.qweight', 'vpm.encoder.layers.2.mlp.fc2.qzeros', 'vpm.encoder.layers.2.mlp.fc2.scales', 'vpm.encoder.layers.2.self_attn.k_proj.g_idx', 'vpm.encoder.layers.2.self_attn.k_proj.qweight', 'vpm.encoder.layers.2.self_attn.k_proj.qzeros', 'vpm.encoder.layers.2.self_attn.k_proj.scales', 'vpm.encoder.layers.2.self_attn.out_proj.g_idx', 'vpm.encoder.layers.2.self_attn.out_proj.qweight', 'vpm.encoder.layers.2.self_attn.out_proj.qzeros', 'vpm.encoder.layers.2.self_attn.out_proj.scales', 'vpm.encoder.layers.2.self_attn.q_proj.g_idx', 'vpm.encoder.layers.2.self_attn.q_proj.qweight', 'vpm.encoder.layers.2.self_attn.q_proj.qzeros', 'vpm.encoder.layers.2.self_attn.q_proj.scales', 'vpm.encoder.layers.2.self_attn.v_proj.g_idx', 'vpm.encoder.layers.2.self_attn.v_proj.qweight', 'vpm.encoder.layers.2.self_attn.v_proj.qzeros', 'vpm.encoder.layers.2.self_attn.v_proj.scales', 'vpm.encoder.layers.20.mlp.fc1.g_idx', 'vpm.encoder.layers.20.mlp.fc1.qweight', 'vpm.encoder.layers.20.mlp.fc1.qzeros', 'vpm.encoder.layers.20.mlp.fc1.scales', 'vpm.encoder.layers.20.mlp.fc2.g_idx', 'vpm.encoder.layers.20.mlp.fc2.qweight', 'vpm.encoder.layers.20.mlp.fc2.qzeros', 'vpm.encoder.layers.20.mlp.fc2.scales', 'vpm.encoder.layers.20.self_attn.k_proj.g_idx', 'vpm.encoder.layers.20.self_attn.k_proj.qweight', 'vpm.encoder.layers.20.self_attn.k_proj.qzeros', 'vpm.encoder.layers.20.self_attn.k_proj.scales', 'vpm.encoder.layers.20.self_attn.out_proj.g_idx', 'vpm.encoder.layers.20.self_attn.out_proj.qweight', 'vpm.encoder.layers.20.self_attn.out_proj.qzeros', 'vpm.encoder.layers.20.self_attn.out_proj.scales', 'vpm.encoder.layers.20.self_attn.q_proj.g_idx', 'vpm.encoder.layers.20.self_attn.q_proj.qweight', 'vpm.encoder.layers.20.self_attn.q_proj.qzeros', 'vpm.encoder.layers.20.self_attn.q_proj.scales', 'vpm.encoder.layers.20.self_attn.v_proj.g_idx', 'vpm.encoder.layers.20.self_attn.v_proj.qweight', 'vpm.encoder.layers.20.self_attn.v_proj.qzeros', 'vpm.encoder.layers.20.self_attn.v_proj.scales', 'vpm.encoder.layers.21.mlp.fc1.g_idx', 'vpm.encoder.layers.21.mlp.fc1.qweight', 'vpm.encoder.layers.21.mlp.fc1.qzeros', 'vpm.encoder.layers.21.mlp.fc1.scales', 'vpm.encoder.layers.21.mlp.fc2.g_idx', 'vpm.encoder.layers.21.mlp.fc2.qweight', 'vpm.encoder.layers.21.mlp.fc2.qzeros', 'vpm.encoder.layers.21.mlp.fc2.scales', 'vpm.encoder.layers.21.self_attn.k_proj.g_idx', 'vpm.encoder.layers.21.self_attn.k_proj.qweight', 'vpm.encoder.layers.21.self_attn.k_proj.qzeros', 'vpm.encoder.layers.21.self_attn.k_proj.scales', 'vpm.encoder.layers.21.self_attn.out_proj.g_idx', 'vpm.encoder.layers.21.self_attn.out_proj.qweight', 'vpm.encoder.layers.21.self_attn.out_proj.qzeros', 'vpm.encoder.layers.21.self_attn.out_proj.scales', 'vpm.encoder.layers.21.self_attn.q_proj.g_idx', 'vpm.encoder.layers.21.self_attn.q_proj.qweight', 'vpm.encoder.layers.21.self_attn.q_proj.qzeros', 'vpm.encoder.layers.21.self_attn.q_proj.scales', 'vpm.encoder.layers.21.self_attn.v_proj.g_idx', 'vpm.encoder.layers.21.self_attn.v_proj.qweight', 'vpm.encoder.layers.21.self_attn.v_proj.qzeros', 'vpm.encoder.layers.21.self_attn.v_proj.scales', 'vpm.encoder.layers.22.mlp.fc1.g_idx', 'vpm.encoder.layers.22.mlp.fc1.qweight', 'vpm.encoder.layers.22.mlp.fc1.qzeros', 'vpm.encoder.layers.22.mlp.fc1.scales', 'vpm.encoder.layers.22.mlp.fc2.g_idx', 'vpm.encoder.layers.22.mlp.fc2.qweight', 'vpm.encoder.layers.22.mlp.fc2.qzeros', 'vpm.encoder.layers.22.mlp.fc2.scales', 'vpm.encoder.layers.22.self_attn.k_proj.g_idx', 'vpm.encoder.layers.22.self_attn.k_proj.qweight', 'vpm.encoder.layers.22.self_attn.k_proj.qzeros', 'vpm.encoder.layers.22.self_attn.k_proj.scales', 'vpm.encoder.layers.22.self_attn.out_proj.g_idx', 'vpm.encoder.layers.22.self_attn.out_proj.qweight', 'vpm.encoder.layers.22.self_attn.out_proj.qzeros', 'vpm.encoder.layers.22.self_attn.out_proj.scales', 'vpm.encoder.layers.22.self_attn.q_proj.g_idx', 'vpm.encoder.layers.22.self_attn.q_proj.qweight', 'vpm.encoder.layers.22.self_attn.q_proj.qzeros', 'vpm.encoder.layers.22.self_attn.q_proj.scales', 'vpm.encoder.layers.22.self_attn.v_proj.g_idx', 'vpm.encoder.layers.22.self_attn.v_proj.qweight', 'vpm.encoder.layers.22.self_attn.v_proj.qzeros', 'vpm.encoder.layers.22.self_attn.v_proj.scales', 'vpm.encoder.layers.23.mlp.fc1.g_idx', 'vpm.encoder.layers.23.mlp.fc1.qweight', 'vpm.encoder.layers.23.mlp.fc1.qzeros', 'vpm.encoder.layers.23.mlp.fc1.scales', 'vpm.encoder.layers.23.mlp.fc2.g_idx', 'vpm.encoder.layers.23.mlp.fc2.qweight', 'vpm.encoder.layers.23.mlp.fc2.qzeros', 'vpm.encoder.layers.23.mlp.fc2.scales', 'vpm.encoder.layers.23.self_attn.k_proj.g_idx', 'vpm.encoder.layers.23.self_attn.k_proj.qweight', 'vpm.encoder.layers.23.self_attn.k_proj.qzeros', 'vpm.encoder.layers.23.self_attn.k_proj.scales', 'vpm.encoder.layers.23.self_attn.out_proj.g_idx', 'vpm.encoder.layers.23.self_attn.out_proj.qweight', 'vpm.encoder.layers.23.self_attn.out_proj.qzeros', 'vpm.encoder.layers.23.self_attn.out_proj.scales', 'vpm.encoder.layers.23.self_attn.q_proj.g_idx', 'vpm.encoder.layers.23.self_attn.q_proj.qweight', 'vpm.encoder.layers.23.self_attn.q_proj.qzeros', 'vpm.encoder.layers.23.self_attn.q_proj.scales', 'vpm.encoder.layers.23.self_attn.v_proj.g_idx', 'vpm.encoder.layers.23.self_attn.v_proj.qweight', 'vpm.encoder.layers.23.self_attn.v_proj.qzeros', 'vpm.encoder.layers.23.self_attn.v_proj.scales', 'vpm.encoder.layers.24.mlp.fc1.g_idx', 'vpm.encoder.layers.24.mlp.fc1.qweight', 'vpm.encoder.layers.24.mlp.fc1.qzeros', 'vpm.encoder.layers.24.mlp.fc1.scales', 'vpm.encoder.layers.24.mlp.fc2.g_idx', 'vpm.encoder.layers.24.mlp.fc2.qweight', 'vpm.encoder.layers.24.mlp.fc2.qzeros', 'vpm.encoder.layers.24.mlp.fc2.scales', 'vpm.encoder.layers.24.self_attn.k_proj.g_idx', 'vpm.encoder.layers.24.self_attn.k_proj.qweight', 'vpm.encoder.layers.24.self_attn.k_proj.qzeros', 'vpm.encoder.layers.24.self_attn.k_proj.scales', 'vpm.encoder.layers.24.self_attn.out_proj.g_idx', 'vpm.encoder.layers.24.self_attn.out_proj.qweight', 'vpm.encoder.layers.24.self_attn.out_proj.qzeros', 'vpm.encoder.layers.24.self_attn.out_proj.scales', 'vpm.encoder.layers.24.self_attn.q_proj.g_idx', 'vpm.encoder.layers.24.self_attn.q_proj.qweight', 'vpm.encoder.layers.24.self_attn.q_proj.qzeros', 'vpm.encoder.layers.24.self_attn.q_proj.scales', 'vpm.encoder.layers.24.self_attn.v_proj.g_idx', 'vpm.encoder.layers.24.self_attn.v_proj.qweight', 'vpm.encoder.layers.24.self_attn.v_proj.qzeros', 'vpm.encoder.layers.24.self_attn.v_proj.scales', 'vpm.encoder.layers.25.mlp.fc1.g_idx', 'vpm.encoder.layers.25.mlp.fc1.qweight', 'vpm.encoder.layers.25.mlp.fc1.qzeros', 'vpm.encoder.layers.25.mlp.fc1.scales', 'vpm.encoder.layers.25.mlp.fc2.g_idx', 'vpm.encoder.layers.25.mlp.fc2.qweight', 'vpm.encoder.layers.25.mlp.fc2.qzeros', 'vpm.encoder.layers.25.mlp.fc2.scales', 'vpm.encoder.layers.25.self_attn.k_proj.g_idx', 'vpm.encoder.layers.25.self_attn.k_proj.qweight', 'vpm.encoder.layers.25.self_attn.k_proj.qzeros', 'vpm.encoder.layers.25.self_attn.k_proj.scales', 'vpm.encoder.layers.25.self_attn.out_proj.g_idx', 'vpm.encoder.layers.25.self_attn.out_proj.qweight', 'vpm.encoder.layers.25.self_attn.out_proj.qzeros', 'vpm.encoder.layers.25.self_attn.out_proj.scales', 'vpm.encoder.layers.25.self_attn.q_proj.g_idx', 'vpm.encoder.layers.25.self_attn.q_proj.qweight', 'vpm.encoder.layers.25.self_attn.q_proj.qzeros', 'vpm.encoder.layers.25.self_attn.q_proj.scales', 'vpm.encoder.layers.25.self_attn.v_proj.g_idx', 'vpm.encoder.layers.25.self_attn.v_proj.qweight', 'vpm.encoder.layers.25.self_attn.v_proj.qzeros', 'vpm.encoder.layers.25.self_attn.v_proj.scales', 'vpm.encoder.layers.26.mlp.fc1.g_idx', 'vpm.encoder.layers.26.mlp.fc1.qweight', 'vpm.encoder.layers.26.mlp.fc1.qzeros', 'vpm.encoder.layers.26.mlp.fc1.scales', 'vpm.encoder.layers.26.mlp.fc2.g_idx', 'vpm.encoder.layers.26.mlp.fc2.qweight', 'vpm.encoder.layers.26.mlp.fc2.qzeros', 'vpm.encoder.layers.26.mlp.fc2.scales', 'vpm.encoder.layers.26.self_attn.k_proj.g_idx', 'vpm.encoder.layers.26.self_attn.k_proj.qweight', 'vpm.encoder.layers.26.self_attn.k_proj.qzeros', 'vpm.encoder.layers.26.self_attn.k_proj.scales', 'vpm.encoder.layers.26.self_attn.out_proj.g_idx', 'vpm.encoder.layers.26.self_attn.out_proj.qweight', 'vpm.encoder.layers.26.self_attn.out_proj.qzeros', 'vpm.encoder.layers.26.self_attn.out_proj.scales', 'vpm.encoder.layers.26.self_attn.q_proj.g_idx', 'vpm.encoder.layers.26.self_attn.q_proj.qweight', 'vpm.encoder.layers.26.self_attn.q_proj.qzeros', 'vpm.encoder.layers.26.self_attn.q_proj.scales', 'vpm.encoder.layers.26.self_attn.v_proj.g_idx', 'vpm.encoder.layers.26.self_attn.v_proj.qweight', 'vpm.encoder.layers.26.self_attn.v_proj.qzeros', 'vpm.encoder.layers.26.self_attn.v_proj.scales', 'vpm.encoder.layers.3.mlp.fc1.g_idx', 'vpm.encoder.layers.3.mlp.fc1.qweight', 'vpm.encoder.layers.3.mlp.fc1.qzeros', 'vpm.encoder.layers.3.mlp.fc1.scales', 'vpm.encoder.layers.3.mlp.fc2.g_idx', 'vpm.encoder.layers.3.mlp.fc2.qweight', 'vpm.encoder.layers.3.mlp.fc2.qzeros', 'vpm.encoder.layers.3.mlp.fc2.scales', 'vpm.encoder.layers.3.self_attn.k_proj.g_idx', 'vpm.encoder.layers.3.self_attn.k_proj.qweight', 'vpm.encoder.layers.3.self_attn.k_proj.qzeros', 'vpm.encoder.layers.3.self_attn.k_proj.scales', 'vpm.encoder.layers.3.self_attn.out_proj.g_idx', 'vpm.encoder.layers.3.self_attn.out_proj.qweight', 'vpm.encoder.layers.3.self_attn.out_proj.qzeros', 'vpm.encoder.layers.3.self_attn.out_proj.scales', 'vpm.encoder.layers.3.self_attn.q_proj.g_idx', 'vpm.encoder.layers.3.self_attn.q_proj.qweight', 'vpm.encoder.layers.3.self_attn.q_proj.qzeros', 'vpm.encoder.layers.3.self_attn.q_proj.scales', 'vpm.encoder.layers.3.self_attn.v_proj.g_idx', 'vpm.encoder.layers.3.self_attn.v_proj.qweight', 'vpm.encoder.layers.3.self_attn.v_proj.qzeros', 'vpm.encoder.layers.3.self_attn.v_proj.scales', 'vpm.encoder.layers.4.mlp.fc1.g_idx', 'vpm.encoder.layers.4.mlp.fc1.qweight', 'vpm.encoder.layers.4.mlp.fc1.qzeros', 'vpm.encoder.layers.4.mlp.fc1.scales', 'vpm.encoder.layers.4.mlp.fc2.g_idx', 'vpm.encoder.layers.4.mlp.fc2.qweight', 'vpm.encoder.layers.4.mlp.fc2.qzeros', 'vpm.encoder.layers.4.mlp.fc2.scales', 'vpm.encoder.layers.4.self_attn.k_proj.g_idx', 'vpm.encoder.layers.4.self_attn.k_proj.qweight', 'vpm.encoder.layers.4.self_attn.k_proj.qzeros', 'vpm.encoder.layers.4.self_attn.k_proj.scales', 'vpm.encoder.layers.4.self_attn.out_proj.g_idx', 'vpm.encoder.layers.4.self_attn.out_proj.qweight', 'vpm.encoder.layers.4.self_attn.out_proj.qzeros', 'vpm.encoder.layers.4.self_attn.out_proj.scales', 'vpm.encoder.layers.4.self_attn.q_proj.g_idx', 'vpm.encoder.layers.4.self_attn.q_proj.qweight', 'vpm.encoder.layers.4.self_attn.q_proj.qzeros', 'vpm.encoder.layers.4.self_attn.q_proj.scales', 'vpm.encoder.layers.4.self_attn.v_proj.g_idx', 'vpm.encoder.layers.4.self_attn.v_proj.qweight', 'vpm.encoder.layers.4.self_attn.v_proj.qzeros', 'vpm.encoder.layers.4.self_attn.v_proj.scales', 'vpm.encoder.layers.5.mlp.fc1.g_idx', 'vpm.encoder.layers.5.mlp.fc1.qweight', 'vpm.encoder.layers.5.mlp.fc1.qzeros', 'vpm.encoder.layers.5.mlp.fc1.scales', 'vpm.encoder.layers.5.mlp.fc2.g_idx', 'vpm.encoder.layers.5.mlp.fc2.qweight', 'vpm.encoder.layers.5.mlp.fc2.qzeros', 'vpm.encoder.layers.5.mlp.fc2.scales', 'vpm.encoder.layers.5.self_attn.k_proj.g_idx', 'vpm.encoder.layers.5.self_attn.k_proj.qweight', 'vpm.encoder.layers.5.self_attn.k_proj.qzeros', 'vpm.encoder.layers.5.self_attn.k_proj.scales', 'vpm.encoder.layers.5.self_attn.out_proj.g_idx', 'vpm.encoder.layers.5.self_attn.out_proj.qweight', 'vpm.encoder.layers.5.self_attn.out_proj.qzeros', 'vpm.encoder.layers.5.self_attn.out_proj.scales', 'vpm.encoder.layers.5.self_attn.q_proj.g_idx', 'vpm.encoder.layers.5.self_attn.q_proj.qweight', 'vpm.encoder.layers.5.self_attn.q_proj.qzeros', 'vpm.encoder.layers.5.self_attn.q_proj.scales', 'vpm.encoder.layers.5.self_attn.v_proj.g_idx', 'vpm.encoder.layers.5.self_attn.v_proj.qweight', 'vpm.encoder.layers.5.self_attn.v_proj.qzeros', 'vpm.encoder.layers.5.self_attn.v_proj.scales', 'vpm.encoder.layers.6.mlp.fc1.g_idx', 'vpm.encoder.layers.6.mlp.fc1.qweight', 'vpm.encoder.layers.6.mlp.fc1.qzeros', 'vpm.encoder.layers.6.mlp.fc1.scales', 'vpm.encoder.layers.6.mlp.fc2.g_idx', 'vpm.encoder.layers.6.mlp.fc2.qweight', 'vpm.encoder.layers.6.mlp.fc2.qzeros', 'vpm.encoder.layers.6.mlp.fc2.scales', 'vpm.encoder.layers.6.self_attn.k_proj.g_idx', 'vpm.encoder.layers.6.self_attn.k_proj.qweight', 'vpm.encoder.layers.6.self_attn.k_proj.qzeros', 'vpm.encoder.layers.6.self_attn.k_proj.scales', 'vpm.encoder.layers.6.self_attn.out_proj.g_idx', 'vpm.encoder.layers.6.self_attn.out_proj.qweight', 'vpm.encoder.layers.6.self_attn.out_proj.qzeros', 'vpm.encoder.layers.6.self_attn.out_proj.scales', 'vpm.encoder.layers.6.self_attn.q_proj.g_idx', 'vpm.encoder.layers.6.self_attn.q_proj.qweight', 'vpm.encoder.layers.6.self_attn.q_proj.qzeros', 'vpm.encoder.layers.6.self_attn.q_proj.scales', 'vpm.encoder.layers.6.self_attn.v_proj.g_idx', 'vpm.encoder.layers.6.self_attn.v_proj.qweight', 'vpm.encoder.layers.6.self_attn.v_proj.qzeros', 'vpm.encoder.layers.6.self_attn.v_proj.scales', 'vpm.encoder.layers.7.mlp.fc1.g_idx', 'vpm.encoder.layers.7.mlp.fc1.qweight', 'vpm.encoder.layers.7.mlp.fc1.qzeros', 'vpm.encoder.layers.7.mlp.fc1.scales', 'vpm.encoder.layers.7.mlp.fc2.g_idx', 'vpm.encoder.layers.7.mlp.fc2.qweight', 'vpm.encoder.layers.7.mlp.fc2.qzeros', 'vpm.encoder.layers.7.mlp.fc2.scales', 'vpm.encoder.layers.7.self_attn.k_proj.g_idx', 'vpm.encoder.layers.7.self_attn.k_proj.qweight', 'vpm.encoder.layers.7.self_attn.k_proj.qzeros', 'vpm.encoder.layers.7.self_attn.k_proj.scales', 'vpm.encoder.layers.7.self_attn.out_proj.g_idx', 'vpm.encoder.layers.7.self_attn.out_proj.qweight', 'vpm.encoder.layers.7.self_attn.out_proj.qzeros', 'vpm.encoder.layers.7.self_attn.out_proj.scales', 'vpm.encoder.layers.7.self_attn.q_proj.g_idx', 'vpm.encoder.layers.7.self_attn.q_proj.qweight', 'vpm.encoder.layers.7.self_attn.q_proj.qzeros', 'vpm.encoder.layers.7.self_attn.q_proj.scales', 'vpm.encoder.layers.7.self_attn.v_proj.g_idx', 'vpm.encoder.layers.7.self_attn.v_proj.qweight', 'vpm.encoder.layers.7.self_attn.v_proj.qzeros', 'vpm.encoder.layers.7.self_attn.v_proj.scales', 'vpm.encoder.layers.8.mlp.fc1.g_idx', 'vpm.encoder.layers.8.mlp.fc1.qweight', 'vpm.encoder.layers.8.mlp.fc1.qzeros', 'vpm.encoder.layers.8.mlp.fc1.scales', 'vpm.encoder.layers.8.mlp.fc2.g_idx', 'vpm.encoder.layers.8.mlp.fc2.qweight', 'vpm.encoder.layers.8.mlp.fc2.qzeros', 'vpm.encoder.layers.8.mlp.fc2.scales', 'vpm.encoder.layers.8.self_attn.k_proj.g_idx', 'vpm.encoder.layers.8.self_attn.k_proj.qweight', 'vpm.encoder.layers.8.self_attn.k_proj.qzeros', 'vpm.encoder.layers.8.self_attn.k_proj.scales', 'vpm.encoder.layers.8.self_attn.out_proj.g_idx', 'vpm.encoder.layers.8.self_attn.out_proj.qweight', 'vpm.encoder.layers.8.self_attn.out_proj.qzeros', 'vpm.encoder.layers.8.self_attn.out_proj.scales', 'vpm.encoder.layers.8.self_attn.q_proj.g_idx', 'vpm.encoder.layers.8.self_attn.q_proj.qweight', 'vpm.encoder.layers.8.self_attn.q_proj.qzeros', 'vpm.encoder.layers.8.self_attn.q_proj.scales', 'vpm.encoder.layers.8.self_attn.v_proj.g_idx', 'vpm.encoder.layers.8.self_attn.v_proj.qweight', 'vpm.encoder.layers.8.self_attn.v_proj.qzeros', 'vpm.encoder.layers.8.self_attn.v_proj.scales', 'vpm.encoder.layers.9.mlp.fc1.g_idx', 'vpm.encoder.layers.9.mlp.fc1.qweight', 'vpm.encoder.layers.9.mlp.fc1.qzeros', 'vpm.encoder.layers.9.mlp.fc1.scales', 'vpm.encoder.layers.9.mlp.fc2.g_idx', 'vpm.encoder.layers.9.mlp.fc2.qweight', 'vpm.encoder.layers.9.mlp.fc2.qzeros', 'vpm.encoder.layers.9.mlp.fc2.scales', 'vpm.encoder.layers.9.self_attn.k_proj.g_idx', 'vpm.encoder.layers.9.self_attn.k_proj.qweight', 'vpm.encoder.layers.9.self_attn.k_proj.qzeros', 'vpm.encoder.layers.9.self_attn.k_proj.scales', 'vpm.encoder.layers.9.self_attn.out_proj.g_idx', 'vpm.encoder.layers.9.self_attn.out_proj.qweight', 'vpm.encoder.layers.9.self_attn.out_proj.qzeros', 'vpm.encoder.layers.9.self_attn.out_proj.scales', 'vpm.encoder.layers.9.self_attn.q_proj.g_idx', 'vpm.encoder.layers.9.self_attn.q_proj.qweight', 'vpm.encoder.layers.9.self_attn.q_proj.qzeros', 'vpm.encoder.layers.9.self_attn.q_proj.scales', 'vpm.encoder.layers.9.self_attn.v_proj.g_idx', 'vpm.encoder.layers.9.self_attn.v_proj.qweight', 'vpm.encoder.layers.9.self_attn.v_proj.qzeros', 'vpm.encoder.layers.9.self_attn.v_proj.scales']\n",
      "- This IS expected if you are initializing MiniCPMV from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MiniCPMV from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MiniCPMV were not initialized from the model checkpoint at /home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256-llm-w4-pc and are newly initialized: ['llm.model.layers.0.mlp.down_proj.weight', 'llm.model.layers.0.mlp.gate_proj.weight', 'llm.model.layers.0.mlp.up_proj.weight', 'llm.model.layers.0.self_attn.k_proj.weight', 'llm.model.layers.0.self_attn.v_proj.weight', 'llm.model.layers.1.mlp.down_proj.weight', 'llm.model.layers.1.mlp.gate_proj.weight', 'llm.model.layers.1.mlp.up_proj.weight', 'llm.model.layers.1.self_attn.k_proj.weight', 'llm.model.layers.1.self_attn.v_proj.weight', 'llm.model.layers.10.mlp.down_proj.weight', 'llm.model.layers.10.mlp.gate_proj.weight', 'llm.model.layers.10.mlp.up_proj.weight', 'llm.model.layers.10.self_attn.k_proj.weight', 'llm.model.layers.10.self_attn.v_proj.weight', 'llm.model.layers.11.mlp.down_proj.weight', 'llm.model.layers.11.mlp.gate_proj.weight', 'llm.model.layers.11.mlp.up_proj.weight', 'llm.model.layers.11.self_attn.k_proj.weight', 'llm.model.layers.11.self_attn.v_proj.weight', 'llm.model.layers.12.mlp.down_proj.weight', 'llm.model.layers.12.mlp.gate_proj.weight', 'llm.model.layers.12.mlp.up_proj.weight', 'llm.model.layers.12.self_attn.k_proj.weight', 'llm.model.layers.12.self_attn.v_proj.weight', 'llm.model.layers.13.mlp.down_proj.weight', 'llm.model.layers.13.mlp.gate_proj.weight', 'llm.model.layers.13.mlp.up_proj.weight', 'llm.model.layers.13.self_attn.k_proj.weight', 'llm.model.layers.13.self_attn.v_proj.weight', 'llm.model.layers.14.mlp.down_proj.weight', 'llm.model.layers.14.mlp.gate_proj.weight', 'llm.model.layers.14.mlp.up_proj.weight', 'llm.model.layers.14.self_attn.k_proj.weight', 'llm.model.layers.14.self_attn.v_proj.weight', 'llm.model.layers.15.mlp.down_proj.weight', 'llm.model.layers.15.mlp.gate_proj.weight', 'llm.model.layers.15.mlp.up_proj.weight', 'llm.model.layers.15.self_attn.k_proj.weight', 'llm.model.layers.15.self_attn.v_proj.weight', 'llm.model.layers.16.mlp.down_proj.weight', 'llm.model.layers.16.mlp.gate_proj.weight', 'llm.model.layers.16.mlp.up_proj.weight', 'llm.model.layers.16.self_attn.k_proj.weight', 'llm.model.layers.16.self_attn.v_proj.weight', 'llm.model.layers.17.mlp.down_proj.weight', 'llm.model.layers.17.mlp.gate_proj.weight', 'llm.model.layers.17.mlp.up_proj.weight', 'llm.model.layers.17.self_attn.k_proj.weight', 'llm.model.layers.17.self_attn.v_proj.weight', 'llm.model.layers.18.mlp.down_proj.weight', 'llm.model.layers.18.mlp.gate_proj.weight', 'llm.model.layers.18.mlp.up_proj.weight', 'llm.model.layers.18.self_attn.k_proj.weight', 'llm.model.layers.18.self_attn.v_proj.weight', 'llm.model.layers.19.mlp.down_proj.weight', 'llm.model.layers.19.mlp.gate_proj.weight', 'llm.model.layers.19.mlp.up_proj.weight', 'llm.model.layers.19.self_attn.k_proj.weight', 'llm.model.layers.19.self_attn.v_proj.weight', 'llm.model.layers.2.mlp.down_proj.weight', 'llm.model.layers.2.mlp.gate_proj.weight', 'llm.model.layers.2.mlp.up_proj.weight', 'llm.model.layers.2.self_attn.k_proj.weight', 'llm.model.layers.2.self_attn.v_proj.weight', 'llm.model.layers.20.mlp.down_proj.weight', 'llm.model.layers.20.mlp.gate_proj.weight', 'llm.model.layers.20.mlp.up_proj.weight', 'llm.model.layers.20.self_attn.k_proj.weight', 'llm.model.layers.20.self_attn.v_proj.weight', 'llm.model.layers.21.mlp.down_proj.weight', 'llm.model.layers.21.mlp.gate_proj.weight', 'llm.model.layers.21.mlp.up_proj.weight', 'llm.model.layers.21.self_attn.k_proj.weight', 'llm.model.layers.21.self_attn.v_proj.weight', 'llm.model.layers.22.mlp.down_proj.weight', 'llm.model.layers.22.mlp.gate_proj.weight', 'llm.model.layers.22.mlp.up_proj.weight', 'llm.model.layers.22.self_attn.k_proj.weight', 'llm.model.layers.22.self_attn.v_proj.weight', 'llm.model.layers.23.mlp.down_proj.weight', 'llm.model.layers.23.mlp.gate_proj.weight', 'llm.model.layers.23.mlp.up_proj.weight', 'llm.model.layers.23.self_attn.k_proj.weight', 'llm.model.layers.23.self_attn.v_proj.weight', 'llm.model.layers.24.mlp.down_proj.weight', 'llm.model.layers.24.mlp.gate_proj.weight', 'llm.model.layers.24.mlp.up_proj.weight', 'llm.model.layers.24.self_attn.k_proj.weight', 'llm.model.layers.24.self_attn.v_proj.weight', 'llm.model.layers.25.mlp.down_proj.weight', 'llm.model.layers.25.mlp.gate_proj.weight', 'llm.model.layers.25.mlp.up_proj.weight', 'llm.model.layers.25.self_attn.k_proj.weight', 'llm.model.layers.25.self_attn.v_proj.weight', 'llm.model.layers.26.mlp.down_proj.weight', 'llm.model.layers.26.mlp.gate_proj.weight', 'llm.model.layers.26.mlp.up_proj.weight', 'llm.model.layers.26.self_attn.k_proj.weight', 'llm.model.layers.26.self_attn.v_proj.weight', 'llm.model.layers.27.mlp.down_proj.weight', 'llm.model.layers.27.mlp.gate_proj.weight', 'llm.model.layers.27.mlp.up_proj.weight', 'llm.model.layers.27.self_attn.k_proj.weight', 'llm.model.layers.27.self_attn.v_proj.weight', 'llm.model.layers.28.mlp.down_proj.weight', 'llm.model.layers.28.mlp.gate_proj.weight', 'llm.model.layers.28.mlp.up_proj.weight', 'llm.model.layers.28.self_attn.k_proj.weight', 'llm.model.layers.28.self_attn.v_proj.weight', 'llm.model.layers.29.mlp.down_proj.weight', 'llm.model.layers.29.mlp.gate_proj.weight', 'llm.model.layers.29.mlp.up_proj.weight', 'llm.model.layers.29.self_attn.k_proj.weight', 'llm.model.layers.29.self_attn.v_proj.weight', 'llm.model.layers.3.mlp.down_proj.weight', 'llm.model.layers.3.mlp.gate_proj.weight', 'llm.model.layers.3.mlp.up_proj.weight', 'llm.model.layers.3.self_attn.k_proj.weight', 'llm.model.layers.3.self_attn.v_proj.weight', 'llm.model.layers.30.mlp.down_proj.weight', 'llm.model.layers.30.mlp.gate_proj.weight', 'llm.model.layers.30.mlp.up_proj.weight', 'llm.model.layers.30.self_attn.k_proj.weight', 'llm.model.layers.30.self_attn.v_proj.weight', 'llm.model.layers.31.mlp.down_proj.weight', 'llm.model.layers.31.mlp.gate_proj.weight', 'llm.model.layers.31.mlp.up_proj.weight', 'llm.model.layers.31.self_attn.k_proj.weight', 'llm.model.layers.31.self_attn.v_proj.weight', 'llm.model.layers.32.mlp.down_proj.weight', 'llm.model.layers.32.mlp.gate_proj.weight', 'llm.model.layers.32.mlp.up_proj.weight', 'llm.model.layers.32.self_attn.k_proj.weight', 'llm.model.layers.32.self_attn.v_proj.weight', 'llm.model.layers.33.mlp.down_proj.weight', 'llm.model.layers.33.mlp.gate_proj.weight', 'llm.model.layers.33.mlp.up_proj.weight', 'llm.model.layers.33.self_attn.k_proj.weight', 'llm.model.layers.33.self_attn.v_proj.weight', 'llm.model.layers.34.mlp.down_proj.weight', 'llm.model.layers.34.mlp.gate_proj.weight', 'llm.model.layers.34.mlp.up_proj.weight', 'llm.model.layers.34.self_attn.k_proj.weight', 'llm.model.layers.34.self_attn.v_proj.weight', 'llm.model.layers.35.mlp.down_proj.weight', 'llm.model.layers.35.mlp.gate_proj.weight', 'llm.model.layers.35.mlp.up_proj.weight', 'llm.model.layers.35.self_attn.k_proj.weight', 'llm.model.layers.35.self_attn.v_proj.weight', 'llm.model.layers.36.mlp.down_proj.weight', 'llm.model.layers.36.mlp.gate_proj.weight', 'llm.model.layers.36.mlp.up_proj.weight', 'llm.model.layers.36.self_attn.k_proj.weight', 'llm.model.layers.36.self_attn.v_proj.weight', 'llm.model.layers.37.mlp.down_proj.weight', 'llm.model.layers.37.mlp.gate_proj.weight', 'llm.model.layers.37.mlp.up_proj.weight', 'llm.model.layers.37.self_attn.k_proj.weight', 'llm.model.layers.37.self_attn.v_proj.weight', 'llm.model.layers.38.mlp.down_proj.weight', 'llm.model.layers.38.mlp.gate_proj.weight', 'llm.model.layers.38.mlp.up_proj.weight', 'llm.model.layers.38.self_attn.k_proj.weight', 'llm.model.layers.38.self_attn.v_proj.weight', 'llm.model.layers.39.mlp.down_proj.weight', 'llm.model.layers.39.mlp.gate_proj.weight', 'llm.model.layers.39.mlp.up_proj.weight', 'llm.model.layers.39.self_attn.k_proj.weight', 'llm.model.layers.39.self_attn.v_proj.weight', 'llm.model.layers.4.mlp.down_proj.weight', 'llm.model.layers.4.mlp.gate_proj.weight', 'llm.model.layers.4.mlp.up_proj.weight', 'llm.model.layers.4.self_attn.k_proj.weight', 'llm.model.layers.4.self_attn.v_proj.weight', 'llm.model.layers.40.mlp.down_proj.weight', 'llm.model.layers.40.mlp.gate_proj.weight', 'llm.model.layers.40.mlp.up_proj.weight', 'llm.model.layers.40.self_attn.k_proj.weight', 'llm.model.layers.40.self_attn.v_proj.weight', 'llm.model.layers.41.mlp.down_proj.weight', 'llm.model.layers.41.mlp.gate_proj.weight', 'llm.model.layers.41.mlp.up_proj.weight', 'llm.model.layers.41.self_attn.k_proj.weight', 'llm.model.layers.41.self_attn.v_proj.weight', 'llm.model.layers.42.mlp.down_proj.weight', 'llm.model.layers.42.mlp.gate_proj.weight', 'llm.model.layers.42.mlp.up_proj.weight', 'llm.model.layers.42.self_attn.k_proj.weight', 'llm.model.layers.42.self_attn.v_proj.weight', 'llm.model.layers.43.mlp.down_proj.weight', 'llm.model.layers.43.mlp.gate_proj.weight', 'llm.model.layers.43.mlp.up_proj.weight', 'llm.model.layers.43.self_attn.k_proj.weight', 'llm.model.layers.43.self_attn.v_proj.weight', 'llm.model.layers.44.mlp.down_proj.weight', 'llm.model.layers.44.mlp.gate_proj.weight', 'llm.model.layers.44.mlp.up_proj.weight', 'llm.model.layers.44.self_attn.k_proj.weight', 'llm.model.layers.44.self_attn.v_proj.weight', 'llm.model.layers.45.mlp.down_proj.weight', 'llm.model.layers.45.mlp.gate_proj.weight', 'llm.model.layers.45.mlp.up_proj.weight', 'llm.model.layers.45.self_attn.k_proj.weight', 'llm.model.layers.45.self_attn.v_proj.weight', 'llm.model.layers.46.mlp.down_proj.weight', 'llm.model.layers.46.mlp.gate_proj.weight', 'llm.model.layers.46.mlp.up_proj.weight', 'llm.model.layers.46.self_attn.k_proj.weight', 'llm.model.layers.46.self_attn.v_proj.weight', 'llm.model.layers.47.mlp.down_proj.weight', 'llm.model.layers.47.mlp.gate_proj.weight', 'llm.model.layers.47.mlp.up_proj.weight', 'llm.model.layers.47.self_attn.k_proj.weight', 'llm.model.layers.47.self_attn.v_proj.weight', 'llm.model.layers.48.mlp.down_proj.weight', 'llm.model.layers.48.mlp.gate_proj.weight', 'llm.model.layers.48.mlp.up_proj.weight', 'llm.model.layers.48.self_attn.k_proj.weight', 'llm.model.layers.48.self_attn.v_proj.weight', 'llm.model.layers.49.mlp.down_proj.weight', 'llm.model.layers.49.mlp.gate_proj.weight', 'llm.model.layers.49.mlp.up_proj.weight', 'llm.model.layers.49.self_attn.k_proj.weight', 'llm.model.layers.49.self_attn.v_proj.weight', 'llm.model.layers.5.mlp.down_proj.weight', 'llm.model.layers.5.mlp.gate_proj.weight', 'llm.model.layers.5.mlp.up_proj.weight', 'llm.model.layers.5.self_attn.k_proj.weight', 'llm.model.layers.5.self_attn.v_proj.weight', 'llm.model.layers.50.mlp.down_proj.weight', 'llm.model.layers.50.mlp.gate_proj.weight', 'llm.model.layers.50.mlp.up_proj.weight', 'llm.model.layers.50.self_attn.k_proj.weight', 'llm.model.layers.50.self_attn.v_proj.weight', 'llm.model.layers.51.mlp.down_proj.weight', 'llm.model.layers.51.mlp.gate_proj.weight', 'llm.model.layers.51.mlp.up_proj.weight', 'llm.model.layers.51.self_attn.k_proj.weight', 'llm.model.layers.51.self_attn.v_proj.weight', 'llm.model.layers.6.mlp.down_proj.weight', 'llm.model.layers.6.mlp.gate_proj.weight', 'llm.model.layers.6.mlp.up_proj.weight', 'llm.model.layers.6.self_attn.k_proj.weight', 'llm.model.layers.6.self_attn.v_proj.weight', 'llm.model.layers.7.mlp.down_proj.weight', 'llm.model.layers.7.mlp.gate_proj.weight', 'llm.model.layers.7.mlp.up_proj.weight', 'llm.model.layers.7.self_attn.k_proj.weight', 'llm.model.layers.7.self_attn.v_proj.weight', 'llm.model.layers.8.mlp.down_proj.weight', 'llm.model.layers.8.mlp.gate_proj.weight', 'llm.model.layers.8.mlp.up_proj.weight', 'llm.model.layers.8.self_attn.k_proj.weight', 'llm.model.layers.8.self_attn.v_proj.weight', 'llm.model.layers.9.mlp.down_proj.weight', 'llm.model.layers.9.mlp.gate_proj.weight', 'llm.model.layers.9.mlp.up_proj.weight', 'llm.model.layers.9.self_attn.k_proj.weight', 'llm.model.layers.9.self_attn.v_proj.weight', 'vpm.encoder.layers.0.mlp.fc1.weight', 'vpm.encoder.layers.0.mlp.fc2.weight', 'vpm.encoder.layers.0.self_attn.k_proj.weight', 'vpm.encoder.layers.0.self_attn.out_proj.weight', 'vpm.encoder.layers.0.self_attn.q_proj.weight', 'vpm.encoder.layers.0.self_attn.v_proj.weight', 'vpm.encoder.layers.1.mlp.fc1.weight', 'vpm.encoder.layers.1.mlp.fc2.weight', 'vpm.encoder.layers.1.self_attn.k_proj.weight', 'vpm.encoder.layers.1.self_attn.out_proj.weight', 'vpm.encoder.layers.1.self_attn.q_proj.weight', 'vpm.encoder.layers.1.self_attn.v_proj.weight', 'vpm.encoder.layers.10.mlp.fc1.weight', 'vpm.encoder.layers.10.mlp.fc2.weight', 'vpm.encoder.layers.10.self_attn.k_proj.weight', 'vpm.encoder.layers.10.self_attn.out_proj.weight', 'vpm.encoder.layers.10.self_attn.q_proj.weight', 'vpm.encoder.layers.10.self_attn.v_proj.weight', 'vpm.encoder.layers.11.mlp.fc1.weight', 'vpm.encoder.layers.11.mlp.fc2.weight', 'vpm.encoder.layers.11.self_attn.k_proj.weight', 'vpm.encoder.layers.11.self_attn.out_proj.weight', 'vpm.encoder.layers.11.self_attn.q_proj.weight', 'vpm.encoder.layers.11.self_attn.v_proj.weight', 'vpm.encoder.layers.12.mlp.fc1.weight', 'vpm.encoder.layers.12.mlp.fc2.weight', 'vpm.encoder.layers.12.self_attn.k_proj.weight', 'vpm.encoder.layers.12.self_attn.out_proj.weight', 'vpm.encoder.layers.12.self_attn.q_proj.weight', 'vpm.encoder.layers.12.self_attn.v_proj.weight', 'vpm.encoder.layers.13.mlp.fc1.weight', 'vpm.encoder.layers.13.mlp.fc2.weight', 'vpm.encoder.layers.13.self_attn.k_proj.weight', 'vpm.encoder.layers.13.self_attn.out_proj.weight', 'vpm.encoder.layers.13.self_attn.q_proj.weight', 'vpm.encoder.layers.13.self_attn.v_proj.weight', 'vpm.encoder.layers.14.mlp.fc1.weight', 'vpm.encoder.layers.14.mlp.fc2.weight', 'vpm.encoder.layers.14.self_attn.k_proj.weight', 'vpm.encoder.layers.14.self_attn.out_proj.weight', 'vpm.encoder.layers.14.self_attn.q_proj.weight', 'vpm.encoder.layers.14.self_attn.v_proj.weight', 'vpm.encoder.layers.15.mlp.fc1.weight', 'vpm.encoder.layers.15.mlp.fc2.weight', 'vpm.encoder.layers.15.self_attn.k_proj.weight', 'vpm.encoder.layers.15.self_attn.out_proj.weight', 'vpm.encoder.layers.15.self_attn.q_proj.weight', 'vpm.encoder.layers.15.self_attn.v_proj.weight', 'vpm.encoder.layers.16.mlp.fc1.weight', 'vpm.encoder.layers.16.mlp.fc2.weight', 'vpm.encoder.layers.16.self_attn.k_proj.weight', 'vpm.encoder.layers.16.self_attn.out_proj.weight', 'vpm.encoder.layers.16.self_attn.q_proj.weight', 'vpm.encoder.layers.16.self_attn.v_proj.weight', 'vpm.encoder.layers.17.mlp.fc1.weight', 'vpm.encoder.layers.17.mlp.fc2.weight', 'vpm.encoder.layers.17.self_attn.k_proj.weight', 'vpm.encoder.layers.17.self_attn.out_proj.weight', 'vpm.encoder.layers.17.self_attn.q_proj.weight', 'vpm.encoder.layers.17.self_attn.v_proj.weight', 'vpm.encoder.layers.18.mlp.fc1.weight', 'vpm.encoder.layers.18.mlp.fc2.weight', 'vpm.encoder.layers.18.self_attn.k_proj.weight', 'vpm.encoder.layers.18.self_attn.out_proj.weight', 'vpm.encoder.layers.18.self_attn.q_proj.weight', 'vpm.encoder.layers.18.self_attn.v_proj.weight', 'vpm.encoder.layers.19.mlp.fc1.weight', 'vpm.encoder.layers.19.mlp.fc2.weight', 'vpm.encoder.layers.19.self_attn.k_proj.weight', 'vpm.encoder.layers.19.self_attn.out_proj.weight', 'vpm.encoder.layers.19.self_attn.q_proj.weight', 'vpm.encoder.layers.19.self_attn.v_proj.weight', 'vpm.encoder.layers.2.mlp.fc1.weight', 'vpm.encoder.layers.2.mlp.fc2.weight', 'vpm.encoder.layers.2.self_attn.k_proj.weight', 'vpm.encoder.layers.2.self_attn.out_proj.weight', 'vpm.encoder.layers.2.self_attn.q_proj.weight', 'vpm.encoder.layers.2.self_attn.v_proj.weight', 'vpm.encoder.layers.20.mlp.fc1.weight', 'vpm.encoder.layers.20.mlp.fc2.weight', 'vpm.encoder.layers.20.self_attn.k_proj.weight', 'vpm.encoder.layers.20.self_attn.out_proj.weight', 'vpm.encoder.layers.20.self_attn.q_proj.weight', 'vpm.encoder.layers.20.self_attn.v_proj.weight', 'vpm.encoder.layers.21.mlp.fc1.weight', 'vpm.encoder.layers.21.mlp.fc2.weight', 'vpm.encoder.layers.21.self_attn.k_proj.weight', 'vpm.encoder.layers.21.self_attn.out_proj.weight', 'vpm.encoder.layers.21.self_attn.q_proj.weight', 'vpm.encoder.layers.21.self_attn.v_proj.weight', 'vpm.encoder.layers.22.mlp.fc1.weight', 'vpm.encoder.layers.22.mlp.fc2.weight', 'vpm.encoder.layers.22.self_attn.k_proj.weight', 'vpm.encoder.layers.22.self_attn.out_proj.weight', 'vpm.encoder.layers.22.self_attn.q_proj.weight', 'vpm.encoder.layers.22.self_attn.v_proj.weight', 'vpm.encoder.layers.23.mlp.fc1.weight', 'vpm.encoder.layers.23.mlp.fc2.weight', 'vpm.encoder.layers.23.self_attn.k_proj.weight', 'vpm.encoder.layers.23.self_attn.out_proj.weight', 'vpm.encoder.layers.23.self_attn.q_proj.weight', 'vpm.encoder.layers.23.self_attn.v_proj.weight', 'vpm.encoder.layers.24.mlp.fc1.weight', 'vpm.encoder.layers.24.mlp.fc2.weight', 'vpm.encoder.layers.24.self_attn.k_proj.weight', 'vpm.encoder.layers.24.self_attn.out_proj.weight', 'vpm.encoder.layers.24.self_attn.q_proj.weight', 'vpm.encoder.layers.24.self_attn.v_proj.weight', 'vpm.encoder.layers.25.mlp.fc1.weight', 'vpm.encoder.layers.25.mlp.fc2.weight', 'vpm.encoder.layers.25.self_attn.k_proj.weight', 'vpm.encoder.layers.25.self_attn.out_proj.weight', 'vpm.encoder.layers.25.self_attn.q_proj.weight', 'vpm.encoder.layers.25.self_attn.v_proj.weight', 'vpm.encoder.layers.26.mlp.fc1.weight', 'vpm.encoder.layers.26.mlp.fc2.weight', 'vpm.encoder.layers.26.self_attn.k_proj.weight', 'vpm.encoder.layers.26.self_attn.out_proj.weight', 'vpm.encoder.layers.26.self_attn.q_proj.weight', 'vpm.encoder.layers.26.self_attn.v_proj.weight', 'vpm.encoder.layers.3.mlp.fc1.weight', 'vpm.encoder.layers.3.mlp.fc2.weight', 'vpm.encoder.layers.3.self_attn.k_proj.weight', 'vpm.encoder.layers.3.self_attn.out_proj.weight', 'vpm.encoder.layers.3.self_attn.q_proj.weight', 'vpm.encoder.layers.3.self_attn.v_proj.weight', 'vpm.encoder.layers.4.mlp.fc1.weight', 'vpm.encoder.layers.4.mlp.fc2.weight', 'vpm.encoder.layers.4.self_attn.k_proj.weight', 'vpm.encoder.layers.4.self_attn.out_proj.weight', 'vpm.encoder.layers.4.self_attn.q_proj.weight', 'vpm.encoder.layers.4.self_attn.v_proj.weight', 'vpm.encoder.layers.5.mlp.fc1.weight', 'vpm.encoder.layers.5.mlp.fc2.weight', 'vpm.encoder.layers.5.self_attn.k_proj.weight', 'vpm.encoder.layers.5.self_attn.out_proj.weight', 'vpm.encoder.layers.5.self_attn.q_proj.weight', 'vpm.encoder.layers.5.self_attn.v_proj.weight', 'vpm.encoder.layers.6.mlp.fc1.weight', 'vpm.encoder.layers.6.mlp.fc2.weight', 'vpm.encoder.layers.6.self_attn.k_proj.weight', 'vpm.encoder.layers.6.self_attn.out_proj.weight', 'vpm.encoder.layers.6.self_attn.q_proj.weight', 'vpm.encoder.layers.6.self_attn.v_proj.weight', 'vpm.encoder.layers.7.mlp.fc1.weight', 'vpm.encoder.layers.7.mlp.fc2.weight', 'vpm.encoder.layers.7.self_attn.k_proj.weight', 'vpm.encoder.layers.7.self_attn.out_proj.weight', 'vpm.encoder.layers.7.self_attn.q_proj.weight', 'vpm.encoder.layers.7.self_attn.v_proj.weight', 'vpm.encoder.layers.8.mlp.fc1.weight', 'vpm.encoder.layers.8.mlp.fc2.weight', 'vpm.encoder.layers.8.self_attn.k_proj.weight', 'vpm.encoder.layers.8.self_attn.out_proj.weight', 'vpm.encoder.layers.8.self_attn.q_proj.weight', 'vpm.encoder.layers.8.self_attn.v_proj.weight', 'vpm.encoder.layers.9.mlp.fc1.weight', 'vpm.encoder.layers.9.mlp.fc2.weight', 'vpm.encoder.layers.9.self_attn.k_proj.weight', 'vpm.encoder.layers.9.self_attn.out_proj.weight', 'vpm.encoder.layers.9.self_attn.q_proj.weight', 'vpm.encoder.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO - The layer llm.lm_head is not quantized.\n",
      "INFO - The layer vpm.embeddings.patch_embedding is not quantized.\n",
      "INFO - The layer resampler.kv_proj is not quantized.\n",
      "INFO - The layer resampler.attn.out_proj is not quantized.\n",
      "                                                                                                               \r"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalMLM, BaseQuantizeConfig\n",
    "from auto_gptq import AutoGPTQForVIT, BaseQuantizeConfig\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,  # quantize model to 8-bit\n",
    "    group_size=-1,  # it is recommended to set the value to -1\n",
    "    desc_act=True,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    ")\n",
    "quantized_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256-llm-w4-pc\"\n",
    "# quantized_model_dir = \"/home/workspace/model/m inicpm-3o-sft-v1-gptq-1112\"\n",
    "\n",
    "model = AutoGPTQForCausalMLM.from_quantized(quantized_model_dir, quantize_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vpm.encoder.layers[0].self_attn.k_proj.qweight\n",
    "model.vpm.encoder.layers[0].self_attn.k_proj.bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.llm.model.layers[0].self_attn.k_proj.qweight\n",
    "model.llm.model.layers[0].self_attn.k_proj.bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniCPMVGPTQ_Llama3(\n",
       "  (model): MiniCPMV(\n",
       "    (llm): MiniCPMForCausalLM(\n",
       "      (model): MiniCPMModel(\n",
       "        (embed_tokens): Embedding(73464, 1536)\n",
       "        (layers): ModuleList(\n",
       "          (0-51): 52 x MiniCPMDecoderLayer(\n",
       "            (self_attn): MiniCPMAttention(\n",
       "              (q_proj): QuantLinear()\n",
       "              (k_proj): QuantLinear()\n",
       "              (v_proj): QuantLinear()\n",
       "              (o_proj): QuantLinear()\n",
       "              (rotary_emb): MiniCPMLongRoPE()\n",
       "            )\n",
       "            (mlp): MiniCPMMLP(\n",
       "              (gate_proj): QuantLinear()\n",
       "              (up_proj): QuantLinear()\n",
       "              (down_proj): QuantLinear()\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MiniCPMRMSNorm()\n",
       "            (post_attention_layernorm): MiniCPMRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MiniCPMRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=73464, bias=False)\n",
       "    )\n",
       "    (vpm): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(4900, 1152)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x SiglipEncoderLayer(\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): QuantLinear()\n",
       "              (v_proj): QuantLinear()\n",
       "              (q_proj): QuantLinear()\n",
       "              (out_proj): QuantLinear()\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): QuantLinear()\n",
       "              (fc2): QuantLinear()\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (resampler): Resampler(\n",
       "      (kv_proj): Linear(in_features=1152, out_features=1536, bias=False)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      )\n",
       "      (ln_q): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (ln_kv): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (ln_post): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoProcessor\n",
    "pretrained_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1\"\n",
    "model.model.processor = AutoProcessor.from_pretrained(pretrained_model_dir, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这张图片展示了一架商用客机，很可能是一架大型的空中客车（A380）。 \n",
      "\n",
      "1. **飞机类型**：\n",
      "   - 这是A380，是空中客车公司生产的全球最大客机之一，也是世界上最大的商用喷气式飞机。\n",
      "   - A380拥有4个引擎，提供了卓越的巡航速度和效率。\n",
      "\n",
      "2. **航空公司**：\n",
      "   - 虽然图片本身并没有提供明确的航空公司标志或名称，但通常这种飞机会属于像东方航空这样的航空公司，该航空公司在亚洲地区有广泛的运营。\n",
      "\n",
      "3. **设计和细节**：\n",
      "   - 飞机涂成了蓝白相间的色彩，并配有独特的标志，这是东方航空的标志性设计。\n",
      "   - 飞机上有“Airbus A380-800”的字样，进一步确认了它是A380型号。\n",
      "\n",
      "4. **飞行状态**：\n",
      "   - 从照片中可以看出，飞机处于起飞阶段，这意味着它刚刚离开地面准备起飞。\n",
      "\n",
      "5. **背景**：\n",
      "   - 飞机在晴朗的蓝天中飞行，表明它处于高空且天气条件良好。\n",
      "\n",
      "总结，这是一张东方航空的大型空中客车A380商用客机的照片，它处于起飞阶段，展示了其标志性的设计和细节。\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image = Image.open('/home/workspace/code/llm-awq/awq/airplane.jpeg').convert('RGB')\n",
    "\n",
    "# First round chat \n",
    "question = \"这是什么飞机\"\n",
    "msgs = [{'role': 'user', 'content': [image, question]}]\n",
    "\n",
    "answer = model.model.chat(\n",
    "    image=None,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully selected and copied 64 images to /home/workspace/dataset/imagenet/calibration.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# 定义源数据集路径和目标保存路径\n",
    "imagenet_train_dir = '/home/workspace/dataset/imagenet/train'  # 替换为你本地 ImageNet 训练集的路径\n",
    "output_dir = '/home/workspace/dataset/imagenet/calibration'  # 替换为你要保存图片的路径\n",
    "num_images_to_select = 64  # 要随机选取的图片数量\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 获取训练集中所有类别文件夹\n",
    "all_images = []\n",
    "for root, _, files in os.walk(imagenet_train_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            all_images.append(os.path.join(root, file))\n",
    "\n",
    "# 随机选取64张图片\n",
    "selected_images = random.sample(all_images, num_images_to_select)\n",
    "\n",
    "# 将选取的图片复制到目标文件夹\n",
    "for img_path in selected_images:\n",
    "    shutil.copy(img_path, output_dir)\n",
    "\n",
    "print(f'Successfully selected and copied {num_images_to_select} images to {output_dir}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIT部分量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_ScienceQA(nsamples, seed, seqlen, processor):\n",
    "    import torch.nn.functional as F\n",
    "    dataset = datasets.load_from_disk(\"/home/workspace/dataset/ScienceQA-2\")[\"train\"]\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "    rng = random.Random(42)\n",
    "    samples, num_tokens = [], 0\n",
    "    prompts_lists = []\n",
    "    input_images_lists = []\n",
    "    for index, _data in enumerate(dataset):\n",
    "        promt = _data[\"question\"]\n",
    "        image_file = _data[\"image\"]\n",
    "        image = np.array(image_file)\n",
    "        if image_file is None:\n",
    "            nsamples = nsamples+1\n",
    "            continue\n",
    "        msgs = [{'role': 'user', 'content': \"(<image>./</image>)\\n\"+ promt}]\n",
    "        prompts_lists.append(processor.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True))\n",
    "        input_images_lists.append([image])\n",
    "        if index >= nsamples-1:\n",
    "            break\n",
    "    # return prompts_lists,input_images_lists\n",
    "     \n",
    "    inputs = processor(\n",
    "        prompts_lists,\n",
    "        input_images_lists,\n",
    "        max_slice_nums=processor.image_processor.max_slice_nums,\n",
    "        use_image_id=processor.image_processor.use_image_id,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=8192\n",
    "    )\n",
    "    # return inputs\n",
    "    traindataset = []\n",
    "    for _ in range(inputs[\"input_ids\"].size(0)):\n",
    "        input_ids = inputs[\"input_ids\"].select(0, _).unsqueeze(0) \n",
    "        attention_mask = inputs[\"attention_mask\"].select(0, _).unsqueeze(0) \n",
    "        pixel_values = inputs[\"pixel_values\"]\n",
    "        image_sizes = inputs[\"image_sizes\"]\n",
    "        image_bound = inputs[\"image_bound\"]\n",
    "        tgt_sizes = inputs[\"tgt_sizes\"]\n",
    "        traindataset.append({\"input_ids\": input_ids, \n",
    "                             \"attention_mask\": attention_mask,\n",
    "                             \"pixel_values\": pixel_values,\n",
    "                             \"image_sizes\": image_sizes,\n",
    "                             \"image_bound\": image_bound,\n",
    "                             \"tgt_sizes\": tgt_sizes})\n",
    "\n",
    "    return traindataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from datasets import load_dataset, load_from_disk\n",
    "# import random\n",
    "# import torch\n",
    "\n",
    "# def get_wikitext2(nsamples, seed, seqlen, tokenizer):\n",
    "#     # set seed\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.random.manual_seed(seed)\n",
    "\n",
    "#     # load dataset and preprocess\n",
    "#     # traindata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "#     # testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "#     traindata = load_dataset(\"/home/workspace/code/git/FlatQuant_mlm/datasets/wikitext\", split=\"train\")\n",
    "#     testdata = load_dataset(\"/home/workspace/code/git/FlatQuant_mlm/datasets/wikitext\", split=\"test\")\n",
    "#     trainenc = tokenizer(\"\\n\\n\".join(traindata[\"text\"]), return_tensors=\"pt\")\n",
    "#     testenc = tokenizer(\"\\n\\n\".join(testdata[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "#     traindataset = []\n",
    "#     for _ in range(nsamples):\n",
    "#         i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "#         j = i + seqlen\n",
    "#         inp = trainenc.input_ids[:, i:j]\n",
    "#         attention_mask = torch.ones_like(inp)\n",
    "#         traindataset.append({\"input_ids\": inp, \"attention_mask\": attention_mask})\n",
    "#     return traindataset, testenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "You are using a model of type minicpmv to instantiate a model of type minicpm. This is not supported for all configurations of models and can yield errors.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "2024-11-07 15:41:31 INFO [auto_gptq.modeling.minicpm.configuration_minicpm] vision_config is None, using default vision config\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "from auto_gptq import AutoGPTQForVIT, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "pretrained_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1\"\n",
    "quantized_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1-g128\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True, trust_remote_code=True)\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=8,  # quantize model to 4-bit\n",
    "    group_size=-1,  # it is recommended to set the value to 128\n",
    "    desc_act=True,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    ")\n",
    "# traindataset, testenc = get_wikitext2(128, 0, model.seqlen, tokenizer)\n",
    "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "model = AutoGPTQForVIT.from_pretrained(pretrained_model_dir, quantize_config)\n",
    "from transformers import AutoProcessor\n",
    "model.model.processor = AutoProcessor.from_pretrained(pretrained_model_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aircraft in the image is an Airbus A380, identifiable by its distinctive hump on the upper deck, which is characteristic of the Airbus A380 model. The A380 is a twin-engine, wide-body, four-engine jet airliner that was developed by Airbus and manufactured by Boeing. It is one of the largest aircraft in the world, capable of carrying more than 800 passengers and has a range of up to 9,500 nautical miles (17,200 km). This particular model is part of the Airbus A380 family, which includes the A380-800 and A380-900 variants.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image = Image.open('/home/workspace/code/llm-awq/awq/airplane.jpeg').convert('RGB')\n",
    "\n",
    "# First round chat \n",
    "question = \"Tell me the model of this aircraft.\"\n",
    "msgs = [{'role': 'user', 'content': [image, question]}]\n",
    "\n",
    "answer = model.model.cuda().chat(\n",
    "    image=None,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MiniCPMVTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# traindataset, testenc = get_wikitext2(128, 0, model.seqlen, tokenizer)\n",
    "traindataset = get_ScienceQA(1024, 0, model.seqlen, model.model.processor)\n",
    "# i,m = get_ScienceQA(32, 0, model.seqlen, model.model.processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/27\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/27...\n",
      "2024-11-07 16:17:12 INFO [auto_gptq.quantization.gptq] duration: 0.4007911682128906\n",
      "2024-11-07 16:17:12 INFO [auto_gptq.quantization.gptq] avg loss: 6.768801540601999e-05\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/27...\n",
      "2024-11-07 16:17:13 INFO [auto_gptq.quantization.gptq] duration: 0.2792811393737793\n",
      "2024-11-07 16:17:13 INFO [auto_gptq.quantization.gptq] avg loss: 1.185271412396105e-05\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/27...\n",
      "2024-11-07 16:17:13 INFO [auto_gptq.quantization.gptq] duration: 0.2770664691925049\n",
      "2024-11-07 16:17:13 INFO [auto_gptq.quantization.gptq] avg loss: 4.957634519087151e-05\n",
      "INFO - Quantizing self_attn.out_proj in layer 1/27...\n",
      "2024-11-07 16:18:27 INFO [auto_gptq.quantization.gptq] duration: 0.21391677856445312\n",
      "2024-11-07 16:18:27 INFO [auto_gptq.quantization.gptq] avg loss: 1.2891981668872177e-06\n",
      "INFO - Quantizing mlp.fc1 in layer 1/27...\n",
      "2024-11-07 16:19:34 INFO [auto_gptq.quantization.gptq] duration: 0.3289053440093994\n",
      "2024-11-07 16:19:34 INFO [auto_gptq.quantization.gptq] avg loss: 0.0005746465176343918\n",
      "INFO - Quantizing mlp.fc2 in layer 1/27...\n",
      "2024-11-07 16:21:15 INFO [auto_gptq.quantization.gptq] duration: 0.7139286994934082\n",
      "2024-11-07 16:21:15 INFO [auto_gptq.quantization.gptq] avg loss: 8.529113256372511e-05\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.06 GiB. GPU 0 has a total capacity of 79.33 GiB of which 1024.00 MiB is free. Process 5139 has 78.32 GiB memory in use. Of the allocated memory 69.98 GiB is allocated by PyTorch, and 7.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mquantize(traindataset)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/modeling/_base_vit.py:391\u001b[0m, in \u001b[0;36mBaseGPTQForVIT.quantize\u001b[0;34m(self, examples, batch_size, use_triton, use_cuda_fp16, autotune_warmup_after_quantized, cache_examples_on_gpu)\u001b[0m\n\u001b[1;32m    385\u001b[0m         layer_output \u001b[38;5;241m=\u001b[39m move_to_device(\n\u001b[1;32m    386\u001b[0m             layer(\u001b[38;5;241m*\u001b[39mlayer_input,attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    387\u001b[0m             cur_layer_device \u001b[38;5;28;01mif\u001b[39;00m cache_examples_on_gpu \u001b[38;5;28;01melse\u001b[39;00m CPU,\n\u001b[1;32m    388\u001b[0m         )\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m         layer_output \u001b[38;5;241m=\u001b[39m move_to_device(\n\u001b[0;32m--> 391\u001b[0m             layer(\u001b[38;5;241m*\u001b[39mlayer_input)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    392\u001b[0m             cur_layer_device \u001b[38;5;28;01mif\u001b[39;00m cache_examples_on_gpu \u001b[38;5;28;01melse\u001b[39;00m CPU,\n\u001b[1;32m    393\u001b[0m         )                \n\u001b[1;32m    394\u001b[0m     layer_outputs\u001b[38;5;241m.\u001b[39mappend([layer_output])\n\u001b[1;32m    396\u001b[0m layers[i] \u001b[38;5;241m=\u001b[39m move_to_device(layer, CPU \u001b[38;5;28;01mif\u001b[39;00m force_layer_back_to_cpu \u001b[38;5;28;01melse\u001b[39;00m cur_layer_device)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/modeling/minicpm/modeling_navit_siglip.py:670\u001b[0m, in \u001b[0;36mSiglipEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    667\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    669\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(hidden_states)\n\u001b[0;32m--> 670\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    671\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    672\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    673\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    674\u001b[0m )\n\u001b[1;32m    675\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    677\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/modeling/minicpm/modeling_navit_siglip.py:399\u001b[0m, in \u001b[0;36mSiglipAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    396\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(batch_size, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    398\u001b[0m k_v_seq_len \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 399\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query_states, key_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_weights\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q_len, k_v_seq_len):\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention weights should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(batch_size,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mq_len,\u001b[38;5;250m \u001b[39mk_v_seq_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_weights\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.06 GiB. GPU 0 has a total capacity of 79.33 GiB of which 1024.00 MiB is free. Process 5139 has 78.32 GiB memory in use. Of the allocated memory 69.98 GiB is allocated by PyTorch, and 7.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model.quantize(traindataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 15:10:31 INFO [auto_gptq.modeling.minicpm.configuration_minicpm] vision_config is None, using default vision config\n"
     ]
    }
   ],
   "source": [
    "# save quantized model\n",
    "model.save_quantized(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "You are using a model of type minicpmv to instantiate a model of type minicpm. This is not supported for all configurations of models and can yield errors.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "/opt/conda/envs/lmquant/lib/python3.11/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256 were not used when initializing MiniCPMV: ['vpm.encoder.layers.0.mlp.fc1.g_idx', 'vpm.encoder.layers.0.mlp.fc1.qweight', 'vpm.encoder.layers.0.mlp.fc1.qzeros', 'vpm.encoder.layers.0.mlp.fc1.scales', 'vpm.encoder.layers.0.mlp.fc2.g_idx', 'vpm.encoder.layers.0.mlp.fc2.qweight', 'vpm.encoder.layers.0.mlp.fc2.qzeros', 'vpm.encoder.layers.0.mlp.fc2.scales', 'vpm.encoder.layers.0.self_attn.k_proj.g_idx', 'vpm.encoder.layers.0.self_attn.k_proj.qweight', 'vpm.encoder.layers.0.self_attn.k_proj.qzeros', 'vpm.encoder.layers.0.self_attn.k_proj.scales', 'vpm.encoder.layers.0.self_attn.out_proj.g_idx', 'vpm.encoder.layers.0.self_attn.out_proj.qweight', 'vpm.encoder.layers.0.self_attn.out_proj.qzeros', 'vpm.encoder.layers.0.self_attn.out_proj.scales', 'vpm.encoder.layers.0.self_attn.q_proj.g_idx', 'vpm.encoder.layers.0.self_attn.q_proj.qweight', 'vpm.encoder.layers.0.self_attn.q_proj.qzeros', 'vpm.encoder.layers.0.self_attn.q_proj.scales', 'vpm.encoder.layers.0.self_attn.v_proj.g_idx', 'vpm.encoder.layers.0.self_attn.v_proj.qweight', 'vpm.encoder.layers.0.self_attn.v_proj.qzeros', 'vpm.encoder.layers.0.self_attn.v_proj.scales', 'vpm.encoder.layers.1.mlp.fc1.g_idx', 'vpm.encoder.layers.1.mlp.fc1.qweight', 'vpm.encoder.layers.1.mlp.fc1.qzeros', 'vpm.encoder.layers.1.mlp.fc1.scales', 'vpm.encoder.layers.1.mlp.fc2.g_idx', 'vpm.encoder.layers.1.mlp.fc2.qweight', 'vpm.encoder.layers.1.mlp.fc2.qzeros', 'vpm.encoder.layers.1.mlp.fc2.scales', 'vpm.encoder.layers.1.self_attn.k_proj.g_idx', 'vpm.encoder.layers.1.self_attn.k_proj.qweight', 'vpm.encoder.layers.1.self_attn.k_proj.qzeros', 'vpm.encoder.layers.1.self_attn.k_proj.scales', 'vpm.encoder.layers.1.self_attn.out_proj.g_idx', 'vpm.encoder.layers.1.self_attn.out_proj.qweight', 'vpm.encoder.layers.1.self_attn.out_proj.qzeros', 'vpm.encoder.layers.1.self_attn.out_proj.scales', 'vpm.encoder.layers.1.self_attn.q_proj.g_idx', 'vpm.encoder.layers.1.self_attn.q_proj.qweight', 'vpm.encoder.layers.1.self_attn.q_proj.qzeros', 'vpm.encoder.layers.1.self_attn.q_proj.scales', 'vpm.encoder.layers.1.self_attn.v_proj.g_idx', 'vpm.encoder.layers.1.self_attn.v_proj.qweight', 'vpm.encoder.layers.1.self_attn.v_proj.qzeros', 'vpm.encoder.layers.1.self_attn.v_proj.scales', 'vpm.encoder.layers.10.mlp.fc1.g_idx', 'vpm.encoder.layers.10.mlp.fc1.qweight', 'vpm.encoder.layers.10.mlp.fc1.qzeros', 'vpm.encoder.layers.10.mlp.fc1.scales', 'vpm.encoder.layers.10.mlp.fc2.g_idx', 'vpm.encoder.layers.10.mlp.fc2.qweight', 'vpm.encoder.layers.10.mlp.fc2.qzeros', 'vpm.encoder.layers.10.mlp.fc2.scales', 'vpm.encoder.layers.10.self_attn.k_proj.g_idx', 'vpm.encoder.layers.10.self_attn.k_proj.qweight', 'vpm.encoder.layers.10.self_attn.k_proj.qzeros', 'vpm.encoder.layers.10.self_attn.k_proj.scales', 'vpm.encoder.layers.10.self_attn.out_proj.g_idx', 'vpm.encoder.layers.10.self_attn.out_proj.qweight', 'vpm.encoder.layers.10.self_attn.out_proj.qzeros', 'vpm.encoder.layers.10.self_attn.out_proj.scales', 'vpm.encoder.layers.10.self_attn.q_proj.g_idx', 'vpm.encoder.layers.10.self_attn.q_proj.qweight', 'vpm.encoder.layers.10.self_attn.q_proj.qzeros', 'vpm.encoder.layers.10.self_attn.q_proj.scales', 'vpm.encoder.layers.10.self_attn.v_proj.g_idx', 'vpm.encoder.layers.10.self_attn.v_proj.qweight', 'vpm.encoder.layers.10.self_attn.v_proj.qzeros', 'vpm.encoder.layers.10.self_attn.v_proj.scales', 'vpm.encoder.layers.11.mlp.fc1.g_idx', 'vpm.encoder.layers.11.mlp.fc1.qweight', 'vpm.encoder.layers.11.mlp.fc1.qzeros', 'vpm.encoder.layers.11.mlp.fc1.scales', 'vpm.encoder.layers.11.mlp.fc2.g_idx', 'vpm.encoder.layers.11.mlp.fc2.qweight', 'vpm.encoder.layers.11.mlp.fc2.qzeros', 'vpm.encoder.layers.11.mlp.fc2.scales', 'vpm.encoder.layers.11.self_attn.k_proj.g_idx', 'vpm.encoder.layers.11.self_attn.k_proj.qweight', 'vpm.encoder.layers.11.self_attn.k_proj.qzeros', 'vpm.encoder.layers.11.self_attn.k_proj.scales', 'vpm.encoder.layers.11.self_attn.out_proj.g_idx', 'vpm.encoder.layers.11.self_attn.out_proj.qweight', 'vpm.encoder.layers.11.self_attn.out_proj.qzeros', 'vpm.encoder.layers.11.self_attn.out_proj.scales', 'vpm.encoder.layers.11.self_attn.q_proj.g_idx', 'vpm.encoder.layers.11.self_attn.q_proj.qweight', 'vpm.encoder.layers.11.self_attn.q_proj.qzeros', 'vpm.encoder.layers.11.self_attn.q_proj.scales', 'vpm.encoder.layers.11.self_attn.v_proj.g_idx', 'vpm.encoder.layers.11.self_attn.v_proj.qweight', 'vpm.encoder.layers.11.self_attn.v_proj.qzeros', 'vpm.encoder.layers.11.self_attn.v_proj.scales', 'vpm.encoder.layers.12.mlp.fc1.g_idx', 'vpm.encoder.layers.12.mlp.fc1.qweight', 'vpm.encoder.layers.12.mlp.fc1.qzeros', 'vpm.encoder.layers.12.mlp.fc1.scales', 'vpm.encoder.layers.12.mlp.fc2.g_idx', 'vpm.encoder.layers.12.mlp.fc2.qweight', 'vpm.encoder.layers.12.mlp.fc2.qzeros', 'vpm.encoder.layers.12.mlp.fc2.scales', 'vpm.encoder.layers.12.self_attn.k_proj.g_idx', 'vpm.encoder.layers.12.self_attn.k_proj.qweight', 'vpm.encoder.layers.12.self_attn.k_proj.qzeros', 'vpm.encoder.layers.12.self_attn.k_proj.scales', 'vpm.encoder.layers.12.self_attn.out_proj.g_idx', 'vpm.encoder.layers.12.self_attn.out_proj.qweight', 'vpm.encoder.layers.12.self_attn.out_proj.qzeros', 'vpm.encoder.layers.12.self_attn.out_proj.scales', 'vpm.encoder.layers.12.self_attn.q_proj.g_idx', 'vpm.encoder.layers.12.self_attn.q_proj.qweight', 'vpm.encoder.layers.12.self_attn.q_proj.qzeros', 'vpm.encoder.layers.12.self_attn.q_proj.scales', 'vpm.encoder.layers.12.self_attn.v_proj.g_idx', 'vpm.encoder.layers.12.self_attn.v_proj.qweight', 'vpm.encoder.layers.12.self_attn.v_proj.qzeros', 'vpm.encoder.layers.12.self_attn.v_proj.scales', 'vpm.encoder.layers.13.mlp.fc1.g_idx', 'vpm.encoder.layers.13.mlp.fc1.qweight', 'vpm.encoder.layers.13.mlp.fc1.qzeros', 'vpm.encoder.layers.13.mlp.fc1.scales', 'vpm.encoder.layers.13.mlp.fc2.g_idx', 'vpm.encoder.layers.13.mlp.fc2.qweight', 'vpm.encoder.layers.13.mlp.fc2.qzeros', 'vpm.encoder.layers.13.mlp.fc2.scales', 'vpm.encoder.layers.13.self_attn.k_proj.g_idx', 'vpm.encoder.layers.13.self_attn.k_proj.qweight', 'vpm.encoder.layers.13.self_attn.k_proj.qzeros', 'vpm.encoder.layers.13.self_attn.k_proj.scales', 'vpm.encoder.layers.13.self_attn.out_proj.g_idx', 'vpm.encoder.layers.13.self_attn.out_proj.qweight', 'vpm.encoder.layers.13.self_attn.out_proj.qzeros', 'vpm.encoder.layers.13.self_attn.out_proj.scales', 'vpm.encoder.layers.13.self_attn.q_proj.g_idx', 'vpm.encoder.layers.13.self_attn.q_proj.qweight', 'vpm.encoder.layers.13.self_attn.q_proj.qzeros', 'vpm.encoder.layers.13.self_attn.q_proj.scales', 'vpm.encoder.layers.13.self_attn.v_proj.g_idx', 'vpm.encoder.layers.13.self_attn.v_proj.qweight', 'vpm.encoder.layers.13.self_attn.v_proj.qzeros', 'vpm.encoder.layers.13.self_attn.v_proj.scales', 'vpm.encoder.layers.14.mlp.fc1.g_idx', 'vpm.encoder.layers.14.mlp.fc1.qweight', 'vpm.encoder.layers.14.mlp.fc1.qzeros', 'vpm.encoder.layers.14.mlp.fc1.scales', 'vpm.encoder.layers.14.mlp.fc2.g_idx', 'vpm.encoder.layers.14.mlp.fc2.qweight', 'vpm.encoder.layers.14.mlp.fc2.qzeros', 'vpm.encoder.layers.14.mlp.fc2.scales', 'vpm.encoder.layers.14.self_attn.k_proj.g_idx', 'vpm.encoder.layers.14.self_attn.k_proj.qweight', 'vpm.encoder.layers.14.self_attn.k_proj.qzeros', 'vpm.encoder.layers.14.self_attn.k_proj.scales', 'vpm.encoder.layers.14.self_attn.out_proj.g_idx', 'vpm.encoder.layers.14.self_attn.out_proj.qweight', 'vpm.encoder.layers.14.self_attn.out_proj.qzeros', 'vpm.encoder.layers.14.self_attn.out_proj.scales', 'vpm.encoder.layers.14.self_attn.q_proj.g_idx', 'vpm.encoder.layers.14.self_attn.q_proj.qweight', 'vpm.encoder.layers.14.self_attn.q_proj.qzeros', 'vpm.encoder.layers.14.self_attn.q_proj.scales', 'vpm.encoder.layers.14.self_attn.v_proj.g_idx', 'vpm.encoder.layers.14.self_attn.v_proj.qweight', 'vpm.encoder.layers.14.self_attn.v_proj.qzeros', 'vpm.encoder.layers.14.self_attn.v_proj.scales', 'vpm.encoder.layers.15.mlp.fc1.g_idx', 'vpm.encoder.layers.15.mlp.fc1.qweight', 'vpm.encoder.layers.15.mlp.fc1.qzeros', 'vpm.encoder.layers.15.mlp.fc1.scales', 'vpm.encoder.layers.15.mlp.fc2.g_idx', 'vpm.encoder.layers.15.mlp.fc2.qweight', 'vpm.encoder.layers.15.mlp.fc2.qzeros', 'vpm.encoder.layers.15.mlp.fc2.scales', 'vpm.encoder.layers.15.self_attn.k_proj.g_idx', 'vpm.encoder.layers.15.self_attn.k_proj.qweight', 'vpm.encoder.layers.15.self_attn.k_proj.qzeros', 'vpm.encoder.layers.15.self_attn.k_proj.scales', 'vpm.encoder.layers.15.self_attn.out_proj.g_idx', 'vpm.encoder.layers.15.self_attn.out_proj.qweight', 'vpm.encoder.layers.15.self_attn.out_proj.qzeros', 'vpm.encoder.layers.15.self_attn.out_proj.scales', 'vpm.encoder.layers.15.self_attn.q_proj.g_idx', 'vpm.encoder.layers.15.self_attn.q_proj.qweight', 'vpm.encoder.layers.15.self_attn.q_proj.qzeros', 'vpm.encoder.layers.15.self_attn.q_proj.scales', 'vpm.encoder.layers.15.self_attn.v_proj.g_idx', 'vpm.encoder.layers.15.self_attn.v_proj.qweight', 'vpm.encoder.layers.15.self_attn.v_proj.qzeros', 'vpm.encoder.layers.15.self_attn.v_proj.scales', 'vpm.encoder.layers.16.mlp.fc1.g_idx', 'vpm.encoder.layers.16.mlp.fc1.qweight', 'vpm.encoder.layers.16.mlp.fc1.qzeros', 'vpm.encoder.layers.16.mlp.fc1.scales', 'vpm.encoder.layers.16.mlp.fc2.g_idx', 'vpm.encoder.layers.16.mlp.fc2.qweight', 'vpm.encoder.layers.16.mlp.fc2.qzeros', 'vpm.encoder.layers.16.mlp.fc2.scales', 'vpm.encoder.layers.16.self_attn.k_proj.g_idx', 'vpm.encoder.layers.16.self_attn.k_proj.qweight', 'vpm.encoder.layers.16.self_attn.k_proj.qzeros', 'vpm.encoder.layers.16.self_attn.k_proj.scales', 'vpm.encoder.layers.16.self_attn.out_proj.g_idx', 'vpm.encoder.layers.16.self_attn.out_proj.qweight', 'vpm.encoder.layers.16.self_attn.out_proj.qzeros', 'vpm.encoder.layers.16.self_attn.out_proj.scales', 'vpm.encoder.layers.16.self_attn.q_proj.g_idx', 'vpm.encoder.layers.16.self_attn.q_proj.qweight', 'vpm.encoder.layers.16.self_attn.q_proj.qzeros', 'vpm.encoder.layers.16.self_attn.q_proj.scales', 'vpm.encoder.layers.16.self_attn.v_proj.g_idx', 'vpm.encoder.layers.16.self_attn.v_proj.qweight', 'vpm.encoder.layers.16.self_attn.v_proj.qzeros', 'vpm.encoder.layers.16.self_attn.v_proj.scales', 'vpm.encoder.layers.17.mlp.fc1.g_idx', 'vpm.encoder.layers.17.mlp.fc1.qweight', 'vpm.encoder.layers.17.mlp.fc1.qzeros', 'vpm.encoder.layers.17.mlp.fc1.scales', 'vpm.encoder.layers.17.mlp.fc2.g_idx', 'vpm.encoder.layers.17.mlp.fc2.qweight', 'vpm.encoder.layers.17.mlp.fc2.qzeros', 'vpm.encoder.layers.17.mlp.fc2.scales', 'vpm.encoder.layers.17.self_attn.k_proj.g_idx', 'vpm.encoder.layers.17.self_attn.k_proj.qweight', 'vpm.encoder.layers.17.self_attn.k_proj.qzeros', 'vpm.encoder.layers.17.self_attn.k_proj.scales', 'vpm.encoder.layers.17.self_attn.out_proj.g_idx', 'vpm.encoder.layers.17.self_attn.out_proj.qweight', 'vpm.encoder.layers.17.self_attn.out_proj.qzeros', 'vpm.encoder.layers.17.self_attn.out_proj.scales', 'vpm.encoder.layers.17.self_attn.q_proj.g_idx', 'vpm.encoder.layers.17.self_attn.q_proj.qweight', 'vpm.encoder.layers.17.self_attn.q_proj.qzeros', 'vpm.encoder.layers.17.self_attn.q_proj.scales', 'vpm.encoder.layers.17.self_attn.v_proj.g_idx', 'vpm.encoder.layers.17.self_attn.v_proj.qweight', 'vpm.encoder.layers.17.self_attn.v_proj.qzeros', 'vpm.encoder.layers.17.self_attn.v_proj.scales', 'vpm.encoder.layers.18.mlp.fc1.g_idx', 'vpm.encoder.layers.18.mlp.fc1.qweight', 'vpm.encoder.layers.18.mlp.fc1.qzeros', 'vpm.encoder.layers.18.mlp.fc1.scales', 'vpm.encoder.layers.18.mlp.fc2.g_idx', 'vpm.encoder.layers.18.mlp.fc2.qweight', 'vpm.encoder.layers.18.mlp.fc2.qzeros', 'vpm.encoder.layers.18.mlp.fc2.scales', 'vpm.encoder.layers.18.self_attn.k_proj.g_idx', 'vpm.encoder.layers.18.self_attn.k_proj.qweight', 'vpm.encoder.layers.18.self_attn.k_proj.qzeros', 'vpm.encoder.layers.18.self_attn.k_proj.scales', 'vpm.encoder.layers.18.self_attn.out_proj.g_idx', 'vpm.encoder.layers.18.self_attn.out_proj.qweight', 'vpm.encoder.layers.18.self_attn.out_proj.qzeros', 'vpm.encoder.layers.18.self_attn.out_proj.scales', 'vpm.encoder.layers.18.self_attn.q_proj.g_idx', 'vpm.encoder.layers.18.self_attn.q_proj.qweight', 'vpm.encoder.layers.18.self_attn.q_proj.qzeros', 'vpm.encoder.layers.18.self_attn.q_proj.scales', 'vpm.encoder.layers.18.self_attn.v_proj.g_idx', 'vpm.encoder.layers.18.self_attn.v_proj.qweight', 'vpm.encoder.layers.18.self_attn.v_proj.qzeros', 'vpm.encoder.layers.18.self_attn.v_proj.scales', 'vpm.encoder.layers.19.mlp.fc1.g_idx', 'vpm.encoder.layers.19.mlp.fc1.qweight', 'vpm.encoder.layers.19.mlp.fc1.qzeros', 'vpm.encoder.layers.19.mlp.fc1.scales', 'vpm.encoder.layers.19.mlp.fc2.g_idx', 'vpm.encoder.layers.19.mlp.fc2.qweight', 'vpm.encoder.layers.19.mlp.fc2.qzeros', 'vpm.encoder.layers.19.mlp.fc2.scales', 'vpm.encoder.layers.19.self_attn.k_proj.g_idx', 'vpm.encoder.layers.19.self_attn.k_proj.qweight', 'vpm.encoder.layers.19.self_attn.k_proj.qzeros', 'vpm.encoder.layers.19.self_attn.k_proj.scales', 'vpm.encoder.layers.19.self_attn.out_proj.g_idx', 'vpm.encoder.layers.19.self_attn.out_proj.qweight', 'vpm.encoder.layers.19.self_attn.out_proj.qzeros', 'vpm.encoder.layers.19.self_attn.out_proj.scales', 'vpm.encoder.layers.19.self_attn.q_proj.g_idx', 'vpm.encoder.layers.19.self_attn.q_proj.qweight', 'vpm.encoder.layers.19.self_attn.q_proj.qzeros', 'vpm.encoder.layers.19.self_attn.q_proj.scales', 'vpm.encoder.layers.19.self_attn.v_proj.g_idx', 'vpm.encoder.layers.19.self_attn.v_proj.qweight', 'vpm.encoder.layers.19.self_attn.v_proj.qzeros', 'vpm.encoder.layers.19.self_attn.v_proj.scales', 'vpm.encoder.layers.2.mlp.fc1.g_idx', 'vpm.encoder.layers.2.mlp.fc1.qweight', 'vpm.encoder.layers.2.mlp.fc1.qzeros', 'vpm.encoder.layers.2.mlp.fc1.scales', 'vpm.encoder.layers.2.mlp.fc2.g_idx', 'vpm.encoder.layers.2.mlp.fc2.qweight', 'vpm.encoder.layers.2.mlp.fc2.qzeros', 'vpm.encoder.layers.2.mlp.fc2.scales', 'vpm.encoder.layers.2.self_attn.k_proj.g_idx', 'vpm.encoder.layers.2.self_attn.k_proj.qweight', 'vpm.encoder.layers.2.self_attn.k_proj.qzeros', 'vpm.encoder.layers.2.self_attn.k_proj.scales', 'vpm.encoder.layers.2.self_attn.out_proj.g_idx', 'vpm.encoder.layers.2.self_attn.out_proj.qweight', 'vpm.encoder.layers.2.self_attn.out_proj.qzeros', 'vpm.encoder.layers.2.self_attn.out_proj.scales', 'vpm.encoder.layers.2.self_attn.q_proj.g_idx', 'vpm.encoder.layers.2.self_attn.q_proj.qweight', 'vpm.encoder.layers.2.self_attn.q_proj.qzeros', 'vpm.encoder.layers.2.self_attn.q_proj.scales', 'vpm.encoder.layers.2.self_attn.v_proj.g_idx', 'vpm.encoder.layers.2.self_attn.v_proj.qweight', 'vpm.encoder.layers.2.self_attn.v_proj.qzeros', 'vpm.encoder.layers.2.self_attn.v_proj.scales', 'vpm.encoder.layers.20.mlp.fc1.g_idx', 'vpm.encoder.layers.20.mlp.fc1.qweight', 'vpm.encoder.layers.20.mlp.fc1.qzeros', 'vpm.encoder.layers.20.mlp.fc1.scales', 'vpm.encoder.layers.20.mlp.fc2.g_idx', 'vpm.encoder.layers.20.mlp.fc2.qweight', 'vpm.encoder.layers.20.mlp.fc2.qzeros', 'vpm.encoder.layers.20.mlp.fc2.scales', 'vpm.encoder.layers.20.self_attn.k_proj.g_idx', 'vpm.encoder.layers.20.self_attn.k_proj.qweight', 'vpm.encoder.layers.20.self_attn.k_proj.qzeros', 'vpm.encoder.layers.20.self_attn.k_proj.scales', 'vpm.encoder.layers.20.self_attn.out_proj.g_idx', 'vpm.encoder.layers.20.self_attn.out_proj.qweight', 'vpm.encoder.layers.20.self_attn.out_proj.qzeros', 'vpm.encoder.layers.20.self_attn.out_proj.scales', 'vpm.encoder.layers.20.self_attn.q_proj.g_idx', 'vpm.encoder.layers.20.self_attn.q_proj.qweight', 'vpm.encoder.layers.20.self_attn.q_proj.qzeros', 'vpm.encoder.layers.20.self_attn.q_proj.scales', 'vpm.encoder.layers.20.self_attn.v_proj.g_idx', 'vpm.encoder.layers.20.self_attn.v_proj.qweight', 'vpm.encoder.layers.20.self_attn.v_proj.qzeros', 'vpm.encoder.layers.20.self_attn.v_proj.scales', 'vpm.encoder.layers.21.mlp.fc1.g_idx', 'vpm.encoder.layers.21.mlp.fc1.qweight', 'vpm.encoder.layers.21.mlp.fc1.qzeros', 'vpm.encoder.layers.21.mlp.fc1.scales', 'vpm.encoder.layers.21.mlp.fc2.g_idx', 'vpm.encoder.layers.21.mlp.fc2.qweight', 'vpm.encoder.layers.21.mlp.fc2.qzeros', 'vpm.encoder.layers.21.mlp.fc2.scales', 'vpm.encoder.layers.21.self_attn.k_proj.g_idx', 'vpm.encoder.layers.21.self_attn.k_proj.qweight', 'vpm.encoder.layers.21.self_attn.k_proj.qzeros', 'vpm.encoder.layers.21.self_attn.k_proj.scales', 'vpm.encoder.layers.21.self_attn.out_proj.g_idx', 'vpm.encoder.layers.21.self_attn.out_proj.qweight', 'vpm.encoder.layers.21.self_attn.out_proj.qzeros', 'vpm.encoder.layers.21.self_attn.out_proj.scales', 'vpm.encoder.layers.21.self_attn.q_proj.g_idx', 'vpm.encoder.layers.21.self_attn.q_proj.qweight', 'vpm.encoder.layers.21.self_attn.q_proj.qzeros', 'vpm.encoder.layers.21.self_attn.q_proj.scales', 'vpm.encoder.layers.21.self_attn.v_proj.g_idx', 'vpm.encoder.layers.21.self_attn.v_proj.qweight', 'vpm.encoder.layers.21.self_attn.v_proj.qzeros', 'vpm.encoder.layers.21.self_attn.v_proj.scales', 'vpm.encoder.layers.22.mlp.fc1.g_idx', 'vpm.encoder.layers.22.mlp.fc1.qweight', 'vpm.encoder.layers.22.mlp.fc1.qzeros', 'vpm.encoder.layers.22.mlp.fc1.scales', 'vpm.encoder.layers.22.mlp.fc2.g_idx', 'vpm.encoder.layers.22.mlp.fc2.qweight', 'vpm.encoder.layers.22.mlp.fc2.qzeros', 'vpm.encoder.layers.22.mlp.fc2.scales', 'vpm.encoder.layers.22.self_attn.k_proj.g_idx', 'vpm.encoder.layers.22.self_attn.k_proj.qweight', 'vpm.encoder.layers.22.self_attn.k_proj.qzeros', 'vpm.encoder.layers.22.self_attn.k_proj.scales', 'vpm.encoder.layers.22.self_attn.out_proj.g_idx', 'vpm.encoder.layers.22.self_attn.out_proj.qweight', 'vpm.encoder.layers.22.self_attn.out_proj.qzeros', 'vpm.encoder.layers.22.self_attn.out_proj.scales', 'vpm.encoder.layers.22.self_attn.q_proj.g_idx', 'vpm.encoder.layers.22.self_attn.q_proj.qweight', 'vpm.encoder.layers.22.self_attn.q_proj.qzeros', 'vpm.encoder.layers.22.self_attn.q_proj.scales', 'vpm.encoder.layers.22.self_attn.v_proj.g_idx', 'vpm.encoder.layers.22.self_attn.v_proj.qweight', 'vpm.encoder.layers.22.self_attn.v_proj.qzeros', 'vpm.encoder.layers.22.self_attn.v_proj.scales', 'vpm.encoder.layers.23.mlp.fc1.g_idx', 'vpm.encoder.layers.23.mlp.fc1.qweight', 'vpm.encoder.layers.23.mlp.fc1.qzeros', 'vpm.encoder.layers.23.mlp.fc1.scales', 'vpm.encoder.layers.23.mlp.fc2.g_idx', 'vpm.encoder.layers.23.mlp.fc2.qweight', 'vpm.encoder.layers.23.mlp.fc2.qzeros', 'vpm.encoder.layers.23.mlp.fc2.scales', 'vpm.encoder.layers.23.self_attn.k_proj.g_idx', 'vpm.encoder.layers.23.self_attn.k_proj.qweight', 'vpm.encoder.layers.23.self_attn.k_proj.qzeros', 'vpm.encoder.layers.23.self_attn.k_proj.scales', 'vpm.encoder.layers.23.self_attn.out_proj.g_idx', 'vpm.encoder.layers.23.self_attn.out_proj.qweight', 'vpm.encoder.layers.23.self_attn.out_proj.qzeros', 'vpm.encoder.layers.23.self_attn.out_proj.scales', 'vpm.encoder.layers.23.self_attn.q_proj.g_idx', 'vpm.encoder.layers.23.self_attn.q_proj.qweight', 'vpm.encoder.layers.23.self_attn.q_proj.qzeros', 'vpm.encoder.layers.23.self_attn.q_proj.scales', 'vpm.encoder.layers.23.self_attn.v_proj.g_idx', 'vpm.encoder.layers.23.self_attn.v_proj.qweight', 'vpm.encoder.layers.23.self_attn.v_proj.qzeros', 'vpm.encoder.layers.23.self_attn.v_proj.scales', 'vpm.encoder.layers.24.mlp.fc1.g_idx', 'vpm.encoder.layers.24.mlp.fc1.qweight', 'vpm.encoder.layers.24.mlp.fc1.qzeros', 'vpm.encoder.layers.24.mlp.fc1.scales', 'vpm.encoder.layers.24.mlp.fc2.g_idx', 'vpm.encoder.layers.24.mlp.fc2.qweight', 'vpm.encoder.layers.24.mlp.fc2.qzeros', 'vpm.encoder.layers.24.mlp.fc2.scales', 'vpm.encoder.layers.24.self_attn.k_proj.g_idx', 'vpm.encoder.layers.24.self_attn.k_proj.qweight', 'vpm.encoder.layers.24.self_attn.k_proj.qzeros', 'vpm.encoder.layers.24.self_attn.k_proj.scales', 'vpm.encoder.layers.24.self_attn.out_proj.g_idx', 'vpm.encoder.layers.24.self_attn.out_proj.qweight', 'vpm.encoder.layers.24.self_attn.out_proj.qzeros', 'vpm.encoder.layers.24.self_attn.out_proj.scales', 'vpm.encoder.layers.24.self_attn.q_proj.g_idx', 'vpm.encoder.layers.24.self_attn.q_proj.qweight', 'vpm.encoder.layers.24.self_attn.q_proj.qzeros', 'vpm.encoder.layers.24.self_attn.q_proj.scales', 'vpm.encoder.layers.24.self_attn.v_proj.g_idx', 'vpm.encoder.layers.24.self_attn.v_proj.qweight', 'vpm.encoder.layers.24.self_attn.v_proj.qzeros', 'vpm.encoder.layers.24.self_attn.v_proj.scales', 'vpm.encoder.layers.25.mlp.fc1.g_idx', 'vpm.encoder.layers.25.mlp.fc1.qweight', 'vpm.encoder.layers.25.mlp.fc1.qzeros', 'vpm.encoder.layers.25.mlp.fc1.scales', 'vpm.encoder.layers.25.mlp.fc2.g_idx', 'vpm.encoder.layers.25.mlp.fc2.qweight', 'vpm.encoder.layers.25.mlp.fc2.qzeros', 'vpm.encoder.layers.25.mlp.fc2.scales', 'vpm.encoder.layers.25.self_attn.k_proj.g_idx', 'vpm.encoder.layers.25.self_attn.k_proj.qweight', 'vpm.encoder.layers.25.self_attn.k_proj.qzeros', 'vpm.encoder.layers.25.self_attn.k_proj.scales', 'vpm.encoder.layers.25.self_attn.out_proj.g_idx', 'vpm.encoder.layers.25.self_attn.out_proj.qweight', 'vpm.encoder.layers.25.self_attn.out_proj.qzeros', 'vpm.encoder.layers.25.self_attn.out_proj.scales', 'vpm.encoder.layers.25.self_attn.q_proj.g_idx', 'vpm.encoder.layers.25.self_attn.q_proj.qweight', 'vpm.encoder.layers.25.self_attn.q_proj.qzeros', 'vpm.encoder.layers.25.self_attn.q_proj.scales', 'vpm.encoder.layers.25.self_attn.v_proj.g_idx', 'vpm.encoder.layers.25.self_attn.v_proj.qweight', 'vpm.encoder.layers.25.self_attn.v_proj.qzeros', 'vpm.encoder.layers.25.self_attn.v_proj.scales', 'vpm.encoder.layers.26.mlp.fc1.g_idx', 'vpm.encoder.layers.26.mlp.fc1.qweight', 'vpm.encoder.layers.26.mlp.fc1.qzeros', 'vpm.encoder.layers.26.mlp.fc1.scales', 'vpm.encoder.layers.26.mlp.fc2.g_idx', 'vpm.encoder.layers.26.mlp.fc2.qweight', 'vpm.encoder.layers.26.mlp.fc2.qzeros', 'vpm.encoder.layers.26.mlp.fc2.scales', 'vpm.encoder.layers.26.self_attn.k_proj.g_idx', 'vpm.encoder.layers.26.self_attn.k_proj.qweight', 'vpm.encoder.layers.26.self_attn.k_proj.qzeros', 'vpm.encoder.layers.26.self_attn.k_proj.scales', 'vpm.encoder.layers.26.self_attn.out_proj.g_idx', 'vpm.encoder.layers.26.self_attn.out_proj.qweight', 'vpm.encoder.layers.26.self_attn.out_proj.qzeros', 'vpm.encoder.layers.26.self_attn.out_proj.scales', 'vpm.encoder.layers.26.self_attn.q_proj.g_idx', 'vpm.encoder.layers.26.self_attn.q_proj.qweight', 'vpm.encoder.layers.26.self_attn.q_proj.qzeros', 'vpm.encoder.layers.26.self_attn.q_proj.scales', 'vpm.encoder.layers.26.self_attn.v_proj.g_idx', 'vpm.encoder.layers.26.self_attn.v_proj.qweight', 'vpm.encoder.layers.26.self_attn.v_proj.qzeros', 'vpm.encoder.layers.26.self_attn.v_proj.scales', 'vpm.encoder.layers.3.mlp.fc1.g_idx', 'vpm.encoder.layers.3.mlp.fc1.qweight', 'vpm.encoder.layers.3.mlp.fc1.qzeros', 'vpm.encoder.layers.3.mlp.fc1.scales', 'vpm.encoder.layers.3.mlp.fc2.g_idx', 'vpm.encoder.layers.3.mlp.fc2.qweight', 'vpm.encoder.layers.3.mlp.fc2.qzeros', 'vpm.encoder.layers.3.mlp.fc2.scales', 'vpm.encoder.layers.3.self_attn.k_proj.g_idx', 'vpm.encoder.layers.3.self_attn.k_proj.qweight', 'vpm.encoder.layers.3.self_attn.k_proj.qzeros', 'vpm.encoder.layers.3.self_attn.k_proj.scales', 'vpm.encoder.layers.3.self_attn.out_proj.g_idx', 'vpm.encoder.layers.3.self_attn.out_proj.qweight', 'vpm.encoder.layers.3.self_attn.out_proj.qzeros', 'vpm.encoder.layers.3.self_attn.out_proj.scales', 'vpm.encoder.layers.3.self_attn.q_proj.g_idx', 'vpm.encoder.layers.3.self_attn.q_proj.qweight', 'vpm.encoder.layers.3.self_attn.q_proj.qzeros', 'vpm.encoder.layers.3.self_attn.q_proj.scales', 'vpm.encoder.layers.3.self_attn.v_proj.g_idx', 'vpm.encoder.layers.3.self_attn.v_proj.qweight', 'vpm.encoder.layers.3.self_attn.v_proj.qzeros', 'vpm.encoder.layers.3.self_attn.v_proj.scales', 'vpm.encoder.layers.4.mlp.fc1.g_idx', 'vpm.encoder.layers.4.mlp.fc1.qweight', 'vpm.encoder.layers.4.mlp.fc1.qzeros', 'vpm.encoder.layers.4.mlp.fc1.scales', 'vpm.encoder.layers.4.mlp.fc2.g_idx', 'vpm.encoder.layers.4.mlp.fc2.qweight', 'vpm.encoder.layers.4.mlp.fc2.qzeros', 'vpm.encoder.layers.4.mlp.fc2.scales', 'vpm.encoder.layers.4.self_attn.k_proj.g_idx', 'vpm.encoder.layers.4.self_attn.k_proj.qweight', 'vpm.encoder.layers.4.self_attn.k_proj.qzeros', 'vpm.encoder.layers.4.self_attn.k_proj.scales', 'vpm.encoder.layers.4.self_attn.out_proj.g_idx', 'vpm.encoder.layers.4.self_attn.out_proj.qweight', 'vpm.encoder.layers.4.self_attn.out_proj.qzeros', 'vpm.encoder.layers.4.self_attn.out_proj.scales', 'vpm.encoder.layers.4.self_attn.q_proj.g_idx', 'vpm.encoder.layers.4.self_attn.q_proj.qweight', 'vpm.encoder.layers.4.self_attn.q_proj.qzeros', 'vpm.encoder.layers.4.self_attn.q_proj.scales', 'vpm.encoder.layers.4.self_attn.v_proj.g_idx', 'vpm.encoder.layers.4.self_attn.v_proj.qweight', 'vpm.encoder.layers.4.self_attn.v_proj.qzeros', 'vpm.encoder.layers.4.self_attn.v_proj.scales', 'vpm.encoder.layers.5.mlp.fc1.g_idx', 'vpm.encoder.layers.5.mlp.fc1.qweight', 'vpm.encoder.layers.5.mlp.fc1.qzeros', 'vpm.encoder.layers.5.mlp.fc1.scales', 'vpm.encoder.layers.5.mlp.fc2.g_idx', 'vpm.encoder.layers.5.mlp.fc2.qweight', 'vpm.encoder.layers.5.mlp.fc2.qzeros', 'vpm.encoder.layers.5.mlp.fc2.scales', 'vpm.encoder.layers.5.self_attn.k_proj.g_idx', 'vpm.encoder.layers.5.self_attn.k_proj.qweight', 'vpm.encoder.layers.5.self_attn.k_proj.qzeros', 'vpm.encoder.layers.5.self_attn.k_proj.scales', 'vpm.encoder.layers.5.self_attn.out_proj.g_idx', 'vpm.encoder.layers.5.self_attn.out_proj.qweight', 'vpm.encoder.layers.5.self_attn.out_proj.qzeros', 'vpm.encoder.layers.5.self_attn.out_proj.scales', 'vpm.encoder.layers.5.self_attn.q_proj.g_idx', 'vpm.encoder.layers.5.self_attn.q_proj.qweight', 'vpm.encoder.layers.5.self_attn.q_proj.qzeros', 'vpm.encoder.layers.5.self_attn.q_proj.scales', 'vpm.encoder.layers.5.self_attn.v_proj.g_idx', 'vpm.encoder.layers.5.self_attn.v_proj.qweight', 'vpm.encoder.layers.5.self_attn.v_proj.qzeros', 'vpm.encoder.layers.5.self_attn.v_proj.scales', 'vpm.encoder.layers.6.mlp.fc1.g_idx', 'vpm.encoder.layers.6.mlp.fc1.qweight', 'vpm.encoder.layers.6.mlp.fc1.qzeros', 'vpm.encoder.layers.6.mlp.fc1.scales', 'vpm.encoder.layers.6.mlp.fc2.g_idx', 'vpm.encoder.layers.6.mlp.fc2.qweight', 'vpm.encoder.layers.6.mlp.fc2.qzeros', 'vpm.encoder.layers.6.mlp.fc2.scales', 'vpm.encoder.layers.6.self_attn.k_proj.g_idx', 'vpm.encoder.layers.6.self_attn.k_proj.qweight', 'vpm.encoder.layers.6.self_attn.k_proj.qzeros', 'vpm.encoder.layers.6.self_attn.k_proj.scales', 'vpm.encoder.layers.6.self_attn.out_proj.g_idx', 'vpm.encoder.layers.6.self_attn.out_proj.qweight', 'vpm.encoder.layers.6.self_attn.out_proj.qzeros', 'vpm.encoder.layers.6.self_attn.out_proj.scales', 'vpm.encoder.layers.6.self_attn.q_proj.g_idx', 'vpm.encoder.layers.6.self_attn.q_proj.qweight', 'vpm.encoder.layers.6.self_attn.q_proj.qzeros', 'vpm.encoder.layers.6.self_attn.q_proj.scales', 'vpm.encoder.layers.6.self_attn.v_proj.g_idx', 'vpm.encoder.layers.6.self_attn.v_proj.qweight', 'vpm.encoder.layers.6.self_attn.v_proj.qzeros', 'vpm.encoder.layers.6.self_attn.v_proj.scales', 'vpm.encoder.layers.7.mlp.fc1.g_idx', 'vpm.encoder.layers.7.mlp.fc1.qweight', 'vpm.encoder.layers.7.mlp.fc1.qzeros', 'vpm.encoder.layers.7.mlp.fc1.scales', 'vpm.encoder.layers.7.mlp.fc2.g_idx', 'vpm.encoder.layers.7.mlp.fc2.qweight', 'vpm.encoder.layers.7.mlp.fc2.qzeros', 'vpm.encoder.layers.7.mlp.fc2.scales', 'vpm.encoder.layers.7.self_attn.k_proj.g_idx', 'vpm.encoder.layers.7.self_attn.k_proj.qweight', 'vpm.encoder.layers.7.self_attn.k_proj.qzeros', 'vpm.encoder.layers.7.self_attn.k_proj.scales', 'vpm.encoder.layers.7.self_attn.out_proj.g_idx', 'vpm.encoder.layers.7.self_attn.out_proj.qweight', 'vpm.encoder.layers.7.self_attn.out_proj.qzeros', 'vpm.encoder.layers.7.self_attn.out_proj.scales', 'vpm.encoder.layers.7.self_attn.q_proj.g_idx', 'vpm.encoder.layers.7.self_attn.q_proj.qweight', 'vpm.encoder.layers.7.self_attn.q_proj.qzeros', 'vpm.encoder.layers.7.self_attn.q_proj.scales', 'vpm.encoder.layers.7.self_attn.v_proj.g_idx', 'vpm.encoder.layers.7.self_attn.v_proj.qweight', 'vpm.encoder.layers.7.self_attn.v_proj.qzeros', 'vpm.encoder.layers.7.self_attn.v_proj.scales', 'vpm.encoder.layers.8.mlp.fc1.g_idx', 'vpm.encoder.layers.8.mlp.fc1.qweight', 'vpm.encoder.layers.8.mlp.fc1.qzeros', 'vpm.encoder.layers.8.mlp.fc1.scales', 'vpm.encoder.layers.8.mlp.fc2.g_idx', 'vpm.encoder.layers.8.mlp.fc2.qweight', 'vpm.encoder.layers.8.mlp.fc2.qzeros', 'vpm.encoder.layers.8.mlp.fc2.scales', 'vpm.encoder.layers.8.self_attn.k_proj.g_idx', 'vpm.encoder.layers.8.self_attn.k_proj.qweight', 'vpm.encoder.layers.8.self_attn.k_proj.qzeros', 'vpm.encoder.layers.8.self_attn.k_proj.scales', 'vpm.encoder.layers.8.self_attn.out_proj.g_idx', 'vpm.encoder.layers.8.self_attn.out_proj.qweight', 'vpm.encoder.layers.8.self_attn.out_proj.qzeros', 'vpm.encoder.layers.8.self_attn.out_proj.scales', 'vpm.encoder.layers.8.self_attn.q_proj.g_idx', 'vpm.encoder.layers.8.self_attn.q_proj.qweight', 'vpm.encoder.layers.8.self_attn.q_proj.qzeros', 'vpm.encoder.layers.8.self_attn.q_proj.scales', 'vpm.encoder.layers.8.self_attn.v_proj.g_idx', 'vpm.encoder.layers.8.self_attn.v_proj.qweight', 'vpm.encoder.layers.8.self_attn.v_proj.qzeros', 'vpm.encoder.layers.8.self_attn.v_proj.scales', 'vpm.encoder.layers.9.mlp.fc1.g_idx', 'vpm.encoder.layers.9.mlp.fc1.qweight', 'vpm.encoder.layers.9.mlp.fc1.qzeros', 'vpm.encoder.layers.9.mlp.fc1.scales', 'vpm.encoder.layers.9.mlp.fc2.g_idx', 'vpm.encoder.layers.9.mlp.fc2.qweight', 'vpm.encoder.layers.9.mlp.fc2.qzeros', 'vpm.encoder.layers.9.mlp.fc2.scales', 'vpm.encoder.layers.9.self_attn.k_proj.g_idx', 'vpm.encoder.layers.9.self_attn.k_proj.qweight', 'vpm.encoder.layers.9.self_attn.k_proj.qzeros', 'vpm.encoder.layers.9.self_attn.k_proj.scales', 'vpm.encoder.layers.9.self_attn.out_proj.g_idx', 'vpm.encoder.layers.9.self_attn.out_proj.qweight', 'vpm.encoder.layers.9.self_attn.out_proj.qzeros', 'vpm.encoder.layers.9.self_attn.out_proj.scales', 'vpm.encoder.layers.9.self_attn.q_proj.g_idx', 'vpm.encoder.layers.9.self_attn.q_proj.qweight', 'vpm.encoder.layers.9.self_attn.q_proj.qzeros', 'vpm.encoder.layers.9.self_attn.q_proj.scales', 'vpm.encoder.layers.9.self_attn.v_proj.g_idx', 'vpm.encoder.layers.9.self_attn.v_proj.qweight', 'vpm.encoder.layers.9.self_attn.v_proj.qzeros', 'vpm.encoder.layers.9.self_attn.v_proj.scales']\n",
      "- This IS expected if you are initializing MiniCPMV from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MiniCPMV from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MiniCPMV were not initialized from the model checkpoint at /home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256 and are newly initialized: ['vpm.encoder.layers.0.mlp.fc1.weight', 'vpm.encoder.layers.0.mlp.fc2.weight', 'vpm.encoder.layers.0.self_attn.k_proj.weight', 'vpm.encoder.layers.0.self_attn.out_proj.weight', 'vpm.encoder.layers.0.self_attn.q_proj.weight', 'vpm.encoder.layers.0.self_attn.v_proj.weight', 'vpm.encoder.layers.1.mlp.fc1.weight', 'vpm.encoder.layers.1.mlp.fc2.weight', 'vpm.encoder.layers.1.self_attn.k_proj.weight', 'vpm.encoder.layers.1.self_attn.out_proj.weight', 'vpm.encoder.layers.1.self_attn.q_proj.weight', 'vpm.encoder.layers.1.self_attn.v_proj.weight', 'vpm.encoder.layers.10.mlp.fc1.weight', 'vpm.encoder.layers.10.mlp.fc2.weight', 'vpm.encoder.layers.10.self_attn.k_proj.weight', 'vpm.encoder.layers.10.self_attn.out_proj.weight', 'vpm.encoder.layers.10.self_attn.q_proj.weight', 'vpm.encoder.layers.10.self_attn.v_proj.weight', 'vpm.encoder.layers.11.mlp.fc1.weight', 'vpm.encoder.layers.11.mlp.fc2.weight', 'vpm.encoder.layers.11.self_attn.k_proj.weight', 'vpm.encoder.layers.11.self_attn.out_proj.weight', 'vpm.encoder.layers.11.self_attn.q_proj.weight', 'vpm.encoder.layers.11.self_attn.v_proj.weight', 'vpm.encoder.layers.12.mlp.fc1.weight', 'vpm.encoder.layers.12.mlp.fc2.weight', 'vpm.encoder.layers.12.self_attn.k_proj.weight', 'vpm.encoder.layers.12.self_attn.out_proj.weight', 'vpm.encoder.layers.12.self_attn.q_proj.weight', 'vpm.encoder.layers.12.self_attn.v_proj.weight', 'vpm.encoder.layers.13.mlp.fc1.weight', 'vpm.encoder.layers.13.mlp.fc2.weight', 'vpm.encoder.layers.13.self_attn.k_proj.weight', 'vpm.encoder.layers.13.self_attn.out_proj.weight', 'vpm.encoder.layers.13.self_attn.q_proj.weight', 'vpm.encoder.layers.13.self_attn.v_proj.weight', 'vpm.encoder.layers.14.mlp.fc1.weight', 'vpm.encoder.layers.14.mlp.fc2.weight', 'vpm.encoder.layers.14.self_attn.k_proj.weight', 'vpm.encoder.layers.14.self_attn.out_proj.weight', 'vpm.encoder.layers.14.self_attn.q_proj.weight', 'vpm.encoder.layers.14.self_attn.v_proj.weight', 'vpm.encoder.layers.15.mlp.fc1.weight', 'vpm.encoder.layers.15.mlp.fc2.weight', 'vpm.encoder.layers.15.self_attn.k_proj.weight', 'vpm.encoder.layers.15.self_attn.out_proj.weight', 'vpm.encoder.layers.15.self_attn.q_proj.weight', 'vpm.encoder.layers.15.self_attn.v_proj.weight', 'vpm.encoder.layers.16.mlp.fc1.weight', 'vpm.encoder.layers.16.mlp.fc2.weight', 'vpm.encoder.layers.16.self_attn.k_proj.weight', 'vpm.encoder.layers.16.self_attn.out_proj.weight', 'vpm.encoder.layers.16.self_attn.q_proj.weight', 'vpm.encoder.layers.16.self_attn.v_proj.weight', 'vpm.encoder.layers.17.mlp.fc1.weight', 'vpm.encoder.layers.17.mlp.fc2.weight', 'vpm.encoder.layers.17.self_attn.k_proj.weight', 'vpm.encoder.layers.17.self_attn.out_proj.weight', 'vpm.encoder.layers.17.self_attn.q_proj.weight', 'vpm.encoder.layers.17.self_attn.v_proj.weight', 'vpm.encoder.layers.18.mlp.fc1.weight', 'vpm.encoder.layers.18.mlp.fc2.weight', 'vpm.encoder.layers.18.self_attn.k_proj.weight', 'vpm.encoder.layers.18.self_attn.out_proj.weight', 'vpm.encoder.layers.18.self_attn.q_proj.weight', 'vpm.encoder.layers.18.self_attn.v_proj.weight', 'vpm.encoder.layers.19.mlp.fc1.weight', 'vpm.encoder.layers.19.mlp.fc2.weight', 'vpm.encoder.layers.19.self_attn.k_proj.weight', 'vpm.encoder.layers.19.self_attn.out_proj.weight', 'vpm.encoder.layers.19.self_attn.q_proj.weight', 'vpm.encoder.layers.19.self_attn.v_proj.weight', 'vpm.encoder.layers.2.mlp.fc1.weight', 'vpm.encoder.layers.2.mlp.fc2.weight', 'vpm.encoder.layers.2.self_attn.k_proj.weight', 'vpm.encoder.layers.2.self_attn.out_proj.weight', 'vpm.encoder.layers.2.self_attn.q_proj.weight', 'vpm.encoder.layers.2.self_attn.v_proj.weight', 'vpm.encoder.layers.20.mlp.fc1.weight', 'vpm.encoder.layers.20.mlp.fc2.weight', 'vpm.encoder.layers.20.self_attn.k_proj.weight', 'vpm.encoder.layers.20.self_attn.out_proj.weight', 'vpm.encoder.layers.20.self_attn.q_proj.weight', 'vpm.encoder.layers.20.self_attn.v_proj.weight', 'vpm.encoder.layers.21.mlp.fc1.weight', 'vpm.encoder.layers.21.mlp.fc2.weight', 'vpm.encoder.layers.21.self_attn.k_proj.weight', 'vpm.encoder.layers.21.self_attn.out_proj.weight', 'vpm.encoder.layers.21.self_attn.q_proj.weight', 'vpm.encoder.layers.21.self_attn.v_proj.weight', 'vpm.encoder.layers.22.mlp.fc1.weight', 'vpm.encoder.layers.22.mlp.fc2.weight', 'vpm.encoder.layers.22.self_attn.k_proj.weight', 'vpm.encoder.layers.22.self_attn.out_proj.weight', 'vpm.encoder.layers.22.self_attn.q_proj.weight', 'vpm.encoder.layers.22.self_attn.v_proj.weight', 'vpm.encoder.layers.23.mlp.fc1.weight', 'vpm.encoder.layers.23.mlp.fc2.weight', 'vpm.encoder.layers.23.self_attn.k_proj.weight', 'vpm.encoder.layers.23.self_attn.out_proj.weight', 'vpm.encoder.layers.23.self_attn.q_proj.weight', 'vpm.encoder.layers.23.self_attn.v_proj.weight', 'vpm.encoder.layers.24.mlp.fc1.weight', 'vpm.encoder.layers.24.mlp.fc2.weight', 'vpm.encoder.layers.24.self_attn.k_proj.weight', 'vpm.encoder.layers.24.self_attn.out_proj.weight', 'vpm.encoder.layers.24.self_attn.q_proj.weight', 'vpm.encoder.layers.24.self_attn.v_proj.weight', 'vpm.encoder.layers.25.mlp.fc1.weight', 'vpm.encoder.layers.25.mlp.fc2.weight', 'vpm.encoder.layers.25.self_attn.k_proj.weight', 'vpm.encoder.layers.25.self_attn.out_proj.weight', 'vpm.encoder.layers.25.self_attn.q_proj.weight', 'vpm.encoder.layers.25.self_attn.v_proj.weight', 'vpm.encoder.layers.26.mlp.fc1.weight', 'vpm.encoder.layers.26.mlp.fc2.weight', 'vpm.encoder.layers.26.self_attn.k_proj.weight', 'vpm.encoder.layers.26.self_attn.out_proj.weight', 'vpm.encoder.layers.26.self_attn.q_proj.weight', 'vpm.encoder.layers.26.self_attn.v_proj.weight', 'vpm.encoder.layers.3.mlp.fc1.weight', 'vpm.encoder.layers.3.mlp.fc2.weight', 'vpm.encoder.layers.3.self_attn.k_proj.weight', 'vpm.encoder.layers.3.self_attn.out_proj.weight', 'vpm.encoder.layers.3.self_attn.q_proj.weight', 'vpm.encoder.layers.3.self_attn.v_proj.weight', 'vpm.encoder.layers.4.mlp.fc1.weight', 'vpm.encoder.layers.4.mlp.fc2.weight', 'vpm.encoder.layers.4.self_attn.k_proj.weight', 'vpm.encoder.layers.4.self_attn.out_proj.weight', 'vpm.encoder.layers.4.self_attn.q_proj.weight', 'vpm.encoder.layers.4.self_attn.v_proj.weight', 'vpm.encoder.layers.5.mlp.fc1.weight', 'vpm.encoder.layers.5.mlp.fc2.weight', 'vpm.encoder.layers.5.self_attn.k_proj.weight', 'vpm.encoder.layers.5.self_attn.out_proj.weight', 'vpm.encoder.layers.5.self_attn.q_proj.weight', 'vpm.encoder.layers.5.self_attn.v_proj.weight', 'vpm.encoder.layers.6.mlp.fc1.weight', 'vpm.encoder.layers.6.mlp.fc2.weight', 'vpm.encoder.layers.6.self_attn.k_proj.weight', 'vpm.encoder.layers.6.self_attn.out_proj.weight', 'vpm.encoder.layers.6.self_attn.q_proj.weight', 'vpm.encoder.layers.6.self_attn.v_proj.weight', 'vpm.encoder.layers.7.mlp.fc1.weight', 'vpm.encoder.layers.7.mlp.fc2.weight', 'vpm.encoder.layers.7.self_attn.k_proj.weight', 'vpm.encoder.layers.7.self_attn.out_proj.weight', 'vpm.encoder.layers.7.self_attn.q_proj.weight', 'vpm.encoder.layers.7.self_attn.v_proj.weight', 'vpm.encoder.layers.8.mlp.fc1.weight', 'vpm.encoder.layers.8.mlp.fc2.weight', 'vpm.encoder.layers.8.self_attn.k_proj.weight', 'vpm.encoder.layers.8.self_attn.out_proj.weight', 'vpm.encoder.layers.8.self_attn.q_proj.weight', 'vpm.encoder.layers.8.self_attn.v_proj.weight', 'vpm.encoder.layers.9.mlp.fc1.weight', 'vpm.encoder.layers.9.mlp.fc2.weight', 'vpm.encoder.layers.9.self_attn.k_proj.weight', 'vpm.encoder.layers.9.self_attn.out_proj.weight', 'vpm.encoder.layers.9.self_attn.q_proj.weight', 'vpm.encoder.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO - The layer llm.model.layers.0.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.lm_head is not quantized.\n",
      "INFO - The layer vpm.embeddings.patch_embedding is not quantized.\n",
      "INFO - The layer resampler.kv_proj is not quantized.\n",
      "INFO - The layer resampler.attn.out_proj is not quantized.\n"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForVIT, BaseQuantizeConfig\n",
    "\n",
    "# quantized_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1-g128\"\n",
    "quantized_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256\"\n",
    "model_quant = AutoGPTQForVIT.from_quantized(quantized_model_dir, device=\"cuda:0\", use_triton=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline,AutoProcessor\n",
    "pretrained_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True, trust_remote_code=True)\n",
    "model_quant.model.processor = AutoProcessor.from_pretrained(pretrained_model_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------scale_value: tensor(104.6875, device='cuda:0', dtype=torch.float16)\n",
      "这是一架中国东方航空公司的客机，具体型号是A380-800。图上的字是“空航方南国中”和“B-6136”，其中“空航方南国中”是航空公司名称的一部分，而“B-6136”是该飞机的注册编号。\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image = Image.open('/home/workspace/code/llm-awq/awq/airplane.jpeg').convert('RGB')\n",
    "\n",
    "# First round chat \n",
    "question = \"这是什么飞机\"\n",
    "msgs = [{'role': 'user', 'content': [image, question]}]\n",
    "\n",
    "answer = model_quant.model.chat(\n",
    "    image=None,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vpm.embeddings', 'vpm.post_layernorm']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant.outside_layer_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniCPMModel(\n",
       "  (embed_tokens): Embedding(73464, 1536)\n",
       "  (layers): ModuleList(\n",
       "    (0-51): 52 x MiniCPMDecoderLayer(\n",
       "      (self_attn): MiniCPMSdpaAttention(\n",
       "        (q_proj): QuantLinear()\n",
       "        (k_proj): QuantLinear()\n",
       "        (v_proj): QuantLinear()\n",
       "        (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        (rotary_emb): MiniCPMLongRoPE()\n",
       "      )\n",
       "      (mlp): MiniCPMMLP(\n",
       "        (gate_proj): Linear(in_features=1536, out_features=3840, bias=False)\n",
       "        (up_proj): Linear(in_features=1536, out_features=3840, bias=False)\n",
       "        (down_proj): Linear(in_features=3840, out_features=1536, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): MiniCPMRMSNorm()\n",
       "      (post_attention_layernorm): MiniCPMRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): MiniCPMRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant.model.llm.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_inputs = []\n",
    "\n",
    "def store_input_hook(_, args,kwargs):\n",
    "    # Positional arguments.\n",
    "    layer_input = []\n",
    "    for inp in args:\n",
    "        layer_input.append(inp)\n",
    "    layer_inputs.append(layer_input)\n",
    "\n",
    "    # # Keyword arguments.\n",
    "    # if kwargs[\"attention_mask\"] is not None:\n",
    "    #     attention_masks.append(kwargs[\"attention_mask\"].to(data_device))\n",
    "    # else:\n",
    "    #     attention_masks.append(None)\n",
    "\n",
    "    # pos_ids = kwargs.get(\"position_ids\", None)\n",
    "    # if pos_ids is not None:\n",
    "    #     position_ids.append(move_to_device(pos_ids, data_device))\n",
    "    # one_kwargs = {}\n",
    "    # for (\n",
    "    #     k,\n",
    "    #     v,\n",
    "    # ) in kwargs.items():  # make sure other arguments also be captured\n",
    "    #     if k not in [\"hidden_states\", \"attention_mask\", \"position_ids\"]:\n",
    "    #         one_kwargs[k] = nested_move_to_device(v, data_device)\n",
    "    # layer_input_kwargs.append(one_kwargs)\n",
    "    # raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = model.vpm.encoder.layers[0].register_forward_pre_hook(store_input_hook, with_kwargs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in traindataset:\n",
    "    for k, v in example.items():\n",
    "        # if len(v.shape) == 1:\n",
    "        #     v = v.unsqueeze(0)\n",
    "        # if k in \"input_ids\" or k in \"attention_mask\":\n",
    "        #     example[k] = v\n",
    "        # elif k in \"pixel_values\":\n",
    "        #     example[k][0][0] =  v[0][0].cuda()\n",
    "        #     example[k][0][1] =  v[0][1].cuda()\n",
    "        #     example[k][0][2] =  v[0][2].cuda()\n",
    "        a = model(example)\n",
    "        break\n",
    "handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7efb9932ca50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vpm.encoder.layers[0].register_forward_pre_hook(store_input_hook, with_kwargs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取激活"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_ScienceQA(nsamples, seed, seqlen, processor, status):\n",
    "    import torch.nn.functional as F\n",
    "    dataset = datasets.load_from_disk(\"/home/workspace/dataset/ScienceQA-2\")[\"train\"]\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "    rng = random.Random(42)\n",
    "\n",
    "    #数据拆分\n",
    "    if status == 0:\n",
    "        traindataset = []\n",
    "        for index, _data in enumerate(dataset):\n",
    "            prompts_lists = []\n",
    "            input_images_lists = []\n",
    "            promt = _data[\"question\"]\n",
    "            # image_file = _data[\"image\"]\n",
    "            image_file = _data[\"image\"]\n",
    "            if image_file is None:\n",
    "                nsamples = nsamples + 1\n",
    "                continue\n",
    "            else:\n",
    "                image = np.array(image_file)\n",
    "                # image = np.array(image_file.resize((448,  448)))\n",
    "            msgs = [{'role': 'user', 'content': \"(<image>./</image>)\\n\"+ promt}]\n",
    "            prompts_lists.append(processor.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True))\n",
    "            \n",
    "            input_images_lists.append([image])\n",
    "            if index >= nsamples:\n",
    "                break\n",
    "     \n",
    "            inputs = processor(\n",
    "                prompts_lists,\n",
    "                input_images_lists,\n",
    "                max_slice_nums=processor.image_processor.max_slice_nums,\n",
    "                use_image_id=processor.image_processor.use_image_id,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=8192\n",
    "            )\n",
    "\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            pixel_values = inputs[\"pixel_values\"]\n",
    "            image_sizes = inputs[\"image_sizes\"]\n",
    "            image_bound = inputs[\"image_bound\"]\n",
    "            tgt_sizes = inputs[\"tgt_sizes\"]\n",
    "            traindataset.append({\"input_ids\": input_ids, \n",
    "                                    \"attention_mask\": attention_mask,\n",
    "                                    \"pixel_values\": pixel_values,\n",
    "                                    \"image_sizes\": image_sizes,\n",
    "                                    \"image_bound\": image_bound,\n",
    "                                    \"tgt_sizes\": tgt_sizes})\n",
    "    elif status == 1:\n",
    "        traindataset = []\n",
    "        prompts_lists = []\n",
    "        input_images_lists = []\n",
    "        for index, _data in enumerate(dataset):\n",
    "            promt = _data[\"question\"]\n",
    "            image_file = _data[\"image\"]\n",
    "            image = np.array(image_file)\n",
    "            if image_file is None:\n",
    "                nsamples = nsamples+1\n",
    "                continue\n",
    "            msgs = [{'role': 'user', 'content': \"(<image>./</image>)\\n\"+ promt}]\n",
    "            prompts_lists.append(processor.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True))\n",
    "            input_images_lists.append([image])\n",
    "            if index >= nsamples-1:\n",
    "                break\n",
    "     \n",
    "        inputs = processor(\n",
    "            prompts_lists,\n",
    "            input_images_lists,\n",
    "            max_slice_nums=processor.image_processor.max_slice_nums,\n",
    "            use_image_id=processor.image_processor.use_image_id,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=8192\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        pixel_values = inputs[\"pixel_values\"]\n",
    "        image_sizes = inputs[\"image_sizes\"]\n",
    "        image_bound = inputs[\"image_bound\"]\n",
    "        tgt_sizes = inputs[\"tgt_sizes\"]\n",
    "        traindataset.append({\"input_ids\": input_ids, \n",
    "                                \"attention_mask\": attention_mask,\n",
    "                                \"pixel_values\": pixel_values,\n",
    "                                \"image_sizes\": image_sizes,\n",
    "                                \"image_bound\": image_bound,\n",
    "                                \"tgt_sizes\": tgt_sizes})\n",
    "\n",
    "    return traindataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIT量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "You are using a model of type minicpmv to instantiate a model of type minicpm. This is not supported for all configurations of models and can yield errors.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "/opt/conda/envs/lmquant/lib/python3.11/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256 were not used when initializing MiniCPMV: ['vpm.encoder.layers.0.mlp.fc1.g_idx', 'vpm.encoder.layers.0.mlp.fc1.qweight', 'vpm.encoder.layers.0.mlp.fc1.qzeros', 'vpm.encoder.layers.0.mlp.fc1.scales', 'vpm.encoder.layers.0.mlp.fc2.g_idx', 'vpm.encoder.layers.0.mlp.fc2.qweight', 'vpm.encoder.layers.0.mlp.fc2.qzeros', 'vpm.encoder.layers.0.mlp.fc2.scales', 'vpm.encoder.layers.0.self_attn.k_proj.g_idx', 'vpm.encoder.layers.0.self_attn.k_proj.qweight', 'vpm.encoder.layers.0.self_attn.k_proj.qzeros', 'vpm.encoder.layers.0.self_attn.k_proj.scales', 'vpm.encoder.layers.0.self_attn.out_proj.g_idx', 'vpm.encoder.layers.0.self_attn.out_proj.qweight', 'vpm.encoder.layers.0.self_attn.out_proj.qzeros', 'vpm.encoder.layers.0.self_attn.out_proj.scales', 'vpm.encoder.layers.0.self_attn.q_proj.g_idx', 'vpm.encoder.layers.0.self_attn.q_proj.qweight', 'vpm.encoder.layers.0.self_attn.q_proj.qzeros', 'vpm.encoder.layers.0.self_attn.q_proj.scales', 'vpm.encoder.layers.0.self_attn.v_proj.g_idx', 'vpm.encoder.layers.0.self_attn.v_proj.qweight', 'vpm.encoder.layers.0.self_attn.v_proj.qzeros', 'vpm.encoder.layers.0.self_attn.v_proj.scales', 'vpm.encoder.layers.1.mlp.fc1.g_idx', 'vpm.encoder.layers.1.mlp.fc1.qweight', 'vpm.encoder.layers.1.mlp.fc1.qzeros', 'vpm.encoder.layers.1.mlp.fc1.scales', 'vpm.encoder.layers.1.mlp.fc2.g_idx', 'vpm.encoder.layers.1.mlp.fc2.qweight', 'vpm.encoder.layers.1.mlp.fc2.qzeros', 'vpm.encoder.layers.1.mlp.fc2.scales', 'vpm.encoder.layers.1.self_attn.k_proj.g_idx', 'vpm.encoder.layers.1.self_attn.k_proj.qweight', 'vpm.encoder.layers.1.self_attn.k_proj.qzeros', 'vpm.encoder.layers.1.self_attn.k_proj.scales', 'vpm.encoder.layers.1.self_attn.out_proj.g_idx', 'vpm.encoder.layers.1.self_attn.out_proj.qweight', 'vpm.encoder.layers.1.self_attn.out_proj.qzeros', 'vpm.encoder.layers.1.self_attn.out_proj.scales', 'vpm.encoder.layers.1.self_attn.q_proj.g_idx', 'vpm.encoder.layers.1.self_attn.q_proj.qweight', 'vpm.encoder.layers.1.self_attn.q_proj.qzeros', 'vpm.encoder.layers.1.self_attn.q_proj.scales', 'vpm.encoder.layers.1.self_attn.v_proj.g_idx', 'vpm.encoder.layers.1.self_attn.v_proj.qweight', 'vpm.encoder.layers.1.self_attn.v_proj.qzeros', 'vpm.encoder.layers.1.self_attn.v_proj.scales', 'vpm.encoder.layers.10.mlp.fc1.g_idx', 'vpm.encoder.layers.10.mlp.fc1.qweight', 'vpm.encoder.layers.10.mlp.fc1.qzeros', 'vpm.encoder.layers.10.mlp.fc1.scales', 'vpm.encoder.layers.10.mlp.fc2.g_idx', 'vpm.encoder.layers.10.mlp.fc2.qweight', 'vpm.encoder.layers.10.mlp.fc2.qzeros', 'vpm.encoder.layers.10.mlp.fc2.scales', 'vpm.encoder.layers.10.self_attn.k_proj.g_idx', 'vpm.encoder.layers.10.self_attn.k_proj.qweight', 'vpm.encoder.layers.10.self_attn.k_proj.qzeros', 'vpm.encoder.layers.10.self_attn.k_proj.scales', 'vpm.encoder.layers.10.self_attn.out_proj.g_idx', 'vpm.encoder.layers.10.self_attn.out_proj.qweight', 'vpm.encoder.layers.10.self_attn.out_proj.qzeros', 'vpm.encoder.layers.10.self_attn.out_proj.scales', 'vpm.encoder.layers.10.self_attn.q_proj.g_idx', 'vpm.encoder.layers.10.self_attn.q_proj.qweight', 'vpm.encoder.layers.10.self_attn.q_proj.qzeros', 'vpm.encoder.layers.10.self_attn.q_proj.scales', 'vpm.encoder.layers.10.self_attn.v_proj.g_idx', 'vpm.encoder.layers.10.self_attn.v_proj.qweight', 'vpm.encoder.layers.10.self_attn.v_proj.qzeros', 'vpm.encoder.layers.10.self_attn.v_proj.scales', 'vpm.encoder.layers.11.mlp.fc1.g_idx', 'vpm.encoder.layers.11.mlp.fc1.qweight', 'vpm.encoder.layers.11.mlp.fc1.qzeros', 'vpm.encoder.layers.11.mlp.fc1.scales', 'vpm.encoder.layers.11.mlp.fc2.g_idx', 'vpm.encoder.layers.11.mlp.fc2.qweight', 'vpm.encoder.layers.11.mlp.fc2.qzeros', 'vpm.encoder.layers.11.mlp.fc2.scales', 'vpm.encoder.layers.11.self_attn.k_proj.g_idx', 'vpm.encoder.layers.11.self_attn.k_proj.qweight', 'vpm.encoder.layers.11.self_attn.k_proj.qzeros', 'vpm.encoder.layers.11.self_attn.k_proj.scales', 'vpm.encoder.layers.11.self_attn.out_proj.g_idx', 'vpm.encoder.layers.11.self_attn.out_proj.qweight', 'vpm.encoder.layers.11.self_attn.out_proj.qzeros', 'vpm.encoder.layers.11.self_attn.out_proj.scales', 'vpm.encoder.layers.11.self_attn.q_proj.g_idx', 'vpm.encoder.layers.11.self_attn.q_proj.qweight', 'vpm.encoder.layers.11.self_attn.q_proj.qzeros', 'vpm.encoder.layers.11.self_attn.q_proj.scales', 'vpm.encoder.layers.11.self_attn.v_proj.g_idx', 'vpm.encoder.layers.11.self_attn.v_proj.qweight', 'vpm.encoder.layers.11.self_attn.v_proj.qzeros', 'vpm.encoder.layers.11.self_attn.v_proj.scales', 'vpm.encoder.layers.12.mlp.fc1.g_idx', 'vpm.encoder.layers.12.mlp.fc1.qweight', 'vpm.encoder.layers.12.mlp.fc1.qzeros', 'vpm.encoder.layers.12.mlp.fc1.scales', 'vpm.encoder.layers.12.mlp.fc2.g_idx', 'vpm.encoder.layers.12.mlp.fc2.qweight', 'vpm.encoder.layers.12.mlp.fc2.qzeros', 'vpm.encoder.layers.12.mlp.fc2.scales', 'vpm.encoder.layers.12.self_attn.k_proj.g_idx', 'vpm.encoder.layers.12.self_attn.k_proj.qweight', 'vpm.encoder.layers.12.self_attn.k_proj.qzeros', 'vpm.encoder.layers.12.self_attn.k_proj.scales', 'vpm.encoder.layers.12.self_attn.out_proj.g_idx', 'vpm.encoder.layers.12.self_attn.out_proj.qweight', 'vpm.encoder.layers.12.self_attn.out_proj.qzeros', 'vpm.encoder.layers.12.self_attn.out_proj.scales', 'vpm.encoder.layers.12.self_attn.q_proj.g_idx', 'vpm.encoder.layers.12.self_attn.q_proj.qweight', 'vpm.encoder.layers.12.self_attn.q_proj.qzeros', 'vpm.encoder.layers.12.self_attn.q_proj.scales', 'vpm.encoder.layers.12.self_attn.v_proj.g_idx', 'vpm.encoder.layers.12.self_attn.v_proj.qweight', 'vpm.encoder.layers.12.self_attn.v_proj.qzeros', 'vpm.encoder.layers.12.self_attn.v_proj.scales', 'vpm.encoder.layers.13.mlp.fc1.g_idx', 'vpm.encoder.layers.13.mlp.fc1.qweight', 'vpm.encoder.layers.13.mlp.fc1.qzeros', 'vpm.encoder.layers.13.mlp.fc1.scales', 'vpm.encoder.layers.13.mlp.fc2.g_idx', 'vpm.encoder.layers.13.mlp.fc2.qweight', 'vpm.encoder.layers.13.mlp.fc2.qzeros', 'vpm.encoder.layers.13.mlp.fc2.scales', 'vpm.encoder.layers.13.self_attn.k_proj.g_idx', 'vpm.encoder.layers.13.self_attn.k_proj.qweight', 'vpm.encoder.layers.13.self_attn.k_proj.qzeros', 'vpm.encoder.layers.13.self_attn.k_proj.scales', 'vpm.encoder.layers.13.self_attn.out_proj.g_idx', 'vpm.encoder.layers.13.self_attn.out_proj.qweight', 'vpm.encoder.layers.13.self_attn.out_proj.qzeros', 'vpm.encoder.layers.13.self_attn.out_proj.scales', 'vpm.encoder.layers.13.self_attn.q_proj.g_idx', 'vpm.encoder.layers.13.self_attn.q_proj.qweight', 'vpm.encoder.layers.13.self_attn.q_proj.qzeros', 'vpm.encoder.layers.13.self_attn.q_proj.scales', 'vpm.encoder.layers.13.self_attn.v_proj.g_idx', 'vpm.encoder.layers.13.self_attn.v_proj.qweight', 'vpm.encoder.layers.13.self_attn.v_proj.qzeros', 'vpm.encoder.layers.13.self_attn.v_proj.scales', 'vpm.encoder.layers.14.mlp.fc1.g_idx', 'vpm.encoder.layers.14.mlp.fc1.qweight', 'vpm.encoder.layers.14.mlp.fc1.qzeros', 'vpm.encoder.layers.14.mlp.fc1.scales', 'vpm.encoder.layers.14.mlp.fc2.g_idx', 'vpm.encoder.layers.14.mlp.fc2.qweight', 'vpm.encoder.layers.14.mlp.fc2.qzeros', 'vpm.encoder.layers.14.mlp.fc2.scales', 'vpm.encoder.layers.14.self_attn.k_proj.g_idx', 'vpm.encoder.layers.14.self_attn.k_proj.qweight', 'vpm.encoder.layers.14.self_attn.k_proj.qzeros', 'vpm.encoder.layers.14.self_attn.k_proj.scales', 'vpm.encoder.layers.14.self_attn.out_proj.g_idx', 'vpm.encoder.layers.14.self_attn.out_proj.qweight', 'vpm.encoder.layers.14.self_attn.out_proj.qzeros', 'vpm.encoder.layers.14.self_attn.out_proj.scales', 'vpm.encoder.layers.14.self_attn.q_proj.g_idx', 'vpm.encoder.layers.14.self_attn.q_proj.qweight', 'vpm.encoder.layers.14.self_attn.q_proj.qzeros', 'vpm.encoder.layers.14.self_attn.q_proj.scales', 'vpm.encoder.layers.14.self_attn.v_proj.g_idx', 'vpm.encoder.layers.14.self_attn.v_proj.qweight', 'vpm.encoder.layers.14.self_attn.v_proj.qzeros', 'vpm.encoder.layers.14.self_attn.v_proj.scales', 'vpm.encoder.layers.15.mlp.fc1.g_idx', 'vpm.encoder.layers.15.mlp.fc1.qweight', 'vpm.encoder.layers.15.mlp.fc1.qzeros', 'vpm.encoder.layers.15.mlp.fc1.scales', 'vpm.encoder.layers.15.mlp.fc2.g_idx', 'vpm.encoder.layers.15.mlp.fc2.qweight', 'vpm.encoder.layers.15.mlp.fc2.qzeros', 'vpm.encoder.layers.15.mlp.fc2.scales', 'vpm.encoder.layers.15.self_attn.k_proj.g_idx', 'vpm.encoder.layers.15.self_attn.k_proj.qweight', 'vpm.encoder.layers.15.self_attn.k_proj.qzeros', 'vpm.encoder.layers.15.self_attn.k_proj.scales', 'vpm.encoder.layers.15.self_attn.out_proj.g_idx', 'vpm.encoder.layers.15.self_attn.out_proj.qweight', 'vpm.encoder.layers.15.self_attn.out_proj.qzeros', 'vpm.encoder.layers.15.self_attn.out_proj.scales', 'vpm.encoder.layers.15.self_attn.q_proj.g_idx', 'vpm.encoder.layers.15.self_attn.q_proj.qweight', 'vpm.encoder.layers.15.self_attn.q_proj.qzeros', 'vpm.encoder.layers.15.self_attn.q_proj.scales', 'vpm.encoder.layers.15.self_attn.v_proj.g_idx', 'vpm.encoder.layers.15.self_attn.v_proj.qweight', 'vpm.encoder.layers.15.self_attn.v_proj.qzeros', 'vpm.encoder.layers.15.self_attn.v_proj.scales', 'vpm.encoder.layers.16.mlp.fc1.g_idx', 'vpm.encoder.layers.16.mlp.fc1.qweight', 'vpm.encoder.layers.16.mlp.fc1.qzeros', 'vpm.encoder.layers.16.mlp.fc1.scales', 'vpm.encoder.layers.16.mlp.fc2.g_idx', 'vpm.encoder.layers.16.mlp.fc2.qweight', 'vpm.encoder.layers.16.mlp.fc2.qzeros', 'vpm.encoder.layers.16.mlp.fc2.scales', 'vpm.encoder.layers.16.self_attn.k_proj.g_idx', 'vpm.encoder.layers.16.self_attn.k_proj.qweight', 'vpm.encoder.layers.16.self_attn.k_proj.qzeros', 'vpm.encoder.layers.16.self_attn.k_proj.scales', 'vpm.encoder.layers.16.self_attn.out_proj.g_idx', 'vpm.encoder.layers.16.self_attn.out_proj.qweight', 'vpm.encoder.layers.16.self_attn.out_proj.qzeros', 'vpm.encoder.layers.16.self_attn.out_proj.scales', 'vpm.encoder.layers.16.self_attn.q_proj.g_idx', 'vpm.encoder.layers.16.self_attn.q_proj.qweight', 'vpm.encoder.layers.16.self_attn.q_proj.qzeros', 'vpm.encoder.layers.16.self_attn.q_proj.scales', 'vpm.encoder.layers.16.self_attn.v_proj.g_idx', 'vpm.encoder.layers.16.self_attn.v_proj.qweight', 'vpm.encoder.layers.16.self_attn.v_proj.qzeros', 'vpm.encoder.layers.16.self_attn.v_proj.scales', 'vpm.encoder.layers.17.mlp.fc1.g_idx', 'vpm.encoder.layers.17.mlp.fc1.qweight', 'vpm.encoder.layers.17.mlp.fc1.qzeros', 'vpm.encoder.layers.17.mlp.fc1.scales', 'vpm.encoder.layers.17.mlp.fc2.g_idx', 'vpm.encoder.layers.17.mlp.fc2.qweight', 'vpm.encoder.layers.17.mlp.fc2.qzeros', 'vpm.encoder.layers.17.mlp.fc2.scales', 'vpm.encoder.layers.17.self_attn.k_proj.g_idx', 'vpm.encoder.layers.17.self_attn.k_proj.qweight', 'vpm.encoder.layers.17.self_attn.k_proj.qzeros', 'vpm.encoder.layers.17.self_attn.k_proj.scales', 'vpm.encoder.layers.17.self_attn.out_proj.g_idx', 'vpm.encoder.layers.17.self_attn.out_proj.qweight', 'vpm.encoder.layers.17.self_attn.out_proj.qzeros', 'vpm.encoder.layers.17.self_attn.out_proj.scales', 'vpm.encoder.layers.17.self_attn.q_proj.g_idx', 'vpm.encoder.layers.17.self_attn.q_proj.qweight', 'vpm.encoder.layers.17.self_attn.q_proj.qzeros', 'vpm.encoder.layers.17.self_attn.q_proj.scales', 'vpm.encoder.layers.17.self_attn.v_proj.g_idx', 'vpm.encoder.layers.17.self_attn.v_proj.qweight', 'vpm.encoder.layers.17.self_attn.v_proj.qzeros', 'vpm.encoder.layers.17.self_attn.v_proj.scales', 'vpm.encoder.layers.18.mlp.fc1.g_idx', 'vpm.encoder.layers.18.mlp.fc1.qweight', 'vpm.encoder.layers.18.mlp.fc1.qzeros', 'vpm.encoder.layers.18.mlp.fc1.scales', 'vpm.encoder.layers.18.mlp.fc2.g_idx', 'vpm.encoder.layers.18.mlp.fc2.qweight', 'vpm.encoder.layers.18.mlp.fc2.qzeros', 'vpm.encoder.layers.18.mlp.fc2.scales', 'vpm.encoder.layers.18.self_attn.k_proj.g_idx', 'vpm.encoder.layers.18.self_attn.k_proj.qweight', 'vpm.encoder.layers.18.self_attn.k_proj.qzeros', 'vpm.encoder.layers.18.self_attn.k_proj.scales', 'vpm.encoder.layers.18.self_attn.out_proj.g_idx', 'vpm.encoder.layers.18.self_attn.out_proj.qweight', 'vpm.encoder.layers.18.self_attn.out_proj.qzeros', 'vpm.encoder.layers.18.self_attn.out_proj.scales', 'vpm.encoder.layers.18.self_attn.q_proj.g_idx', 'vpm.encoder.layers.18.self_attn.q_proj.qweight', 'vpm.encoder.layers.18.self_attn.q_proj.qzeros', 'vpm.encoder.layers.18.self_attn.q_proj.scales', 'vpm.encoder.layers.18.self_attn.v_proj.g_idx', 'vpm.encoder.layers.18.self_attn.v_proj.qweight', 'vpm.encoder.layers.18.self_attn.v_proj.qzeros', 'vpm.encoder.layers.18.self_attn.v_proj.scales', 'vpm.encoder.layers.19.mlp.fc1.g_idx', 'vpm.encoder.layers.19.mlp.fc1.qweight', 'vpm.encoder.layers.19.mlp.fc1.qzeros', 'vpm.encoder.layers.19.mlp.fc1.scales', 'vpm.encoder.layers.19.mlp.fc2.g_idx', 'vpm.encoder.layers.19.mlp.fc2.qweight', 'vpm.encoder.layers.19.mlp.fc2.qzeros', 'vpm.encoder.layers.19.mlp.fc2.scales', 'vpm.encoder.layers.19.self_attn.k_proj.g_idx', 'vpm.encoder.layers.19.self_attn.k_proj.qweight', 'vpm.encoder.layers.19.self_attn.k_proj.qzeros', 'vpm.encoder.layers.19.self_attn.k_proj.scales', 'vpm.encoder.layers.19.self_attn.out_proj.g_idx', 'vpm.encoder.layers.19.self_attn.out_proj.qweight', 'vpm.encoder.layers.19.self_attn.out_proj.qzeros', 'vpm.encoder.layers.19.self_attn.out_proj.scales', 'vpm.encoder.layers.19.self_attn.q_proj.g_idx', 'vpm.encoder.layers.19.self_attn.q_proj.qweight', 'vpm.encoder.layers.19.self_attn.q_proj.qzeros', 'vpm.encoder.layers.19.self_attn.q_proj.scales', 'vpm.encoder.layers.19.self_attn.v_proj.g_idx', 'vpm.encoder.layers.19.self_attn.v_proj.qweight', 'vpm.encoder.layers.19.self_attn.v_proj.qzeros', 'vpm.encoder.layers.19.self_attn.v_proj.scales', 'vpm.encoder.layers.2.mlp.fc1.g_idx', 'vpm.encoder.layers.2.mlp.fc1.qweight', 'vpm.encoder.layers.2.mlp.fc1.qzeros', 'vpm.encoder.layers.2.mlp.fc1.scales', 'vpm.encoder.layers.2.mlp.fc2.g_idx', 'vpm.encoder.layers.2.mlp.fc2.qweight', 'vpm.encoder.layers.2.mlp.fc2.qzeros', 'vpm.encoder.layers.2.mlp.fc2.scales', 'vpm.encoder.layers.2.self_attn.k_proj.g_idx', 'vpm.encoder.layers.2.self_attn.k_proj.qweight', 'vpm.encoder.layers.2.self_attn.k_proj.qzeros', 'vpm.encoder.layers.2.self_attn.k_proj.scales', 'vpm.encoder.layers.2.self_attn.out_proj.g_idx', 'vpm.encoder.layers.2.self_attn.out_proj.qweight', 'vpm.encoder.layers.2.self_attn.out_proj.qzeros', 'vpm.encoder.layers.2.self_attn.out_proj.scales', 'vpm.encoder.layers.2.self_attn.q_proj.g_idx', 'vpm.encoder.layers.2.self_attn.q_proj.qweight', 'vpm.encoder.layers.2.self_attn.q_proj.qzeros', 'vpm.encoder.layers.2.self_attn.q_proj.scales', 'vpm.encoder.layers.2.self_attn.v_proj.g_idx', 'vpm.encoder.layers.2.self_attn.v_proj.qweight', 'vpm.encoder.layers.2.self_attn.v_proj.qzeros', 'vpm.encoder.layers.2.self_attn.v_proj.scales', 'vpm.encoder.layers.20.mlp.fc1.g_idx', 'vpm.encoder.layers.20.mlp.fc1.qweight', 'vpm.encoder.layers.20.mlp.fc1.qzeros', 'vpm.encoder.layers.20.mlp.fc1.scales', 'vpm.encoder.layers.20.mlp.fc2.g_idx', 'vpm.encoder.layers.20.mlp.fc2.qweight', 'vpm.encoder.layers.20.mlp.fc2.qzeros', 'vpm.encoder.layers.20.mlp.fc2.scales', 'vpm.encoder.layers.20.self_attn.k_proj.g_idx', 'vpm.encoder.layers.20.self_attn.k_proj.qweight', 'vpm.encoder.layers.20.self_attn.k_proj.qzeros', 'vpm.encoder.layers.20.self_attn.k_proj.scales', 'vpm.encoder.layers.20.self_attn.out_proj.g_idx', 'vpm.encoder.layers.20.self_attn.out_proj.qweight', 'vpm.encoder.layers.20.self_attn.out_proj.qzeros', 'vpm.encoder.layers.20.self_attn.out_proj.scales', 'vpm.encoder.layers.20.self_attn.q_proj.g_idx', 'vpm.encoder.layers.20.self_attn.q_proj.qweight', 'vpm.encoder.layers.20.self_attn.q_proj.qzeros', 'vpm.encoder.layers.20.self_attn.q_proj.scales', 'vpm.encoder.layers.20.self_attn.v_proj.g_idx', 'vpm.encoder.layers.20.self_attn.v_proj.qweight', 'vpm.encoder.layers.20.self_attn.v_proj.qzeros', 'vpm.encoder.layers.20.self_attn.v_proj.scales', 'vpm.encoder.layers.21.mlp.fc1.g_idx', 'vpm.encoder.layers.21.mlp.fc1.qweight', 'vpm.encoder.layers.21.mlp.fc1.qzeros', 'vpm.encoder.layers.21.mlp.fc1.scales', 'vpm.encoder.layers.21.mlp.fc2.g_idx', 'vpm.encoder.layers.21.mlp.fc2.qweight', 'vpm.encoder.layers.21.mlp.fc2.qzeros', 'vpm.encoder.layers.21.mlp.fc2.scales', 'vpm.encoder.layers.21.self_attn.k_proj.g_idx', 'vpm.encoder.layers.21.self_attn.k_proj.qweight', 'vpm.encoder.layers.21.self_attn.k_proj.qzeros', 'vpm.encoder.layers.21.self_attn.k_proj.scales', 'vpm.encoder.layers.21.self_attn.out_proj.g_idx', 'vpm.encoder.layers.21.self_attn.out_proj.qweight', 'vpm.encoder.layers.21.self_attn.out_proj.qzeros', 'vpm.encoder.layers.21.self_attn.out_proj.scales', 'vpm.encoder.layers.21.self_attn.q_proj.g_idx', 'vpm.encoder.layers.21.self_attn.q_proj.qweight', 'vpm.encoder.layers.21.self_attn.q_proj.qzeros', 'vpm.encoder.layers.21.self_attn.q_proj.scales', 'vpm.encoder.layers.21.self_attn.v_proj.g_idx', 'vpm.encoder.layers.21.self_attn.v_proj.qweight', 'vpm.encoder.layers.21.self_attn.v_proj.qzeros', 'vpm.encoder.layers.21.self_attn.v_proj.scales', 'vpm.encoder.layers.22.mlp.fc1.g_idx', 'vpm.encoder.layers.22.mlp.fc1.qweight', 'vpm.encoder.layers.22.mlp.fc1.qzeros', 'vpm.encoder.layers.22.mlp.fc1.scales', 'vpm.encoder.layers.22.mlp.fc2.g_idx', 'vpm.encoder.layers.22.mlp.fc2.qweight', 'vpm.encoder.layers.22.mlp.fc2.qzeros', 'vpm.encoder.layers.22.mlp.fc2.scales', 'vpm.encoder.layers.22.self_attn.k_proj.g_idx', 'vpm.encoder.layers.22.self_attn.k_proj.qweight', 'vpm.encoder.layers.22.self_attn.k_proj.qzeros', 'vpm.encoder.layers.22.self_attn.k_proj.scales', 'vpm.encoder.layers.22.self_attn.out_proj.g_idx', 'vpm.encoder.layers.22.self_attn.out_proj.qweight', 'vpm.encoder.layers.22.self_attn.out_proj.qzeros', 'vpm.encoder.layers.22.self_attn.out_proj.scales', 'vpm.encoder.layers.22.self_attn.q_proj.g_idx', 'vpm.encoder.layers.22.self_attn.q_proj.qweight', 'vpm.encoder.layers.22.self_attn.q_proj.qzeros', 'vpm.encoder.layers.22.self_attn.q_proj.scales', 'vpm.encoder.layers.22.self_attn.v_proj.g_idx', 'vpm.encoder.layers.22.self_attn.v_proj.qweight', 'vpm.encoder.layers.22.self_attn.v_proj.qzeros', 'vpm.encoder.layers.22.self_attn.v_proj.scales', 'vpm.encoder.layers.23.mlp.fc1.g_idx', 'vpm.encoder.layers.23.mlp.fc1.qweight', 'vpm.encoder.layers.23.mlp.fc1.qzeros', 'vpm.encoder.layers.23.mlp.fc1.scales', 'vpm.encoder.layers.23.mlp.fc2.g_idx', 'vpm.encoder.layers.23.mlp.fc2.qweight', 'vpm.encoder.layers.23.mlp.fc2.qzeros', 'vpm.encoder.layers.23.mlp.fc2.scales', 'vpm.encoder.layers.23.self_attn.k_proj.g_idx', 'vpm.encoder.layers.23.self_attn.k_proj.qweight', 'vpm.encoder.layers.23.self_attn.k_proj.qzeros', 'vpm.encoder.layers.23.self_attn.k_proj.scales', 'vpm.encoder.layers.23.self_attn.out_proj.g_idx', 'vpm.encoder.layers.23.self_attn.out_proj.qweight', 'vpm.encoder.layers.23.self_attn.out_proj.qzeros', 'vpm.encoder.layers.23.self_attn.out_proj.scales', 'vpm.encoder.layers.23.self_attn.q_proj.g_idx', 'vpm.encoder.layers.23.self_attn.q_proj.qweight', 'vpm.encoder.layers.23.self_attn.q_proj.qzeros', 'vpm.encoder.layers.23.self_attn.q_proj.scales', 'vpm.encoder.layers.23.self_attn.v_proj.g_idx', 'vpm.encoder.layers.23.self_attn.v_proj.qweight', 'vpm.encoder.layers.23.self_attn.v_proj.qzeros', 'vpm.encoder.layers.23.self_attn.v_proj.scales', 'vpm.encoder.layers.24.mlp.fc1.g_idx', 'vpm.encoder.layers.24.mlp.fc1.qweight', 'vpm.encoder.layers.24.mlp.fc1.qzeros', 'vpm.encoder.layers.24.mlp.fc1.scales', 'vpm.encoder.layers.24.mlp.fc2.g_idx', 'vpm.encoder.layers.24.mlp.fc2.qweight', 'vpm.encoder.layers.24.mlp.fc2.qzeros', 'vpm.encoder.layers.24.mlp.fc2.scales', 'vpm.encoder.layers.24.self_attn.k_proj.g_idx', 'vpm.encoder.layers.24.self_attn.k_proj.qweight', 'vpm.encoder.layers.24.self_attn.k_proj.qzeros', 'vpm.encoder.layers.24.self_attn.k_proj.scales', 'vpm.encoder.layers.24.self_attn.out_proj.g_idx', 'vpm.encoder.layers.24.self_attn.out_proj.qweight', 'vpm.encoder.layers.24.self_attn.out_proj.qzeros', 'vpm.encoder.layers.24.self_attn.out_proj.scales', 'vpm.encoder.layers.24.self_attn.q_proj.g_idx', 'vpm.encoder.layers.24.self_attn.q_proj.qweight', 'vpm.encoder.layers.24.self_attn.q_proj.qzeros', 'vpm.encoder.layers.24.self_attn.q_proj.scales', 'vpm.encoder.layers.24.self_attn.v_proj.g_idx', 'vpm.encoder.layers.24.self_attn.v_proj.qweight', 'vpm.encoder.layers.24.self_attn.v_proj.qzeros', 'vpm.encoder.layers.24.self_attn.v_proj.scales', 'vpm.encoder.layers.25.mlp.fc1.g_idx', 'vpm.encoder.layers.25.mlp.fc1.qweight', 'vpm.encoder.layers.25.mlp.fc1.qzeros', 'vpm.encoder.layers.25.mlp.fc1.scales', 'vpm.encoder.layers.25.mlp.fc2.g_idx', 'vpm.encoder.layers.25.mlp.fc2.qweight', 'vpm.encoder.layers.25.mlp.fc2.qzeros', 'vpm.encoder.layers.25.mlp.fc2.scales', 'vpm.encoder.layers.25.self_attn.k_proj.g_idx', 'vpm.encoder.layers.25.self_attn.k_proj.qweight', 'vpm.encoder.layers.25.self_attn.k_proj.qzeros', 'vpm.encoder.layers.25.self_attn.k_proj.scales', 'vpm.encoder.layers.25.self_attn.out_proj.g_idx', 'vpm.encoder.layers.25.self_attn.out_proj.qweight', 'vpm.encoder.layers.25.self_attn.out_proj.qzeros', 'vpm.encoder.layers.25.self_attn.out_proj.scales', 'vpm.encoder.layers.25.self_attn.q_proj.g_idx', 'vpm.encoder.layers.25.self_attn.q_proj.qweight', 'vpm.encoder.layers.25.self_attn.q_proj.qzeros', 'vpm.encoder.layers.25.self_attn.q_proj.scales', 'vpm.encoder.layers.25.self_attn.v_proj.g_idx', 'vpm.encoder.layers.25.self_attn.v_proj.qweight', 'vpm.encoder.layers.25.self_attn.v_proj.qzeros', 'vpm.encoder.layers.25.self_attn.v_proj.scales', 'vpm.encoder.layers.26.mlp.fc1.g_idx', 'vpm.encoder.layers.26.mlp.fc1.qweight', 'vpm.encoder.layers.26.mlp.fc1.qzeros', 'vpm.encoder.layers.26.mlp.fc1.scales', 'vpm.encoder.layers.26.mlp.fc2.g_idx', 'vpm.encoder.layers.26.mlp.fc2.qweight', 'vpm.encoder.layers.26.mlp.fc2.qzeros', 'vpm.encoder.layers.26.mlp.fc2.scales', 'vpm.encoder.layers.26.self_attn.k_proj.g_idx', 'vpm.encoder.layers.26.self_attn.k_proj.qweight', 'vpm.encoder.layers.26.self_attn.k_proj.qzeros', 'vpm.encoder.layers.26.self_attn.k_proj.scales', 'vpm.encoder.layers.26.self_attn.out_proj.g_idx', 'vpm.encoder.layers.26.self_attn.out_proj.qweight', 'vpm.encoder.layers.26.self_attn.out_proj.qzeros', 'vpm.encoder.layers.26.self_attn.out_proj.scales', 'vpm.encoder.layers.26.self_attn.q_proj.g_idx', 'vpm.encoder.layers.26.self_attn.q_proj.qweight', 'vpm.encoder.layers.26.self_attn.q_proj.qzeros', 'vpm.encoder.layers.26.self_attn.q_proj.scales', 'vpm.encoder.layers.26.self_attn.v_proj.g_idx', 'vpm.encoder.layers.26.self_attn.v_proj.qweight', 'vpm.encoder.layers.26.self_attn.v_proj.qzeros', 'vpm.encoder.layers.26.self_attn.v_proj.scales', 'vpm.encoder.layers.3.mlp.fc1.g_idx', 'vpm.encoder.layers.3.mlp.fc1.qweight', 'vpm.encoder.layers.3.mlp.fc1.qzeros', 'vpm.encoder.layers.3.mlp.fc1.scales', 'vpm.encoder.layers.3.mlp.fc2.g_idx', 'vpm.encoder.layers.3.mlp.fc2.qweight', 'vpm.encoder.layers.3.mlp.fc2.qzeros', 'vpm.encoder.layers.3.mlp.fc2.scales', 'vpm.encoder.layers.3.self_attn.k_proj.g_idx', 'vpm.encoder.layers.3.self_attn.k_proj.qweight', 'vpm.encoder.layers.3.self_attn.k_proj.qzeros', 'vpm.encoder.layers.3.self_attn.k_proj.scales', 'vpm.encoder.layers.3.self_attn.out_proj.g_idx', 'vpm.encoder.layers.3.self_attn.out_proj.qweight', 'vpm.encoder.layers.3.self_attn.out_proj.qzeros', 'vpm.encoder.layers.3.self_attn.out_proj.scales', 'vpm.encoder.layers.3.self_attn.q_proj.g_idx', 'vpm.encoder.layers.3.self_attn.q_proj.qweight', 'vpm.encoder.layers.3.self_attn.q_proj.qzeros', 'vpm.encoder.layers.3.self_attn.q_proj.scales', 'vpm.encoder.layers.3.self_attn.v_proj.g_idx', 'vpm.encoder.layers.3.self_attn.v_proj.qweight', 'vpm.encoder.layers.3.self_attn.v_proj.qzeros', 'vpm.encoder.layers.3.self_attn.v_proj.scales', 'vpm.encoder.layers.4.mlp.fc1.g_idx', 'vpm.encoder.layers.4.mlp.fc1.qweight', 'vpm.encoder.layers.4.mlp.fc1.qzeros', 'vpm.encoder.layers.4.mlp.fc1.scales', 'vpm.encoder.layers.4.mlp.fc2.g_idx', 'vpm.encoder.layers.4.mlp.fc2.qweight', 'vpm.encoder.layers.4.mlp.fc2.qzeros', 'vpm.encoder.layers.4.mlp.fc2.scales', 'vpm.encoder.layers.4.self_attn.k_proj.g_idx', 'vpm.encoder.layers.4.self_attn.k_proj.qweight', 'vpm.encoder.layers.4.self_attn.k_proj.qzeros', 'vpm.encoder.layers.4.self_attn.k_proj.scales', 'vpm.encoder.layers.4.self_attn.out_proj.g_idx', 'vpm.encoder.layers.4.self_attn.out_proj.qweight', 'vpm.encoder.layers.4.self_attn.out_proj.qzeros', 'vpm.encoder.layers.4.self_attn.out_proj.scales', 'vpm.encoder.layers.4.self_attn.q_proj.g_idx', 'vpm.encoder.layers.4.self_attn.q_proj.qweight', 'vpm.encoder.layers.4.self_attn.q_proj.qzeros', 'vpm.encoder.layers.4.self_attn.q_proj.scales', 'vpm.encoder.layers.4.self_attn.v_proj.g_idx', 'vpm.encoder.layers.4.self_attn.v_proj.qweight', 'vpm.encoder.layers.4.self_attn.v_proj.qzeros', 'vpm.encoder.layers.4.self_attn.v_proj.scales', 'vpm.encoder.layers.5.mlp.fc1.g_idx', 'vpm.encoder.layers.5.mlp.fc1.qweight', 'vpm.encoder.layers.5.mlp.fc1.qzeros', 'vpm.encoder.layers.5.mlp.fc1.scales', 'vpm.encoder.layers.5.mlp.fc2.g_idx', 'vpm.encoder.layers.5.mlp.fc2.qweight', 'vpm.encoder.layers.5.mlp.fc2.qzeros', 'vpm.encoder.layers.5.mlp.fc2.scales', 'vpm.encoder.layers.5.self_attn.k_proj.g_idx', 'vpm.encoder.layers.5.self_attn.k_proj.qweight', 'vpm.encoder.layers.5.self_attn.k_proj.qzeros', 'vpm.encoder.layers.5.self_attn.k_proj.scales', 'vpm.encoder.layers.5.self_attn.out_proj.g_idx', 'vpm.encoder.layers.5.self_attn.out_proj.qweight', 'vpm.encoder.layers.5.self_attn.out_proj.qzeros', 'vpm.encoder.layers.5.self_attn.out_proj.scales', 'vpm.encoder.layers.5.self_attn.q_proj.g_idx', 'vpm.encoder.layers.5.self_attn.q_proj.qweight', 'vpm.encoder.layers.5.self_attn.q_proj.qzeros', 'vpm.encoder.layers.5.self_attn.q_proj.scales', 'vpm.encoder.layers.5.self_attn.v_proj.g_idx', 'vpm.encoder.layers.5.self_attn.v_proj.qweight', 'vpm.encoder.layers.5.self_attn.v_proj.qzeros', 'vpm.encoder.layers.5.self_attn.v_proj.scales', 'vpm.encoder.layers.6.mlp.fc1.g_idx', 'vpm.encoder.layers.6.mlp.fc1.qweight', 'vpm.encoder.layers.6.mlp.fc1.qzeros', 'vpm.encoder.layers.6.mlp.fc1.scales', 'vpm.encoder.layers.6.mlp.fc2.g_idx', 'vpm.encoder.layers.6.mlp.fc2.qweight', 'vpm.encoder.layers.6.mlp.fc2.qzeros', 'vpm.encoder.layers.6.mlp.fc2.scales', 'vpm.encoder.layers.6.self_attn.k_proj.g_idx', 'vpm.encoder.layers.6.self_attn.k_proj.qweight', 'vpm.encoder.layers.6.self_attn.k_proj.qzeros', 'vpm.encoder.layers.6.self_attn.k_proj.scales', 'vpm.encoder.layers.6.self_attn.out_proj.g_idx', 'vpm.encoder.layers.6.self_attn.out_proj.qweight', 'vpm.encoder.layers.6.self_attn.out_proj.qzeros', 'vpm.encoder.layers.6.self_attn.out_proj.scales', 'vpm.encoder.layers.6.self_attn.q_proj.g_idx', 'vpm.encoder.layers.6.self_attn.q_proj.qweight', 'vpm.encoder.layers.6.self_attn.q_proj.qzeros', 'vpm.encoder.layers.6.self_attn.q_proj.scales', 'vpm.encoder.layers.6.self_attn.v_proj.g_idx', 'vpm.encoder.layers.6.self_attn.v_proj.qweight', 'vpm.encoder.layers.6.self_attn.v_proj.qzeros', 'vpm.encoder.layers.6.self_attn.v_proj.scales', 'vpm.encoder.layers.7.mlp.fc1.g_idx', 'vpm.encoder.layers.7.mlp.fc1.qweight', 'vpm.encoder.layers.7.mlp.fc1.qzeros', 'vpm.encoder.layers.7.mlp.fc1.scales', 'vpm.encoder.layers.7.mlp.fc2.g_idx', 'vpm.encoder.layers.7.mlp.fc2.qweight', 'vpm.encoder.layers.7.mlp.fc2.qzeros', 'vpm.encoder.layers.7.mlp.fc2.scales', 'vpm.encoder.layers.7.self_attn.k_proj.g_idx', 'vpm.encoder.layers.7.self_attn.k_proj.qweight', 'vpm.encoder.layers.7.self_attn.k_proj.qzeros', 'vpm.encoder.layers.7.self_attn.k_proj.scales', 'vpm.encoder.layers.7.self_attn.out_proj.g_idx', 'vpm.encoder.layers.7.self_attn.out_proj.qweight', 'vpm.encoder.layers.7.self_attn.out_proj.qzeros', 'vpm.encoder.layers.7.self_attn.out_proj.scales', 'vpm.encoder.layers.7.self_attn.q_proj.g_idx', 'vpm.encoder.layers.7.self_attn.q_proj.qweight', 'vpm.encoder.layers.7.self_attn.q_proj.qzeros', 'vpm.encoder.layers.7.self_attn.q_proj.scales', 'vpm.encoder.layers.7.self_attn.v_proj.g_idx', 'vpm.encoder.layers.7.self_attn.v_proj.qweight', 'vpm.encoder.layers.7.self_attn.v_proj.qzeros', 'vpm.encoder.layers.7.self_attn.v_proj.scales', 'vpm.encoder.layers.8.mlp.fc1.g_idx', 'vpm.encoder.layers.8.mlp.fc1.qweight', 'vpm.encoder.layers.8.mlp.fc1.qzeros', 'vpm.encoder.layers.8.mlp.fc1.scales', 'vpm.encoder.layers.8.mlp.fc2.g_idx', 'vpm.encoder.layers.8.mlp.fc2.qweight', 'vpm.encoder.layers.8.mlp.fc2.qzeros', 'vpm.encoder.layers.8.mlp.fc2.scales', 'vpm.encoder.layers.8.self_attn.k_proj.g_idx', 'vpm.encoder.layers.8.self_attn.k_proj.qweight', 'vpm.encoder.layers.8.self_attn.k_proj.qzeros', 'vpm.encoder.layers.8.self_attn.k_proj.scales', 'vpm.encoder.layers.8.self_attn.out_proj.g_idx', 'vpm.encoder.layers.8.self_attn.out_proj.qweight', 'vpm.encoder.layers.8.self_attn.out_proj.qzeros', 'vpm.encoder.layers.8.self_attn.out_proj.scales', 'vpm.encoder.layers.8.self_attn.q_proj.g_idx', 'vpm.encoder.layers.8.self_attn.q_proj.qweight', 'vpm.encoder.layers.8.self_attn.q_proj.qzeros', 'vpm.encoder.layers.8.self_attn.q_proj.scales', 'vpm.encoder.layers.8.self_attn.v_proj.g_idx', 'vpm.encoder.layers.8.self_attn.v_proj.qweight', 'vpm.encoder.layers.8.self_attn.v_proj.qzeros', 'vpm.encoder.layers.8.self_attn.v_proj.scales', 'vpm.encoder.layers.9.mlp.fc1.g_idx', 'vpm.encoder.layers.9.mlp.fc1.qweight', 'vpm.encoder.layers.9.mlp.fc1.qzeros', 'vpm.encoder.layers.9.mlp.fc1.scales', 'vpm.encoder.layers.9.mlp.fc2.g_idx', 'vpm.encoder.layers.9.mlp.fc2.qweight', 'vpm.encoder.layers.9.mlp.fc2.qzeros', 'vpm.encoder.layers.9.mlp.fc2.scales', 'vpm.encoder.layers.9.self_attn.k_proj.g_idx', 'vpm.encoder.layers.9.self_attn.k_proj.qweight', 'vpm.encoder.layers.9.self_attn.k_proj.qzeros', 'vpm.encoder.layers.9.self_attn.k_proj.scales', 'vpm.encoder.layers.9.self_attn.out_proj.g_idx', 'vpm.encoder.layers.9.self_attn.out_proj.qweight', 'vpm.encoder.layers.9.self_attn.out_proj.qzeros', 'vpm.encoder.layers.9.self_attn.out_proj.scales', 'vpm.encoder.layers.9.self_attn.q_proj.g_idx', 'vpm.encoder.layers.9.self_attn.q_proj.qweight', 'vpm.encoder.layers.9.self_attn.q_proj.qzeros', 'vpm.encoder.layers.9.self_attn.q_proj.scales', 'vpm.encoder.layers.9.self_attn.v_proj.g_idx', 'vpm.encoder.layers.9.self_attn.v_proj.qweight', 'vpm.encoder.layers.9.self_attn.v_proj.qzeros', 'vpm.encoder.layers.9.self_attn.v_proj.scales']\n",
      "- This IS expected if you are initializing MiniCPMV from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MiniCPMV from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MiniCPMV were not initialized from the model checkpoint at /home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256 and are newly initialized: ['vpm.encoder.layers.0.mlp.fc1.weight', 'vpm.encoder.layers.0.mlp.fc2.weight', 'vpm.encoder.layers.0.self_attn.k_proj.weight', 'vpm.encoder.layers.0.self_attn.out_proj.weight', 'vpm.encoder.layers.0.self_attn.q_proj.weight', 'vpm.encoder.layers.0.self_attn.v_proj.weight', 'vpm.encoder.layers.1.mlp.fc1.weight', 'vpm.encoder.layers.1.mlp.fc2.weight', 'vpm.encoder.layers.1.self_attn.k_proj.weight', 'vpm.encoder.layers.1.self_attn.out_proj.weight', 'vpm.encoder.layers.1.self_attn.q_proj.weight', 'vpm.encoder.layers.1.self_attn.v_proj.weight', 'vpm.encoder.layers.10.mlp.fc1.weight', 'vpm.encoder.layers.10.mlp.fc2.weight', 'vpm.encoder.layers.10.self_attn.k_proj.weight', 'vpm.encoder.layers.10.self_attn.out_proj.weight', 'vpm.encoder.layers.10.self_attn.q_proj.weight', 'vpm.encoder.layers.10.self_attn.v_proj.weight', 'vpm.encoder.layers.11.mlp.fc1.weight', 'vpm.encoder.layers.11.mlp.fc2.weight', 'vpm.encoder.layers.11.self_attn.k_proj.weight', 'vpm.encoder.layers.11.self_attn.out_proj.weight', 'vpm.encoder.layers.11.self_attn.q_proj.weight', 'vpm.encoder.layers.11.self_attn.v_proj.weight', 'vpm.encoder.layers.12.mlp.fc1.weight', 'vpm.encoder.layers.12.mlp.fc2.weight', 'vpm.encoder.layers.12.self_attn.k_proj.weight', 'vpm.encoder.layers.12.self_attn.out_proj.weight', 'vpm.encoder.layers.12.self_attn.q_proj.weight', 'vpm.encoder.layers.12.self_attn.v_proj.weight', 'vpm.encoder.layers.13.mlp.fc1.weight', 'vpm.encoder.layers.13.mlp.fc2.weight', 'vpm.encoder.layers.13.self_attn.k_proj.weight', 'vpm.encoder.layers.13.self_attn.out_proj.weight', 'vpm.encoder.layers.13.self_attn.q_proj.weight', 'vpm.encoder.layers.13.self_attn.v_proj.weight', 'vpm.encoder.layers.14.mlp.fc1.weight', 'vpm.encoder.layers.14.mlp.fc2.weight', 'vpm.encoder.layers.14.self_attn.k_proj.weight', 'vpm.encoder.layers.14.self_attn.out_proj.weight', 'vpm.encoder.layers.14.self_attn.q_proj.weight', 'vpm.encoder.layers.14.self_attn.v_proj.weight', 'vpm.encoder.layers.15.mlp.fc1.weight', 'vpm.encoder.layers.15.mlp.fc2.weight', 'vpm.encoder.layers.15.self_attn.k_proj.weight', 'vpm.encoder.layers.15.self_attn.out_proj.weight', 'vpm.encoder.layers.15.self_attn.q_proj.weight', 'vpm.encoder.layers.15.self_attn.v_proj.weight', 'vpm.encoder.layers.16.mlp.fc1.weight', 'vpm.encoder.layers.16.mlp.fc2.weight', 'vpm.encoder.layers.16.self_attn.k_proj.weight', 'vpm.encoder.layers.16.self_attn.out_proj.weight', 'vpm.encoder.layers.16.self_attn.q_proj.weight', 'vpm.encoder.layers.16.self_attn.v_proj.weight', 'vpm.encoder.layers.17.mlp.fc1.weight', 'vpm.encoder.layers.17.mlp.fc2.weight', 'vpm.encoder.layers.17.self_attn.k_proj.weight', 'vpm.encoder.layers.17.self_attn.out_proj.weight', 'vpm.encoder.layers.17.self_attn.q_proj.weight', 'vpm.encoder.layers.17.self_attn.v_proj.weight', 'vpm.encoder.layers.18.mlp.fc1.weight', 'vpm.encoder.layers.18.mlp.fc2.weight', 'vpm.encoder.layers.18.self_attn.k_proj.weight', 'vpm.encoder.layers.18.self_attn.out_proj.weight', 'vpm.encoder.layers.18.self_attn.q_proj.weight', 'vpm.encoder.layers.18.self_attn.v_proj.weight', 'vpm.encoder.layers.19.mlp.fc1.weight', 'vpm.encoder.layers.19.mlp.fc2.weight', 'vpm.encoder.layers.19.self_attn.k_proj.weight', 'vpm.encoder.layers.19.self_attn.out_proj.weight', 'vpm.encoder.layers.19.self_attn.q_proj.weight', 'vpm.encoder.layers.19.self_attn.v_proj.weight', 'vpm.encoder.layers.2.mlp.fc1.weight', 'vpm.encoder.layers.2.mlp.fc2.weight', 'vpm.encoder.layers.2.self_attn.k_proj.weight', 'vpm.encoder.layers.2.self_attn.out_proj.weight', 'vpm.encoder.layers.2.self_attn.q_proj.weight', 'vpm.encoder.layers.2.self_attn.v_proj.weight', 'vpm.encoder.layers.20.mlp.fc1.weight', 'vpm.encoder.layers.20.mlp.fc2.weight', 'vpm.encoder.layers.20.self_attn.k_proj.weight', 'vpm.encoder.layers.20.self_attn.out_proj.weight', 'vpm.encoder.layers.20.self_attn.q_proj.weight', 'vpm.encoder.layers.20.self_attn.v_proj.weight', 'vpm.encoder.layers.21.mlp.fc1.weight', 'vpm.encoder.layers.21.mlp.fc2.weight', 'vpm.encoder.layers.21.self_attn.k_proj.weight', 'vpm.encoder.layers.21.self_attn.out_proj.weight', 'vpm.encoder.layers.21.self_attn.q_proj.weight', 'vpm.encoder.layers.21.self_attn.v_proj.weight', 'vpm.encoder.layers.22.mlp.fc1.weight', 'vpm.encoder.layers.22.mlp.fc2.weight', 'vpm.encoder.layers.22.self_attn.k_proj.weight', 'vpm.encoder.layers.22.self_attn.out_proj.weight', 'vpm.encoder.layers.22.self_attn.q_proj.weight', 'vpm.encoder.layers.22.self_attn.v_proj.weight', 'vpm.encoder.layers.23.mlp.fc1.weight', 'vpm.encoder.layers.23.mlp.fc2.weight', 'vpm.encoder.layers.23.self_attn.k_proj.weight', 'vpm.encoder.layers.23.self_attn.out_proj.weight', 'vpm.encoder.layers.23.self_attn.q_proj.weight', 'vpm.encoder.layers.23.self_attn.v_proj.weight', 'vpm.encoder.layers.24.mlp.fc1.weight', 'vpm.encoder.layers.24.mlp.fc2.weight', 'vpm.encoder.layers.24.self_attn.k_proj.weight', 'vpm.encoder.layers.24.self_attn.out_proj.weight', 'vpm.encoder.layers.24.self_attn.q_proj.weight', 'vpm.encoder.layers.24.self_attn.v_proj.weight', 'vpm.encoder.layers.25.mlp.fc1.weight', 'vpm.encoder.layers.25.mlp.fc2.weight', 'vpm.encoder.layers.25.self_attn.k_proj.weight', 'vpm.encoder.layers.25.self_attn.out_proj.weight', 'vpm.encoder.layers.25.self_attn.q_proj.weight', 'vpm.encoder.layers.25.self_attn.v_proj.weight', 'vpm.encoder.layers.26.mlp.fc1.weight', 'vpm.encoder.layers.26.mlp.fc2.weight', 'vpm.encoder.layers.26.self_attn.k_proj.weight', 'vpm.encoder.layers.26.self_attn.out_proj.weight', 'vpm.encoder.layers.26.self_attn.q_proj.weight', 'vpm.encoder.layers.26.self_attn.v_proj.weight', 'vpm.encoder.layers.3.mlp.fc1.weight', 'vpm.encoder.layers.3.mlp.fc2.weight', 'vpm.encoder.layers.3.self_attn.k_proj.weight', 'vpm.encoder.layers.3.self_attn.out_proj.weight', 'vpm.encoder.layers.3.self_attn.q_proj.weight', 'vpm.encoder.layers.3.self_attn.v_proj.weight', 'vpm.encoder.layers.4.mlp.fc1.weight', 'vpm.encoder.layers.4.mlp.fc2.weight', 'vpm.encoder.layers.4.self_attn.k_proj.weight', 'vpm.encoder.layers.4.self_attn.out_proj.weight', 'vpm.encoder.layers.4.self_attn.q_proj.weight', 'vpm.encoder.layers.4.self_attn.v_proj.weight', 'vpm.encoder.layers.5.mlp.fc1.weight', 'vpm.encoder.layers.5.mlp.fc2.weight', 'vpm.encoder.layers.5.self_attn.k_proj.weight', 'vpm.encoder.layers.5.self_attn.out_proj.weight', 'vpm.encoder.layers.5.self_attn.q_proj.weight', 'vpm.encoder.layers.5.self_attn.v_proj.weight', 'vpm.encoder.layers.6.mlp.fc1.weight', 'vpm.encoder.layers.6.mlp.fc2.weight', 'vpm.encoder.layers.6.self_attn.k_proj.weight', 'vpm.encoder.layers.6.self_attn.out_proj.weight', 'vpm.encoder.layers.6.self_attn.q_proj.weight', 'vpm.encoder.layers.6.self_attn.v_proj.weight', 'vpm.encoder.layers.7.mlp.fc1.weight', 'vpm.encoder.layers.7.mlp.fc2.weight', 'vpm.encoder.layers.7.self_attn.k_proj.weight', 'vpm.encoder.layers.7.self_attn.out_proj.weight', 'vpm.encoder.layers.7.self_attn.q_proj.weight', 'vpm.encoder.layers.7.self_attn.v_proj.weight', 'vpm.encoder.layers.8.mlp.fc1.weight', 'vpm.encoder.layers.8.mlp.fc2.weight', 'vpm.encoder.layers.8.self_attn.k_proj.weight', 'vpm.encoder.layers.8.self_attn.out_proj.weight', 'vpm.encoder.layers.8.self_attn.q_proj.weight', 'vpm.encoder.layers.8.self_attn.v_proj.weight', 'vpm.encoder.layers.9.mlp.fc1.weight', 'vpm.encoder.layers.9.mlp.fc2.weight', 'vpm.encoder.layers.9.self_attn.k_proj.weight', 'vpm.encoder.layers.9.self_attn.out_proj.weight', 'vpm.encoder.layers.9.self_attn.q_proj.weight', 'vpm.encoder.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO - The layer llm.model.layers.0.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.lm_head is not quantized.\n",
      "INFO - The layer vpm.embeddings.patch_embedding is not quantized.\n",
      "INFO - The layer resampler.kv_proj is not quantized.\n",
      "INFO - The layer resampler.attn.out_proj is not quantized.\n"
     ]
    }
   ],
   "source": [
    "# 全整模型\n",
    "from auto_gptq import AutoGPTQForVIT, BaseQuantizeConfig\n",
    "# quantized_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1-g128\"\n",
    "quantized_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256\"\n",
    "# model_quant = AutoGPTQForVIT.from_quantized(quantized_model_dir, device=\"cuda:0\", use_triton=False)\n",
    "model_quant = AutoGPTQForVIT.from_quantized(quantized_model_dir, use_triton=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "model_path='/home/workspace/model/MiniCPM-3o-1B-sft-v1'\n",
    "model_quant.processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "model_quant.seqlen = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 浮点模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from auto_gptq.modeling.minicpm.modeling_minicpmv import MiniCPMV\n",
    "from torch import nn\n",
    "\n",
    "model_path='/home/workspace/model/MiniCPM-3o-1B-sft-v1'\n",
    "model = MiniCPMV.from_pretrained(model_path, trust_remote_code=True).to(dtype=torch.float32)\n",
    "from transformers import AutoProcessor\n",
    "model.processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "model.seqlen = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindataset = get_ScienceQA(32, 0, model.seqlen, model.processor,0)\n",
    "len(traindataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    " \n",
    " \n",
    "# 保存到文件 \n",
    "with open('data.pkl',  'wb') as f: \n",
    "    pickle.dump(traindataset,  f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pkl',  'rb') as f: \n",
    "    loaded_data = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5, 28780,  1379,  1358,  2800,  2286, 12344,\n",
       "           59361, 59328,  2076,  2087,    74, 73440,     5, 73441, 43686,     5]],\n",
       "         dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[-0.1608, -0.1765, -0.2000,  ..., -0.4275, -0.2471, -0.1373],\n",
       "             [-0.1765, -0.0980,  0.0118,  ..., -0.3412, -0.2157, -0.1451],\n",
       "             [-0.2000,  0.0196,  0.3412,  ..., -0.2392, -0.1843, -0.1608],\n",
       "             ...,\n",
       "             [-0.2000,  0.0353,  0.4039,  ..., -0.4275, -0.2549, -0.1373],\n",
       "             [-0.2078,  0.0431,  0.4118,  ..., -0.2549, -0.1922, -0.1529],\n",
       "             [-0.2078,  0.0431,  0.4118,  ..., -0.1373, -0.1529, -0.1608]],\n",
       "    \n",
       "            [[-0.1529, -0.1843, -0.2157,  ..., -0.1765, -0.1529, -0.1608],\n",
       "             [-0.1765, -0.0745,  0.0902,  ..., -0.0902, -0.1216, -0.1686],\n",
       "             [-0.2157,  0.1059,  0.5843,  ...,  0.0118, -0.0980, -0.1765],\n",
       "             ...,\n",
       "             [-0.2235,  0.1216,  0.6392,  ..., -0.1765, -0.1608, -0.1608],\n",
       "             [-0.2235,  0.1137,  0.6314,  ..., -0.1686, -0.1608, -0.1608],\n",
       "             [-0.2235,  0.1137,  0.6314,  ..., -0.1608, -0.1608, -0.1608]],\n",
       "    \n",
       "            [[-0.1529, -0.1843, -0.2314,  ...,  0.1294, -0.0510, -0.1765],\n",
       "             [-0.1843, -0.0510,  0.1451,  ...,  0.2157, -0.0196, -0.1843],\n",
       "             [-0.2314,  0.1608,  0.7490,  ...,  0.3333,  0.0196, -0.2000],\n",
       "             ...,\n",
       "             [-0.2314,  0.1765,  0.8039,  ...,  0.1294, -0.0588, -0.1843],\n",
       "             [-0.2314,  0.1765,  0.7961,  ..., -0.0667, -0.1294, -0.1686],\n",
       "             [-0.2314,  0.1765,  0.7961,  ..., -0.1843, -0.1686, -0.1608]]])]],\n",
       "  'image_sizes': [[tensor([302, 232])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[28, 36]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459, 73458,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0, 73459,     5,  5856,  1410,  1358,  8107,\n",
       "            1379, 24333,    74, 73440,     5, 73441, 43686,     5]],\n",
       "         dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True]]),\n",
       "  'pixel_values': [[tensor([[[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]]]),\n",
       "    tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]]]),\n",
       "    tensor([[[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]])]],\n",
       "  'image_sizes': [[tensor([750, 429])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [140, 204]])],\n",
       "  'tgt_sizes': [tensor([[24, 42],\n",
       "           [34, 30],\n",
       "           [34, 30]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459, 73458,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0, 73459,     5, 73458,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0, 73459, 73458,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0, 73459,     5, 28780,\n",
       "            4451,  1410, 23834,    74, 73440,     5, 73441, 43686,     5]],\n",
       "         dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True]]),\n",
       "  'pixel_values': [[tensor([[[1.0000, 0.9373, 0.7725,  ..., 0.0588, 0.2627, 0.6863],\n",
       "             [0.9294, 0.7020, 0.6235,  ..., 0.0588, 0.2627, 0.6863],\n",
       "             [0.7725, 0.6235, 0.7490,  ..., 0.0588, 0.2627, 0.6863],\n",
       "             ...,\n",
       "             [0.6784, 0.3961, 0.5608,  ..., 0.0824, 0.4745, 0.7961],\n",
       "             [0.6863, 0.2549, 0.0275,  ..., 0.4745, 0.7176, 0.9373],\n",
       "             [0.6941, 0.2392, 0.1294,  ..., 0.7961, 0.9373, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9294, 0.7647,  ..., 0.5059, 0.5608, 0.6706],\n",
       "             [0.9294, 0.7098, 0.6784,  ..., 0.5059, 0.5608, 0.6706],\n",
       "             [0.7647, 0.6784, 0.7569,  ..., 0.5059, 0.5608, 0.6706],\n",
       "             ...,\n",
       "             [0.6784, 0.4353, 0.5765,  ..., 0.5216, 0.6314, 0.7882],\n",
       "             [0.6784, 0.4667, 0.2471,  ..., 0.6392, 0.7176, 0.9373],\n",
       "             [0.6706, 0.5451, 0.3333,  ..., 0.7804, 0.9373, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9294, 0.7647,  ..., 0.9608, 0.8667, 0.6471],\n",
       "             [0.9294, 0.7176, 0.7333,  ..., 0.9608, 0.8667, 0.6471],\n",
       "             [0.7647, 0.7333, 0.7725,  ..., 0.9608, 0.8667, 0.6471],\n",
       "             ...,\n",
       "             [0.6706, 0.4824, 0.6000,  ..., 0.9608, 0.8039, 0.7725],\n",
       "             [0.6627, 0.6706, 0.4745,  ..., 0.7961, 0.7255, 0.9373],\n",
       "             [0.6549, 0.8510, 0.5451,  ..., 0.7725, 0.9373, 1.0000]]]),\n",
       "    tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.0824, 0.0824, 0.0824],\n",
       "             [1.0000, 1.0000, 0.9373,  ..., 0.0824, 0.0824, 0.0824],\n",
       "             [1.0000, 0.9373, 0.8039,  ..., 0.0824, 0.0824, 0.0824],\n",
       "             ...,\n",
       "             [0.6627, 0.6549, 0.7020,  ..., 0.0824, 0.0824, 0.0824],\n",
       "             [0.6627, 0.6549, 0.7020,  ..., 0.0824, 0.0824, 0.0824],\n",
       "             [0.6627, 0.6549, 0.7020,  ..., 0.0824, 0.0824, 0.0824]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.5137, 0.5137, 0.5137],\n",
       "             [1.0000, 1.0000, 0.9373,  ..., 0.5137, 0.5137, 0.5137],\n",
       "             [1.0000, 0.9373, 0.8039,  ..., 0.5137, 0.5137, 0.5137],\n",
       "             ...,\n",
       "             [0.6627, 0.6549, 0.7020,  ..., 0.5137, 0.5137, 0.5137],\n",
       "             [0.6627, 0.6549, 0.7020,  ..., 0.5137, 0.5137, 0.5137],\n",
       "             [0.6627, 0.6549, 0.7020,  ..., 0.5137, 0.5137, 0.5137]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.9529, 0.9529, 0.9529],\n",
       "             [1.0000, 1.0000, 0.9373,  ..., 0.9529, 0.9529, 0.9529],\n",
       "             [1.0000, 0.9373, 0.7961,  ..., 0.9529, 0.9529, 0.9529],\n",
       "             ...,\n",
       "             [0.6627, 0.6549, 0.7020,  ..., 0.9529, 0.9529, 0.9529],\n",
       "             [0.6627, 0.6549, 0.7020,  ..., 0.9529, 0.9529, 0.9529],\n",
       "             [0.6627, 0.6549, 0.7020,  ..., 0.9529, 0.9529, 0.9529]]]),\n",
       "    tensor([[[0.6627, 0.6627, 0.6627,  ..., 0.4353, 0.7020, 0.6627],\n",
       "             [0.7020, 0.7020, 0.7020,  ..., 0.4353, 0.7020, 0.6627],\n",
       "             [0.4275, 0.4275, 0.4275,  ..., 0.4353, 0.7020, 0.6627],\n",
       "             ...,\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 0.4353, 0.7020, 0.6627],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 0.4353, 0.7020, 0.6627],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 0.4353, 0.7020, 0.6627]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.6078, 0.6706, 0.6627],\n",
       "             [0.6706, 0.6706, 0.6706,  ..., 0.6078, 0.6706, 0.6627],\n",
       "             [0.6000, 0.6000, 0.6000,  ..., 0.6078, 0.6706, 0.6627],\n",
       "             ...,\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 0.6078, 0.6706, 0.6627],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 0.6078, 0.6706, 0.6627],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 0.6078, 0.6706, 0.6627]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.7804, 0.6392, 0.6627],\n",
       "             [0.6392, 0.6392, 0.6392,  ..., 0.7804, 0.6392, 0.6627],\n",
       "             [0.7804, 0.7804, 0.7804,  ..., 0.7804, 0.6392, 0.6627],\n",
       "             ...,\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 0.7804, 0.6392, 0.6627],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 0.7804, 0.6392, 0.6627],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 0.7804, 0.6392, 0.6627]]]),\n",
       "    tensor([[[0.6627, 0.7020, 0.4353,  ..., 0.0824, 0.0824, 0.0824],\n",
       "             [0.6627, 0.7020, 0.4353,  ..., 0.0824, 0.0824, 0.0824],\n",
       "             [0.6627, 0.7020, 0.4353,  ..., 0.0824, 0.0824, 0.0824],\n",
       "             ...,\n",
       "             [0.6627, 0.7020, 0.4353,  ..., 0.4275, 0.4275, 0.4275],\n",
       "             [0.6627, 0.7020, 0.4353,  ..., 0.7020, 0.7020, 0.7020],\n",
       "             [0.6627, 0.7020, 0.4353,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[0.6627, 0.6706, 0.6078,  ..., 0.5137, 0.5137, 0.5137],\n",
       "             [0.6627, 0.6706, 0.6078,  ..., 0.5137, 0.5137, 0.5137],\n",
       "             [0.6627, 0.6706, 0.6078,  ..., 0.5137, 0.5137, 0.5137],\n",
       "             ...,\n",
       "             [0.6627, 0.6706, 0.6078,  ..., 0.6000, 0.6000, 0.6000],\n",
       "             [0.6627, 0.6706, 0.6078,  ..., 0.6706, 0.6706, 0.6706],\n",
       "             [0.6627, 0.6706, 0.6078,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[0.6627, 0.6392, 0.7804,  ..., 0.9529, 0.9529, 0.9529],\n",
       "             [0.6627, 0.6392, 0.7804,  ..., 0.9529, 0.9529, 0.9529],\n",
       "             [0.6627, 0.6392, 0.7804,  ..., 0.9529, 0.9529, 0.9529],\n",
       "             ...,\n",
       "             [0.6627, 0.6392, 0.7804,  ..., 0.7804, 0.7804, 0.7804],\n",
       "             [0.6627, 0.6392, 0.7804,  ..., 0.6392, 0.6392, 0.6392],\n",
       "             [0.6627, 0.6392, 0.7804,  ..., 0.6627, 0.6627, 0.6627]]]),\n",
       "    tensor([[[0.0824, 0.0824, 0.0824,  ..., 0.4353, 0.7020, 0.6627],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 0.4353, 0.7020, 0.6627],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 0.4353, 0.7020, 0.6627],\n",
       "             ...,\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 0.8118, 0.9451, 1.0000],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 0.9373, 1.0000, 1.0000],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.5137, 0.5137, 0.5137,  ..., 0.6078, 0.6706, 0.6627],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 0.6078, 0.6706, 0.6627],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 0.6078, 0.6706, 0.6627],\n",
       "             ...,\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 0.8118, 0.9451, 1.0000],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 0.9373, 1.0000, 1.0000],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.9529, 0.9529, 0.9529,  ..., 0.7804, 0.6392, 0.6627],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 0.7804, 0.6392, 0.6627],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 0.7804, 0.6392, 0.6627],\n",
       "             ...,\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 0.8039, 0.9451, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 0.9373, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000]]])]],\n",
       "  'image_sizes': [[tensor([750, 625])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [140, 204],\n",
       "           [207, 271],\n",
       "           [273, 337]])],\n",
       "  'tgt_sizes': [tensor([[29, 35],\n",
       "           [29, 35],\n",
       "           [29, 35],\n",
       "           [29, 35],\n",
       "           [29, 35]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5, 28780,  8968,  1410,  1751, 20911,  1385,\n",
       "            1441,  6413,  1375,  3907,  5269,  1377,  1358, 12192,    74, 73440,\n",
       "               5, 73441, 43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[-0.1529, -0.1686, -0.2078,  ...,  0.1608, -0.0980, -0.2078],\n",
       "             [-0.1686, -0.1529, -0.1059,  ..., -0.1294, -0.1529, -0.1686],\n",
       "             [-0.2078, -0.0980,  0.1294,  ..., -0.2235, -0.1686, -0.1529],\n",
       "             ...,\n",
       "             [-0.2235, -0.0745,  0.2941,  ...,  0.0275, -0.1216, -0.1922],\n",
       "             [-0.2235, -0.0745,  0.2863,  ..., -0.1216, -0.1529, -0.1686],\n",
       "             [-0.2235, -0.0745,  0.2863,  ..., -0.1843, -0.1686, -0.1608]],\n",
       "    \n",
       "            [[-0.1529, -0.1686, -0.2078,  ...,  0.1608, -0.0980, -0.2078],\n",
       "             [-0.1686, -0.1451, -0.0980,  ..., -0.1765, -0.1686, -0.1608],\n",
       "             [-0.2078, -0.0980,  0.1608,  ..., -0.2706, -0.1843, -0.1451],\n",
       "             ...,\n",
       "             [-0.2314, -0.0667,  0.3255,  ...,  0.0353, -0.1294, -0.1922],\n",
       "             [-0.2314, -0.0667,  0.3176,  ..., -0.1216, -0.1529, -0.1686],\n",
       "             [-0.2314, -0.0667,  0.3255,  ..., -0.1843, -0.1686, -0.1608]],\n",
       "    \n",
       "            [[-0.1529, -0.1686, -0.2157,  ...,  0.1608, -0.0980, -0.2078],\n",
       "             [-0.1686, -0.1451, -0.0902,  ..., -0.2078, -0.1686, -0.1529],\n",
       "             [-0.2157, -0.0902,  0.1922,  ..., -0.3098, -0.1922, -0.1373],\n",
       "             ...,\n",
       "             [-0.2392, -0.0588,  0.3804,  ...,  0.0824, -0.1137, -0.1922],\n",
       "             [-0.2392, -0.0588,  0.3647,  ..., -0.1137, -0.1529, -0.1686],\n",
       "             [-0.2392, -0.0588,  0.3569,  ..., -0.1922, -0.1686, -0.1529]]])]],\n",
       "  'image_sizes': [[tensor([202, 202])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[32, 32]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5, 28780,  1379,  1358,  2800,  2286,  7341,\n",
       "            2371, 59361, 59328,  2076,  2087,    74, 73440,     5, 73441, 43686,\n",
       "               5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[-0.1608, -0.1451, -0.1216,  ...,  0.2235, -0.0275, -0.1843],\n",
       "             [-0.1451, -0.2157, -0.3255,  ...,  0.1922, -0.0275, -0.1843],\n",
       "             [-0.1216, -0.3255, -0.6314,  ...,  0.3098,  0.0196, -0.2000],\n",
       "             ...,\n",
       "             [-0.1294, -0.3255, -0.6235,  ...,  0.4353,  0.0588, -0.2078],\n",
       "             [-0.1294, -0.3255, -0.6235,  ...,  0.0510, -0.0824, -0.1765],\n",
       "             [-0.1294, -0.3255, -0.6235,  ..., -0.2078, -0.1765, -0.1608]],\n",
       "    \n",
       "            [[-0.1608, -0.1529, -0.1451,  ...,  0.0275, -0.0980, -0.1686],\n",
       "             [-0.1529, -0.1843, -0.2235,  ..., -0.0039, -0.0980, -0.1686],\n",
       "             [-0.1451, -0.2235, -0.3490,  ...,  0.0980, -0.0510, -0.1843],\n",
       "             ...,\n",
       "             [-0.1451, -0.2235, -0.3333,  ...,  0.2235, -0.0118, -0.1922],\n",
       "             [-0.1451, -0.2235, -0.3333,  ..., -0.0196, -0.1059, -0.1686],\n",
       "             [-0.1451, -0.2235, -0.3333,  ..., -0.1922, -0.1686, -0.1608]],\n",
       "    \n",
       "            [[-0.1608, -0.1765, -0.2000,  ..., -0.3020, -0.2157, -0.1529],\n",
       "             [-0.1765, -0.0980,  0.0118,  ..., -0.3255, -0.2078, -0.1451],\n",
       "             [-0.2000,  0.0196,  0.3412,  ..., -0.2157, -0.1608, -0.1608],\n",
       "             ...,\n",
       "             [-0.2000,  0.0196,  0.3647,  ..., -0.0667, -0.1137, -0.1686],\n",
       "             [-0.2000,  0.0196,  0.3647,  ..., -0.1216, -0.1451, -0.1608],\n",
       "             [-0.2000,  0.0196,  0.3647,  ..., -0.1686, -0.1608, -0.1608]]])]],\n",
       "  'image_sizes': [[tensor([302, 232])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[28, 36]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459, 73458,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0, 73459,     5,  5856,  1410,  1358,  8107,\n",
       "            1379, 13172,    74, 73440,     5, 73441, 43686,     5]],\n",
       "         dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True]]),\n",
       "  'pixel_values': [[tensor([[[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]]]),\n",
       "    tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]]]),\n",
       "    tensor([[[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]])]],\n",
       "  'image_sizes': [[tensor([750, 429])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [140, 204]])],\n",
       "  'tgt_sizes': [tensor([[24, 42],\n",
       "           [34, 30],\n",
       "           [34, 30]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459,     5,\n",
       "           73458,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0, 73459,     5,  5856,  1410,  1358,\n",
       "            2258,  1379,  1358, 30495,  5023,    74, 73440,     5, 73441, 43686,\n",
       "               5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[1.0000, 1.0000, 0.9373,  ..., 0.0510, 0.5294, 0.6784],\n",
       "             [1.0000, 0.8745, 0.6863,  ..., 0.0510, 0.5294, 0.6784],\n",
       "             [0.9294, 0.6941, 0.5843,  ..., 0.0510, 0.5294, 0.6784],\n",
       "             ...,\n",
       "             [0.6706, 0.5843, 0.3098,  ..., 0.5451, 0.7020, 0.9451],\n",
       "             [0.6706, 0.5843, 0.3098,  ..., 0.6941, 0.8902, 1.0000],\n",
       "             [0.6706, 0.5843, 0.3098,  ..., 0.9373, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 0.9294,  ..., 0.5059, 0.6314, 0.6706],\n",
       "             [1.0000, 0.8745, 0.6863,  ..., 0.5059, 0.6314, 0.6706],\n",
       "             [0.9294, 0.6863, 0.6784,  ..., 0.5059, 0.6314, 0.6706],\n",
       "             ...,\n",
       "             [0.6627, 0.6627, 0.6627,  ..., 0.6627, 0.7020, 0.9451],\n",
       "             [0.6627, 0.6627, 0.6627,  ..., 0.6941, 0.8824, 1.0000],\n",
       "             [0.6627, 0.6627, 0.6627,  ..., 0.9294, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 0.9294,  ..., 0.9686, 0.7333, 0.6549],\n",
       "             [1.0000, 0.8745, 0.6863,  ..., 0.9686, 0.7333, 0.6549],\n",
       "             [0.9294, 0.6863, 0.7647,  ..., 0.9686, 0.7333, 0.6549],\n",
       "             ...,\n",
       "             [0.6549, 0.7412, 1.0000,  ..., 0.7725, 0.6941, 0.9451],\n",
       "             [0.6549, 0.7412, 1.0000,  ..., 0.6863, 0.8824, 1.0000],\n",
       "             [0.6549, 0.7412, 1.0000,  ..., 0.9294, 1.0000, 1.0000]]]),\n",
       "    tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.3255, 0.7020, 0.6627],\n",
       "             [1.0000, 1.0000, 0.8902,  ..., 0.3255, 0.7020, 0.6627],\n",
       "             [1.0000, 0.8980, 0.7490,  ..., 0.3255, 0.7020, 0.6627],\n",
       "             ...,\n",
       "             [0.6627, 0.6863, 0.4667,  ..., 0.3255, 0.7020, 0.6627],\n",
       "             [0.6627, 0.6863, 0.4667,  ..., 0.3255, 0.7020, 0.6627],\n",
       "             [0.6627, 0.6863, 0.4667,  ..., 0.3255, 0.7020, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.5765, 0.6706, 0.6627],\n",
       "             [1.0000, 1.0000, 0.8902,  ..., 0.5765, 0.6706, 0.6627],\n",
       "             [1.0000, 0.8980, 0.7490,  ..., 0.5765, 0.6706, 0.6627],\n",
       "             ...,\n",
       "             [0.6627, 0.6627, 0.6627,  ..., 0.5765, 0.6706, 0.6627],\n",
       "             [0.6627, 0.6627, 0.6627,  ..., 0.5765, 0.6706, 0.6627],\n",
       "             [0.6627, 0.6627, 0.6627,  ..., 0.5765, 0.6706, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.8353, 0.6392, 0.6627],\n",
       "             [1.0000, 1.0000, 0.8902,  ..., 0.8353, 0.6392, 0.6627],\n",
       "             [1.0000, 0.8980, 0.7412,  ..., 0.8353, 0.6392, 0.6627],\n",
       "             ...,\n",
       "             [0.6627, 0.6392, 0.8510,  ..., 0.8353, 0.6392, 0.6627],\n",
       "             [0.6627, 0.6392, 0.8510,  ..., 0.8353, 0.6392, 0.6627],\n",
       "             [0.6627, 0.6392, 0.8510,  ..., 0.8353, 0.6392, 0.6627]]]),\n",
       "    tensor([[[0.6627, 0.6549, 0.7176,  ..., 0.3255, 0.7020, 0.6627],\n",
       "             [0.6627, 0.6549, 0.7176,  ..., 0.3255, 0.7020, 0.6627],\n",
       "             [0.6627, 0.6549, 0.7176,  ..., 0.3255, 0.7020, 0.6627],\n",
       "             ...,\n",
       "             [0.6627, 0.6549, 0.7176,  ..., 0.7647, 0.9137, 1.0000],\n",
       "             [0.6627, 0.6549, 0.7176,  ..., 0.8980, 1.0000, 1.0000],\n",
       "             [0.6627, 0.6549, 0.7176,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6549, 0.7176,  ..., 0.5765, 0.6706, 0.6627],\n",
       "             [0.6627, 0.6549, 0.7176,  ..., 0.5765, 0.6706, 0.6627],\n",
       "             [0.6627, 0.6549, 0.7176,  ..., 0.5765, 0.6706, 0.6627],\n",
       "             ...,\n",
       "             [0.6627, 0.6549, 0.7176,  ..., 0.7569, 0.9137, 1.0000],\n",
       "             [0.6627, 0.6549, 0.7176,  ..., 0.8980, 1.0000, 1.0000],\n",
       "             [0.6627, 0.6549, 0.7176,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6549, 0.7176,  ..., 0.8353, 0.6392, 0.6627],\n",
       "             [0.6627, 0.6549, 0.7176,  ..., 0.8353, 0.6392, 0.6627],\n",
       "             [0.6627, 0.6549, 0.7176,  ..., 0.8353, 0.6392, 0.6627],\n",
       "             ...,\n",
       "             [0.6627, 0.6549, 0.7176,  ..., 0.7412, 0.9137, 1.0000],\n",
       "             [0.6627, 0.6549, 0.7176,  ..., 0.8980, 1.0000, 1.0000],\n",
       "             [0.6627, 0.6549, 0.7176,  ..., 1.0000, 1.0000, 1.0000]]])]],\n",
       "  'image_sizes': [[tensor([452, 595])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [141, 205]])],\n",
       "  'tgt_sizes': [tensor([[37, 28],\n",
       "           [26, 39],\n",
       "           [26, 39]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459, 73458,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0, 73459,     5,  5856,  1410,  1358,  8107,\n",
       "            1379,  8225,    74, 73440,     5, 73441, 43686,     5]],\n",
       "         dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True]]),\n",
       "  'pixel_values': [[tensor([[[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]]]),\n",
       "    tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]]]),\n",
       "    tensor([[[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]])]],\n",
       "  'image_sizes': [[tensor([750, 429])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [140, 204]])],\n",
       "  'tgt_sizes': [tensor([[24, 42],\n",
       "           [34, 30],\n",
       "           [34, 30]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5, 15134,  2294,  1358,  3361,  1457,  2748,\n",
       "            1373, 59361, 59328,  8753,  1571,  2558,  3978,    72, 73440,     5,\n",
       "           73441, 43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[-0.1529, -0.1922, -0.2392,  ..., -0.4431, -0.2863, -0.1451],\n",
       "             [-0.1922,  0.0980,  0.4039,  ..., -0.4431, -0.2863, -0.1451],\n",
       "             [-0.2392,  0.4039,  1.0000,  ..., -0.4588, -0.3020, -0.1451],\n",
       "             ...,\n",
       "             [-0.2314,  0.3804,  1.0000,  ..., -0.4510, -0.2941, -0.1451],\n",
       "             [-0.2314,  0.3804,  1.0000,  ..., -0.2941, -0.2235, -0.1529],\n",
       "             [-0.2314,  0.3804,  1.0000,  ..., -0.1451, -0.1529, -0.1608]],\n",
       "    \n",
       "            [[-0.1529, -0.1922, -0.2392,  ..., -0.4588, -0.2941, -0.1451],\n",
       "             [-0.1922,  0.0980,  0.4039,  ..., -0.4588, -0.2941, -0.1451],\n",
       "             [-0.2392,  0.4118,  1.0000,  ..., -0.4824, -0.3098, -0.1373],\n",
       "             ...,\n",
       "             [-0.2314,  0.3882,  1.0000,  ..., -0.4667, -0.3020, -0.1451],\n",
       "             [-0.2314,  0.3882,  1.0000,  ..., -0.3020, -0.2235, -0.1529],\n",
       "             [-0.2314,  0.3882,  1.0000,  ..., -0.1451, -0.1529, -0.1608]],\n",
       "    \n",
       "            [[-0.1529, -0.2000, -0.2392,  ..., -0.2863, -0.2157, -0.1529],\n",
       "             [-0.2000,  0.1059,  0.4039,  ..., -0.2863, -0.2157, -0.1529],\n",
       "             [-0.2471,  0.4275,  1.0000,  ..., -0.3020, -0.2314, -0.1529],\n",
       "             ...,\n",
       "             [-0.2392,  0.4039,  1.0000,  ..., -0.2863, -0.2157, -0.1529],\n",
       "             [-0.2392,  0.4039,  1.0000,  ..., -0.2157, -0.1843, -0.1608],\n",
       "             [-0.2392,  0.4039,  1.0000,  ..., -0.1529, -0.1608, -0.1608]]])]],\n",
       "  'image_sizes': [[tensor([302, 302])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[32, 32]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5, 28780,  3819,  1649,  1348,  4031,  7987,\n",
       "            1379, 13548, 12636,    74, 73440,     5, 73441, 43686,     5]],\n",
       "         dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]]])]],\n",
       "  'image_sizes': [[tensor([378, 251])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[26, 39]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5, 28780,  3819,  1649,  1348,  4031,  7987,\n",
       "            1379,  7262, 12636,    74, 73440,     5, 73441, 43686,     5]],\n",
       "         dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]]])]],\n",
       "  'image_sizes': [[tensor([378, 249])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[26, 39]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5, 15134,  2294,  1358,  3361,  1457,  1491,\n",
       "           41625, 59361, 59328,  8753,  1571,  2558,  3978,    72, 73440,     5,\n",
       "           73441, 43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[-0.1608, -0.1451, -0.1216,  ..., -0.9922, -0.4588, -0.0980],\n",
       "             [-0.1451, -0.2235, -0.3490,  ..., -1.0000, -0.4588, -0.0980],\n",
       "             [-0.1216, -0.3569, -0.7098,  ..., -0.9529, -0.4431, -0.1059],\n",
       "             ...,\n",
       "             [-0.1216, -0.3333, -0.6549,  ...,  0.5922,  0.0980, -0.2157],\n",
       "             [-0.1216, -0.3333, -0.6627,  ...,  0.0980, -0.0745, -0.1765],\n",
       "             [-0.1216, -0.3333, -0.6549,  ..., -0.2157, -0.1843, -0.1529]],\n",
       "    \n",
       "            [[-0.1608, -0.1686, -0.1765,  ..., -0.9373, -0.4353, -0.0980],\n",
       "             [-0.1686, -0.1373, -0.0980,  ..., -0.9373, -0.4353, -0.0980],\n",
       "             [-0.1765, -0.1059,  0.0118,  ..., -0.8902, -0.4118, -0.1059],\n",
       "             ...,\n",
       "             [-0.1294, -0.3255, -0.6314,  ...,  0.6078,  0.1059, -0.2157],\n",
       "             [-0.1216, -0.3490, -0.6863,  ...,  0.1059, -0.0667, -0.1765],\n",
       "             [-0.1137, -0.3569, -0.7255,  ..., -0.2235, -0.1843, -0.1529]],\n",
       "    \n",
       "            [[-0.1608, -0.1451, -0.1137,  ..., -0.8588, -0.4039, -0.1059],\n",
       "             [-0.1451, -0.2392, -0.3804,  ..., -0.8118, -0.3882, -0.1137],\n",
       "             [-0.1137, -0.3961, -0.8118,  ..., -0.6941, -0.3490, -0.1216],\n",
       "             ...,\n",
       "             [-0.1216, -0.3569, -0.7098,  ...,  0.9137,  0.2157, -0.2392],\n",
       "             [-0.1216, -0.3490, -0.7020,  ...,  0.2078, -0.0353, -0.1843],\n",
       "             [-0.1216, -0.3490, -0.6941,  ..., -0.2471, -0.1922, -0.1529]]])]],\n",
       "  'image_sizes': [[tensor([302, 232])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[28, 36]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5, 28780,  6822,  1410,  8040,  1520,  2009,\n",
       "            9146,    74, 73440,     5, 73441, 43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[1.0000, 1.0000, 0.9843,  ..., 1.0000, 0.9137, 0.5843],\n",
       "             [1.0000, 0.9922, 0.7804,  ..., 1.0000, 0.9137, 0.5843],\n",
       "             [0.9843, 0.7882, 0.7804,  ..., 1.0000, 0.9137, 0.5843],\n",
       "             ...,\n",
       "             [0.5843, 0.9137, 1.0000,  ..., 0.7725, 0.7882, 0.9843],\n",
       "             [0.5843, 0.9137, 1.0000,  ..., 0.7804, 0.9922, 1.0000],\n",
       "             [0.5843, 0.9137, 1.0000,  ..., 0.9843, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 0.9843,  ..., 1.0000, 0.9137, 0.5843],\n",
       "             [1.0000, 0.9922, 0.7804,  ..., 1.0000, 0.9137, 0.5843],\n",
       "             [0.9843, 0.7882, 0.7804,  ..., 1.0000, 0.9137, 0.5843],\n",
       "             ...,\n",
       "             [0.5843, 0.9137, 1.0000,  ..., 0.7725, 0.7882, 0.9843],\n",
       "             [0.5843, 0.9137, 1.0000,  ..., 0.7804, 0.9922, 1.0000],\n",
       "             [0.5843, 0.9137, 1.0000,  ..., 0.9843, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 0.9843,  ..., 1.0000, 0.9137, 0.5843],\n",
       "             [1.0000, 0.9922, 0.7804,  ..., 1.0000, 0.9137, 0.5843],\n",
       "             [0.9843, 0.7882, 0.7804,  ..., 1.0000, 0.9137, 0.5843],\n",
       "             ...,\n",
       "             [0.5843, 0.9137, 1.0000,  ..., 0.7725, 0.7882, 0.9843],\n",
       "             [0.5843, 0.9137, 1.0000,  ..., 0.7804, 0.9922, 1.0000],\n",
       "             [0.5843, 0.9137, 1.0000,  ..., 0.9843, 1.0000, 1.0000]]])]],\n",
       "  'image_sizes': [[tensor([504, 278])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[24, 43]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459, 73458,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0, 73459,     5, 28780,  2506,  1410, 23834,\n",
       "              74, 73440,     5, 73441, 43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]]]),\n",
       "    tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]]]),\n",
       "    tensor([[[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]])]],\n",
       "  'image_sizes': [[tensor([750, 429])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [140, 204]])],\n",
       "  'tgt_sizes': [tensor([[24, 42],\n",
       "           [34, 30],\n",
       "           [34, 30]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459, 73458,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0, 73459,     5, 28780,  1379,  2009,  6134,\n",
       "            1410,  1368,  1522,  4609,  7941,    74, 73440,     5, 73441, 43686,\n",
       "               5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[1.0000, 0.9843, 0.8745,  ..., 0.0510, 0.4196, 0.6941],\n",
       "             [0.9843, 0.8118, 0.6157,  ..., 0.0510, 0.4196, 0.6941],\n",
       "             [0.8745, 0.6078, 0.6314,  ..., 0.0510, 0.4196, 0.6941],\n",
       "             ...,\n",
       "             [0.6784, 0.5216, 0.5765,  ..., 0.3569, 0.6549, 0.8902],\n",
       "             [0.6706, 0.5686, 0.6627,  ..., 0.6549, 0.8118, 0.9922],\n",
       "             [0.6706, 0.5686, 0.6549,  ..., 0.8745, 0.9843, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9843, 0.8745,  ..., 0.5059, 0.6000, 0.6706],\n",
       "             [0.9843, 0.8118, 0.6392,  ..., 0.5059, 0.6000, 0.6706],\n",
       "             [0.8667, 0.6392, 0.6784,  ..., 0.5059, 0.6000, 0.6706],\n",
       "             ...,\n",
       "             [0.6784, 0.5294, 0.5843,  ..., 0.6078, 0.6784, 0.8902],\n",
       "             [0.6706, 0.5765, 0.6627,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6706, 0.5765, 0.6549,  ..., 0.8745, 0.9843, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9843, 0.8745,  ..., 0.9686, 0.7882, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6706,  ..., 0.9686, 0.7882, 0.6471],\n",
       "             [0.8667, 0.6627, 0.7176,  ..., 0.9686, 0.7882, 0.6471],\n",
       "             ...,\n",
       "             [0.6784, 0.5294, 0.5922,  ..., 0.8667, 0.6941, 0.8902],\n",
       "             [0.6706, 0.5765, 0.6549,  ..., 0.6941, 0.8039, 0.9922],\n",
       "             [0.6706, 0.5765, 0.6549,  ..., 0.8667, 0.9843, 1.0000]]]),\n",
       "    tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.0824, 0.0824, 0.0824],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 0.0824, 0.0824, 0.0824],\n",
       "             [1.0000, 0.8353, 0.7176,  ..., 0.0824, 0.0824, 0.0824],\n",
       "             ...,\n",
       "             [0.6627, 0.6706, 0.5059,  ..., 0.2000, 0.2000, 0.2000],\n",
       "             [0.6627, 0.6706, 0.5216,  ..., 0.6941, 0.6941, 0.6941],\n",
       "             [0.6627, 0.6784, 0.4039,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.5137, 0.5137, 0.5137],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 0.5137, 0.5137, 0.5137],\n",
       "             [1.0000, 0.8353, 0.7020,  ..., 0.5137, 0.5137, 0.5137],\n",
       "             ...,\n",
       "             [0.6627, 0.6706, 0.5059,  ..., 0.5451, 0.5451, 0.5451],\n",
       "             [0.6627, 0.6706, 0.5137,  ..., 0.6706, 0.6706, 0.6706],\n",
       "             [0.6627, 0.6784, 0.3961,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.9529, 0.9529, 0.9529],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 0.9529, 0.9529, 0.9529],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 0.9529, 0.9529, 0.9529],\n",
       "             ...,\n",
       "             [0.6627, 0.6706, 0.5059,  ..., 0.8980, 0.8980, 0.8980],\n",
       "             [0.6627, 0.6706, 0.5137,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6784, 0.3882,  ..., 0.6627, 0.6627, 0.6627]]]),\n",
       "    tensor([[[0.6627, 0.6627, 0.6627,  ..., 0.2157, 0.6941, 0.6627],\n",
       "             [0.6706, 0.6706, 0.6706,  ..., 0.2157, 0.6941, 0.6627],\n",
       "             [0.5059, 0.5059, 0.5059,  ..., 0.2157, 0.6941, 0.6627],\n",
       "             ...,\n",
       "             [0.7569, 0.7569, 0.7569,  ..., 0.7098, 0.8431, 1.0000],\n",
       "             [0.7569, 0.7569, 0.7569,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [0.7569, 0.7569, 0.7569,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.5451, 0.6706, 0.6627],\n",
       "             [0.6706, 0.6706, 0.6706,  ..., 0.5451, 0.6706, 0.6627],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 0.5451, 0.6706, 0.6627],\n",
       "             ...,\n",
       "             [0.7569, 0.7569, 0.7569,  ..., 0.7020, 0.8431, 1.0000],\n",
       "             [0.7569, 0.7569, 0.7569,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [0.7569, 0.7569, 0.7569,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.8902, 0.6471, 0.6627],\n",
       "             [0.6706, 0.6706, 0.6706,  ..., 0.8902, 0.6471, 0.6627],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 0.8902, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [0.7569, 0.7569, 0.7569,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [0.7569, 0.7569, 0.7569,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [0.7569, 0.7569, 0.7569,  ..., 1.0000, 1.0000, 1.0000]]])]],\n",
       "  'image_sizes': [[tensor([750, 429])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [140, 204]])],\n",
       "  'tgt_sizes': [tensor([[24, 42],\n",
       "           [34, 30],\n",
       "           [34, 30]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459, 73458,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0, 73459,     5,  5856,  1410,  1358,  8107,\n",
       "            1379, 12850,    74, 73440,     5, 73441, 43686,     5]],\n",
       "         dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True]]),\n",
       "  'pixel_values': [[tensor([[[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]]]),\n",
       "    tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]]]),\n",
       "    tensor([[[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]])]],\n",
       "  'image_sizes': [[tensor([750, 429])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [140, 204]])],\n",
       "  'tgt_sizes': [tensor([[24, 42],\n",
       "           [34, 30],\n",
       "           [34, 30]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459, 73458,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0, 73459,     5,  5856,  1410,  1358,  8107,\n",
       "            1379, 31836,    74, 73440,     5, 73441, 43686,     5]],\n",
       "         dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True]]),\n",
       "  'pixel_values': [[tensor([[[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 0.9843, 0.8745,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.9843, 0.8039, 0.6549,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             [0.8667, 0.6549, 0.8039,  ..., 1.0000, 0.8039, 0.6471],\n",
       "             ...,\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8510, 0.6784, 0.8902],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.6784, 0.8039, 0.9922],\n",
       "             [0.6471, 0.8039, 1.0000,  ..., 0.8667, 0.9843, 1.0000]]]),\n",
       "    tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 0.8353,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 0.8353, 0.6941,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.9294, 0.9294, 0.9294],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6471, 0.6471, 0.6471],\n",
       "             [0.6627, 0.6471, 0.9216,  ..., 0.6627, 0.6627, 0.6627]]]),\n",
       "    tensor([[[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6627, 0.6627, 0.6627,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.6471, 0.6471, 0.6471,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             [0.9294, 0.9294, 0.9294,  ..., 0.9216, 0.6471, 0.6627],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8431, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8353, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]])]],\n",
       "  'image_sizes': [[tensor([750, 429])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [140, 204]])],\n",
       "  'tgt_sizes': [tensor([[24, 42],\n",
       "           [34, 30],\n",
       "           [34, 30]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5,  5415,  1348, 24278, 59337,  1348,  7098,\n",
       "            1508,  1348, 10816,    74, 73440,     5, 73441, 43686,     5]],\n",
       "         dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]]])]],\n",
       "  'image_sizes': [[tensor([132, 183])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[38, 27]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5,  5856,  1410,  1358,  5051,  6850,  1379,\n",
       "           38960,  1450,  1348, 23088,  3795,  1385, 38960,  1450,  1348, 23475,\n",
       "            3795,    74, 25406,  1358,  2121,  5156,  6850,    72, 73440,     5,\n",
       "           73441, 43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.5686, 0.5765, 0.5922],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5686, 0.5765, 0.5922],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5686, 0.5765, 0.5922],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5686, 0.5765, 0.5922],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5843, 0.5922, 0.5922],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5922, 0.5922, 0.5922]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.5686, 0.5765, 0.5922],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5686, 0.5765, 0.5922],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5686, 0.5765, 0.5922],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5686, 0.5765, 0.5922],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5843, 0.5922, 0.5922],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5922, 0.5922, 0.5922]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.5686, 0.5765, 0.5922],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5686, 0.5765, 0.5922],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5686, 0.5765, 0.5922],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5686, 0.5765, 0.5922],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5843, 0.5922, 0.5922],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5922, 0.5922, 0.5922]]])]],\n",
       "  'image_sizes': [[tensor([246, 241])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[32, 32]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459, 73458,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0, 73459,     5, 28780, 44188,  1410, 23834,\n",
       "              74, 73440,     5, 73441, 43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       "    tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5451, 0.5451, 0.5451],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.4275, 0.4275, 0.4275],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5765, 0.5765, 0.5765],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.4745, 0.4745, 0.4745],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6157, 0.6157, 0.6157],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5216, 0.5216, 0.5216],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196]]]),\n",
       "    tensor([[[0.3176, 0.3176, 0.3176,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.1451, 0.1451, 0.1451,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.0667, 0.0667, 0.0667,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6392, 0.6392, 0.6392,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5451, 0.5451, 0.5451,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5059, 0.5059, 0.5059,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.9608, 0.9608, 0.9608,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000]]])]],\n",
       "  'image_sizes': [[tensor([704, 358])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [140, 204]])],\n",
       "  'tgt_sizes': [tensor([[23, 45],\n",
       "           [32, 32],\n",
       "           [32, 32]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459, 73458,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0, 73459,     5, 44917,  1358,  5090, 22121,\n",
       "           29265,  1379,  1358, 12636,  1377,  2351,  5871,    72, 10363,  5871,\n",
       "            1649,  1358,  4031,  5945,    74, 73440,     5, 73441, 43686,     5]],\n",
       "         dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True]]),\n",
       "  'pixel_values': [[tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8902, 1.0000],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6784, 0.8667, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8902, 1.0000],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6784, 0.8667, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6863, 0.8902, 1.0000],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6784, 0.8667, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]]),\n",
       "    tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       "    tensor([[[ 1.0000,  1.0000,  1.0000,  ..., -0.5137, -0.3882,  0.5451],\n",
       "             [ 1.0000,  1.0000,  1.0000,  ...,  0.6784,  0.6157,  0.8353],\n",
       "             [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  0.9922,  0.9765],\n",
       "             ...,\n",
       "             [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
       "    \n",
       "            [[ 1.0000,  1.0000,  1.0000,  ..., -0.5137, -0.3882,  0.5451],\n",
       "             [ 1.0000,  1.0000,  1.0000,  ...,  0.6784,  0.6157,  0.8353],\n",
       "             [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  0.9922,  0.9765],\n",
       "             ...,\n",
       "             [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
       "    \n",
       "            [[ 1.0000,  1.0000,  1.0000,  ..., -0.5137, -0.3882,  0.5451],\n",
       "             [ 1.0000,  1.0000,  1.0000,  ...,  0.6784,  0.6157,  0.8353],\n",
       "             [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  0.9922,  0.9765],\n",
       "             ...,\n",
       "             [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]])]],\n",
       "  'image_sizes': [[tensor([563, 405])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [140, 204]])],\n",
       "  'tgt_sizes': [tensor([[27, 38],\n",
       "           [38, 27],\n",
       "           [38, 27]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5, 15134,  2294,  1358,  3361,  1457,  2758,\n",
       "            2232, 59361, 59328,  8753,  1571,  2558,  3978,    72, 73440,     5,\n",
       "           73441, 43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[-0.1529, -0.1922, -0.2471,  ...,  0.9686,  0.2314, -0.2471],\n",
       "             [-0.1922, -0.0196,  0.2314,  ...,  0.9686,  0.2314, -0.2471],\n",
       "             [-0.2471,  0.2392,  0.9843,  ...,  0.9686,  0.2314, -0.2471],\n",
       "             ...,\n",
       "             [-0.2471,  0.2471,  1.0000,  ...,  0.9529,  0.2235, -0.2471],\n",
       "             [-0.2471,  0.2471,  1.0000,  ...,  0.2157, -0.0275, -0.1922],\n",
       "             [-0.2471,  0.2471,  1.0000,  ..., -0.2471, -0.1922, -0.1529]],\n",
       "    \n",
       "            [[-0.1529, -0.1922, -0.2471,  ...,  0.9686,  0.2314, -0.2471],\n",
       "             [-0.1922, -0.0196,  0.2314,  ...,  0.9686,  0.2314, -0.2471],\n",
       "             [-0.2471,  0.2392,  0.9843,  ...,  0.9686,  0.2314, -0.2471],\n",
       "             ...,\n",
       "             [-0.2471,  0.2471,  1.0000,  ...,  0.9529,  0.2235, -0.2471],\n",
       "             [-0.2471,  0.2471,  1.0000,  ...,  0.2157, -0.0275, -0.1922],\n",
       "             [-0.2471,  0.2471,  1.0000,  ..., -0.2471, -0.1922, -0.1529]],\n",
       "    \n",
       "            [[-0.1529, -0.1922, -0.2471,  ...,  0.9686,  0.2314, -0.2471],\n",
       "             [-0.1922, -0.0196,  0.2314,  ...,  0.9686,  0.2314, -0.2471],\n",
       "             [-0.2471,  0.2392,  0.9843,  ...,  0.9686,  0.2314, -0.2471],\n",
       "             ...,\n",
       "             [-0.2471,  0.2471,  1.0000,  ...,  0.9529,  0.2235, -0.2471],\n",
       "             [-0.2471,  0.2471,  1.0000,  ...,  0.2157, -0.0275, -0.1922],\n",
       "             [-0.2471,  0.2471,  1.0000,  ..., -0.2471, -0.1922, -0.1529]]])]],\n",
       "  'image_sizes': [[tensor([302, 232])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[28, 36]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5,  5415, 37223,  1348, 19913,    74, 73440,\n",
       "               5, 73441, 43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[-0.1529, -0.1765, -0.2235,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.1765, -0.1451, -0.0745,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.2314, -0.0745,  0.2863,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             ...,\n",
       "             [-0.2706, -0.0275,  0.5451,  ...,  0.5686,  0.5686,  0.5686],\n",
       "             [-0.2706, -0.0275,  0.5451,  ..., -0.0196, -0.0196, -0.0196],\n",
       "             [-0.2706, -0.0275,  0.5451,  ..., -0.2706, -0.2706, -0.2706]],\n",
       "    \n",
       "            [[-0.1529, -0.1765, -0.2235,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.1765, -0.1451, -0.0745,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.2314, -0.0745,  0.2863,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             ...,\n",
       "             [-0.2706, -0.0275,  0.5451,  ...,  0.5686,  0.5686,  0.5686],\n",
       "             [-0.2706, -0.0275,  0.5451,  ..., -0.0196, -0.0196, -0.0196],\n",
       "             [-0.2706, -0.0275,  0.5451,  ..., -0.2706, -0.2706, -0.2706]],\n",
       "    \n",
       "            [[-0.1529, -0.1765, -0.2235,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.1765, -0.1451, -0.0745,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.2314, -0.0745,  0.2863,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             ...,\n",
       "             [-0.2706, -0.0275,  0.5451,  ...,  0.5686,  0.5686,  0.5686],\n",
       "             [-0.2706, -0.0275,  0.5451,  ..., -0.0196, -0.0196, -0.0196],\n",
       "             [-0.2706, -0.0275,  0.5451,  ..., -0.2706, -0.2706, -0.2706]]])]],\n",
       "  'image_sizes': [[tensor([200, 202])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[32, 32]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5, 28780,  3819,  1649,  1348,  4031,  7987,\n",
       "            1379, 13548, 12636,    74, 73440,     5, 73441, 43686,     5]],\n",
       "         dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]]])]],\n",
       "  'image_sizes': [[tensor([378, 251])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[26, 39]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5, 28780,  8968, 59361, 59328, 14935,  1410,\n",
       "            1751, 20911,  1385, 25263,  2128, 15909,    74, 73440,     5, 73441,\n",
       "           43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[-0.1608, -0.1529, -0.1373,  ..., -0.4039, -0.2078, -0.1216],\n",
       "             [-0.1608, -0.1686, -0.1922,  ..., -0.4039, -0.2078, -0.1216],\n",
       "             [-0.1373, -0.1922, -0.3020,  ..., -0.4039, -0.2078, -0.1216],\n",
       "             ...,\n",
       "             [-0.1216, -0.2078, -0.4196,  ..., -0.3412, -0.1922, -0.1294],\n",
       "             [-0.1216, -0.2078, -0.4196,  ..., -0.2000, -0.1686, -0.1529],\n",
       "             [-0.1216, -0.2078, -0.4196,  ..., -0.1373, -0.1529, -0.1686]],\n",
       "    \n",
       "            [[-0.1608, -0.1529, -0.1373,  ..., -0.4353, -0.2157, -0.1216],\n",
       "             [-0.1608, -0.1686, -0.1922,  ..., -0.4353, -0.2157, -0.1216],\n",
       "             [-0.1451, -0.1922, -0.3020,  ..., -0.4353, -0.2157, -0.1216],\n",
       "             ...,\n",
       "             [-0.1216, -0.2157, -0.4353,  ..., -0.3490, -0.1922, -0.1294],\n",
       "             [-0.1216, -0.2157, -0.4353,  ..., -0.2000, -0.1686, -0.1529],\n",
       "             [-0.1216, -0.2157, -0.4275,  ..., -0.1294, -0.1529, -0.1686]],\n",
       "    \n",
       "            [[-0.1686, -0.1529, -0.1294,  ..., -0.5137, -0.2314, -0.1059],\n",
       "             [-0.1529, -0.1686, -0.2000,  ..., -0.5137, -0.2314, -0.1059],\n",
       "             [-0.1294, -0.2000, -0.3725,  ..., -0.5137, -0.2314, -0.1059],\n",
       "             ...,\n",
       "             [-0.1059, -0.2314, -0.5137,  ..., -0.3961, -0.2078, -0.1294],\n",
       "             [-0.1059, -0.2314, -0.5137,  ..., -0.2078, -0.1686, -0.1529],\n",
       "             [-0.1137, -0.2235, -0.5059,  ..., -0.1216, -0.1529, -0.1686]]])]],\n",
       "  'image_sizes': [[tensor([202, 202])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[32, 32]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5, 30486,  2009,  9361,  2379,  7707,  1508,\n",
       "            2437,  1391,  2351,  1875,    74, 73440,     5, 73441, 43686,     5]],\n",
       "         dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]]])]],\n",
       "  'image_sizes': [[tensor([183, 437])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[49, 21]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459, 73458,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0, 73459,     5, 28780, 18605,  1410, 23834,\n",
       "              74, 73440,     5, 73441, 43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       "    tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5294, 0.5294, 0.5294],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.4118, 0.4118, 0.4118],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5686, 0.5686, 0.5686],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.4588, 0.4588, 0.4588],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6078, 0.6078, 0.6078],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5216, 0.5216, 0.5216],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196]]]),\n",
       "    tensor([[[0.1373, 0.1373, 0.1373,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.0980, 0.0980, 0.0980,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.5373, 0.5373, 0.5373,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5216, 0.5216, 0.5216,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000]]])]],\n",
       "  'image_sizes': [[tensor([704, 358])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [140, 204]])],\n",
       "  'tgt_sizes': [tensor([[23, 45],\n",
       "           [32, 32],\n",
       "           [32, 32]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459, 73458,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0, 73459,     5, 28780, 44188,  1410, 23834,\n",
       "              74, 73440,     5, 73441, 43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       "    tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5451, 0.5451, 0.5451],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.4275, 0.4275, 0.4275],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5765, 0.5765, 0.5765],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.4745, 0.4745, 0.4745],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6157, 0.6157, 0.6157],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5216, 0.5216, 0.5216],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196]]]),\n",
       "    tensor([[[0.3176, 0.3176, 0.3176,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.1451, 0.1451, 0.1451,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.0667, 0.0667, 0.0667,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.6392, 0.6392, 0.6392,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5451, 0.5451, 0.5451,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5059, 0.5059, 0.5059,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.9608, 0.9608, 0.9608,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000]]])]],\n",
       "  'image_sizes': [[tensor([704, 358])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [140, 204]])],\n",
       "  'tgt_sizes': [tensor([[23, 45],\n",
       "           [32, 32],\n",
       "           [32, 32]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459, 73458,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0, 73459,     5, 28780,  2957, 20336,  1421,\n",
       "           23423,    74, 73440,     5, 73441, 43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True]]),\n",
       "  'pixel_values': [[tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       "    tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       "    tensor([[[-0.5451, -0.6706, -0.6392,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.2314, -0.5765, -0.6549,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.0196, -0.5216, -0.6706,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             ...,\n",
       "             [-0.0510, -0.5294, -0.6706,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.0510, -0.5294, -0.6706,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.0510, -0.5294, -0.6706,  ...,  1.0000,  1.0000,  1.0000]],\n",
       "    \n",
       "            [[-0.5451, -0.6706, -0.6392,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.2314, -0.5765, -0.6549,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.0196, -0.5216, -0.6706,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             ...,\n",
       "             [-0.0510, -0.5294, -0.6706,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.0510, -0.5294, -0.6706,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.0510, -0.5294, -0.6706,  ...,  1.0000,  1.0000,  1.0000]],\n",
       "    \n",
       "            [[-0.5451, -0.6706, -0.6392,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.2314, -0.5765, -0.6549,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.0196, -0.5216, -0.6706,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             ...,\n",
       "             [-0.0510, -0.5294, -0.6706,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.0510, -0.5294, -0.6706,  ...,  1.0000,  1.0000,  1.0000],\n",
       "             [-0.0510, -0.5294, -0.6706,  ...,  1.0000,  1.0000,  1.0000]]])]],\n",
       "  'image_sizes': [[tensor([586, 387])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [140, 204]])],\n",
       "  'tgt_sizes': [tensor([[26, 39],\n",
       "           [37, 28],\n",
       "           [37, 28]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449,     5,  5415,  1348,  7653,  1348,  7098, 59342,\n",
       "            1348, 10816, 59342,  1508,  1348,  5845,    74, 73440,     5, 73441,\n",
       "           43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]]])]],\n",
       "  'image_sizes': [[tensor([179, 101])]],\n",
       "  'image_bound': [tensor([[ 8, 72]])],\n",
       "  'tgt_sizes': [tensor([[24, 42]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459,     5,\n",
       "           73458,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0, 73459,     5, 28780,  9072, 16627,\n",
       "            1358, 21929, 10486,  1520,  1358, 15626,  3491, 59324, 10790,  2356,\n",
       "              74, 73440,     5, 73441, 43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[0.3255, 0.0588, 0.0745,  ..., 0.0824, 0.0824, 0.3961],\n",
       "             [0.3882, 0.0824, 0.0824,  ..., 0.0824, 0.0824, 0.3961],\n",
       "             [0.3882, 0.0824, 0.0824,  ..., 0.0824, 0.0824, 0.3961],\n",
       "             ...,\n",
       "             [0.3882, 0.0824, 0.0824,  ..., 0.0824, 0.0824, 0.3961],\n",
       "             [0.3882, 0.0824, 0.0824,  ..., 0.0824, 0.0824, 0.3961],\n",
       "             [0.3882, 0.0824, 0.0824,  ..., 0.0824, 0.0824, 0.3961]],\n",
       "    \n",
       "            [[0.5137, 0.2549, 0.5137,  ..., 0.5137, 0.5137, 0.6706],\n",
       "             [0.6706, 0.5137, 0.5137,  ..., 0.5137, 0.5137, 0.6706],\n",
       "             [0.6706, 0.5137, 0.5137,  ..., 0.5137, 0.5137, 0.6706],\n",
       "             ...,\n",
       "             [0.6706, 0.5137, 0.5137,  ..., 0.5137, 0.5137, 0.6706],\n",
       "             [0.6706, 0.5137, 0.5137,  ..., 0.5137, 0.5137, 0.6706],\n",
       "             [0.6706, 0.5137, 0.5137,  ..., 0.5137, 0.5137, 0.6706]],\n",
       "    \n",
       "            [[0.6941, 0.4588, 0.9529,  ..., 0.9529, 0.9529, 0.9686],\n",
       "             [0.9686, 0.9529, 0.9529,  ..., 0.9529, 0.9529, 0.9686],\n",
       "             [0.9686, 0.9529, 0.9529,  ..., 0.9529, 0.9529, 0.9686],\n",
       "             ...,\n",
       "             [0.9686, 0.9529, 0.9529,  ..., 0.9529, 0.9529, 0.9686],\n",
       "             [0.9686, 0.9529, 0.9529,  ..., 0.9529, 0.9529, 0.9686],\n",
       "             [0.9686, 0.9529, 0.9529,  ..., 0.9529, 0.9529, 0.9686]]]),\n",
       "    tensor([[[0.3490, 0.1686, 0.0431,  ..., 0.0588, 0.2157, 0.4196],\n",
       "             [0.3882, 0.1922, 0.0510,  ..., 0.0588, 0.2157, 0.4196],\n",
       "             [0.4196, 0.2078, 0.0588,  ..., 0.0588, 0.2157, 0.4196],\n",
       "             ...,\n",
       "             [0.4118, 0.2078, 0.0588,  ..., 0.0588, 0.2157, 0.4196],\n",
       "             [0.4118, 0.2078, 0.0588,  ..., 0.0588, 0.2157, 0.4196],\n",
       "             [0.4118, 0.2078, 0.0588,  ..., 0.0588, 0.2157, 0.4196]],\n",
       "    \n",
       "            [[0.5216, 0.3176, 0.2863,  ..., 0.4980, 0.5843, 0.6863],\n",
       "             [0.6235, 0.4824, 0.4196,  ..., 0.4980, 0.5843, 0.6863],\n",
       "             [0.6941, 0.6000, 0.5137,  ..., 0.4980, 0.5843, 0.6863],\n",
       "             ...,\n",
       "             [0.6863, 0.5843, 0.4980,  ..., 0.4980, 0.5843, 0.6863],\n",
       "             [0.6863, 0.5843, 0.4980,  ..., 0.4980, 0.5843, 0.6863],\n",
       "             [0.6863, 0.5843, 0.4980,  ..., 0.4980, 0.5843, 0.6863]],\n",
       "    \n",
       "            [[0.6941, 0.4745, 0.5216,  ..., 0.9529, 0.9608, 0.9686],\n",
       "             [0.8667, 0.7804, 0.7961,  ..., 0.9529, 0.9608, 0.9686],\n",
       "             [0.9843, 0.9922, 0.9843,  ..., 0.9529, 0.9608, 0.9686],\n",
       "             ...,\n",
       "             [0.9686, 0.9608, 0.9529,  ..., 0.9529, 0.9608, 0.9686],\n",
       "             [0.9686, 0.9608, 0.9529,  ..., 0.9529, 0.9608, 0.9686],\n",
       "             [0.9686, 0.9608, 0.9529,  ..., 0.9529, 0.9608, 0.9686]]]),\n",
       "    tensor([[[0.4118, 0.2078, 0.0588,  ..., 0.0588, 0.2157, 0.4196],\n",
       "             [0.4118, 0.2078, 0.0588,  ..., 0.0588, 0.2157, 0.4196],\n",
       "             [0.4118, 0.2078, 0.0588,  ..., 0.0588, 0.2157, 0.4196],\n",
       "             ...,\n",
       "             [0.4118, 0.2078, 0.0588,  ..., 0.0588, 0.2157, 0.4196],\n",
       "             [0.4118, 0.2078, 0.0588,  ..., 0.0588, 0.2157, 0.4196],\n",
       "             [0.4118, 0.2078, 0.0588,  ..., 0.0588, 0.2157, 0.4196]],\n",
       "    \n",
       "            [[0.6863, 0.5843, 0.4980,  ..., 0.4980, 0.5843, 0.6863],\n",
       "             [0.6863, 0.5843, 0.4980,  ..., 0.4980, 0.5843, 0.6863],\n",
       "             [0.6863, 0.5843, 0.4980,  ..., 0.4980, 0.5843, 0.6863],\n",
       "             ...,\n",
       "             [0.6863, 0.5843, 0.4980,  ..., 0.4980, 0.5843, 0.6863],\n",
       "             [0.6863, 0.5843, 0.4980,  ..., 0.4980, 0.5843, 0.6863],\n",
       "             [0.6863, 0.5843, 0.4980,  ..., 0.4980, 0.5843, 0.6863]],\n",
       "    \n",
       "            [[0.9686, 0.9608, 0.9529,  ..., 0.9529, 0.9608, 0.9686],\n",
       "             [0.9686, 0.9608, 0.9529,  ..., 0.9529, 0.9608, 0.9686],\n",
       "             [0.9686, 0.9608, 0.9529,  ..., 0.9529, 0.9608, 0.9686],\n",
       "             ...,\n",
       "             [0.9686, 0.9608, 0.9529,  ..., 0.9529, 0.9608, 0.9686],\n",
       "             [0.9686, 0.9608, 0.9529,  ..., 0.9529, 0.9608, 0.9686],\n",
       "             [0.9686, 0.9608, 0.9529,  ..., 0.9529, 0.9608, 0.9686]]])]],\n",
       "  'image_sizes': [[tensor([450, 477])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [141, 205]])],\n",
       "  'tgt_sizes': [tensor([[33, 31],\n",
       "           [23, 44],\n",
       "           [23, 44]])]},\n",
       " {'input_ids': tensor([[    1, 73441,  1836,     5, 73460, 59344, 73461, 73448,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0, 73449, 73458,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0, 73459, 73458,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0, 73459,     5, 28780, 18605,  1410, 23834,\n",
       "              74, 73440,     5, 73441, 43686,     5]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "           True, True, True, True, True, True, True, True, True, True, True, True]]),\n",
       "  'pixel_values': [[tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "    \n",
       "            [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             ...,\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "             [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       "    tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5294, 0.5294, 0.5294],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.4118, 0.4118, 0.4118],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5686, 0.5686, 0.5686],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.4588, 0.4588, 0.4588],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196]],\n",
       "    \n",
       "            [[1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196],\n",
       "             ...,\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.6078, 0.6078, 0.6078],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.5216, 0.5216, 0.5216],\n",
       "             [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8196, 0.8196]]]),\n",
       "    tensor([[[0.1373, 0.1373, 0.1373,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.0980, 0.0980, 0.0980,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.5373, 0.5373, 0.5373,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5216, 0.5216, 0.5216,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.5137, 0.5137, 0.5137,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "    \n",
       "            [[0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             ...,\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000],\n",
       "             [0.9529, 0.9529, 0.9529,  ..., 1.0000, 1.0000, 1.0000]]])]],\n",
       "  'image_sizes': [[tensor([704, 358])]],\n",
       "  'image_bound': [tensor([[  8,  72],\n",
       "           [ 74, 138],\n",
       "           [140, 204]])],\n",
       "  'tgt_sizes': [tensor([[23, 45],\n",
       "           [32, 32],\n",
       "           [32, 32]])]}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering hook for vpm\n"
     ]
    }
   ],
   "source": [
    "hidden_states = {}\n",
    "module_name_count = {}\n",
    "def hook_fn(module, input, output):\n",
    "    # 获取模块名称（假设模块有一个唯一标识的 `name` 属性）\n",
    "    module_name = module.name  # 使用类名作为基础模块名称\n",
    "    \n",
    "    # 检查模块名称是否已存在，若存在则增加计数器\n",
    "    if module_name in module_name_count:\n",
    "        module_name_count[module_name] += 1\n",
    "        module_key = f\"{module_name}_{module_name_count[module_name]}\"  # 为重复名称模块编号\n",
    "    else:\n",
    "        module_name_count[module_name] = 1\n",
    "        module_key = module_name  # 首次遇到该模块时，不加编号\n",
    "\n",
    "    hidden_states[module_key] = {'input': input, 'output': output}\n",
    "\n",
    "handle_list = []\n",
    "ignore_layers = [\"resampler\",\"llm.model\",\"llm.lm_head\"]\n",
    "# for name, module in model_quant.model.named_modules():\n",
    "for name, module in model.named_modules():\n",
    "    if name.endswith(\"vpm\") :\n",
    "        # 选择你想要添加 hook 的模块，比如 Linear, Conv2d, 或 LayerNorm\n",
    "        module.name = name\n",
    "        print(f\"Registering hook for {name}\")\n",
    "        handle_list.append(module.register_forward_hook(hook_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------scale_value: tensor(73.2500, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>)\n",
      "----------------------scale_value: tensor(73.2310, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for example in traindataset:\n",
    "    example[\"input_ids\"] = example[\"input_ids\"].cuda()\n",
    "    example[\"attention_mask\"] = example[\"attention_mask\"].cuda()\n",
    "    example[\"pixel_values\"][0][0] = example[\"pixel_values\"][0][0].cuda()\n",
    "    model_quant.model.cuda()(example)\n",
    "    model.cuda()(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = hidden_states[\"vpm\"][\"output\"][\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9971,  2.3770,  0.1758,  ...,  0.4910,  0.6113,  1.0439],\n",
       "         [-1.5332,  2.1875, -0.2476,  ..., -0.1013,  0.1482,  1.7949],\n",
       "         [-0.1750, -0.0800, -0.5186,  ..., -1.0283,  0.6602,  0.1801],\n",
       "         ...,\n",
       "         [ 0.2646, -0.2050, -0.5308,  ..., -0.2505,  0.6084,  1.9346],\n",
       "         [-2.7012,  1.2871,  0.6060,  ..., -1.7246, -0.1388,  0.9688],\n",
       "         [-1.9326,  1.8760, -0.5083,  ...,  0.1836, -0.3635,  0.5562]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = hidden_states[\"vpm\"][\"output\"][\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ss, \"fp_vit.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(s, \"quant_vit.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for handle in handle_list:\n",
    "    handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_folder = \"/home/workspace/code/git/AutoGPTQ_mlm/hidden_states\" \n",
    "for k, v in hidden_states.items():  \n",
    "    file_path = os.path.join(output_folder,  str(k)) \n",
    "    v[\"output\"].detach().numpy().tofile(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取激活"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_cpu(data):\n",
    "    if isinstance(data,torch.Tensor):\n",
    "        return data.cpu()\n",
    "    elif isinstance(data,tuple):\n",
    "        new_data = []\n",
    "        for i in data:\n",
    "            if isinstance(i,torch.Tensor):\n",
    "                i = i.cpu()\n",
    "            new_data.append(i)\n",
    "        return tuple(new_data)\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported data type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering hook for vpm.encoder.layers.0.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.0.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.0.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.0.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.0.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.0.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.1.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.1.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.1.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.1.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.1.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.1.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.2.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.2.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.2.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.2.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.2.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.2.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.3.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.3.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.3.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.3.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.3.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.3.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.4.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.4.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.4.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.4.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.4.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.4.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.5.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.5.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.5.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.5.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.5.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.5.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.6.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.6.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.6.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.6.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.6.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.6.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.7.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.7.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.7.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.7.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.7.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.7.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.8.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.8.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.8.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.8.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.8.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.8.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.9.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.9.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.9.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.9.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.9.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.9.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.10.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.10.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.10.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.10.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.10.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.10.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.11.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.11.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.11.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.11.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.11.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.11.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.12.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.12.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.12.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.12.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.12.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.12.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.13.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.13.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.13.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.13.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.13.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.13.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.14.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.14.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.14.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.14.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.14.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.14.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.15.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.15.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.15.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.15.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.15.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.15.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.16.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.16.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.16.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.16.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.16.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.16.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.17.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.17.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.17.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.17.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.17.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.17.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.18.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.18.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.18.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.18.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.18.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.18.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.19.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.19.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.19.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.19.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.19.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.19.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.20.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.20.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.20.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.20.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.20.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.20.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.21.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.21.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.21.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.21.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.21.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.21.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.22.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.22.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.22.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.22.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.22.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.22.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.23.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.23.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.23.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.23.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.23.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.23.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.24.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.24.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.24.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.24.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.24.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.24.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.25.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.25.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.25.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.25.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.25.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.25.mlp.fc2\n",
      "Registering hook for vpm.encoder.layers.26.self_attn.k_proj\n",
      "Registering hook for vpm.encoder.layers.26.self_attn.v_proj\n",
      "Registering hook for vpm.encoder.layers.26.self_attn.q_proj\n",
      "Registering hook for vpm.encoder.layers.26.self_attn.out_proj\n",
      "Registering hook for vpm.encoder.layers.26.mlp.fc1\n",
      "Registering hook for vpm.encoder.layers.26.mlp.fc2\n"
     ]
    }
   ],
   "source": [
    "activations = {}\n",
    "import torch.nn as nn\n",
    "# 定义 hook 函数\n",
    "def hook_fn(module, input, output):\n",
    "    # 使用模块名作为键来存储输入和输出\n",
    "    # activations[module.name] = {'input': move_to_cpu(input), 'output': move_to_cpu(output)}\n",
    "    activations[module.name] = {'input': input, 'output': output}\n",
    "from auto_gptq.nn_modules.qlinear.qlinear_cuda_old import QuantLinear\n",
    "handle_list = []\n",
    "ignore_layers = [\"resampler\",\"llm.model\",\"llm.lm_head\"]\n",
    "for name, module in model.named_modules():\n",
    "    if (isinstance(module, nn.Linear) or isinstance(module, QuantLinear)) and not any(name.startswith(ignore_layer) for ignore_layer in ignore_layers):\n",
    "        # 选择你想要添加 hook 的模块，比如 Linear, Conv2d, 或 LayerNorm\n",
    "        module.name = name\n",
    "        print(f\"Registering hook for {name}\")\n",
    "        handle_list.append(module.register_forward_hook(hook_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:6 and cuda:0! (when checking argument for argument weight in method wrapper_CUDA__cudnn_convolution)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      4\u001b[0m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mcuda()(example)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/modeling/minicpm/modeling_minicpmv.py:193\u001b[0m, in \u001b[0;36mMiniCPMV.forward\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 193\u001b[0m     vllm_embedding, vision_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vllm_embedding(data)\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(data):\n\u001b[1;32m    196\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/modeling/minicpm/modeling_minicpmv.py:134\u001b[0m, in \u001b[0;36mMiniCPMV.get_vllm_embedding\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    132\u001b[0m     start_idx \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m    133\u001b[0m     end_idx \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m vision_batch_size\n\u001b[0;32m--> 134\u001b[0m     tmp_hs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvpm(all_pixel_values[start_idx:end_idx], patch_attention_mask\u001b[38;5;241m=\u001b[39mpatch_attn_mask[start_idx:end_idx], tgt_sizes\u001b[38;5;241m=\u001b[39mtgt_sizes[start_idx:end_idx])\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    135\u001b[0m     hs\u001b[38;5;241m.\u001b[39mappend(tmp_hs)\n\u001b[1;32m    136\u001b[0m vision_embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(hs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/modeling/minicpm/modeling_navit_siglip.py:968\u001b[0m, in \u001b[0;36mSiglipVisionTransformer.forward\u001b[0;34m(self, pixel_values, patch_attention_mask, tgt_sizes, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m patch_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     patch_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\n\u001b[1;32m    959\u001b[0m         size\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    960\u001b[0m             batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    965\u001b[0m         device\u001b[38;5;241m=\u001b[39mpixel_values\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    966\u001b[0m     )\n\u001b[0;32m--> 968\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values\u001b[38;5;241m=\u001b[39mpixel_values, patch_attention_mask\u001b[38;5;241m=\u001b[39mpatch_attention_mask, tgt_sizes\u001b[38;5;241m=\u001b[39mtgt_sizes)\n\u001b[1;32m    970\u001b[0m patch_attention_mask \u001b[38;5;241m=\u001b[39m patch_attention_mask\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# The call to `_upad_input` in `_flash_attention_forward` is expensive\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# So when the `patch_attention_mask` is full of 1s (i.e. attending to the whole sequence),\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# avoiding passing the attention_mask, which is equivalent to attending to the full sequence\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/modeling/minicpm/modeling_navit_siglip.py:345\u001b[0m, in \u001b[0;36mSiglipVisionEmbeddings.forward\u001b[0;34m(self, pixel_values, patch_attention_mask, tgt_sizes)\u001b[0m\n\u001b[1;32m    342\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# patch_embeds = self.patch_embedding(pixel_values.cpu())\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m patch_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embedding(pixel_values)\n\u001b[1;32m    346\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m patch_embeds\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    348\u001b[0m max_im_h, max_im_w \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m), pixel_values\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:6 and cuda:0! (when checking argument for argument weight in method wrapper_CUDA__cudnn_convolution)"
     ]
    }
   ],
   "source": [
    "for example in traindataset:\n",
    "    # example[\"input_ids\"] = example[\"input_ids\"].cuda()\n",
    "    # example[\"attention_mask\"] = example[\"attention_mask\"].cuda()\n",
    "    # example[\"pixel_values\"][0][0] = example[\"pixel_values\"][0][0].cuda()\n",
    "    # model.cuda()(example)\n",
    "    model(example)\n",
    "# for handle in handle_list:\n",
    "#     handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for handle in handle_list:\n",
    "    handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vpm.encoder.layers.0.self_attn.q_proj': {'input': (tensor([[[ 4.8578e-04, -3.4855e-03,  1.1160e-03,  ...,  2.4031e-01,\n",
       "              2.4336e-01, -3.4616e-04],\n",
       "            [ 1.1152e-02, -3.5248e-03, -1.5366e-03,  ...,  2.0528e-01,\n",
       "              2.3838e-01, -3.3171e-04],\n",
       "            [ 7.0265e-02, -3.8696e-03, -1.7894e-03,  ...,  3.3367e-01,\n",
       "              1.6415e-01, -4.2620e-04],\n",
       "            ...,\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04],\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04],\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04]],\n",
       "   \n",
       "           [[ 4.1334e-03, -1.7801e-03, -1.1118e-05,  ...,  2.6957e-01,\n",
       "              2.3497e-01, -4.3304e-04],\n",
       "            [ 2.7526e-03, -1.9192e-03, -3.7443e-03,  ...,  8.9450e-02,\n",
       "              2.4055e-01, -4.0075e-04],\n",
       "            [-4.6041e-02, -2.5858e-03,  5.4265e-04,  ...,  3.9427e-01,\n",
       "              1.9231e-01, -4.9419e-04],\n",
       "            ...,\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04],\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04],\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.5870, -0.0699,  0.9994,  ...,  0.1951,  0.0285,  3.4388],\n",
       "           [ 0.6740, -0.0681,  0.9254,  ...,  0.2324,  0.0794,  3.4636],\n",
       "           [ 0.6160, -0.1392,  0.9712,  ...,  0.1755,  0.0444,  3.3299],\n",
       "           ...,\n",
       "           [-0.7442, -0.6847,  0.7900,  ...,  0.2888,  0.1856,  3.6038],\n",
       "           [-0.7442, -0.6847,  0.7900,  ...,  0.2888,  0.1856,  3.6038],\n",
       "           [-0.7442, -0.6847,  0.7900,  ...,  0.2888,  0.1856,  3.6038]],\n",
       "  \n",
       "          [[-0.5194, -0.3677,  0.9688,  ...,  0.1249,  0.1101,  3.4699],\n",
       "           [-0.3596, -0.1607,  0.8792,  ...,  0.3571,  0.2006,  3.4687],\n",
       "           [-0.2516, -0.1029,  0.7730,  ...,  0.1532,  0.2065,  3.2524],\n",
       "           ...,\n",
       "           [-0.7442, -0.6847,  0.7900,  ...,  0.2888,  0.1856,  3.6038],\n",
       "           [-0.7442, -0.6847,  0.7900,  ...,  0.2888,  0.1856,  3.6038],\n",
       "           [-0.7442, -0.6847,  0.7900,  ...,  0.2888,  0.1856,  3.6038]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.0.self_attn.k_proj': {'input': (tensor([[[ 4.8578e-04, -3.4855e-03,  1.1160e-03,  ...,  2.4031e-01,\n",
       "              2.4336e-01, -3.4616e-04],\n",
       "            [ 1.1152e-02, -3.5248e-03, -1.5366e-03,  ...,  2.0528e-01,\n",
       "              2.3838e-01, -3.3171e-04],\n",
       "            [ 7.0265e-02, -3.8696e-03, -1.7894e-03,  ...,  3.3367e-01,\n",
       "              1.6415e-01, -4.2620e-04],\n",
       "            ...,\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04],\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04],\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04]],\n",
       "   \n",
       "           [[ 4.1334e-03, -1.7801e-03, -1.1118e-05,  ...,  2.6957e-01,\n",
       "              2.3497e-01, -4.3304e-04],\n",
       "            [ 2.7526e-03, -1.9192e-03, -3.7443e-03,  ...,  8.9450e-02,\n",
       "              2.4055e-01, -4.0075e-04],\n",
       "            [-4.6041e-02, -2.5858e-03,  5.4265e-04,  ...,  3.9427e-01,\n",
       "              1.9231e-01, -4.9419e-04],\n",
       "            ...,\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04],\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04],\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.8435, -0.2601,  0.0235,  ..., -0.9185,  0.0111, -0.7057],\n",
       "           [ 0.8503, -0.2557, -0.0849,  ..., -0.7627,  0.1419, -0.7030],\n",
       "           [ 0.8590, -0.3188, -0.0512,  ..., -0.4433,  0.4314, -0.6995],\n",
       "           ...,\n",
       "           [-0.4741, -0.7599, -0.1210,  ..., -0.9240, -0.6570, -0.7182],\n",
       "           [-0.4741, -0.7599, -0.1210,  ..., -0.9240, -0.6570, -0.7182],\n",
       "           [-0.4741, -0.7599, -0.1210,  ..., -0.9240, -0.6570, -0.7182]],\n",
       "  \n",
       "          [[-0.2007, -0.6378, -0.0417,  ..., -0.9536, -0.3260, -0.6877],\n",
       "           [-0.1111, -0.4836, -0.0803,  ..., -0.8251, -0.1789, -0.6515],\n",
       "           [ 0.0726, -0.3375, -0.2408,  ..., -0.2529,  0.0817, -0.6328],\n",
       "           ...,\n",
       "           [-0.4741, -0.7599, -0.1210,  ..., -0.9240, -0.6570, -0.7182],\n",
       "           [-0.4741, -0.7599, -0.1210,  ..., -0.9240, -0.6570, -0.7182],\n",
       "           [-0.4741, -0.7599, -0.1210,  ..., -0.9240, -0.6570, -0.7182]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.0.self_attn.v_proj': {'input': (tensor([[[ 4.8578e-04, -3.4855e-03,  1.1160e-03,  ...,  2.4031e-01,\n",
       "              2.4336e-01, -3.4616e-04],\n",
       "            [ 1.1152e-02, -3.5248e-03, -1.5366e-03,  ...,  2.0528e-01,\n",
       "              2.3838e-01, -3.3171e-04],\n",
       "            [ 7.0265e-02, -3.8696e-03, -1.7894e-03,  ...,  3.3367e-01,\n",
       "              1.6415e-01, -4.2620e-04],\n",
       "            ...,\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04],\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04],\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04]],\n",
       "   \n",
       "           [[ 4.1334e-03, -1.7801e-03, -1.1118e-05,  ...,  2.6957e-01,\n",
       "              2.3497e-01, -4.3304e-04],\n",
       "            [ 2.7526e-03, -1.9192e-03, -3.7443e-03,  ...,  8.9450e-02,\n",
       "              2.4055e-01, -4.0075e-04],\n",
       "            [-4.6041e-02, -2.5858e-03,  5.4265e-04,  ...,  3.9427e-01,\n",
       "              1.9231e-01, -4.9419e-04],\n",
       "            ...,\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04],\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04],\n",
       "            [ 5.7611e-03, -1.0862e-03,  5.4058e-03,  ...,  2.2158e-02,\n",
       "              2.4040e-01, -4.5663e-04]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.2470,  0.0440,  0.0050,  ...,  0.0023,  0.0059, -0.1112],\n",
       "           [ 0.1903,  0.1118,  0.0121,  ...,  0.0103, -0.0078, -0.1059],\n",
       "           [ 0.2624,  0.2359,  0.0029,  ...,  0.0402,  0.0362, -0.0746],\n",
       "           ...,\n",
       "           [-0.0096,  0.0189,  0.0331,  ..., -0.0424,  0.0195, -0.0662],\n",
       "           [-0.0096,  0.0189,  0.0331,  ..., -0.0424,  0.0195, -0.0662],\n",
       "           [-0.0096,  0.0189,  0.0331,  ..., -0.0424,  0.0195, -0.0662]],\n",
       "  \n",
       "          [[ 0.0889,  0.0555,  0.0062,  ...,  0.0131,  0.0277, -0.1183],\n",
       "           [-0.0685,  0.0935,  0.0233,  ...,  0.0478,  0.0137, -0.1196],\n",
       "           [-0.0781,  0.1271,  0.0534,  ...,  0.0216,  0.0500,  0.0116],\n",
       "           ...,\n",
       "           [-0.0096,  0.0189,  0.0331,  ..., -0.0424,  0.0195, -0.0662],\n",
       "           [-0.0096,  0.0189,  0.0331,  ..., -0.0424,  0.0195, -0.0662],\n",
       "           [-0.0096,  0.0189,  0.0331,  ..., -0.0424,  0.0195, -0.0662]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.0.self_attn.out_proj': {'input': (tensor([[[ 0.0510,  0.0545,  0.0084,  ...,  0.0036, -0.0053, -0.0335],\n",
       "            [ 0.0545,  0.0588,  0.0086,  ...,  0.0036, -0.0060, -0.0338],\n",
       "            [ 0.0517,  0.0584,  0.0090,  ...,  0.0032, -0.0028, -0.0319],\n",
       "            ...,\n",
       "            [ 0.0368,  0.0123,  0.0101,  ...,  0.0044, -0.0051, -0.0244],\n",
       "            [ 0.0368,  0.0123,  0.0101,  ...,  0.0044, -0.0051, -0.0244],\n",
       "            [ 0.0368,  0.0123,  0.0101,  ...,  0.0044, -0.0051, -0.0244]],\n",
       "   \n",
       "           [[-0.0212,  0.0213, -0.0222,  ..., -0.0016, -0.0044, -0.0246],\n",
       "            [-0.0295,  0.0219, -0.0240,  ..., -0.0021, -0.0055, -0.0266],\n",
       "            [-0.0360,  0.0201, -0.0125,  ..., -0.0019, -0.0021, -0.0271],\n",
       "            ...,\n",
       "            [ 0.0528,  0.0081,  0.0071,  ..., -0.0005, -0.0055, -0.0219],\n",
       "            [ 0.0528,  0.0081,  0.0071,  ..., -0.0005, -0.0055, -0.0219],\n",
       "            [ 0.0528,  0.0081,  0.0071,  ..., -0.0005, -0.0055, -0.0219]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[ 7.7349e-02, -6.9585e-04,  2.4580e-01,  ..., -7.0063e-02,\n",
       "            -1.4263e+00, -4.2995e-02],\n",
       "           [ 7.3345e-02,  8.6299e-03,  2.4049e-01,  ..., -7.0296e-02,\n",
       "            -1.2800e+00, -3.5534e-02],\n",
       "           [ 8.2072e-02, -9.9094e-03,  2.4859e-01,  ..., -7.1972e-02,\n",
       "            -1.5374e+00, -5.7161e-02],\n",
       "           ...,\n",
       "           [ 7.3774e-02, -6.4740e-03,  2.7110e-01,  ..., -3.1369e-03,\n",
       "            -1.3096e+00,  1.2098e-02],\n",
       "           [ 7.3774e-02, -6.4740e-03,  2.7110e-01,  ..., -3.1369e-03,\n",
       "            -1.3096e+00,  1.2098e-02],\n",
       "           [ 7.3774e-02, -6.4740e-03,  2.7110e-01,  ..., -3.1369e-03,\n",
       "            -1.3096e+00,  1.2098e-02]],\n",
       "  \n",
       "          [[ 7.8805e-02,  2.8457e-03,  2.3264e-01,  ..., -5.2739e-02,\n",
       "            -1.4082e+00, -1.4970e-02],\n",
       "           [ 7.4467e-02,  9.6397e-03,  2.3000e-01,  ..., -5.2699e-02,\n",
       "            -1.3462e+00, -6.9992e-03],\n",
       "           [ 1.0016e-01, -2.4731e-02,  2.4389e-01,  ..., -5.9125e-02,\n",
       "            -1.8798e+00, -4.6456e-02],\n",
       "           ...,\n",
       "           [ 8.6274e-02, -3.3460e-02,  2.9144e-01,  ..., -2.0438e-03,\n",
       "            -1.3768e+00,  1.5727e-02],\n",
       "           [ 8.6274e-02, -3.3460e-02,  2.9144e-01,  ..., -2.0438e-03,\n",
       "            -1.3768e+00,  1.5727e-02],\n",
       "           [ 8.6274e-02, -3.3460e-02,  2.9144e-01,  ..., -2.0438e-03,\n",
       "            -1.3768e+00,  1.5727e-02]]], grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.0.mlp.fc1': {'input': (tensor([[[ 1.2077e-01,  2.2833e-01,  3.5057e-02,  ...,  1.4115e-01,\n",
       "             -1.9882e-03,  4.7185e-02],\n",
       "            [ 1.5984e-01,  2.3959e-01,  1.3577e-02,  ..., -9.9540e-03,\n",
       "             -2.0041e-03,  5.8020e-02],\n",
       "            [ 3.5597e-01,  2.5067e-01,  7.2339e-03,  ...,  4.0409e-01,\n",
       "             -1.2974e-03, -4.6807e-03],\n",
       "            ...,\n",
       "            [ 1.4942e-01,  5.4916e-05,  1.1415e-01,  ..., -1.7235e-01,\n",
       "             -1.7674e-03, -4.9916e-03],\n",
       "            [ 1.4942e-01,  5.4916e-05,  1.1415e-01,  ..., -1.7235e-01,\n",
       "             -1.7674e-03, -4.9916e-03],\n",
       "            [ 1.4942e-01,  5.4916e-05,  1.1415e-01,  ..., -1.7235e-01,\n",
       "             -1.7674e-03, -4.9916e-03]],\n",
       "   \n",
       "           [[ 1.3613e-01,  7.0531e-02,  2.6637e-02,  ...,  2.4027e-01,\n",
       "             -1.7769e-03,  2.1268e-04],\n",
       "            [ 1.2893e-01,  8.5644e-02, -4.7656e-03,  ..., -3.7710e-01,\n",
       "             -1.7788e-03,  2.1116e-02],\n",
       "            [-1.1533e-02,  1.1909e-01,  2.7243e-02,  ...,  5.4373e-01,\n",
       "             -1.1425e-03, -4.2177e-02],\n",
       "            ...,\n",
       "            [ 1.6865e-01, -1.5376e-02,  1.2259e-01,  ..., -1.5706e-01,\n",
       "             -1.6924e-03, -3.5975e-03],\n",
       "            [ 1.6865e-01, -1.5376e-02,  1.2259e-01,  ..., -1.5706e-01,\n",
       "             -1.6924e-03, -3.5975e-03],\n",
       "            [ 1.6865e-01, -1.5376e-02,  1.2259e-01,  ..., -1.5706e-01,\n",
       "             -1.6924e-03, -3.5975e-03]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-5.5595, -5.8353, -4.8752,  ..., -3.6615, -1.2016, -5.6580],\n",
       "           [-5.7242, -6.0277, -5.1385,  ..., -3.6267, -1.2934, -5.7259],\n",
       "           [-5.8807, -6.2029, -4.8068,  ..., -3.5906, -1.3272, -5.5984],\n",
       "           ...,\n",
       "           [-5.2753, -5.4477, -5.3434,  ..., -4.6624, -1.0043, -5.3023],\n",
       "           [-5.2753, -5.4477, -5.3434,  ..., -4.6624, -1.0043, -5.3023],\n",
       "           [-5.2753, -5.4477, -5.3434,  ..., -4.6624, -1.0043, -5.3023]],\n",
       "  \n",
       "          [[-5.4734, -5.5019, -4.8837,  ..., -3.5984, -1.2425, -5.5649],\n",
       "           [-5.5059, -5.5720, -4.9598,  ..., -3.5716, -1.4192, -5.4451],\n",
       "           [-6.1025, -5.6255, -4.7636,  ..., -3.5653, -1.4478, -5.1731],\n",
       "           ...,\n",
       "           [-5.2609, -5.4168, -5.3744,  ..., -4.7755, -0.9536, -5.2572],\n",
       "           [-5.2609, -5.4168, -5.3744,  ..., -4.7755, -0.9536, -5.2572],\n",
       "           [-5.2609, -5.4168, -5.3744,  ..., -4.7755, -0.9536, -5.2572]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.0.mlp.fc2': {'input': (tensor([[[-0.0000e+00, -0.0000e+00, -5.8116e-07,  ..., -3.1973e-04,\n",
       "             -1.3811e-01, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.6965e-04,\n",
       "             -1.2691e-01, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -8.5952e-07,  ..., -4.2868e-04,\n",
       "             -1.2263e-01, -0.0000e+00],\n",
       "            ...,\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -1.9453e-06,\n",
       "             -1.5845e-01, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -1.9453e-06,\n",
       "             -1.5845e-01, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -1.9453e-06,\n",
       "             -1.5845e-01, -0.0000e+00]],\n",
       "   \n",
       "           [[-0.0000e+00, -0.0000e+00, -4.3664e-07,  ..., -4.1534e-04,\n",
       "             -1.3321e-01, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -2.9563e-07,  ..., -4.6323e-04,\n",
       "             -1.1082e-01, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -9.9376e-07,  ..., -4.7496e-04,\n",
       "             -1.0713e-01, -0.0000e+00],\n",
       "            ...,\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -9.9625e-07,\n",
       "             -1.6238e-01, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -9.9625e-07,\n",
       "             -1.6238e-01, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -9.9625e-07,\n",
       "             -1.6238e-01, -0.0000e+00]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[-0.0146, -0.0348,  0.3768,  ...,  0.0674,  0.6286,  0.0158],\n",
       "           [-0.0558,  0.0384,  0.4178,  ...,  0.1553,  0.5078,  0.0550],\n",
       "           [-0.0547,  0.0134,  0.2897,  ..., -0.0702,  0.7742,  0.0841],\n",
       "           ...,\n",
       "           [ 0.1100,  0.1034,  0.2081,  ...,  0.0441,  0.8924,  0.0336],\n",
       "           [ 0.1100,  0.1034,  0.2081,  ...,  0.0441,  0.8924,  0.0336],\n",
       "           [ 0.1100,  0.1034,  0.2081,  ...,  0.0441,  0.8924,  0.0336]],\n",
       "  \n",
       "          [[-0.0145,  0.0382,  0.3944,  ..., -0.0713,  0.8372,  0.0178],\n",
       "           [ 0.0436, -0.0435,  0.6755,  ...,  0.0085,  0.7448,  0.0574],\n",
       "           [ 0.1190, -0.3031,  1.2386,  ..., -0.1553,  1.0016, -0.1339],\n",
       "           ...,\n",
       "           [ 0.0921,  0.1029,  0.2125,  ...,  0.0188,  0.9016,  0.0297],\n",
       "           [ 0.0921,  0.1029,  0.2125,  ...,  0.0188,  0.9016,  0.0297],\n",
       "           [ 0.0921,  0.1029,  0.2125,  ...,  0.0188,  0.9016,  0.0297]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.1.self_attn.q_proj': {'input': (tensor([[[-0.0364,  0.1178, -0.0008,  ..., -0.0017,  0.1648,  0.0298],\n",
       "            [-0.0537,  0.1380, -0.0008,  ...,  0.0346,  0.1541,  0.0393],\n",
       "            [ 0.0900,  0.1484, -0.0004,  ..., -0.0520,  0.0940,  0.0133],\n",
       "            ...,\n",
       "            [ 0.0732,  0.0157, -0.0008,  ..., -0.0404,  0.1775,  0.0038],\n",
       "            [ 0.0732,  0.0157, -0.0008,  ..., -0.0404,  0.1775,  0.0038],\n",
       "            [ 0.0732,  0.0157, -0.0008,  ..., -0.0404,  0.1775,  0.0038]],\n",
       "   \n",
       "           [[-0.0422,  0.0355, -0.0008,  ..., -0.0793,  0.1682,  0.0038],\n",
       "            [ 0.0013,  0.0165, -0.0013,  ..., -0.0772,  0.1583,  0.0195],\n",
       "            [-0.0140, -0.0232, -0.0025,  ..., -0.0850,  0.0985, -0.0372],\n",
       "            ...,\n",
       "            [ 0.0684,  0.0072, -0.0008,  ..., -0.0544,  0.1704,  0.0038],\n",
       "            [ 0.0684,  0.0072, -0.0008,  ..., -0.0544,  0.1704,  0.0038],\n",
       "            [ 0.0684,  0.0072, -0.0008,  ..., -0.0544,  0.1704,  0.0038]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.2127,  0.2691,  0.4794,  ..., -5.4212, -0.2269,  0.3774],\n",
       "           [ 0.1969,  0.2862,  0.5513,  ..., -5.3832, -0.2376,  0.3632],\n",
       "           [ 0.1438,  0.1116,  0.3858,  ..., -5.4117, -0.0216,  0.5183],\n",
       "           ...,\n",
       "           [-0.0286,  0.1703, -0.1253,  ..., -5.1495, -0.5602,  0.0950],\n",
       "           [-0.0286,  0.1703, -0.1253,  ..., -5.1495, -0.5602,  0.0950],\n",
       "           [-0.0286,  0.1703, -0.1253,  ..., -5.1495, -0.5602,  0.0950]],\n",
       "  \n",
       "          [[-0.0904,  0.2251,  0.5109,  ..., -5.4288, -0.4852,  0.4002],\n",
       "           [-0.2835,  0.2589,  0.3913,  ..., -5.1523, -0.4250,  0.3072],\n",
       "           [-0.2743,  0.1506,  0.3261,  ..., -5.1400,  0.0318,  0.3842],\n",
       "           ...,\n",
       "           [-0.0197,  0.1381, -0.1512,  ..., -5.1468, -0.5723,  0.1130],\n",
       "           [-0.0197,  0.1381, -0.1512,  ..., -5.1468, -0.5723,  0.1130],\n",
       "           [-0.0197,  0.1381, -0.1512,  ..., -5.1468, -0.5723,  0.1130]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.1.self_attn.k_proj': {'input': (tensor([[[-0.0364,  0.1178, -0.0008,  ..., -0.0017,  0.1648,  0.0298],\n",
       "            [-0.0537,  0.1380, -0.0008,  ...,  0.0346,  0.1541,  0.0393],\n",
       "            [ 0.0900,  0.1484, -0.0004,  ..., -0.0520,  0.0940,  0.0133],\n",
       "            ...,\n",
       "            [ 0.0732,  0.0157, -0.0008,  ..., -0.0404,  0.1775,  0.0038],\n",
       "            [ 0.0732,  0.0157, -0.0008,  ..., -0.0404,  0.1775,  0.0038],\n",
       "            [ 0.0732,  0.0157, -0.0008,  ..., -0.0404,  0.1775,  0.0038]],\n",
       "   \n",
       "           [[-0.0422,  0.0355, -0.0008,  ..., -0.0793,  0.1682,  0.0038],\n",
       "            [ 0.0013,  0.0165, -0.0013,  ..., -0.0772,  0.1583,  0.0195],\n",
       "            [-0.0140, -0.0232, -0.0025,  ..., -0.0850,  0.0985, -0.0372],\n",
       "            ...,\n",
       "            [ 0.0684,  0.0072, -0.0008,  ..., -0.0544,  0.1704,  0.0038],\n",
       "            [ 0.0684,  0.0072, -0.0008,  ..., -0.0544,  0.1704,  0.0038],\n",
       "            [ 0.0684,  0.0072, -0.0008,  ..., -0.0544,  0.1704,  0.0038]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.1983,  0.1816,  0.2794,  ..., -0.1072,  0.0257, -0.3847],\n",
       "           [-0.1723,  0.1983,  0.3212,  ..., -0.0329,  0.0074, -0.3837],\n",
       "           [-0.2869,  0.2545,  0.1580,  ..., -0.0610,  0.0353, -0.3157],\n",
       "           ...,\n",
       "           [-0.3074, -0.1590,  0.0031,  ..., -0.4552,  0.2638,  0.0290],\n",
       "           [-0.3074, -0.1590,  0.0031,  ..., -0.4552,  0.2638,  0.0290],\n",
       "           [-0.3074, -0.1590,  0.0031,  ..., -0.4552,  0.2638,  0.0290]],\n",
       "  \n",
       "          [[-0.3879, -0.0830,  0.3937,  ..., -0.0620, -0.0863, -0.2270],\n",
       "           [-0.3679, -0.3113,  0.5122,  ..., -0.1869,  0.0057, -0.2467],\n",
       "           [-0.3844, -0.3113,  0.1344,  ..., -0.4191,  0.5783, -0.1469],\n",
       "           ...,\n",
       "           [-0.2871, -0.2133,  0.0014,  ..., -0.5078,  0.3049,  0.0704],\n",
       "           [-0.2871, -0.2133,  0.0014,  ..., -0.5078,  0.3049,  0.0704],\n",
       "           [-0.2871, -0.2133,  0.0014,  ..., -0.5078,  0.3049,  0.0704]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.1.self_attn.v_proj': {'input': (tensor([[[-0.0364,  0.1178, -0.0008,  ..., -0.0017,  0.1648,  0.0298],\n",
       "            [-0.0537,  0.1380, -0.0008,  ...,  0.0346,  0.1541,  0.0393],\n",
       "            [ 0.0900,  0.1484, -0.0004,  ..., -0.0520,  0.0940,  0.0133],\n",
       "            ...,\n",
       "            [ 0.0732,  0.0157, -0.0008,  ..., -0.0404,  0.1775,  0.0038],\n",
       "            [ 0.0732,  0.0157, -0.0008,  ..., -0.0404,  0.1775,  0.0038],\n",
       "            [ 0.0732,  0.0157, -0.0008,  ..., -0.0404,  0.1775,  0.0038]],\n",
       "   \n",
       "           [[-0.0422,  0.0355, -0.0008,  ..., -0.0793,  0.1682,  0.0038],\n",
       "            [ 0.0013,  0.0165, -0.0013,  ..., -0.0772,  0.1583,  0.0195],\n",
       "            [-0.0140, -0.0232, -0.0025,  ..., -0.0850,  0.0985, -0.0372],\n",
       "            ...,\n",
       "            [ 0.0684,  0.0072, -0.0008,  ..., -0.0544,  0.1704,  0.0038],\n",
       "            [ 0.0684,  0.0072, -0.0008,  ..., -0.0544,  0.1704,  0.0038],\n",
       "            [ 0.0684,  0.0072, -0.0008,  ..., -0.0544,  0.1704,  0.0038]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.2868, -0.1100,  0.1465,  ...,  0.0113,  0.1809,  0.0114],\n",
       "           [-0.2652, -0.1210,  0.1615,  ...,  0.0818,  0.1324,  0.0724],\n",
       "           [-0.2223, -0.1950,  0.2549,  ...,  0.1214,  0.2415, -0.1191],\n",
       "           ...,\n",
       "           [-0.3008, -0.1397,  0.0889,  ..., -0.0173,  0.1845, -0.0476],\n",
       "           [-0.3008, -0.1397,  0.0889,  ..., -0.0173,  0.1845, -0.0476],\n",
       "           [-0.3008, -0.1397,  0.0889,  ..., -0.0173,  0.1845, -0.0476]],\n",
       "  \n",
       "          [[-0.3249, -0.1664, -0.3561,  ...,  0.0604,  0.2063,  0.0226],\n",
       "           [-0.1078, -0.2787, -0.2501,  ...,  0.2115,  0.1391,  0.1355],\n",
       "           [ 0.0030, -0.4320, -0.3636,  ...,  0.1025,  0.1857, -0.2780],\n",
       "           ...,\n",
       "           [-0.3110, -0.1236,  0.0686,  ..., -0.0211,  0.1909, -0.0144],\n",
       "           [-0.3110, -0.1236,  0.0686,  ..., -0.0211,  0.1909, -0.0144],\n",
       "           [-0.3110, -0.1236,  0.0686,  ..., -0.0211,  0.1909, -0.0144]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.1.self_attn.out_proj': {'input': (tensor([[[ 0.0034, -0.0040,  0.0325,  ...,  0.0645,  0.0760, -0.0503],\n",
       "            [ 0.0045, -0.0051,  0.0325,  ...,  0.0642,  0.0789, -0.0526],\n",
       "            [ 0.0021, -0.0043,  0.0409,  ...,  0.0700,  0.0774, -0.0367],\n",
       "            ...,\n",
       "            [ 0.0241, -0.0195, -0.0448,  ...,  0.0591,  0.0730, -0.0705],\n",
       "            [ 0.0241, -0.0195, -0.0448,  ...,  0.0591,  0.0730, -0.0705],\n",
       "            [ 0.0241, -0.0195, -0.0448,  ...,  0.0591,  0.0730, -0.0705]],\n",
       "   \n",
       "           [[ 0.0311, -0.0280, -0.0134,  ...,  0.0288,  0.0860, -0.0991],\n",
       "            [ 0.0434, -0.0308, -0.0386,  ...,  0.0287,  0.0948, -0.1066],\n",
       "            [ 0.0386, -0.0280, -0.0248,  ...,  0.0266,  0.0991, -0.1141],\n",
       "            ...,\n",
       "            [ 0.0395, -0.0279, -0.0424,  ...,  0.0377,  0.0829, -0.1067],\n",
       "            [ 0.0395, -0.0279, -0.0424,  ...,  0.0377,  0.0829, -0.1067],\n",
       "            [ 0.0395, -0.0279, -0.0424,  ...,  0.0377,  0.0829, -0.1067]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[ 0.0820, -0.2562, -0.3529,  ..., -0.0190,  0.4111, -0.0494],\n",
       "           [ 0.0821, -0.2708, -0.3598,  ..., -0.0122,  0.3964, -0.0431],\n",
       "           [ 0.0707, -0.2653, -0.3616,  ..., -0.0247,  0.3786, -0.0409],\n",
       "           ...,\n",
       "           [ 0.0639, -0.1660, -0.3524,  ...,  0.0088,  0.2659, -0.0823],\n",
       "           [ 0.0639, -0.1660, -0.3524,  ...,  0.0088,  0.2659, -0.0823],\n",
       "           [ 0.0639, -0.1660, -0.3524,  ...,  0.0088,  0.2659, -0.0823]],\n",
       "  \n",
       "          [[ 0.0742, -0.0944, -0.3947,  ...,  0.0075,  0.1622, -0.0916],\n",
       "           [ 0.0658, -0.1365, -0.4399,  ...,  0.0128,  0.0921, -0.0843],\n",
       "           [ 0.0628, -0.2016, -0.4698,  ...,  0.0084,  0.0616, -0.0920],\n",
       "           ...,\n",
       "           [ 0.0397, -0.1735, -0.3537,  ..., -0.0188,  0.1292, -0.0859],\n",
       "           [ 0.0397, -0.1735, -0.3537,  ..., -0.0188,  0.1292, -0.0859],\n",
       "           [ 0.0397, -0.1735, -0.3537,  ..., -0.0188,  0.1292, -0.0859]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.1.mlp.fc1': {'input': (tensor([[[ 0.0364,  0.0944,  0.1534,  ...,  0.0432,  0.1372,  0.0974],\n",
       "            [ 0.0096,  0.1224,  0.1401,  ...,  0.1260,  0.1289,  0.1283],\n",
       "            [ 0.2264,  0.1549,  0.0755,  ..., -0.0651,  0.0799,  0.0520],\n",
       "            ...,\n",
       "            [ 0.1649, -0.0690,  0.1271,  ..., -0.0093,  0.1276,  0.0007],\n",
       "            [ 0.1649, -0.0690,  0.1271,  ..., -0.0093,  0.1276,  0.0007],\n",
       "            [ 0.1649, -0.0690,  0.1271,  ..., -0.0093,  0.1276,  0.0007]],\n",
       "   \n",
       "           [[ 0.0007,  0.0070,  0.1147,  ..., -0.0994,  0.1156, -0.0084],\n",
       "            [ 0.0570, -0.0506,  0.1955,  ..., -0.0796,  0.1007,  0.0428],\n",
       "            [ 0.0307, -0.1309,  0.4377,  ..., -0.0776,  0.0566, -0.0979],\n",
       "            ...,\n",
       "            [ 0.1227, -0.0886,  0.1385,  ..., -0.0670,  0.1112, -0.0005],\n",
       "            [ 0.1227, -0.0886,  0.1385,  ..., -0.0670,  0.1112, -0.0005],\n",
       "            [ 0.1227, -0.0886,  0.1385,  ..., -0.0670,  0.1112, -0.0005]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-3.1259, -1.1880, -5.7500,  ..., -1.2829, -1.6736, -4.4147],\n",
       "           [-3.1180, -1.2527, -5.7500,  ..., -1.3609, -1.5985, -4.2813],\n",
       "           [-2.9036, -1.1598, -5.7500,  ..., -1.4072, -1.5732, -4.3087],\n",
       "           ...,\n",
       "           [-3.7524, -1.2422, -5.7501,  ..., -0.9975, -2.0227, -3.2776],\n",
       "           [-3.7524, -1.2422, -5.7501,  ..., -0.9975, -2.0227, -3.2776],\n",
       "           [-3.7524, -1.2422, -5.7501,  ..., -0.9975, -2.0227, -3.2776]],\n",
       "  \n",
       "          [[-2.6289, -1.2897, -5.7500,  ..., -1.1154, -2.3182, -4.0188],\n",
       "           [-2.6680, -1.9860, -5.7500,  ..., -0.7337, -2.4015, -3.8483],\n",
       "           [-2.3553, -0.3718, -5.7500,  ..., -1.1219, -2.5373, -3.6046],\n",
       "           ...,\n",
       "           [-3.7102, -1.2700, -5.7500,  ..., -1.0198, -2.0347, -3.3179],\n",
       "           [-3.7102, -1.2700, -5.7500,  ..., -1.0198, -2.0347, -3.3179],\n",
       "           [-3.7102, -1.2700, -5.7500,  ..., -1.0198, -2.0347, -3.3179]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.1.mlp.fc2': {'input': (tensor([[[-2.4084e-03, -1.3970e-01, -0.0000e+00,  ..., -1.2822e-01,\n",
       "             -7.8983e-02, -8.2888e-06],\n",
       "            [-2.4734e-03, -1.3196e-01, -0.0000e+00,  ..., -1.1833e-01,\n",
       "             -8.8053e-02, -1.7098e-05],\n",
       "            [-4.9121e-03, -1.4294e-01, -0.0000e+00,  ..., -1.1236e-01,\n",
       "             -9.1181e-02, -1.4767e-05],\n",
       "            ...,\n",
       "            [-2.1706e-04, -1.3324e-01, -0.0000e+00,  ..., -1.5902e-01,\n",
       "             -4.3474e-02, -1.4214e-03],\n",
       "            [-2.1706e-04, -1.3324e-01, -0.0000e+00,  ..., -1.5902e-01,\n",
       "             -4.3474e-02, -1.4214e-03],\n",
       "            [-2.1706e-04, -1.3324e-01, -0.0000e+00,  ..., -1.5902e-01,\n",
       "             -4.3474e-02, -1.4214e-03]],\n",
       "   \n",
       "           [[-1.0791e-02, -1.2737e-01, -0.0000e+00,  ..., -1.4781e-01,\n",
       "             -2.3340e-02, -6.4196e-05],\n",
       "            [-9.7068e-03, -4.6616e-02, -0.0000e+00,  ..., -1.6996e-01,\n",
       "             -1.9209e-02, -1.4198e-04],\n",
       "            [-2.1422e-02, -1.3201e-01, -0.0000e+00,  ..., -1.4711e-01,\n",
       "             -1.3722e-02, -4.0489e-04],\n",
       "            ...,\n",
       "            [-2.6029e-04, -1.2982e-01, -0.0000e+00,  ..., -1.5712e-01,\n",
       "             -4.2477e-02, -1.2286e-03],\n",
       "            [-2.6029e-04, -1.2982e-01, -0.0000e+00,  ..., -1.5712e-01,\n",
       "             -4.2477e-02, -1.2286e-03],\n",
       "            [-2.6029e-04, -1.2982e-01, -0.0000e+00,  ..., -1.5712e-01,\n",
       "             -4.2477e-02, -1.2286e-03]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[ 0.4265, -0.2106,  0.4340,  ...,  0.1788,  0.4529, -0.2890],\n",
       "           [ 0.3376, -0.0988,  0.3984,  ...,  0.2506,  0.5693, -0.2114],\n",
       "           [ 0.3471, -0.1002,  0.2939,  ...,  0.1634,  0.7942, -0.3352],\n",
       "           ...,\n",
       "           [ 0.1941,  0.0377,  0.6390,  ..., -0.1211,  0.8966, -0.1108],\n",
       "           [ 0.1941,  0.0377,  0.6390,  ..., -0.1211,  0.8966, -0.1108],\n",
       "           [ 0.1941,  0.0377,  0.6390,  ..., -0.1211,  0.8966, -0.1108]],\n",
       "  \n",
       "          [[ 0.0816,  0.3253,  0.4539,  ..., -0.0126,  0.7552, -0.1591],\n",
       "           [ 0.2011,  0.3154,  0.7463,  ...,  0.3620,  0.8896, -0.1558],\n",
       "           [ 0.1684,  0.2238,  1.0778,  ...,  0.3892,  1.3720, -0.4079],\n",
       "           ...,\n",
       "           [ 0.1426, -0.0044,  0.6395,  ..., -0.0407,  0.8992, -0.0967],\n",
       "           [ 0.1426, -0.0044,  0.6395,  ..., -0.0407,  0.8992, -0.0967],\n",
       "           [ 0.1426, -0.0044,  0.6395,  ..., -0.0407,  0.8992, -0.0967]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.2.self_attn.q_proj': {'input': (tensor([[[ 0.3765, -0.0607, -0.0013,  ...,  0.0549,  0.3214, -0.0535],\n",
       "            [ 0.2893, -0.0064, -0.0012,  ...,  0.1378,  0.3313,  0.0053],\n",
       "            [ 0.4439,  0.0165, -0.0006,  ..., -0.0057,  0.2449, -0.1170],\n",
       "            ...,\n",
       "            [ 0.2352, -0.0660, -0.0015,  ..., -0.1223,  0.3734, -0.0604],\n",
       "            [ 0.2352, -0.0660, -0.0015,  ..., -0.1223,  0.3734, -0.0604],\n",
       "            [ 0.2352, -0.0660, -0.0015,  ..., -0.1223,  0.3734, -0.0604]],\n",
       "   \n",
       "           [[ 0.0401,  0.0767, -0.0012,  ..., -0.1125,  0.3306, -0.0963],\n",
       "            [ 0.1955,  0.0385, -0.0022,  ...,  0.0995,  0.3356, -0.0517],\n",
       "            [ 0.1453, -0.0699, -0.0044,  ...,  0.0932,  0.3191, -0.3341],\n",
       "            ...,\n",
       "            [ 0.1655, -0.0926, -0.0016,  ..., -0.1097,  0.3344, -0.0581],\n",
       "            [ 0.1655, -0.0926, -0.0016,  ..., -0.1097,  0.3344, -0.0581],\n",
       "            [ 0.1655, -0.0926, -0.0016,  ..., -0.1097,  0.3344, -0.0581]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.1211,  0.3244,  0.4168,  ...,  1.1332,  0.4438,  0.3037],\n",
       "           [ 1.1472,  0.2840,  0.4579,  ...,  0.9478,  0.6049,  0.4407],\n",
       "           [ 0.7134,  0.7501,  0.2614,  ...,  0.7862,  0.4157,  0.1850],\n",
       "           ...,\n",
       "           [-1.5455, -0.2883,  0.7723,  ...,  0.4066, -0.0379,  1.3210],\n",
       "           [-1.5455, -0.2883,  0.7723,  ...,  0.4066, -0.0379,  1.3210],\n",
       "           [-1.5455, -0.2883,  0.7723,  ...,  0.4066, -0.0379,  1.3210]],\n",
       "  \n",
       "          [[ 0.2685,  0.0264,  0.3626,  ...,  0.2953,  0.0629,  0.5442],\n",
       "           [ 0.0187,  0.3524,  0.1977,  ...,  0.2253, -0.2118,  0.7402],\n",
       "           [ 0.1206,  1.2139, -0.2891,  ...,  0.4657, -0.6095,  0.5156],\n",
       "           ...,\n",
       "           [-1.7627, -0.3496,  0.7873,  ...,  0.3013, -0.0377,  1.3238],\n",
       "           [-1.7627, -0.3496,  0.7873,  ...,  0.3013, -0.0377,  1.3238],\n",
       "           [-1.7627, -0.3496,  0.7873,  ...,  0.3013, -0.0377,  1.3238]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.2.self_attn.k_proj': {'input': (tensor([[[ 0.3765, -0.0607, -0.0013,  ...,  0.0549,  0.3214, -0.0535],\n",
       "            [ 0.2893, -0.0064, -0.0012,  ...,  0.1378,  0.3313,  0.0053],\n",
       "            [ 0.4439,  0.0165, -0.0006,  ..., -0.0057,  0.2449, -0.1170],\n",
       "            ...,\n",
       "            [ 0.2352, -0.0660, -0.0015,  ..., -0.1223,  0.3734, -0.0604],\n",
       "            [ 0.2352, -0.0660, -0.0015,  ..., -0.1223,  0.3734, -0.0604],\n",
       "            [ 0.2352, -0.0660, -0.0015,  ..., -0.1223,  0.3734, -0.0604]],\n",
       "   \n",
       "           [[ 0.0401,  0.0767, -0.0012,  ..., -0.1125,  0.3306, -0.0963],\n",
       "            [ 0.1955,  0.0385, -0.0022,  ...,  0.0995,  0.3356, -0.0517],\n",
       "            [ 0.1453, -0.0699, -0.0044,  ...,  0.0932,  0.3191, -0.3341],\n",
       "            ...,\n",
       "            [ 0.1655, -0.0926, -0.0016,  ..., -0.1097,  0.3344, -0.0581],\n",
       "            [ 0.1655, -0.0926, -0.0016,  ..., -0.1097,  0.3344, -0.0581],\n",
       "            [ 0.1655, -0.0926, -0.0016,  ..., -0.1097,  0.3344, -0.0581]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.0809e+00,  5.0665e-01,  1.4143e+00,  ...,  1.1284e+00,\n",
       "            -2.2171e-01, -2.8036e-01],\n",
       "           [ 1.1527e+00,  3.2782e-01,  1.4212e+00,  ...,  1.0270e+00,\n",
       "            -4.4486e-02, -1.2147e-01],\n",
       "           [ 8.3765e-01,  8.2326e-01,  1.2401e+00,  ...,  3.8018e-01,\n",
       "            -2.0597e-01, -1.8747e-03],\n",
       "           ...,\n",
       "           [-1.7829e+00, -5.8282e-01,  1.2629e+00,  ...,  1.1881e-01,\n",
       "            -7.1215e-01,  4.4346e-01],\n",
       "           [-1.7829e+00, -5.8282e-01,  1.2629e+00,  ...,  1.1881e-01,\n",
       "            -7.1215e-01,  4.4346e-01],\n",
       "           [-1.7829e+00, -5.8282e-01,  1.2629e+00,  ...,  1.1881e-01,\n",
       "            -7.1215e-01,  4.4346e-01]],\n",
       "  \n",
       "          [[-1.4430e-01, -4.1193e-01,  9.3811e-01,  ...,  3.5308e-01,\n",
       "            -1.4417e+00,  6.4311e-02],\n",
       "           [-5.2660e-01, -3.8375e-01,  3.1704e-01,  ...,  4.6883e-02,\n",
       "            -1.5342e+00,  3.1517e-01],\n",
       "           [-1.3352e-01,  4.0390e-01, -2.6593e-01,  ..., -6.0393e-02,\n",
       "            -1.8328e+00,  4.0712e-01],\n",
       "           ...,\n",
       "           [-1.9746e+00, -5.9257e-01,  1.3180e+00,  ...,  1.0948e-01,\n",
       "            -7.4389e-01,  3.4245e-01],\n",
       "           [-1.9746e+00, -5.9257e-01,  1.3180e+00,  ...,  1.0948e-01,\n",
       "            -7.4389e-01,  3.4245e-01],\n",
       "           [-1.9746e+00, -5.9257e-01,  1.3180e+00,  ...,  1.0948e-01,\n",
       "            -7.4389e-01,  3.4245e-01]]], grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.2.self_attn.v_proj': {'input': (tensor([[[ 0.3765, -0.0607, -0.0013,  ...,  0.0549,  0.3214, -0.0535],\n",
       "            [ 0.2893, -0.0064, -0.0012,  ...,  0.1378,  0.3313,  0.0053],\n",
       "            [ 0.4439,  0.0165, -0.0006,  ..., -0.0057,  0.2449, -0.1170],\n",
       "            ...,\n",
       "            [ 0.2352, -0.0660, -0.0015,  ..., -0.1223,  0.3734, -0.0604],\n",
       "            [ 0.2352, -0.0660, -0.0015,  ..., -0.1223,  0.3734, -0.0604],\n",
       "            [ 0.2352, -0.0660, -0.0015,  ..., -0.1223,  0.3734, -0.0604]],\n",
       "   \n",
       "           [[ 0.0401,  0.0767, -0.0012,  ..., -0.1125,  0.3306, -0.0963],\n",
       "            [ 0.1955,  0.0385, -0.0022,  ...,  0.0995,  0.3356, -0.0517],\n",
       "            [ 0.1453, -0.0699, -0.0044,  ...,  0.0932,  0.3191, -0.3341],\n",
       "            ...,\n",
       "            [ 0.1655, -0.0926, -0.0016,  ..., -0.1097,  0.3344, -0.0581],\n",
       "            [ 0.1655, -0.0926, -0.0016,  ..., -0.1097,  0.3344, -0.0581],\n",
       "            [ 0.1655, -0.0926, -0.0016,  ..., -0.1097,  0.3344, -0.0581]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.0656, -0.2508, -0.3690,  ...,  0.2412, -0.3168,  0.1313],\n",
       "           [-0.2948, -0.0518, -0.2034,  ...,  0.1388, -0.2937, -0.0079],\n",
       "           [ 0.0593,  0.0477, -0.4189,  ...,  0.0974, -0.3143,  0.0583],\n",
       "           ...,\n",
       "           [-0.0391, -0.3810,  0.1834,  ...,  0.1933, -0.1557,  0.3683],\n",
       "           [-0.0391, -0.3810,  0.1834,  ...,  0.1933, -0.1557,  0.3683],\n",
       "           [-0.0391, -0.3810,  0.1834,  ...,  0.1933, -0.1557,  0.3683]],\n",
       "  \n",
       "          [[ 0.0645, -0.2006,  0.0377,  ...,  0.4144, -0.2864,  0.1859],\n",
       "           [ 0.3204, -0.0676,  1.1825,  ...,  0.5857, -0.3263, -0.3761],\n",
       "           [-0.0109,  0.2012,  1.0372,  ...,  0.5798, -0.3220,  0.2218],\n",
       "           ...,\n",
       "           [-0.0804, -0.5466, -0.0404,  ...,  0.1968, -0.2190,  0.3288],\n",
       "           [-0.0804, -0.5466, -0.0404,  ...,  0.1968, -0.2190,  0.3288],\n",
       "           [-0.0804, -0.5466, -0.0404,  ...,  0.1968, -0.2190,  0.3288]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.2.self_attn.out_proj': {'input': (tensor([[[-7.5909e-02,  8.5931e-03, -1.0014e-01,  ...,  3.5075e-02,\n",
       "             -3.5858e-02,  1.9815e-02],\n",
       "            [-7.7184e-02,  1.0303e-02, -9.5808e-02,  ...,  3.0518e-02,\n",
       "             -2.7446e-02,  1.3759e-02],\n",
       "            [-8.0250e-02,  2.5553e-02, -8.7969e-02,  ...,  4.9486e-02,\n",
       "             -3.2821e-02,  1.8698e-02],\n",
       "            ...,\n",
       "            [-2.3612e-02,  9.4555e-02,  5.2638e-02,  ...,  2.6210e-02,\n",
       "             -1.2386e-02,  2.5780e-02],\n",
       "            [-2.3612e-02,  9.4555e-02,  5.2638e-02,  ...,  2.6210e-02,\n",
       "             -1.2386e-02,  2.5780e-02],\n",
       "            [-2.3612e-02,  9.4555e-02,  5.2638e-02,  ...,  2.6210e-02,\n",
       "             -1.2386e-02,  2.5780e-02]],\n",
       "   \n",
       "           [[-5.1286e-02, -1.5239e-02,  5.3086e-02,  ...,  2.5826e-02,\n",
       "             -2.1992e-02,  5.4639e-05],\n",
       "            [-4.4982e-02, -4.0726e-02,  7.5199e-02,  ...,  1.7996e-02,\n",
       "             -1.0779e-02, -5.8400e-03],\n",
       "            [-3.0559e-02, -7.1235e-03,  1.8826e-01,  ...,  2.9887e-02,\n",
       "             -2.9999e-02,  5.5615e-03],\n",
       "            ...,\n",
       "            [-6.2373e-02,  1.2976e-01,  8.9616e-02,  ...,  2.2997e-02,\n",
       "             -1.4731e-02,  1.2882e-02],\n",
       "            [-6.2373e-02,  1.2976e-01,  8.9616e-02,  ...,  2.2997e-02,\n",
       "             -1.4731e-02,  1.2882e-02],\n",
       "            [-6.2373e-02,  1.2976e-01,  8.9616e-02,  ...,  2.2997e-02,\n",
       "             -1.4731e-02,  1.2882e-02]]], grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.3115,  0.3829,  0.2294,  ..., -0.0287, -0.0032,  0.5020],\n",
       "           [-0.2075,  0.2862,  0.1439,  ...,  0.1544, -0.0718,  0.5431],\n",
       "           [-0.1136,  0.2835,  0.1856,  ...,  0.1324, -0.0887,  0.5404],\n",
       "           ...,\n",
       "           [-0.1427,  0.1697, -0.0765,  ...,  0.0670,  0.1008,  0.2796],\n",
       "           [-0.1427,  0.1697, -0.0765,  ...,  0.0670,  0.1008,  0.2796],\n",
       "           [-0.1427,  0.1697, -0.0765,  ...,  0.0670,  0.1008,  0.2796]],\n",
       "  \n",
       "          [[-0.1584, -0.0387, -0.1610,  ...,  0.0288, -0.0246,  0.2259],\n",
       "           [-0.3991, -0.0471, -0.4214,  ..., -0.0457,  0.0014,  0.0321],\n",
       "           [ 0.0644,  0.1696, -0.4549,  ...,  0.1386,  0.1447,  0.0392],\n",
       "           ...,\n",
       "           [-0.1517,  0.0333, -0.1596,  ...,  0.0145,  0.0489,  0.0891],\n",
       "           [-0.1517,  0.0333, -0.1596,  ...,  0.0145,  0.0489,  0.0891],\n",
       "           [-0.1517,  0.0333, -0.1596,  ...,  0.0145,  0.0489,  0.0891]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.2.mlp.fc1': {'input': (tensor([[[ 0.3227,  0.1940,  0.3064,  ...,  0.1539,  0.4425,  0.2962],\n",
       "            [ 0.3203,  0.2553,  0.2525,  ...,  0.7404,  0.4286,  0.5180],\n",
       "            [ 0.8420,  0.2814,  0.1567,  ...,  0.2205,  0.2760,  0.1639],\n",
       "            ...,\n",
       "            [ 0.3511, -0.0192,  0.2181,  ..., -0.2352,  0.5013,  0.0106],\n",
       "            [ 0.3511, -0.0192,  0.2181,  ..., -0.2352,  0.5013,  0.0106],\n",
       "            [ 0.3511, -0.0192,  0.2181,  ..., -0.2352,  0.5013,  0.0106]],\n",
       "   \n",
       "           [[-0.1491,  0.1683,  0.1265,  ..., -0.2765,  0.4377, -0.1223],\n",
       "            [-0.2925,  0.0543,  0.1898,  ...,  0.2344,  0.3967, -0.1954],\n",
       "            [ 0.5012, -0.0086,  0.4809,  ...,  0.4704,  0.3804, -0.7610],\n",
       "            ...,\n",
       "            [ 0.1779, -0.1872,  0.1971,  ..., -0.2692,  0.4316, -0.1661],\n",
       "            [ 0.1779, -0.1872,  0.1971,  ..., -0.2692,  0.4316, -0.1661],\n",
       "            [ 0.1779, -0.1872,  0.1971,  ..., -0.2692,  0.4316, -0.1661]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-2.8437, -2.5146, -0.9673,  ..., -0.4452, -3.5778, -3.7207],\n",
       "           [-3.4472, -2.6797, -0.9714,  ..., -0.2766, -3.2453, -3.3056],\n",
       "           [-2.4167, -2.4373, -1.0314,  ..., -0.1188, -3.0086, -3.0951],\n",
       "           ...,\n",
       "           [-4.5261, -3.9993, -3.4790,  ..., -1.1255, -4.4239, -3.1530],\n",
       "           [-4.5261, -3.9993, -3.4790,  ..., -1.1255, -4.4239, -3.1530],\n",
       "           [-4.5261, -3.9993, -3.4790,  ..., -1.1255, -4.4239, -3.1530]],\n",
       "  \n",
       "          [[-2.9536, -2.9643, -0.9850,  ..., -0.9482, -3.6030, -3.3108],\n",
       "           [-2.0966, -3.6853, -1.3997,  ..., -0.8416, -3.6436, -2.8767],\n",
       "           [-3.1949, -3.0722, -1.9151,  ..., -0.5982, -3.5027, -3.5814],\n",
       "           ...,\n",
       "           [-3.8772, -4.0944, -1.8409,  ..., -1.2371, -4.1838, -3.3046],\n",
       "           [-3.8772, -4.0944, -1.8409,  ..., -1.2371, -4.1838, -3.3046],\n",
       "           [-3.8772, -4.0944, -1.8409,  ..., -1.2371, -4.1838, -3.3046]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.2.mlp.fc2': {'input': (tensor([[[-5.8831e-03, -1.4539e-02, -1.6138e-01,  ..., -1.4607e-01,\n",
       "             -4.5167e-04, -2.4883e-04],\n",
       "            [-7.5665e-04, -9.3995e-03, -1.6108e-01,  ..., -1.0817e-01,\n",
       "             -1.5951e-03, -1.2850e-03],\n",
       "            [-1.8519e-02, -1.7620e-02, -1.5609e-01,  ..., -5.3778e-02,\n",
       "             -3.5388e-03, -2.6697e-03],\n",
       "            ...,\n",
       "            [-4.4513e-06, -7.0440e-05, -6.6906e-04,  ..., -1.4672e-01,\n",
       "             -7.9106e-06, -2.1974e-03],\n",
       "            [-4.4513e-06, -7.0440e-05, -6.6906e-04,  ..., -1.4672e-01,\n",
       "             -7.9106e-06, -2.1974e-03],\n",
       "            [-4.4513e-06, -7.0440e-05, -6.6906e-04,  ..., -1.4672e-01,\n",
       "             -7.9106e-06, -2.1974e-03]],\n",
       "   \n",
       "           [[-4.2100e-03, -4.0723e-03, -1.6002e-01,  ..., -1.6276e-01,\n",
       "             -4.0750e-04, -1.2608e-03],\n",
       "            [-3.7591e-02, -2.8929e-04, -1.1333e-01,  ..., -1.6842e-01,\n",
       "             -3.4466e-04, -5.3294e-03],\n",
       "            [-1.9030e-03, -2.8798e-03, -5.3097e-02,  ..., -1.6445e-01,\n",
       "             -6.0963e-04, -4.4508e-04],\n",
       "            ...,\n",
       "            [-1.2456e-04, -4.4416e-05, -6.0446e-02,  ..., -1.3386e-01,\n",
       "             -2.8304e-05, -1.2897e-03],\n",
       "            [-1.2456e-04, -4.4416e-05, -6.0446e-02,  ..., -1.3386e-01,\n",
       "             -2.8304e-05, -1.2897e-03],\n",
       "            [-1.2456e-04, -4.4416e-05, -6.0446e-02,  ..., -1.3386e-01,\n",
       "             -2.8304e-05, -1.2897e-03]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[-0.3841, -0.2271,  0.7946,  ...,  0.1725, -0.1028, -0.2716],\n",
       "           [-0.1935, -0.2101,  0.4877,  ...,  0.0987,  0.0172, -0.3231],\n",
       "           [ 0.3500, -0.2031,  0.6599,  ..., -0.4486, -0.0307, -0.4183],\n",
       "           ...,\n",
       "           [ 0.1101,  0.3048, -0.4074,  ..., -0.7885, -0.2835,  0.0544],\n",
       "           [ 0.1101,  0.3048, -0.4074,  ..., -0.7885, -0.2835,  0.0544],\n",
       "           [ 0.1101,  0.3048, -0.4074,  ..., -0.7885, -0.2835,  0.0544]],\n",
       "  \n",
       "          [[ 0.0418, -0.3476,  0.6311,  ...,  0.0481, -0.3608, -0.2958],\n",
       "           [ 0.1363, -0.2659,  1.0611,  ...,  0.6979, -0.9744, -0.3095],\n",
       "           [ 0.4386, -0.2977,  0.9632,  ..., -0.0942, -0.4214,  0.3718],\n",
       "           ...,\n",
       "           [-0.0189,  0.0599, -0.0451,  ..., -0.4544, -0.3629, -0.0803],\n",
       "           [-0.0189,  0.0599, -0.0451,  ..., -0.4544, -0.3629, -0.0803],\n",
       "           [-0.0189,  0.0599, -0.0451,  ..., -0.4544, -0.3629, -0.0803]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.3.self_attn.q_proj': {'input': (tensor([[[-0.0925,  0.0289,  0.2977,  ...,  0.1468,  0.5073,  0.0637],\n",
       "            [ 0.0388,  0.0636,  0.1997,  ...,  0.3735,  0.5401,  0.1424],\n",
       "            [ 0.5816,  0.0904,  0.1616,  ..., -0.3024,  0.3437, -0.0674],\n",
       "            ...,\n",
       "            [ 0.2695,  0.1485, -0.0232,  ..., -0.7604,  0.5256,  0.1054],\n",
       "            [ 0.2695,  0.1485, -0.0232,  ..., -0.7604,  0.5256,  0.1054],\n",
       "            [ 0.2695,  0.1485, -0.0232,  ..., -0.7604,  0.5256,  0.1054]],\n",
       "   \n",
       "           [[ 0.0632, -0.0364,  0.1513,  ..., -0.1637,  0.4081, -0.1640],\n",
       "            [ 0.0737, -0.0538,  0.2972,  ...,  0.6258,  0.1975, -0.2212],\n",
       "            [ 0.5682, -0.0953,  0.4637,  ...,  0.1166,  0.4059, -0.1806],\n",
       "            ...,\n",
       "            [ 0.1315, -0.0345,  0.0412,  ..., -0.5398,  0.4328, -0.0589],\n",
       "            [ 0.1315, -0.0345,  0.0412,  ..., -0.5398,  0.4328, -0.0589],\n",
       "            [ 0.1315, -0.0345,  0.0412,  ..., -0.5398,  0.4328, -0.0589]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-1.0940e-01, -2.9855e+00,  1.3183e+00,  ...,  2.2447e-01,\n",
       "            -3.9468e-01,  1.2313e-01],\n",
       "           [-1.1717e-01, -2.9730e+00,  9.3035e-01,  ..., -3.0275e-01,\n",
       "            -8.9047e-01, -8.2061e-01],\n",
       "           [ 2.3189e-01, -1.1508e+00,  1.4873e+00,  ..., -2.5962e-01,\n",
       "             4.1356e-01, -3.4801e-01],\n",
       "           ...,\n",
       "           [ 1.2143e-01, -1.8818e+00, -8.3415e-01,  ..., -7.0548e-01,\n",
       "             6.9108e-01,  1.3095e+00],\n",
       "           [ 1.2143e-01, -1.8818e+00, -8.3415e-01,  ..., -7.0548e-01,\n",
       "             6.9108e-01,  1.3095e+00],\n",
       "           [ 1.2143e-01, -1.8818e+00, -8.3415e-01,  ..., -7.0548e-01,\n",
       "             6.9108e-01,  1.3095e+00]],\n",
       "  \n",
       "          [[ 5.7462e-01, -2.3264e+00, -6.2715e-03,  ...,  3.1363e-01,\n",
       "            -5.2031e-01,  2.7112e-03],\n",
       "           [ 5.2739e-01, -2.4709e+00, -5.5003e-01,  ..., -1.6677e-02,\n",
       "            -7.2150e-01, -2.4401e-01],\n",
       "           [ 8.4050e-01, -1.6338e+00,  1.0984e+00,  ...,  7.5656e-01,\n",
       "            -1.3439e+00, -4.8307e-01],\n",
       "           ...,\n",
       "           [-5.1479e-01, -1.9721e+00, -5.9867e-01,  ..., -4.4715e-01,\n",
       "             2.7609e-01,  5.2053e-01],\n",
       "           [-5.1479e-01, -1.9721e+00, -5.9867e-01,  ..., -4.4715e-01,\n",
       "             2.7609e-01,  5.2053e-01],\n",
       "           [-5.1479e-01, -1.9721e+00, -5.9867e-01,  ..., -4.4715e-01,\n",
       "             2.7609e-01,  5.2053e-01]]], grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.3.self_attn.k_proj': {'input': (tensor([[[-0.0925,  0.0289,  0.2977,  ...,  0.1468,  0.5073,  0.0637],\n",
       "            [ 0.0388,  0.0636,  0.1997,  ...,  0.3735,  0.5401,  0.1424],\n",
       "            [ 0.5816,  0.0904,  0.1616,  ..., -0.3024,  0.3437, -0.0674],\n",
       "            ...,\n",
       "            [ 0.2695,  0.1485, -0.0232,  ..., -0.7604,  0.5256,  0.1054],\n",
       "            [ 0.2695,  0.1485, -0.0232,  ..., -0.7604,  0.5256,  0.1054],\n",
       "            [ 0.2695,  0.1485, -0.0232,  ..., -0.7604,  0.5256,  0.1054]],\n",
       "   \n",
       "           [[ 0.0632, -0.0364,  0.1513,  ..., -0.1637,  0.4081, -0.1640],\n",
       "            [ 0.0737, -0.0538,  0.2972,  ...,  0.6258,  0.1975, -0.2212],\n",
       "            [ 0.5682, -0.0953,  0.4637,  ...,  0.1166,  0.4059, -0.1806],\n",
       "            ...,\n",
       "            [ 0.1315, -0.0345,  0.0412,  ..., -0.5398,  0.4328, -0.0589],\n",
       "            [ 0.1315, -0.0345,  0.0412,  ..., -0.5398,  0.4328, -0.0589],\n",
       "            [ 0.1315, -0.0345,  0.0412,  ..., -0.5398,  0.4328, -0.0589]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.9740, -1.8269, -1.0502,  ..., -0.2436, -1.7939, -0.3093],\n",
       "           [-0.8761, -1.7897, -1.2371,  ..., -0.6255, -2.4126, -1.5367],\n",
       "           [-1.0358, -1.0484,  0.0967,  ..., -1.4136, -1.6831, -2.2030],\n",
       "           ...,\n",
       "           [-0.7672, -1.1751, -1.5455,  ..., -1.0005, -1.1618, -0.9263],\n",
       "           [-0.7672, -1.1751, -1.5455,  ..., -1.0005, -1.1618, -0.9263],\n",
       "           [-0.7672, -1.1751, -1.5455,  ..., -1.0005, -1.1618, -0.9263]],\n",
       "  \n",
       "          [[-0.3980, -1.3185, -1.2207,  ..., -0.3896, -1.6238, -0.5676],\n",
       "           [-0.9580, -1.3487, -0.9652,  ..., -0.9754, -1.9865, -2.3384],\n",
       "           [-0.5793, -1.3748, -0.1857,  ..., -1.5266, -1.8901, -3.6617],\n",
       "           ...,\n",
       "           [-0.8231, -1.0706, -1.4510,  ..., -0.6791, -1.3450, -0.7460],\n",
       "           [-0.8231, -1.0706, -1.4510,  ..., -0.6791, -1.3450, -0.7460],\n",
       "           [-0.8231, -1.0706, -1.4510,  ..., -0.6791, -1.3450, -0.7460]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.3.self_attn.v_proj': {'input': (tensor([[[-0.0925,  0.0289,  0.2977,  ...,  0.1468,  0.5073,  0.0637],\n",
       "            [ 0.0388,  0.0636,  0.1997,  ...,  0.3735,  0.5401,  0.1424],\n",
       "            [ 0.5816,  0.0904,  0.1616,  ..., -0.3024,  0.3437, -0.0674],\n",
       "            ...,\n",
       "            [ 0.2695,  0.1485, -0.0232,  ..., -0.7604,  0.5256,  0.1054],\n",
       "            [ 0.2695,  0.1485, -0.0232,  ..., -0.7604,  0.5256,  0.1054],\n",
       "            [ 0.2695,  0.1485, -0.0232,  ..., -0.7604,  0.5256,  0.1054]],\n",
       "   \n",
       "           [[ 0.0632, -0.0364,  0.1513,  ..., -0.1637,  0.4081, -0.1640],\n",
       "            [ 0.0737, -0.0538,  0.2972,  ...,  0.6258,  0.1975, -0.2212],\n",
       "            [ 0.5682, -0.0953,  0.4637,  ...,  0.1166,  0.4059, -0.1806],\n",
       "            ...,\n",
       "            [ 0.1315, -0.0345,  0.0412,  ..., -0.5398,  0.4328, -0.0589],\n",
       "            [ 0.1315, -0.0345,  0.0412,  ..., -0.5398,  0.4328, -0.0589],\n",
       "            [ 0.1315, -0.0345,  0.0412,  ..., -0.5398,  0.4328, -0.0589]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.1913,  0.7516,  0.3234,  ..., -0.5616, -0.1790, -0.2820],\n",
       "           [-0.0169,  0.7653, -0.0927,  ..., -0.4549, -0.1616, -0.2058],\n",
       "           [ 0.1260, -0.7671, -0.3856,  ...,  0.2646, -0.6743,  0.4208],\n",
       "           ...,\n",
       "           [ 0.2611,  0.1821, -0.0143,  ..., -0.1093, -0.3049, -0.3900],\n",
       "           [ 0.2611,  0.1821, -0.0143,  ..., -0.1093, -0.3049, -0.3900],\n",
       "           [ 0.2611,  0.1821, -0.0143,  ..., -0.1093, -0.3049, -0.3900]],\n",
       "  \n",
       "          [[ 0.3237,  0.0646,  0.1087,  ..., -0.4239, -0.0599, -0.1948],\n",
       "           [ 0.5222,  0.2497, -0.3330,  ..., -0.1019,  0.4963,  0.0180],\n",
       "           [-0.0406, -0.6093, -0.4670,  ...,  0.2299, -0.2536,  0.3071],\n",
       "           ...,\n",
       "           [ 0.3949,  0.1974,  0.0584,  ..., -0.3527, -0.2782, -0.3376],\n",
       "           [ 0.3949,  0.1974,  0.0584,  ..., -0.3527, -0.2782, -0.3376],\n",
       "           [ 0.3949,  0.1974,  0.0584,  ..., -0.3527, -0.2782, -0.3376]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.3.self_attn.out_proj': {'input': (tensor([[[ 0.0674,  0.1096, -0.0924,  ..., -0.1954,  0.0216,  0.0475],\n",
       "            [ 0.0824,  0.1344, -0.0700,  ..., -0.1108, -0.0734, -0.0225],\n",
       "            [ 0.0741,  0.1682, -0.0178,  ..., -0.1036,  0.0228,  0.0796],\n",
       "            ...,\n",
       "            [ 0.1019, -0.0232, -0.0619,  ..., -0.1086,  0.0381,  0.0794],\n",
       "            [ 0.1019, -0.0232, -0.0619,  ..., -0.1086,  0.0381,  0.0794],\n",
       "            [ 0.1019, -0.0232, -0.0619,  ..., -0.1086,  0.0381,  0.0794]],\n",
       "   \n",
       "           [[ 0.1067, -0.1586, -0.0793,  ..., -0.1327,  0.0837, -0.1043],\n",
       "            [-0.0053, -0.1205, -0.0629,  ...,  0.3461, -0.0417, -0.0475],\n",
       "            [-0.0323, -0.0548, -0.0857,  ...,  0.0014, -0.2440,  0.0621],\n",
       "            ...,\n",
       "            [ 0.0684, -0.1378, -0.0599,  ..., -0.1053,  0.0385,  0.0827],\n",
       "            [ 0.0684, -0.1378, -0.0599,  ..., -0.1053,  0.0385,  0.0827],\n",
       "            [ 0.0684, -0.1378, -0.0599,  ..., -0.1053,  0.0385,  0.0827]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.0666,  0.3946, -0.3789,  ..., -0.5327,  0.2171,  0.0810],\n",
       "           [ 0.0267,  0.1811, -0.4327,  ..., -0.4324,  0.2448,  0.0216],\n",
       "           [ 0.1255,  0.1022, -0.3676,  ..., -0.3185,  0.3336,  0.0158],\n",
       "           ...,\n",
       "           [-0.0996,  0.2280, -0.2411,  ..., -0.4410,  0.1168, -0.1665],\n",
       "           [-0.0996,  0.2280, -0.2411,  ..., -0.4410,  0.1168, -0.1665],\n",
       "           [-0.0996,  0.2280, -0.2411,  ..., -0.4410,  0.1168, -0.1665]],\n",
       "  \n",
       "          [[-0.0095,  0.3892, -0.3643,  ..., -0.4805,  0.1511,  0.1612],\n",
       "           [ 0.1600,  0.1782, -0.4401,  ..., -0.3728,  0.1435,  0.0216],\n",
       "           [ 0.3309,  0.4343, -0.3268,  ..., -0.2354,  0.2994,  0.1963],\n",
       "           ...,\n",
       "           [-0.0467,  0.2047, -0.2434,  ..., -0.3525,  0.1619, -0.0977],\n",
       "           [-0.0467,  0.2047, -0.2434,  ..., -0.3525,  0.1619, -0.0977],\n",
       "           [-0.0467,  0.2047, -0.2434,  ..., -0.3525,  0.1619, -0.0977]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.3.mlp.fc1': {'input': (tensor([[[-0.2065,  0.2755,  0.5811,  ..., -0.3000,  0.9073,  0.1749],\n",
       "            [ 0.0806,  0.1612,  0.3411,  ...,  0.1265,  0.9886,  0.2522],\n",
       "            [ 0.9289,  0.1201,  0.3103,  ..., -0.6332,  0.6716, -0.1167],\n",
       "            ...,\n",
       "            [ 0.2803,  0.3081, -0.0101,  ..., -1.3380,  0.8119,  0.0190],\n",
       "            [ 0.2803,  0.3081, -0.0101,  ..., -1.3380,  0.8119,  0.0190],\n",
       "            [ 0.2803,  0.3081, -0.0101,  ..., -1.3380,  0.8119,  0.0190]],\n",
       "   \n",
       "           [[ 0.0771,  0.1331,  0.2795,  ..., -0.6676,  0.6996, -0.1405],\n",
       "            [ 0.2546, -0.0838,  0.5166,  ...,  0.5479,  0.3388, -0.3534],\n",
       "            [ 1.0611,  0.0314,  0.8973,  ...,  0.0287,  0.7195, -0.1292],\n",
       "            ...,\n",
       "            [ 0.1390, -0.0412,  0.1151,  ..., -0.9761,  0.7058, -0.1974],\n",
       "            [ 0.1390, -0.0412,  0.1151,  ..., -0.9761,  0.7058, -0.1974],\n",
       "            [ 0.1390, -0.0412,  0.1151,  ..., -0.9761,  0.7058, -0.1974]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.1422, -2.9197, -0.1601,  ..., -3.9501, -0.5553, -3.9925],\n",
       "           [ 0.6976, -2.9649, -0.2447,  ..., -2.9740, -0.4892, -3.9204],\n",
       "           [-1.8497, -2.5110, -0.1926,  ..., -2.5784, -1.4210, -4.3665],\n",
       "           ...,\n",
       "           [ 1.4615, -4.5220, -1.3508,  ..., -3.9021,  0.6151, -4.2122],\n",
       "           [ 1.4615, -4.5220, -1.3508,  ..., -3.9021,  0.6151, -4.2122],\n",
       "           [ 1.4615, -4.5220, -1.3508,  ..., -3.9021,  0.6151, -4.2122]],\n",
       "  \n",
       "          [[ 0.7703, -3.3376, -0.4991,  ..., -3.2113, -0.4944, -3.8861],\n",
       "           [ 0.1556, -1.9347, -1.8944,  ..., -2.3992, -1.1065, -4.6090],\n",
       "           [-0.9913, -1.7352, -0.5640,  ..., -3.3208, -0.9201, -4.2286],\n",
       "           ...,\n",
       "           [ 0.9456, -3.9745, -0.7936,  ..., -4.0036,  0.6986, -3.8872],\n",
       "           [ 0.9456, -3.9745, -0.7936,  ..., -4.0036,  0.6986, -3.8872],\n",
       "           [ 0.9456, -3.9745, -0.7936,  ..., -4.0036,  0.6986, -3.8872]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.3.mlp.fc2': {'input': (tensor([[[-6.3057e-02, -4.6764e-03, -6.9857e-02,  ..., -8.8881e-05,\n",
       "             -1.6070e-01, -7.2820e-05],\n",
       "            [ 5.2821e-01, -4.0643e-03, -9.8688e-02,  ..., -3.9494e-03,\n",
       "             -1.5282e-01, -1.0212e-04],\n",
       "            [-5.9551e-02, -1.4672e-02, -8.1603e-02,  ..., -1.2336e-02,\n",
       "             -1.1058e-01, -1.0801e-05],\n",
       "            ...,\n",
       "            [ 1.3561e+00, -4.5820e-06, -1.1961e-01,  ..., -1.1117e-04,\n",
       "              4.4946e-01, -2.4479e-05],\n",
       "            [ 1.3561e+00, -4.5820e-06, -1.1961e-01,  ..., -1.1117e-04,\n",
       "              4.4946e-01, -2.4479e-05],\n",
       "            [ 1.3561e+00, -4.5820e-06, -1.1961e-01,  ..., -1.1117e-04,\n",
       "              4.4946e-01, -2.4479e-05]],\n",
       "   \n",
       "           [[ 6.0033e-01, -1.1429e-03, -1.5417e-01,  ..., -1.7973e-03,\n",
       "             -1.5353e-01, -1.1952e-04],\n",
       "            [ 8.7408e-02, -5.1256e-02, -5.5093e-02,  ..., -1.9314e-02,\n",
       "             -1.4874e-01, -2.7472e-06],\n",
       "            [-1.5952e-01, -7.1862e-02, -1.6154e-01,  ..., -1.2157e-03,\n",
       "             -1.6460e-01, -2.2558e-05],\n",
       "            ...,\n",
       "            [ 7.8263e-01, -7.9243e-05, -1.6969e-01,  ..., -6.9085e-05,\n",
       "              5.2919e-01, -1.1898e-04],\n",
       "            [ 7.8263e-01, -7.9243e-05, -1.6969e-01,  ..., -6.9085e-05,\n",
       "              5.2919e-01, -1.1898e-04],\n",
       "            [ 7.8263e-01, -7.9243e-05, -1.6969e-01,  ..., -6.9085e-05,\n",
       "              5.2919e-01, -1.1898e-04]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[-0.0453, -0.3636, -0.3690,  ...,  0.3086, -0.4150, -0.1050],\n",
       "           [ 0.2114,  0.0202, -0.3913,  ...,  0.0631, -0.2601, -0.1631],\n",
       "           [-0.3368, -0.1626, -0.0164,  ...,  0.1810, -0.5319, -0.1842],\n",
       "           ...,\n",
       "           [ 0.1708, -0.2793,  0.2845,  ...,  0.7203,  0.2604,  0.0154],\n",
       "           [ 0.1708, -0.2793,  0.2845,  ...,  0.7203,  0.2604,  0.0154],\n",
       "           [ 0.1708, -0.2793,  0.2845,  ...,  0.7203,  0.2604,  0.0154]],\n",
       "  \n",
       "          [[-0.2025, -0.2674, -0.2756,  ...,  0.1397, -0.0468,  0.0303],\n",
       "           [ 0.0865, -0.2494,  0.1134,  ..., -0.2274, -0.3098,  0.3622],\n",
       "           [-0.2519, -0.3123, -0.3099,  ..., -0.2612, -0.5724,  0.4346],\n",
       "           ...,\n",
       "           [ 0.0724, -0.3526,  0.0393,  ...,  0.6278,  0.2769,  0.0824],\n",
       "           [ 0.0724, -0.3526,  0.0393,  ...,  0.6278,  0.2769,  0.0824],\n",
       "           [ 0.0724, -0.3526,  0.0393,  ...,  0.6278,  0.2769,  0.0824]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.4.self_attn.q_proj': {'input': (tensor([[[-0.2276,  0.0654,  0.3614,  ..., -0.0533,  0.5119, -0.0041],\n",
       "            [ 0.2011,  0.2280,  0.1001,  ...,  0.0493,  0.6652,  0.0015],\n",
       "            [ 0.4472,  0.0985,  0.2804,  ..., -0.4420,  0.3247, -0.2981],\n",
       "            ...,\n",
       "            [ 0.2795,  0.1694,  0.0634,  ..., -0.5554,  0.7078, -0.0300],\n",
       "            [ 0.2795,  0.1694,  0.0634,  ..., -0.5554,  0.7078, -0.0300],\n",
       "            [ 0.2795,  0.1694,  0.0634,  ..., -0.5554,  0.7078, -0.0300]],\n",
       "   \n",
       "           [[-0.1417,  0.0304,  0.1097,  ..., -0.4874,  0.5339, -0.1539],\n",
       "            [ 0.2338, -0.1182,  0.6030,  ...,  0.1345,  0.1452, -0.0807],\n",
       "            [ 0.6564, -0.0584,  0.8365,  ..., -0.3104,  0.3785,  0.1648],\n",
       "            ...,\n",
       "            [ 0.1138, -0.1109,  0.0899,  ..., -0.3500,  0.6412, -0.1585],\n",
       "            [ 0.1138, -0.1109,  0.0899,  ..., -0.3500,  0.6412, -0.1585],\n",
       "            [ 0.1138, -0.1109,  0.0899,  ..., -0.3500,  0.6412, -0.1585]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.0308,  2.0511, -0.6999,  ..., -2.6577,  0.0856, -1.6510],\n",
       "           [ 0.2391,  1.1197, -1.4112,  ..., -2.1583, -0.0901, -1.2372],\n",
       "           [-0.0274,  0.8545, -1.2633,  ..., -0.9412, -1.1882,  0.1159],\n",
       "           ...,\n",
       "           [-2.8141,  1.6811,  0.2214,  ..., -2.2916, -1.1051,  0.1512],\n",
       "           [-2.8141,  1.6811,  0.2214,  ..., -2.2916, -1.1051,  0.1512],\n",
       "           [-2.8141,  1.6811,  0.2214,  ..., -2.2916, -1.1051,  0.1512]],\n",
       "  \n",
       "          [[-0.6541,  0.8895, -0.6968,  ..., -1.4987,  0.0051, -1.6912],\n",
       "           [-1.1432,  0.3090,  1.4800,  ..., -1.2760, -0.3706, -0.1770],\n",
       "           [-0.3678,  0.6031,  1.3718,  ..., -0.3267, -1.0307,  0.3669],\n",
       "           ...,\n",
       "           [-2.6164,  2.1813, -0.1538,  ..., -2.1760, -0.9572, -0.5150],\n",
       "           [-2.6164,  2.1813, -0.1538,  ..., -2.1760, -0.9572, -0.5150],\n",
       "           [-2.6164,  2.1813, -0.1538,  ..., -2.1760, -0.9572, -0.5150]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.4.self_attn.k_proj': {'input': (tensor([[[-0.2276,  0.0654,  0.3614,  ..., -0.0533,  0.5119, -0.0041],\n",
       "            [ 0.2011,  0.2280,  0.1001,  ...,  0.0493,  0.6652,  0.0015],\n",
       "            [ 0.4472,  0.0985,  0.2804,  ..., -0.4420,  0.3247, -0.2981],\n",
       "            ...,\n",
       "            [ 0.2795,  0.1694,  0.0634,  ..., -0.5554,  0.7078, -0.0300],\n",
       "            [ 0.2795,  0.1694,  0.0634,  ..., -0.5554,  0.7078, -0.0300],\n",
       "            [ 0.2795,  0.1694,  0.0634,  ..., -0.5554,  0.7078, -0.0300]],\n",
       "   \n",
       "           [[-0.1417,  0.0304,  0.1097,  ..., -0.4874,  0.5339, -0.1539],\n",
       "            [ 0.2338, -0.1182,  0.6030,  ...,  0.1345,  0.1452, -0.0807],\n",
       "            [ 0.6564, -0.0584,  0.8365,  ..., -0.3104,  0.3785,  0.1648],\n",
       "            ...,\n",
       "            [ 0.1138, -0.1109,  0.0899,  ..., -0.3500,  0.6412, -0.1585],\n",
       "            [ 0.1138, -0.1109,  0.0899,  ..., -0.3500,  0.6412, -0.1585],\n",
       "            [ 0.1138, -0.1109,  0.0899,  ..., -0.3500,  0.6412, -0.1585]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.0091,  0.7403,  0.1592,  ..., -1.3879,  2.4595, -1.5271],\n",
       "           [ 1.9332,  0.3009,  0.4162,  ..., -1.4458,  2.1218, -1.4996],\n",
       "           [ 0.7371,  0.9246,  0.7677,  ...,  0.6457,  0.2102, -0.3326],\n",
       "           ...,\n",
       "           [-1.3012,  1.2378,  1.5569,  ..., -1.8648,  0.1102, -0.3811],\n",
       "           [-1.3012,  1.2378,  1.5569,  ..., -1.8648,  0.1102, -0.3811],\n",
       "           [-1.3012,  1.2378,  1.5569,  ..., -1.8648,  0.1102, -0.3811]],\n",
       "  \n",
       "          [[-0.2075, -0.0337,  0.4333,  ..., -0.3886,  1.5307, -1.8986],\n",
       "           [-0.2550, -0.1545, -0.1059,  ..., -0.0841,  0.7132, -1.2495],\n",
       "           [-1.0451,  0.9306, -0.4039,  ...,  0.0237,  0.4952,  0.1489],\n",
       "           ...,\n",
       "           [-1.3631,  1.3577,  1.2810,  ..., -1.4490,  0.1015, -0.8751],\n",
       "           [-1.3631,  1.3577,  1.2810,  ..., -1.4490,  0.1015, -0.8751],\n",
       "           [-1.3631,  1.3577,  1.2810,  ..., -1.4490,  0.1015, -0.8751]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.4.self_attn.v_proj': {'input': (tensor([[[-0.2276,  0.0654,  0.3614,  ..., -0.0533,  0.5119, -0.0041],\n",
       "            [ 0.2011,  0.2280,  0.1001,  ...,  0.0493,  0.6652,  0.0015],\n",
       "            [ 0.4472,  0.0985,  0.2804,  ..., -0.4420,  0.3247, -0.2981],\n",
       "            ...,\n",
       "            [ 0.2795,  0.1694,  0.0634,  ..., -0.5554,  0.7078, -0.0300],\n",
       "            [ 0.2795,  0.1694,  0.0634,  ..., -0.5554,  0.7078, -0.0300],\n",
       "            [ 0.2795,  0.1694,  0.0634,  ..., -0.5554,  0.7078, -0.0300]],\n",
       "   \n",
       "           [[-0.1417,  0.0304,  0.1097,  ..., -0.4874,  0.5339, -0.1539],\n",
       "            [ 0.2338, -0.1182,  0.6030,  ...,  0.1345,  0.1452, -0.0807],\n",
       "            [ 0.6564, -0.0584,  0.8365,  ..., -0.3104,  0.3785,  0.1648],\n",
       "            ...,\n",
       "            [ 0.1138, -0.1109,  0.0899,  ..., -0.3500,  0.6412, -0.1585],\n",
       "            [ 0.1138, -0.1109,  0.0899,  ..., -0.3500,  0.6412, -0.1585],\n",
       "            [ 0.1138, -0.1109,  0.0899,  ..., -0.3500,  0.6412, -0.1585]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.1761, -0.3422,  0.3622,  ..., -0.4986, -0.8738,  0.1943],\n",
       "           [ 0.0423,  0.3152,  0.1757,  ..., -0.7152, -1.1767, -0.3354],\n",
       "           [ 0.8038, -0.2113,  0.8388,  ..., -0.1127, -0.5046, -1.3632],\n",
       "           ...,\n",
       "           [-0.1458, -0.3139, -0.3047,  ..., -0.1137,  1.0770, -1.0510],\n",
       "           [-0.1458, -0.3139, -0.3047,  ..., -0.1137,  1.0770, -1.0510],\n",
       "           [-0.1458, -0.3139, -0.3047,  ..., -0.1137,  1.0770, -1.0510]],\n",
       "  \n",
       "          [[-0.0171, -0.4077, -0.0347,  ..., -0.1726, -0.3695, -0.0856],\n",
       "           [-0.2084, -0.0544, -0.1774,  ...,  0.3273,  0.3979,  0.1866],\n",
       "           [ 0.1920, -0.2362,  0.4846,  ...,  0.3632,  0.3169, -0.8664],\n",
       "           ...,\n",
       "           [-0.1585, -0.2676, -0.1531,  ..., -0.1378,  0.6603, -1.0577],\n",
       "           [-0.1585, -0.2676, -0.1531,  ..., -0.1378,  0.6603, -1.0577],\n",
       "           [-0.1585, -0.2676, -0.1531,  ..., -0.1378,  0.6603, -1.0577]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.4.self_attn.out_proj': {'input': (tensor([[[ 0.2136,  0.2422, -0.0652,  ..., -0.1386, -0.0570,  0.0722],\n",
       "            [ 0.1818,  0.2019,  0.0145,  ..., -0.1989, -0.0624, -0.0500],\n",
       "            [ 0.0861,  0.1114,  0.1295,  ..., -0.0462,  0.0995, -0.0958],\n",
       "            ...,\n",
       "            [ 0.0067, -0.0229,  0.2422,  ..., -0.0743, -0.1294, -0.4668],\n",
       "            [ 0.0067, -0.0229,  0.2422,  ..., -0.0743, -0.1294, -0.4668],\n",
       "            [ 0.0067, -0.0229,  0.2422,  ..., -0.0743, -0.1294, -0.4668]],\n",
       "   \n",
       "           [[ 0.0261, -0.1405,  0.1771,  ..., -0.1143,  0.2735,  0.0833],\n",
       "            [ 0.0133, -0.0960, -0.0080,  ..., -0.2259,  0.1123,  0.1083],\n",
       "            [-0.0596, -0.0250,  0.0587,  ..., -0.0609,  0.0465, -0.1623],\n",
       "            ...,\n",
       "            [ 0.0140,  0.0342,  0.1754,  ..., -0.0681,  0.0485, -0.2928],\n",
       "            [ 0.0140,  0.0342,  0.1754,  ..., -0.0681,  0.0485, -0.2928],\n",
       "            [ 0.0140,  0.0342,  0.1754,  ..., -0.0681,  0.0485, -0.2928]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.5630,  0.1940, -0.4901,  ...,  0.1389, -0.3033, -0.4374],\n",
       "           [-0.4603,  0.2289, -0.4548,  ...,  0.1720, -0.3024, -0.3033],\n",
       "           [-0.2740,  0.0149, -0.4461,  ...,  0.3620, -0.0652, -0.5548],\n",
       "           ...,\n",
       "           [-0.1580,  0.3793, -0.3849,  ...,  0.2447, -0.3642, -0.4446],\n",
       "           [-0.1580,  0.3793, -0.3849,  ...,  0.2447, -0.3642, -0.4446],\n",
       "           [-0.1580,  0.3793, -0.3849,  ...,  0.2447, -0.3642, -0.4446]],\n",
       "  \n",
       "          [[-0.1427,  0.1077, -0.3643,  ...,  0.2773, -0.5688, -0.5203],\n",
       "           [-0.2011,  0.0973, -0.5627,  ...,  0.2090, -0.7292, -0.2958],\n",
       "           [-0.1134,  0.1708, -0.6143,  ...,  0.2275, -0.5837, -0.0925],\n",
       "           ...,\n",
       "           [-0.1715,  0.4362, -0.3887,  ...,  0.2945, -0.3905, -0.4163],\n",
       "           [-0.1715,  0.4362, -0.3887,  ...,  0.2945, -0.3905, -0.4163],\n",
       "           [-0.1715,  0.4362, -0.3887,  ...,  0.2945, -0.3905, -0.4163]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.4.mlp.fc1': {'input': (tensor([[[-7.2701e-01,  2.3051e-01,  2.0332e-01,  ...,  4.0535e-03,\n",
       "              7.9538e-01, -9.6955e-02],\n",
       "            [-2.1822e-01,  4.6179e-01, -8.2520e-02,  ...,  1.4569e-01,\n",
       "              1.0101e+00, -1.0213e-03],\n",
       "            [ 2.5036e-01,  1.3729e-01,  1.5778e-01,  ..., -2.1353e-01,\n",
       "              6.6812e-01, -4.5362e-01],\n",
       "            ...,\n",
       "            [ 1.7741e-01,  4.3470e-01, -2.2460e-02,  ..., -4.4081e-01,\n",
       "              1.1041e+00, -6.0162e-02],\n",
       "            [ 1.7741e-01,  4.3470e-01, -2.2460e-02,  ..., -4.4081e-01,\n",
       "              1.1041e+00, -6.0162e-02],\n",
       "            [ 1.7741e-01,  4.3470e-01, -2.2460e-02,  ..., -4.4081e-01,\n",
       "              1.1041e+00, -6.0163e-02]],\n",
       "   \n",
       "           [[-2.8153e-01,  1.2487e-01, -1.7184e-03,  ..., -3.0143e-01,\n",
       "              6.3666e-01, -3.1465e-01],\n",
       "            [ 5.8504e-02, -5.4164e-02,  4.0388e-01,  ...,  2.4264e-01,\n",
       "             -7.4190e-02, -5.9437e-02],\n",
       "            [ 5.8431e-01,  6.2483e-02,  6.4494e-01,  ..., -1.7838e-01,\n",
       "              3.9921e-01,  3.6301e-01],\n",
       "            ...,\n",
       "            [-1.4177e-02,  1.4563e-01, -2.7848e-03,  ..., -1.9484e-01,\n",
       "              9.7356e-01, -1.8238e-01],\n",
       "            [-1.4177e-02,  1.4563e-01, -2.7848e-03,  ..., -1.9484e-01,\n",
       "              9.7356e-01, -1.8238e-01],\n",
       "            [-1.4178e-02,  1.4563e-01, -2.7848e-03,  ..., -1.9484e-01,\n",
       "              9.7356e-01, -1.8238e-01]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.3340, -2.5478, -1.2903,  ..., -1.2920, -1.6543, -3.5031],\n",
       "           [-0.1954, -2.7664, -0.8395,  ..., -0.6328, -1.5140, -3.2313],\n",
       "           [-0.3857, -1.1247, -1.6108,  ...,  0.0548, -2.2440, -1.6316],\n",
       "           ...,\n",
       "           [-0.0650, -2.8710, -0.8361,  ..., -1.4169, -1.7819, -2.5130],\n",
       "           [-0.0650, -2.8710, -0.8361,  ..., -1.4169, -1.7819, -2.5130],\n",
       "           [-0.0650, -2.8710, -0.8361,  ..., -1.4169, -1.7819, -2.5130]],\n",
       "  \n",
       "          [[-0.0605, -2.9022, -0.1288,  ..., -1.0485, -2.1932, -3.5536],\n",
       "           [-0.8162, -2.2653, -0.5414,  ..., -1.3482, -1.0856, -3.1374],\n",
       "           [-0.8014, -1.8355, -1.4051,  ..., -0.7675, -0.9974, -1.1866],\n",
       "           ...,\n",
       "           [-0.1091, -2.4365, -0.5694,  ..., -1.4433, -1.4985, -2.4069],\n",
       "           [-0.1091, -2.4365, -0.5694,  ..., -1.4433, -1.4985, -2.4069],\n",
       "           [-0.1091, -2.4365, -0.5694,  ..., -1.4433, -1.4985, -2.4069]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.4.mlp.fc2': {'input': (tensor([[[-0.1233, -0.0134, -0.1273,  ..., -0.1271, -0.0813, -0.0006],\n",
       "            [-0.0826, -0.0074, -0.1685,  ..., -0.1667, -0.0986, -0.0017],\n",
       "            [-0.1349, -0.1468, -0.0865,  ...,  0.0286, -0.0276, -0.0840],\n",
       "            ...,\n",
       "            [-0.0308, -0.0054, -0.1686,  ..., -0.1111, -0.0667, -0.0146],\n",
       "            [-0.0308, -0.0054, -0.1686,  ..., -0.1111, -0.0667, -0.0146],\n",
       "            [-0.0308, -0.0054, -0.1686,  ..., -0.1111, -0.0667, -0.0146]],\n",
       "   \n",
       "           [[-0.0288, -0.0049, -0.0578,  ..., -0.1545, -0.0308, -0.0005],\n",
       "            [-0.1692, -0.0263, -0.1593,  ..., -0.1199, -0.1509, -0.0023],\n",
       "            [-0.1695, -0.0610, -0.1126,  ..., -0.1700, -0.1590, -0.1399],\n",
       "            ...,\n",
       "            [-0.0498, -0.0177, -0.1620,  ..., -0.1077, -0.1006, -0.0190],\n",
       "            [-0.0498, -0.0177, -0.1620,  ..., -0.1077, -0.1006, -0.0190],\n",
       "            [-0.0498, -0.0177, -0.1620,  ..., -0.1077, -0.1006, -0.0190]]],\n",
       "          grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[ 0.1964,  0.0069, -0.1393,  ...,  0.2479,  0.2093, -0.0462],\n",
       "           [ 0.2917,  0.0728, -0.0788,  ...,  0.1113, -0.0732,  0.1529],\n",
       "           [ 0.3900,  0.0559, -0.1315,  ..., -0.0193, -0.6134,  0.4036],\n",
       "           ...,\n",
       "           [ 0.3465, -0.0345,  0.2214,  ...,  0.3771,  0.1315,  0.0760],\n",
       "           [ 0.3465, -0.0345,  0.2214,  ...,  0.3771,  0.1315,  0.0760],\n",
       "           [ 0.3465, -0.0345,  0.2214,  ...,  0.3771,  0.1315,  0.0760]],\n",
       "  \n",
       "          [[ 0.3739,  0.2650, -0.1175,  ..., -0.0487,  0.0458,  0.1387],\n",
       "           [ 0.2796, -0.0582,  0.0380,  ...,  0.1551, -0.2777,  0.3013],\n",
       "           [ 0.5937,  0.0525, -0.2668,  ...,  0.0998, -0.3081,  0.5345],\n",
       "           ...,\n",
       "           [ 0.3768,  0.0247,  0.0839,  ...,  0.1309,  0.0942,  0.0419],\n",
       "           [ 0.3768,  0.0247,  0.0839,  ...,  0.1309,  0.0942,  0.0419],\n",
       "           [ 0.3768,  0.0247,  0.0839,  ...,  0.1309,  0.0942,  0.0419]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.5.self_attn.q_proj': {'input': (tensor([[[-0.4720,  0.1343,  0.0881,  ...,  0.1786,  0.5177, -0.2852],\n",
       "            [-0.0244,  0.3135, -0.1715,  ...,  0.2084,  0.5420, -0.0928],\n",
       "            [ 0.3847,  0.1060,  0.0585,  ..., -0.1330,  0.0970, -0.3230],\n",
       "            ...,\n",
       "            [ 0.2542,  0.2525,  0.0371,  ..., -0.0950,  0.7047, -0.2042],\n",
       "            [ 0.2542,  0.2525,  0.0371,  ..., -0.0950,  0.7047, -0.2042],\n",
       "            [ 0.2542,  0.2525,  0.0371,  ..., -0.0950,  0.7047, -0.2042]],\n",
       "   \n",
       "           [[-0.0397,  0.1882, -0.1096,  ..., -0.2146,  0.3520, -0.3445],\n",
       "            [ 0.1711, -0.0754,  0.4129,  ...,  0.3187, -0.3024, -0.0557],\n",
       "            [ 0.8186,  0.0538,  0.5183,  ..., -0.0323,  0.0271,  0.4217],\n",
       "            ...,\n",
       "            [ 0.1335,  0.0927, -0.0039,  ..., -0.0379,  0.6088, -0.3137],\n",
       "            [ 0.1335,  0.0927, -0.0039,  ..., -0.0379,  0.6088, -0.3137],\n",
       "            [ 0.1335,  0.0927, -0.0039,  ..., -0.0379,  0.6088, -0.3137]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-1.7513,  1.5023,  0.9333,  ..., -0.6034,  0.2989, -1.3044],\n",
       "           [-0.3002,  1.2217,  0.4472,  ...,  0.1545, -0.4193, -0.2608],\n",
       "           [-0.5544,  0.8202,  0.8145,  ...,  0.2690,  0.0511,  1.2612],\n",
       "           ...,\n",
       "           [-0.0691,  2.5341, -2.5711,  ...,  0.6420, -1.2799, -0.1181],\n",
       "           [-0.0691,  2.5341, -2.5711,  ...,  0.6420, -1.2799, -0.1181],\n",
       "           [-0.0691,  2.5341, -2.5711,  ...,  0.6420, -1.2799, -0.1181]],\n",
       "  \n",
       "          [[-0.5822,  0.3882, -1.3972,  ...,  0.4243,  0.6458,  0.0067],\n",
       "           [-0.1536,  0.3184, -1.1575,  ...,  0.2612, -0.3006,  0.5885],\n",
       "           [-0.4156,  0.9883,  0.0918,  ...,  0.7196, -0.1973,  1.1293],\n",
       "           ...,\n",
       "           [-0.1626,  2.5374, -2.2001,  ...,  0.6727, -1.0425,  0.1158],\n",
       "           [-0.1626,  2.5374, -2.2001,  ...,  0.6727, -1.0425,  0.1158],\n",
       "           [-0.1626,  2.5374, -2.2001,  ...,  0.6727, -1.0425,  0.1158]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.5.self_attn.k_proj': {'input': (tensor([[[-0.4720,  0.1343,  0.0881,  ...,  0.1786,  0.5177, -0.2852],\n",
       "            [-0.0244,  0.3135, -0.1715,  ...,  0.2084,  0.5420, -0.0928],\n",
       "            [ 0.3847,  0.1060,  0.0585,  ..., -0.1330,  0.0970, -0.3230],\n",
       "            ...,\n",
       "            [ 0.2542,  0.2525,  0.0371,  ..., -0.0950,  0.7047, -0.2042],\n",
       "            [ 0.2542,  0.2525,  0.0371,  ..., -0.0950,  0.7047, -0.2042],\n",
       "            [ 0.2542,  0.2525,  0.0371,  ..., -0.0950,  0.7047, -0.2042]],\n",
       "   \n",
       "           [[-0.0397,  0.1882, -0.1096,  ..., -0.2146,  0.3520, -0.3445],\n",
       "            [ 0.1711, -0.0754,  0.4129,  ...,  0.3187, -0.3024, -0.0557],\n",
       "            [ 0.8186,  0.0538,  0.5183,  ..., -0.0323,  0.0271,  0.4217],\n",
       "            ...,\n",
       "            [ 0.1335,  0.0927, -0.0039,  ..., -0.0379,  0.6088, -0.3137],\n",
       "            [ 0.1335,  0.0927, -0.0039,  ..., -0.0379,  0.6088, -0.3137],\n",
       "            [ 0.1335,  0.0927, -0.0039,  ..., -0.0379,  0.6088, -0.3137]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.2686,  1.0216,  2.3411,  ...,  1.3319, -0.0728,  0.0676],\n",
       "           [-0.7097,  0.6211,  2.7143,  ...,  1.7119, -0.5917, -0.0917],\n",
       "           [-1.5171,  0.5338,  1.8265,  ...,  0.9507, -0.1676, -0.1723],\n",
       "           ...,\n",
       "           [-0.5871,  1.3755, -0.6181,  ...,  0.0179, -0.4082,  0.4698],\n",
       "           [-0.5871,  1.3755, -0.6181,  ...,  0.0179, -0.4082,  0.4698],\n",
       "           [-0.5871,  1.3755, -0.6181,  ...,  0.0179, -0.4082,  0.4698]],\n",
       "  \n",
       "          [[ 0.7074, -0.7050,  1.0125,  ...,  0.9725, -0.4512,  0.5005],\n",
       "           [-0.0486, -0.9098,  0.9758,  ...,  0.8888, -1.2077, -0.0039],\n",
       "           [ 0.2233,  0.3054,  0.8243,  ...,  0.4369, -1.1992,  0.1260],\n",
       "           ...,\n",
       "           [-0.3617,  0.9036, -0.2200,  ...,  0.3120, -0.3372,  0.7088],\n",
       "           [-0.3617,  0.9036, -0.2200,  ...,  0.3120, -0.3372,  0.7088],\n",
       "           [-0.3617,  0.9036, -0.2200,  ...,  0.3120, -0.3372,  0.7088]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.5.self_attn.v_proj': {'input': (tensor([[[-0.4720,  0.1343,  0.0881,  ...,  0.1786,  0.5177, -0.2852],\n",
       "            [-0.0244,  0.3135, -0.1715,  ...,  0.2084,  0.5420, -0.0928],\n",
       "            [ 0.3847,  0.1060,  0.0585,  ..., -0.1330,  0.0970, -0.3230],\n",
       "            ...,\n",
       "            [ 0.2542,  0.2525,  0.0371,  ..., -0.0950,  0.7047, -0.2042],\n",
       "            [ 0.2542,  0.2525,  0.0371,  ..., -0.0950,  0.7047, -0.2042],\n",
       "            [ 0.2542,  0.2525,  0.0371,  ..., -0.0950,  0.7047, -0.2042]],\n",
       "   \n",
       "           [[-0.0397,  0.1882, -0.1096,  ..., -0.2146,  0.3520, -0.3445],\n",
       "            [ 0.1711, -0.0754,  0.4129,  ...,  0.3187, -0.3024, -0.0557],\n",
       "            [ 0.8186,  0.0538,  0.5183,  ..., -0.0323,  0.0271,  0.4217],\n",
       "            ...,\n",
       "            [ 0.1335,  0.0927, -0.0039,  ..., -0.0379,  0.6088, -0.3137],\n",
       "            [ 0.1335,  0.0927, -0.0039,  ..., -0.0379,  0.6088, -0.3137],\n",
       "            [ 0.1335,  0.0927, -0.0039,  ..., -0.0379,  0.6088, -0.3137]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-1.0836, -0.1278, -0.2110,  ...,  0.3784,  1.5454,  0.3871],\n",
       "           [-0.5163,  0.1223,  0.4497,  ...,  0.0061,  0.6838,  0.7307],\n",
       "           [ 0.3769,  0.3003,  0.2768,  ...,  0.2593,  0.1529,  1.4674],\n",
       "           ...,\n",
       "           [ 0.0272, -0.5519,  0.3745,  ...,  0.5565,  0.4094, -0.6350],\n",
       "           [ 0.0272, -0.5519,  0.3745,  ...,  0.5565,  0.4094, -0.6350],\n",
       "           [ 0.0272, -0.5519,  0.3745,  ...,  0.5565,  0.4094, -0.6350]],\n",
       "  \n",
       "          [[-0.5869,  0.0542,  0.1801,  ...,  0.2532,  0.8899,  0.4759],\n",
       "           [-0.2426, -0.1472,  0.1119,  ...,  1.4838,  1.2128,  0.8555],\n",
       "           [ 0.0938, -0.0541, -0.6281,  ...,  1.2312,  0.5247,  1.5461],\n",
       "           ...,\n",
       "           [-0.0702, -0.3238, -0.0361,  ...,  0.3259,  0.1087, -0.2494],\n",
       "           [-0.0702, -0.3238, -0.0361,  ...,  0.3259,  0.1087, -0.2494],\n",
       "           [-0.0702, -0.3238, -0.0361,  ...,  0.3259,  0.1087, -0.2494]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.5.self_attn.out_proj': {'input': (tensor([[[-8.5311e-01,  2.7907e-02, -1.3012e+00,  ...,  5.1609e-02,\n",
       "              8.2369e-01, -2.3476e-01],\n",
       "            [-4.5954e-01, -4.7604e-04, -8.3999e-01,  ...,  1.8028e-01,\n",
       "              2.1163e-01,  3.6803e-02],\n",
       "            [ 2.4563e-01,  1.1125e-01,  9.6424e-02,  ...,  3.8430e-03,\n",
       "              2.3955e-01,  1.5842e-01],\n",
       "            ...,\n",
       "            [ 5.0414e-02,  3.2196e-01,  2.0147e-01,  ...,  1.8154e-01,\n",
       "              1.5843e-01,  5.6515e-02],\n",
       "            [ 5.0414e-02,  3.2196e-01,  2.0147e-01,  ...,  1.8154e-01,\n",
       "              1.5843e-01,  5.6515e-02],\n",
       "            [ 5.0414e-02,  3.2196e-01,  2.0147e-01,  ...,  1.8154e-01,\n",
       "              1.5843e-01,  5.6515e-02]],\n",
       "   \n",
       "           [[-2.2952e-01, -1.4410e-01, -4.7628e-01,  ...,  3.7310e-01,\n",
       "              1.1607e-01, -1.9160e-02],\n",
       "            [-1.2879e-01, -2.1798e-01, -2.6307e-01,  ...,  1.2106e-01,\n",
       "              5.4135e-02,  4.5804e-02],\n",
       "            [ 8.8415e-03, -1.3069e-03,  1.2605e-01,  ..., -2.1402e-02,\n",
       "              9.4261e-02,  7.4098e-02],\n",
       "            ...,\n",
       "            [-7.6071e-02,  2.5573e-01,  1.8372e-02,  ...,  1.0886e-01,\n",
       "              5.0390e-02,  2.3161e-02],\n",
       "            [-7.6071e-02,  2.5573e-01,  1.8372e-02,  ...,  1.0886e-01,\n",
       "              5.0390e-02,  2.3161e-02],\n",
       "            [-7.6071e-02,  2.5573e-01,  1.8372e-02,  ...,  1.0886e-01,\n",
       "              5.0390e-02,  2.3161e-02]]], grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.2565, -0.0209,  0.0071,  ..., -0.1818,  0.5558,  0.3459],\n",
       "           [-0.2355,  0.0929, -0.0958,  ..., -0.0435,  0.4326,  0.1381],\n",
       "           [ 0.3026,  0.0902, -0.0158,  ..., -0.2885,  0.2701, -0.1056],\n",
       "           ...,\n",
       "           [ 0.2258,  0.3363, -0.0944,  ...,  0.2925,  0.2139, -0.0076],\n",
       "           [ 0.2258,  0.3363, -0.0944,  ...,  0.2925,  0.2139, -0.0076],\n",
       "           [ 0.2258,  0.3363, -0.0944,  ...,  0.2925,  0.2139, -0.0076]],\n",
       "  \n",
       "          [[ 0.0361,  0.8342, -0.0932,  ..., -0.1241,  0.4184, -0.0016],\n",
       "           [-0.3536,  0.4116, -0.3796,  ..., -0.1594, -0.1040, -0.2074],\n",
       "           [-0.0241,  0.4463, -0.1884,  ..., -0.2158,  0.5975, -0.1725],\n",
       "           ...,\n",
       "           [ 0.0702,  0.5703,  0.0400,  ..., -0.0393,  0.3417,  0.1632],\n",
       "           [ 0.0702,  0.5703,  0.0400,  ..., -0.0393,  0.3417,  0.1632],\n",
       "           [ 0.0702,  0.5703,  0.0400,  ..., -0.0393,  0.3417,  0.1632]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.5.mlp.fc1': {'input': (tensor([[[-0.8764,  0.0397,  0.1479,  ...,  0.1392,  0.9640, -0.1316],\n",
       "            [-0.3425,  0.4150, -0.2367,  ...,  0.2763,  0.9668, -0.0316],\n",
       "            [ 0.5985,  0.0763,  0.0975,  ..., -0.3304,  0.2702, -0.5491],\n",
       "            ...,\n",
       "            [ 0.3452,  0.4482,  0.0315,  ...,  0.1209,  1.0076, -0.3052],\n",
       "            [ 0.3452,  0.4482,  0.0315,  ...,  0.1209,  1.0076, -0.3052],\n",
       "            [ 0.3452,  0.4482,  0.0315,  ...,  0.1209,  1.0076, -0.3052]],\n",
       "   \n",
       "           [[-0.1369,  0.7371, -0.1545,  ..., -0.3045,  0.6840, -0.4827],\n",
       "            [-0.1960,  0.0307,  0.2567,  ...,  0.3100, -0.4678, -0.2760],\n",
       "            [ 0.8550,  0.2649,  0.5171,  ..., -0.1581,  0.3946,  0.3721],\n",
       "            ...,\n",
       "            [ 0.0936,  0.3400,  0.0566,  ..., -0.0189,  0.9402, -0.3274],\n",
       "            [ 0.0936,  0.3400,  0.0566,  ..., -0.0189,  0.9402, -0.3274],\n",
       "            [ 0.0936,  0.3400,  0.0566,  ..., -0.0189,  0.9402, -0.3274]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-2.6009, -1.9040, -1.6832,  ..., -2.1537, -3.3635, -1.7153],\n",
       "           [-2.8439, -1.5724, -0.9906,  ..., -1.8595, -3.0141, -1.8225],\n",
       "           [-2.6780, -1.4342, -1.9900,  ..., -2.3435, -1.8587, -2.8434],\n",
       "           ...,\n",
       "           [-3.2013, -1.6741, -2.5910,  ..., -0.2274, -1.4845, -2.5901],\n",
       "           [-3.2013, -1.6741, -2.5910,  ..., -0.2274, -1.4845, -2.5901],\n",
       "           [-3.2013, -1.6741, -2.5910,  ..., -0.2274, -1.4845, -2.5901]],\n",
       "  \n",
       "          [[-3.1318, -2.3712, -0.9590,  ..., -1.2564, -2.7427,  0.6800],\n",
       "           [-3.8052, -1.8134, -0.0957,  ..., -1.5832, -4.1966, -0.2792],\n",
       "           [-3.5679, -1.5437, -1.9436,  ..., -2.0634, -4.7549, -1.7035],\n",
       "           ...,\n",
       "           [-3.2345, -1.4742, -1.5389,  ..., -0.3428, -1.2135, -2.5223],\n",
       "           [-3.2345, -1.4742, -1.5389,  ..., -0.3428, -1.2135, -2.5223],\n",
       "           [-3.2345, -1.4742, -1.5389,  ..., -0.3428, -1.2135, -2.5223]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.5.mlp.fc2': {'input': (tensor([[[-1.1627e-02, -5.4166e-02, -7.7854e-02,  ..., -3.3435e-02,\n",
       "             -1.0386e-03, -7.4133e-02],\n",
       "            [-5.8786e-03, -9.1284e-02, -1.5958e-01,  ..., -5.8551e-02,\n",
       "             -3.4775e-03, -6.2362e-02],\n",
       "            [-9.4447e-03, -1.0888e-01, -4.6267e-02,  ..., -2.2019e-02,\n",
       "             -5.8630e-02, -5.8882e-03],\n",
       "            ...,\n",
       "            [-1.8608e-03, -7.8920e-02, -1.1935e-02,  ..., -9.3258e-02,\n",
       "             -1.0241e-01, -1.1964e-02],\n",
       "            [-1.8608e-03, -7.8920e-02, -1.1935e-02,  ..., -9.3258e-02,\n",
       "             -1.0241e-01, -1.1964e-02],\n",
       "            [-1.8608e-03, -7.8920e-02, -1.1935e-02,  ..., -9.3258e-02,\n",
       "             -1.0241e-01, -1.1964e-02]],\n",
       "   \n",
       "           [[-2.3612e-03, -2.0639e-02, -1.6200e-01,  ..., -1.3150e-01,\n",
       "             -7.8842e-03,  5.1116e-01],\n",
       "            [-1.7215e-04, -6.3319e-02, -4.4186e-02,  ..., -8.9937e-02,\n",
       "             -2.6514e-05, -1.0890e-01],\n",
       "            [-4.7009e-04, -9.4884e-02, -5.0430e-02,  ..., -4.0157e-02,\n",
       "             -1.1336e-06, -7.5492e-02],\n",
       "            ...,\n",
       "            [-1.6568e-03, -1.0374e-01, -9.5491e-02,  ..., -1.2543e-01,\n",
       "             -1.3670e-01, -1.4255e-02],\n",
       "            [-1.6568e-03, -1.0374e-01, -9.5491e-02,  ..., -1.2543e-01,\n",
       "             -1.3670e-01, -1.4255e-02],\n",
       "            [-1.6568e-03, -1.0374e-01, -9.5491e-02,  ..., -1.2543e-01,\n",
       "             -1.3670e-01, -1.4255e-02]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[ 0.1823, -0.2087, -0.1152,  ...,  0.1094, -0.0316,  0.1824],\n",
       "           [-0.1129, -0.1462,  0.5552,  ..., -0.0116, -0.1190,  0.2053],\n",
       "           [-0.0217, -0.1595,  0.2099,  ...,  0.0374, -0.1622,  0.2606],\n",
       "           ...,\n",
       "           [-0.2549,  0.1573, -0.2718,  ..., -0.2128,  0.5051, -0.4960],\n",
       "           [-0.2549,  0.1573, -0.2718,  ..., -0.2128,  0.5051, -0.4960],\n",
       "           [-0.2549,  0.1573, -0.2718,  ..., -0.2128,  0.5051, -0.4960]],\n",
       "  \n",
       "          [[-0.1252,  0.0565, -0.1448,  ..., -0.2994,  0.1904,  0.1368],\n",
       "           [-0.0714,  0.0908,  0.2598,  ...,  0.3160,  0.2297,  0.2231],\n",
       "           [ 0.0541, -0.5972,  0.0505,  ...,  0.3260, -0.1793, -0.1718],\n",
       "           ...,\n",
       "           [-0.2719,  0.1826, -0.1191,  ..., -0.2185,  0.2433, -0.2236],\n",
       "           [-0.2719,  0.1826, -0.1191,  ..., -0.2185,  0.2433, -0.2236],\n",
       "           [-0.2719,  0.1826, -0.1191,  ..., -0.2185,  0.2433, -0.2236]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.6.self_attn.q_proj': {'input': (tensor([[[-0.5404, -0.0060,  0.0866,  ...,  0.1153,  0.7462,  0.0747],\n",
       "            [-0.2954,  0.2685,  0.1723,  ...,  0.1470,  0.6945,  0.1596],\n",
       "            [ 0.5279,  0.0415,  0.2450,  ..., -0.2507,  0.1394, -0.1431],\n",
       "            ...,\n",
       "            [ 0.1865,  0.4196, -0.0875,  ..., -0.0470,  0.9566, -0.3098],\n",
       "            [ 0.1865,  0.4196, -0.0875,  ..., -0.0470,  0.9566, -0.3098],\n",
       "            [ 0.1865,  0.4196, -0.0875,  ..., -0.0470,  0.9566, -0.3098]],\n",
       "   \n",
       "           [[-0.1351,  0.5910, -0.2158,  ..., -0.4059,  0.6230, -0.1599],\n",
       "            [-0.1526,  0.1315,  0.4256,  ...,  0.3517, -0.2584,  0.0124],\n",
       "            [ 0.7765, -0.0649,  0.5387,  ...,  0.0330,  0.2223,  0.2089],\n",
       "            ...,\n",
       "            [-0.0194,  0.3654,  0.0083,  ..., -0.1480,  0.8195, -0.2217],\n",
       "            [-0.0194,  0.3654,  0.0083,  ..., -0.1480,  0.8195, -0.2217],\n",
       "            [-0.0194,  0.3654,  0.0083,  ..., -0.1480,  0.8195, -0.2217]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.3085, -0.5573,  0.0857,  ...,  0.6040,  0.1351,  3.3855],\n",
       "           [ 0.5410, -0.2976,  0.3218,  ..., -0.2145,  0.2509,  1.5146],\n",
       "           [ 0.2376, -1.8171, -0.1505,  ...,  0.0094,  0.5496,  0.3915],\n",
       "           ...,\n",
       "           [ 2.3319, -1.6355,  0.6620,  ..., -0.6823,  0.2243,  1.0861],\n",
       "           [ 2.3319, -1.6355,  0.6620,  ..., -0.6823,  0.2243,  1.0861],\n",
       "           [ 2.3319, -1.6355,  0.6620,  ..., -0.6823,  0.2243,  1.0861]],\n",
       "  \n",
       "          [[ 0.6687, -0.3335,  0.1404,  ...,  0.1514, -0.3509,  3.6027],\n",
       "           [ 0.7600, -2.2010,  0.5911,  ...,  0.5154, -0.6513,  1.4601],\n",
       "           [ 1.9728, -1.4865, -0.3340,  ...,  0.6031, -0.3527,  0.5798],\n",
       "           ...,\n",
       "           [ 2.2849, -1.7249,  0.9046,  ..., -0.6040,  0.4171,  1.1842],\n",
       "           [ 2.2849, -1.7249,  0.9046,  ..., -0.6040,  0.4171,  1.1842],\n",
       "           [ 2.2849, -1.7249,  0.9046,  ..., -0.6040,  0.4171,  1.1842]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.6.self_attn.k_proj': {'input': (tensor([[[-0.5404, -0.0060,  0.0866,  ...,  0.1153,  0.7462,  0.0747],\n",
       "            [-0.2954,  0.2685,  0.1723,  ...,  0.1470,  0.6945,  0.1596],\n",
       "            [ 0.5279,  0.0415,  0.2450,  ..., -0.2507,  0.1394, -0.1431],\n",
       "            ...,\n",
       "            [ 0.1865,  0.4196, -0.0875,  ..., -0.0470,  0.9566, -0.3098],\n",
       "            [ 0.1865,  0.4196, -0.0875,  ..., -0.0470,  0.9566, -0.3098],\n",
       "            [ 0.1865,  0.4196, -0.0875,  ..., -0.0470,  0.9566, -0.3098]],\n",
       "   \n",
       "           [[-0.1351,  0.5910, -0.2158,  ..., -0.4059,  0.6230, -0.1599],\n",
       "            [-0.1526,  0.1315,  0.4256,  ...,  0.3517, -0.2584,  0.0124],\n",
       "            [ 0.7765, -0.0649,  0.5387,  ...,  0.0330,  0.2223,  0.2089],\n",
       "            ...,\n",
       "            [-0.0194,  0.3654,  0.0083,  ..., -0.1480,  0.8195, -0.2217],\n",
       "            [-0.0194,  0.3654,  0.0083,  ..., -0.1480,  0.8195, -0.2217],\n",
       "            [-0.0194,  0.3654,  0.0083,  ..., -0.1480,  0.8195, -0.2217]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.3468,  1.1314,  2.1134,  ..., -1.5536, -0.0922,  2.3402],\n",
       "           [ 0.1039,  1.5859,  1.7440,  ..., -1.0478,  1.0244,  1.0584],\n",
       "           [ 0.1806, -1.2474,  0.8803,  ..., -0.7038,  0.6914,  0.7940],\n",
       "           ...,\n",
       "           [ 2.1591, -0.3916,  1.7784,  ..., -1.1450, -0.0505,  3.6363],\n",
       "           [ 2.1591, -0.3916,  1.7784,  ..., -1.1450, -0.0505,  3.6363],\n",
       "           [ 2.1591, -0.3916,  1.7784,  ..., -1.1450, -0.0505,  3.6363]],\n",
       "  \n",
       "          [[ 1.2125,  1.3224,  0.7246,  ..., -0.3435, -0.3184,  2.6231],\n",
       "           [ 2.4304, -0.2310,  0.8541,  ..., -1.1068, -0.1602,  0.4467],\n",
       "           [ 1.9090, -0.7544,  0.4117,  ..., -0.7630,  0.0834,  1.2041],\n",
       "           ...,\n",
       "           [ 2.5166, -0.3719,  1.7101,  ..., -0.7664,  0.0290,  3.6352],\n",
       "           [ 2.5166, -0.3719,  1.7101,  ..., -0.7664,  0.0290,  3.6352],\n",
       "           [ 2.5166, -0.3719,  1.7101,  ..., -0.7664,  0.0290,  3.6352]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.6.self_attn.v_proj': {'input': (tensor([[[-0.5404, -0.0060,  0.0866,  ...,  0.1153,  0.7462,  0.0747],\n",
       "            [-0.2954,  0.2685,  0.1723,  ...,  0.1470,  0.6945,  0.1596],\n",
       "            [ 0.5279,  0.0415,  0.2450,  ..., -0.2507,  0.1394, -0.1431],\n",
       "            ...,\n",
       "            [ 0.1865,  0.4196, -0.0875,  ..., -0.0470,  0.9566, -0.3098],\n",
       "            [ 0.1865,  0.4196, -0.0875,  ..., -0.0470,  0.9566, -0.3098],\n",
       "            [ 0.1865,  0.4196, -0.0875,  ..., -0.0470,  0.9566, -0.3098]],\n",
       "   \n",
       "           [[-0.1351,  0.5910, -0.2158,  ..., -0.4059,  0.6230, -0.1599],\n",
       "            [-0.1526,  0.1315,  0.4256,  ...,  0.3517, -0.2584,  0.0124],\n",
       "            [ 0.7765, -0.0649,  0.5387,  ...,  0.0330,  0.2223,  0.2089],\n",
       "            ...,\n",
       "            [-0.0194,  0.3654,  0.0083,  ..., -0.1480,  0.8195, -0.2217],\n",
       "            [-0.0194,  0.3654,  0.0083,  ..., -0.1480,  0.8195, -0.2217],\n",
       "            [-0.0194,  0.3654,  0.0083,  ..., -0.1480,  0.8195, -0.2217]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.5953, -0.1983, -0.4645,  ...,  0.2336, -0.5504, -0.6320],\n",
       "           [ 1.0277,  0.0356,  0.2746,  ..., -0.0841, -0.1526, -0.3383],\n",
       "           [-0.1449,  1.2555,  0.0555,  ..., -0.1297,  0.1764, -0.6104],\n",
       "           ...,\n",
       "           [ 0.6205,  1.3552, -0.0383,  ..., -1.6458,  0.0506, -0.3607],\n",
       "           [ 0.6205,  1.3552, -0.0383,  ..., -1.6458,  0.0506, -0.3607],\n",
       "           [ 0.6205,  1.3552, -0.0383,  ..., -1.6458,  0.0506, -0.3607]],\n",
       "  \n",
       "          [[ 0.1212,  0.8549, -1.0579,  ..., -0.1686, -0.5193, -0.7441],\n",
       "           [-0.1359,  0.7351, -0.7409,  ..., -0.8703,  0.0043, -1.0233],\n",
       "           [ 0.6271,  0.1966, -0.1137,  ..., -0.6516,  0.4979, -0.1395],\n",
       "           ...,\n",
       "           [ 0.3952,  1.4715, -0.3956,  ..., -1.5415,  0.2825, -0.4883],\n",
       "           [ 0.3952,  1.4715, -0.3956,  ..., -1.5415,  0.2825, -0.4883],\n",
       "           [ 0.3953,  1.4715, -0.3956,  ..., -1.5415,  0.2825, -0.4883]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.6.self_attn.out_proj': {'input': (tensor([[[ 0.7164,  0.0485,  0.0518,  ...,  0.3593, -0.0531, -0.4882],\n",
       "            [ 0.7836, -0.0043,  0.0041,  ...,  0.1333, -0.0700, -0.0472],\n",
       "            [ 0.0300,  0.5346, -0.1245,  ...,  0.1423, -0.0926,  0.0183],\n",
       "            ...,\n",
       "            [ 0.4219,  0.5372,  0.0292,  ...,  0.2216, -0.0980,  0.0033],\n",
       "            [ 0.4219,  0.5372,  0.0292,  ...,  0.2216, -0.0980,  0.0033],\n",
       "            [ 0.4219,  0.5372,  0.0292,  ...,  0.2216, -0.0980,  0.0033]],\n",
       "   \n",
       "           [[-0.5061,  0.5265,  0.1403,  ...,  0.1352, -0.0143, -0.1136],\n",
       "            [-0.1409,  0.7549, -0.9487,  ...,  0.1491, -0.0193, -0.0650],\n",
       "            [-0.0219,  0.6817, -0.6983,  ...,  0.1547, -0.0518,  0.0543],\n",
       "            ...,\n",
       "            [ 0.7040,  0.5134,  0.0977,  ...,  0.1875, -0.0218, -0.0010],\n",
       "            [ 0.7040,  0.5134,  0.0977,  ...,  0.1875, -0.0218, -0.0010],\n",
       "            [ 0.7040,  0.5134,  0.0977,  ...,  0.1875, -0.0218, -0.0010]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[ 0.2058, -0.0655,  0.2263,  ...,  0.0430, -0.1416,  0.2283],\n",
       "           [ 0.0399, -0.0195,  0.0579,  ..., -0.0241, -0.3546,  0.1351],\n",
       "           [-0.1256,  0.0208,  0.0551,  ...,  0.2402, -0.3195,  0.5059],\n",
       "           ...,\n",
       "           [ 0.0484,  0.0470,  0.1658,  ...,  0.4671, -0.3392,  0.3782],\n",
       "           [ 0.0484,  0.0470,  0.1658,  ...,  0.4671, -0.3392,  0.3782],\n",
       "           [ 0.0484,  0.0470,  0.1658,  ...,  0.4671, -0.3392,  0.3782]],\n",
       "  \n",
       "          [[-0.0300,  0.0404,  0.1972,  ...,  0.3663, -0.3307,  0.7303],\n",
       "           [ 0.1124, -0.0588, -0.1691,  ..., -0.0312, -0.1372,  0.5026],\n",
       "           [ 0.3223,  0.0223, -0.3060,  ..., -0.0788, -0.2282,  0.4443],\n",
       "           ...,\n",
       "           [-0.0594, -0.0559,  0.0125,  ...,  0.2679, -0.3493,  0.6909],\n",
       "           [-0.0594, -0.0559,  0.0125,  ...,  0.2679, -0.3493,  0.6909],\n",
       "           [-0.0594, -0.0559,  0.0125,  ...,  0.2679, -0.3493,  0.6909]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.6.mlp.fc1': {'input': (tensor([[[-0.3895, -0.1562,  0.2441,  ...,  0.1080,  0.9262, -0.0630],\n",
       "            [-0.2322,  0.2332,  0.2334,  ...,  0.1048,  0.7150,  0.0088],\n",
       "            [ 0.5049, -0.0446,  0.2962,  ..., -0.2000,  0.0392, -0.1709],\n",
       "            ...,\n",
       "            [ 0.2568,  0.4454,  0.0071,  ...,  0.1118,  1.0742, -0.5581],\n",
       "            [ 0.2568,  0.4454,  0.0071,  ...,  0.1118,  1.0742, -0.5581],\n",
       "            [ 0.2568,  0.4454,  0.0071,  ...,  0.1118,  1.0742, -0.5581]],\n",
       "   \n",
       "           [[-0.1140,  0.6590, -0.0845,  ..., -0.3074,  0.6275, -0.0532],\n",
       "            [-0.0405,  0.0175,  0.3307,  ...,  0.3275, -0.3500,  0.0556],\n",
       "            [ 1.0357, -0.1794,  0.3455,  ..., -0.0748,  0.1871,  0.2919],\n",
       "            ...,\n",
       "            [-0.0061,  0.3278,  0.0270,  ..., -0.0927,  0.9002, -0.2375],\n",
       "            [-0.0061,  0.3278,  0.0270,  ..., -0.0927,  0.9002, -0.2375],\n",
       "            [-0.0061,  0.3278,  0.0270,  ..., -0.0927,  0.9002, -0.2375]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-1.5337, -2.0765,  0.7378,  ..., -2.8017, -0.6796, -1.7676],\n",
       "           [-1.1004, -1.7685,  0.6024,  ..., -2.8577, -1.0058, -1.6851],\n",
       "           [-2.0401, -2.6889, -0.7568,  ..., -2.5394, -1.6951, -2.0566],\n",
       "           ...,\n",
       "           [-1.5565, -2.3575,  0.2131,  ..., -2.8764,  0.1711, -1.9317],\n",
       "           [-1.5565, -2.3575,  0.2131,  ..., -2.8764,  0.1711, -1.9317],\n",
       "           [-1.5565, -2.3575,  0.2131,  ..., -2.8764,  0.1711, -1.9317]],\n",
       "  \n",
       "          [[-0.9171, -2.6518,  1.2783,  ..., -3.6721, -0.6497, -1.7469],\n",
       "           [-0.8361, -3.5164,  0.5158,  ..., -3.6381, -0.7094, -1.6474],\n",
       "           [-1.4470, -3.3476, -0.7277,  ..., -3.9775, -0.4526, -1.4933],\n",
       "           ...,\n",
       "           [-1.5860, -2.3243,  0.2735,  ..., -3.3047, -0.2057, -1.7335],\n",
       "           [-1.5860, -2.3243,  0.2735,  ..., -3.3047, -0.2057, -1.7335],\n",
       "           [-1.5860, -2.3243,  0.2735,  ..., -3.3047, -0.2057, -1.7335]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.6.mlp.fc2': {'input': (tensor([[[-9.6150e-02, -3.9129e-02,  5.6784e-01,  ..., -6.6568e-03,\n",
       "             -1.6885e-01, -6.8256e-02],\n",
       "            [-1.4938e-01, -6.8153e-02,  4.3765e-01,  ..., -5.6428e-03,\n",
       "             -1.5832e-01, -7.7629e-02],\n",
       "            [-4.2035e-02, -9.1643e-03, -1.7004e-01,  ..., -1.3646e-02,\n",
       "             -7.6464e-02, -4.0700e-02],\n",
       "            ...,\n",
       "            [-9.3276e-02, -2.1314e-02,  1.2454e-01,  ..., -5.3340e-03,\n",
       "              9.7188e-02, -5.1531e-02],\n",
       "            [-9.3276e-02, -2.1314e-02,  1.2454e-01,  ..., -5.3340e-03,\n",
       "              9.7188e-02, -5.1531e-02],\n",
       "            [-9.3276e-02, -2.1314e-02,  1.2454e-01,  ..., -5.3340e-03,\n",
       "              9.7188e-02, -5.1531e-02]],\n",
       "   \n",
       "           [[-1.6478e-01, -1.0144e-02,  1.1495e+00,  ..., -3.0587e-04,\n",
       "             -1.6763e-01, -7.0546e-02],\n",
       "            [-1.6861e-01, -5.7764e-04,  3.5946e-01,  ..., -3.5260e-04,\n",
       "             -1.6963e-01, -8.2100e-02],\n",
       "            [-1.0723e-01, -1.1015e-03, -1.6991e-01,  ..., -7.8118e-05,\n",
       "             -1.4730e-01, -1.0129e-01],\n",
       "            ...,\n",
       "            [-8.9591e-02, -2.3018e-02,  1.6619e-01,  ..., -1.2892e-03,\n",
       "             -8.6093e-02, -7.2059e-02],\n",
       "            [-8.9591e-02, -2.3018e-02,  1.6619e-01,  ..., -1.2892e-03,\n",
       "             -8.6093e-02, -7.2059e-02],\n",
       "            [-8.9591e-02, -2.3018e-02,  1.6619e-01,  ..., -1.2892e-03,\n",
       "             -8.6093e-02, -7.2059e-02]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[-0.4390,  0.0762, -0.0749,  ...,  0.2707,  1.0480, -0.2357],\n",
       "           [-0.5090,  0.2746,  0.1793,  ...,  0.0012,  0.3952, -0.4216],\n",
       "           [-0.0811, -0.0892,  0.2865,  ..., -0.3143,  0.5178, -0.3321],\n",
       "           ...,\n",
       "           [ 0.2987,  0.7167, -0.3280,  ...,  1.0956, -0.3060,  0.5866],\n",
       "           [ 0.2987,  0.7167, -0.3280,  ...,  1.0956, -0.3060,  0.5866],\n",
       "           [ 0.2987,  0.7167, -0.3280,  ...,  1.0956, -0.3060,  0.5866]],\n",
       "  \n",
       "          [[-0.3907,  0.6239,  0.1526,  ...,  0.2621,  0.3729, -0.1761],\n",
       "           [-0.1792,  0.6036, -0.0995,  ..., -0.4462,  0.8780, -0.3837],\n",
       "           [-0.4867, -0.4611,  0.0152,  ..., -0.2434,  0.8799, -0.7919],\n",
       "           ...,\n",
       "           [ 0.2993,  0.6198, -0.4815,  ...,  1.1291, -0.0496,  0.3149],\n",
       "           [ 0.2993,  0.6198, -0.4815,  ...,  1.1291, -0.0496,  0.3149],\n",
       "           [ 0.2993,  0.6198, -0.4815,  ...,  1.1291, -0.0496,  0.3149]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.7.self_attn.q_proj': {'input': (tensor([[[-0.5306, -0.0143,  0.1568,  ...,  0.2376,  1.0380,  0.0391],\n",
       "            [-0.4715,  0.3697,  0.3006,  ...,  0.1191,  0.6457, -0.0155],\n",
       "            [ 0.3588, -0.0058,  0.4279,  ..., -0.2493,  0.2222, -0.0707],\n",
       "            ...,\n",
       "            [ 0.2825,  0.6074, -0.1492,  ...,  0.4758,  0.5859,  0.0366],\n",
       "            [ 0.2825,  0.6074, -0.1492,  ...,  0.4758,  0.5859,  0.0366],\n",
       "            [ 0.2825,  0.6074, -0.1492,  ...,  0.4758,  0.5859,  0.0366]],\n",
       "   \n",
       "           [[-0.3008,  0.8294, -0.0140,  ..., -0.0705,  0.5633,  0.0672],\n",
       "            [-0.1443,  0.3646,  0.2134,  ...,  0.0711,  0.0951,  0.0343],\n",
       "            [ 0.5674, -0.2985,  0.3050,  ..., -0.1291,  0.4962, -0.0059],\n",
       "            ...,\n",
       "            [ 0.0948,  0.5097, -0.2106,  ...,  0.3707,  0.5528,  0.1362],\n",
       "            [ 0.0948,  0.5097, -0.2106,  ...,  0.3707,  0.5528,  0.1362],\n",
       "            [ 0.0948,  0.5097, -0.2106,  ...,  0.3707,  0.5528,  0.1362]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.3462e-03,  1.4808e-01,  6.5597e-01,  ..., -3.1014e-01,\n",
       "             1.1390e+00,  3.0541e-01],\n",
       "           [-2.7376e-01, -6.8703e-01,  8.1554e-01,  ..., -5.8430e-01,\n",
       "             1.0058e+00,  7.6240e-01],\n",
       "           [ 3.1590e-02, -8.0715e-01,  1.2411e-01,  ..., -4.9276e-01,\n",
       "             1.2298e+00, -3.3664e-01],\n",
       "           ...,\n",
       "           [-7.1188e-01,  6.5960e-01,  1.7532e+00,  ..., -3.8854e+00,\n",
       "            -3.0427e-01, -1.9926e-01],\n",
       "           [-7.1188e-01,  6.5960e-01,  1.7532e+00,  ..., -3.8854e+00,\n",
       "            -3.0427e-01, -1.9926e-01],\n",
       "           [-7.1188e-01,  6.5960e-01,  1.7532e+00,  ..., -3.8854e+00,\n",
       "            -3.0427e-01, -1.9926e-01]],\n",
       "  \n",
       "          [[-6.7806e-01,  3.8228e-01,  4.6175e-01,  ..., -2.8201e+00,\n",
       "            -8.1822e-01,  3.4074e-01],\n",
       "           [ 5.5077e-02, -2.8869e-01, -7.6322e-01,  ..., -2.2103e+00,\n",
       "            -1.3609e+00, -1.2179e+00],\n",
       "           [-2.6601e-01, -1.8329e+00, -1.6429e+00,  ..., -2.4125e+00,\n",
       "            -4.3190e-01, -6.8268e-01],\n",
       "           ...,\n",
       "           [-9.3322e-01,  1.4794e+00,  9.0867e-01,  ..., -3.7691e+00,\n",
       "            -8.7957e-02, -2.7410e-02],\n",
       "           [-9.3322e-01,  1.4794e+00,  9.0867e-01,  ..., -3.7691e+00,\n",
       "            -8.7957e-02, -2.7410e-02],\n",
       "           [-9.3322e-01,  1.4794e+00,  9.0867e-01,  ..., -3.7691e+00,\n",
       "            -8.7957e-02, -2.7410e-02]]], grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.7.self_attn.k_proj': {'input': (tensor([[[-0.5306, -0.0143,  0.1568,  ...,  0.2376,  1.0380,  0.0391],\n",
       "            [-0.4715,  0.3697,  0.3006,  ...,  0.1191,  0.6457, -0.0155],\n",
       "            [ 0.3588, -0.0058,  0.4279,  ..., -0.2493,  0.2222, -0.0707],\n",
       "            ...,\n",
       "            [ 0.2825,  0.6074, -0.1492,  ...,  0.4758,  0.5859,  0.0366],\n",
       "            [ 0.2825,  0.6074, -0.1492,  ...,  0.4758,  0.5859,  0.0366],\n",
       "            [ 0.2825,  0.6074, -0.1492,  ...,  0.4758,  0.5859,  0.0366]],\n",
       "   \n",
       "           [[-0.3008,  0.8294, -0.0140,  ..., -0.0705,  0.5633,  0.0672],\n",
       "            [-0.1443,  0.3646,  0.2134,  ...,  0.0711,  0.0951,  0.0343],\n",
       "            [ 0.5674, -0.2985,  0.3050,  ..., -0.1291,  0.4962, -0.0059],\n",
       "            ...,\n",
       "            [ 0.0948,  0.5097, -0.2106,  ...,  0.3707,  0.5528,  0.1362],\n",
       "            [ 0.0948,  0.5097, -0.2106,  ...,  0.3707,  0.5528,  0.1362],\n",
       "            [ 0.0948,  0.5097, -0.2106,  ...,  0.3707,  0.5528,  0.1362]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-1.2168, -1.3425,  1.8424,  ..., -0.9559,  0.4596, -0.4940],\n",
       "           [-1.6575, -0.0665,  2.1047,  ..., -0.5434,  0.9645, -0.1768],\n",
       "           [-1.2994,  0.4487, -0.2001,  ...,  0.1855,  0.6775, -0.4243],\n",
       "           ...,\n",
       "           [-1.0871, -0.3843,  2.7580,  ..., -0.5388,  1.0459,  0.7010],\n",
       "           [-1.0871, -0.3843,  2.7580,  ..., -0.5388,  1.0459,  0.7010],\n",
       "           [-1.0871, -0.3843,  2.7580,  ..., -0.5388,  1.0459,  0.7010]],\n",
       "  \n",
       "          [[-0.9321, -1.4491,  2.0035,  ..., -2.8060, -1.3958, -0.4748],\n",
       "           [-1.4400, -0.3844,  1.6283,  ..., -1.9709, -1.2328, -0.9474],\n",
       "           [-0.9838,  1.1226,  1.5892,  ..., -0.8201, -0.0794, -1.4064],\n",
       "           ...,\n",
       "           [-1.0214, -0.5212,  2.7829,  ..., -0.6644,  0.9801,  0.9349],\n",
       "           [-1.0214, -0.5212,  2.7829,  ..., -0.6644,  0.9801,  0.9349],\n",
       "           [-1.0214, -0.5212,  2.7829,  ..., -0.6644,  0.9801,  0.9349]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.7.self_attn.v_proj': {'input': (tensor([[[-0.5306, -0.0143,  0.1568,  ...,  0.2376,  1.0380,  0.0391],\n",
       "            [-0.4715,  0.3697,  0.3006,  ...,  0.1191,  0.6457, -0.0155],\n",
       "            [ 0.3588, -0.0058,  0.4279,  ..., -0.2493,  0.2222, -0.0707],\n",
       "            ...,\n",
       "            [ 0.2825,  0.6074, -0.1492,  ...,  0.4758,  0.5859,  0.0366],\n",
       "            [ 0.2825,  0.6074, -0.1492,  ...,  0.4758,  0.5859,  0.0366],\n",
       "            [ 0.2825,  0.6074, -0.1492,  ...,  0.4758,  0.5859,  0.0366]],\n",
       "   \n",
       "           [[-0.3008,  0.8294, -0.0140,  ..., -0.0705,  0.5633,  0.0672],\n",
       "            [-0.1443,  0.3646,  0.2134,  ...,  0.0711,  0.0951,  0.0343],\n",
       "            [ 0.5674, -0.2985,  0.3050,  ..., -0.1291,  0.4962, -0.0059],\n",
       "            ...,\n",
       "            [ 0.0948,  0.5097, -0.2106,  ...,  0.3707,  0.5528,  0.1362],\n",
       "            [ 0.0948,  0.5097, -0.2106,  ...,  0.3707,  0.5528,  0.1362],\n",
       "            [ 0.0948,  0.5097, -0.2106,  ...,  0.3707,  0.5528,  0.1362]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.0617,  0.3335, -0.0599,  ..., -0.1031, -0.5967,  0.3157],\n",
       "           [ 1.4520,  0.6562, -0.2343,  ..., -0.5153,  0.2091,  0.0751],\n",
       "           [ 1.2553,  0.2968, -0.1813,  ...,  0.2041,  0.1276, -0.7013],\n",
       "           ...,\n",
       "           [ 1.8146,  1.0291, -0.0595,  ...,  0.2455,  1.4282,  0.1050],\n",
       "           [ 1.8146,  1.0291, -0.0595,  ...,  0.2455,  1.4282,  0.1050],\n",
       "           [ 1.8146,  1.0291, -0.0595,  ...,  0.2455,  1.4282,  0.1050]],\n",
       "  \n",
       "          [[ 0.6642,  0.2394,  0.6498,  ..., -0.4992,  0.6528,  0.6939],\n",
       "           [-0.6733, -0.4949,  0.2641,  ...,  0.0171, -0.1061, -0.3160],\n",
       "           [-0.4208, -0.2073,  0.0430,  ..., -0.1595, -0.2575,  0.0361],\n",
       "           ...,\n",
       "           [ 1.6053,  0.5163,  0.1927,  ...,  0.5701,  1.4105,  0.2997],\n",
       "           [ 1.6053,  0.5163,  0.1927,  ...,  0.5701,  1.4105,  0.2997],\n",
       "           [ 1.6053,  0.5163,  0.1927,  ...,  0.5701,  1.4105,  0.2997]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.7.self_attn.out_proj': {'input': (tensor([[[ 8.7351e-01,  1.9496e-02,  4.0575e-02,  ..., -1.4881e-01,\n",
       "             -4.4673e-01,  4.1173e-01],\n",
       "            [ 8.6592e-01,  1.2978e-01,  3.5351e-02,  ..., -1.9727e-01,\n",
       "             -2.7004e-01,  3.8147e-01],\n",
       "            [ 3.9984e-01,  1.5405e-01,  1.7830e-02,  ..., -9.3013e-02,\n",
       "              7.1696e-03, -4.3277e-02],\n",
       "            ...,\n",
       "            [ 8.1317e-01,  1.6221e-01,  1.9564e-02,  ..., -5.4184e-01,\n",
       "              1.2379e-01,  4.0017e-01],\n",
       "            [ 8.1317e-01,  1.6221e-01,  1.9564e-02,  ..., -5.4184e-01,\n",
       "              1.2379e-01,  4.0017e-01],\n",
       "            [ 8.1317e-01,  1.6221e-01,  1.9564e-02,  ..., -5.4184e-01,\n",
       "              1.2379e-01,  4.0017e-01]],\n",
       "   \n",
       "           [[ 5.2266e-01,  2.8510e-01,  6.3791e-02,  ..., -2.9110e-01,\n",
       "              1.0638e-01,  4.2231e-01],\n",
       "            [ 5.9967e-01, -1.1059e-01, -1.1906e-01,  ...,  8.4897e-02,\n",
       "              7.9550e-02,  1.1764e-01],\n",
       "            [ 2.7845e-01,  4.0103e-04, -2.8281e-01,  ...,  1.7337e-01,\n",
       "              1.4507e-01,  1.6649e-01],\n",
       "            ...,\n",
       "            [ 7.6362e-01,  8.8644e-02,  1.4844e-02,  ..., -6.5106e-01,\n",
       "              1.9151e-01,  7.0659e-01],\n",
       "            [ 7.6362e-01,  8.8644e-02,  1.4844e-02,  ..., -6.5106e-01,\n",
       "              1.9151e-01,  7.0659e-01],\n",
       "            [ 7.6362e-01,  8.8644e-02,  1.4844e-02,  ..., -6.5106e-01,\n",
       "              1.9151e-01,  7.0659e-01]]], grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.3049,  0.4097, -0.3674,  ..., -0.2044, -0.1577, -0.3392],\n",
       "           [-0.4241,  0.3319, -0.4555,  ..., -0.3677, -0.1678, -0.3609],\n",
       "           [-0.1946,  0.1325, -0.4300,  ...,  0.0967, -0.1482, -0.0931],\n",
       "           ...,\n",
       "           [-0.1476,  0.0908, -0.4561,  ..., -0.2822, -0.2246, -0.2266],\n",
       "           [-0.1476,  0.0908, -0.4561,  ..., -0.2822, -0.2246, -0.2266],\n",
       "           [-0.1476,  0.0908, -0.4561,  ..., -0.2822, -0.2246, -0.2266]],\n",
       "  \n",
       "          [[-0.5768,  0.0747,  0.2314,  ..., -0.1878, -0.4349,  0.1914],\n",
       "           [-0.3887,  0.2593,  0.0515,  ..., -0.2644, -0.2322,  0.0015],\n",
       "           [-0.2597,  0.2167, -0.0814,  ..., -0.2884, -0.4322, -0.0543],\n",
       "           ...,\n",
       "           [-0.5247,  0.2169, -0.0665,  ..., -0.4520, -0.4596, -0.1706],\n",
       "           [-0.5247,  0.2169, -0.0665,  ..., -0.4520, -0.4596, -0.1706],\n",
       "           [-0.5247,  0.2169, -0.0665,  ..., -0.4520, -0.4596, -0.1706]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.7.mlp.fc1': {'input': (tensor([[[-0.6304,  0.3560, -0.1002,  ...,  0.2381,  1.4272,  0.0362],\n",
       "            [-0.6426,  0.9406, -0.0138,  ..., -0.0481,  0.7555, -0.0979],\n",
       "            [ 0.4332,  0.1736,  0.1749,  ..., -0.2460,  0.0933,  0.0176],\n",
       "            ...,\n",
       "            [ 0.3932,  1.0707, -0.4579,  ...,  0.5316,  0.6439,  0.1563],\n",
       "            [ 0.3932,  1.0707, -0.4579,  ...,  0.5316,  0.6439,  0.1563],\n",
       "            [ 0.3932,  1.0707, -0.4579,  ...,  0.5316,  0.6439,  0.1563]],\n",
       "   \n",
       "           [[-0.5037,  1.4463,  0.1075,  ..., -0.1743,  0.4520,  0.4628],\n",
       "            [-0.2383,  0.8466,  0.2655,  ..., -0.0408, -0.1663,  0.2715],\n",
       "            [ 0.6166, -0.2196,  0.2789,  ..., -0.3288,  0.3148,  0.1601],\n",
       "            ...,\n",
       "            [ 0.0208,  1.0037, -0.3376,  ...,  0.3101,  0.4719,  0.3467],\n",
       "            [ 0.0208,  1.0037, -0.3376,  ...,  0.3101,  0.4719,  0.3467],\n",
       "            [ 0.0208,  1.0037, -0.3376,  ...,  0.3101,  0.4719,  0.3467]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-2.3146, -2.0258, -2.2834,  ..., -2.3557,  0.3448, -1.9585],\n",
       "           [-2.1125, -1.4773, -1.3626,  ..., -2.2209,  0.9489, -1.3242],\n",
       "           [-1.5519, -2.7415, -3.0415,  ..., -2.2151, -0.7182, -1.5930],\n",
       "           ...,\n",
       "           [-3.0150, -2.1759, -2.3026,  ..., -4.2393, -1.8312,  0.5661],\n",
       "           [-3.0150, -2.1759, -2.3026,  ..., -4.2393, -1.8312,  0.5661],\n",
       "           [-3.0150, -2.1759, -2.3026,  ..., -4.2393, -1.8312,  0.5661]],\n",
       "  \n",
       "          [[-2.4985, -2.3717, -1.5739,  ..., -3.6693, -0.3052, -0.8524],\n",
       "           [-1.3900, -3.1455, -0.7042,  ..., -2.4185,  0.0315, -0.3800],\n",
       "           [-1.4675, -2.9696, -0.4276,  ..., -2.1362,  0.3002,  0.3081],\n",
       "           ...,\n",
       "           [-2.6941, -2.4864, -2.4796,  ..., -4.2870, -1.3976,  0.9269],\n",
       "           [-2.6941, -2.4864, -2.4796,  ..., -4.2870, -1.3976,  0.9269],\n",
       "           [-2.6941, -2.4864, -2.4796,  ..., -4.2870, -1.3976,  0.9269]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.7.mlp.fc2': {'input': (tensor([[[-2.3535e-02, -4.3214e-02, -2.5256e-02,  ..., -2.1402e-02,\n",
       "              2.1893e-01, -4.9067e-02],\n",
       "            [-3.6398e-02, -1.0333e-01, -1.1811e-01,  ..., -2.8985e-02,\n",
       "              7.8622e-01, -1.2301e-01],\n",
       "            [-9.3850e-02, -7.9115e-03, -3.1832e-03,  ..., -2.9351e-02,\n",
       "             -1.6978e-01, -8.8728e-02],\n",
       "            ...,\n",
       "            [-3.4674e-03, -3.1913e-02, -2.4185e-02,  ..., -2.1352e-05,\n",
       "             -6.1454e-02,  4.0439e-01],\n",
       "            [-3.4674e-03, -3.1913e-02, -2.4185e-02,  ..., -2.1352e-05,\n",
       "             -6.1454e-02,  4.0439e-01],\n",
       "            [-3.4674e-03, -3.1913e-02, -2.4185e-02,  ..., -2.1352e-05,\n",
       "             -6.1454e-02,  4.0439e-01]],\n",
       "   \n",
       "           [[-1.5143e-02, -2.0613e-02, -9.1095e-02,  ..., -3.0958e-04,\n",
       "             -1.1602e-01, -1.6802e-01],\n",
       "            [-1.1458e-01, -2.2539e-03, -1.6952e-01,  ..., -1.8441e-02,\n",
       "              1.6164e-02, -1.3375e-01],\n",
       "            [-1.0460e-01, -4.0047e-03, -1.4303e-01,  ..., -3.4671e-02,\n",
       "              1.8555e-01,  1.9129e-01],\n",
       "            ...,\n",
       "            [-9.0350e-03, -1.5608e-02, -1.5876e-02,  ..., -1.6609e-05,\n",
       "             -1.1361e-01,  7.6272e-01],\n",
       "            [-9.0350e-03, -1.5608e-02, -1.5876e-02,  ..., -1.6609e-05,\n",
       "             -1.1361e-01,  7.6272e-01],\n",
       "            [-9.0350e-03, -1.5608e-02, -1.5876e-02,  ..., -1.6609e-05,\n",
       "             -1.1361e-01,  7.6272e-01]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[ 0.1005, -0.2832,  0.2313,  ...,  0.1344, -0.3850,  0.1273],\n",
       "           [-0.3352, -0.1832,  0.1379,  ...,  0.4855, -0.1391, -0.1245],\n",
       "           [-0.0459, -0.2293, -0.3140,  ...,  0.0033, -0.3681,  0.1272],\n",
       "           ...,\n",
       "           [ 0.1252,  0.2071,  0.8257,  ..., -0.0054,  0.4287, -0.1145],\n",
       "           [ 0.1252,  0.2071,  0.8257,  ..., -0.0054,  0.4287, -0.1145],\n",
       "           [ 0.1252,  0.2071,  0.8257,  ..., -0.0054,  0.4287, -0.1145]],\n",
       "  \n",
       "          [[-0.1415, -0.6296, -0.2294,  ...,  0.1639, -0.1429,  0.3795],\n",
       "           [-0.3074,  0.1875, -0.1565,  ..., -0.0872,  0.5873,  0.3738],\n",
       "           [-0.4233, -0.8243,  0.0678,  ...,  0.1943,  0.0689, -0.2982],\n",
       "           ...,\n",
       "           [ 0.3594,  0.1710,  0.7486,  ..., -0.0614,  0.4603, -0.2872],\n",
       "           [ 0.3594,  0.1710,  0.7486,  ..., -0.0614,  0.4603, -0.2872],\n",
       "           [ 0.3594,  0.1710,  0.7486,  ..., -0.0614,  0.4603, -0.2872]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.8.self_attn.q_proj': {'input': (tensor([[[-7.1373e-01,  1.0867e-01,  9.9640e-02,  ...,  2.5961e-01,\n",
       "              1.1202e+00, -4.9499e-02],\n",
       "            [-1.0027e+00,  6.7133e-01,  1.2633e-01,  ...,  2.1643e-01,\n",
       "              6.8043e-01, -3.0562e-01],\n",
       "            [ 3.1815e-01, -3.0233e-02, -3.7133e-02,  ..., -2.5675e-01,\n",
       "             -1.4974e-02, -6.6886e-02],\n",
       "            ...,\n",
       "            [ 3.1451e-01,  9.3486e-01,  1.1317e-02,  ...,  4.0942e-01,\n",
       "              7.8166e-01, -6.3954e-02],\n",
       "            [ 3.1451e-01,  9.3486e-01,  1.1317e-02,  ...,  4.0942e-01,\n",
       "              7.8166e-01, -6.3954e-02],\n",
       "            [ 3.1451e-01,  9.3487e-01,  1.1317e-02,  ...,  4.0942e-01,\n",
       "              7.8166e-01, -6.3954e-02]],\n",
       "   \n",
       "           [[-7.2230e-01,  8.5282e-01, -1.9091e-02,  ..., -9.6538e-02,\n",
       "              4.3290e-01,  4.2199e-01],\n",
       "            [-5.3480e-01,  8.2224e-01,  1.8295e-01,  ..., -1.1094e-01,\n",
       "              3.1845e-01,  2.7443e-01],\n",
       "            [ 2.6077e-01, -7.8678e-01,  3.6883e-01,  ..., -2.1179e-01,\n",
       "              4.3759e-01, -2.0592e-01],\n",
       "            ...,\n",
       "            [ 5.9249e-02,  8.6949e-01,  1.1374e-01,  ...,  2.0287e-01,\n",
       "              6.7119e-01,  3.2257e-04],\n",
       "            [ 5.9249e-02,  8.6949e-01,  1.1374e-01,  ...,  2.0287e-01,\n",
       "              6.7119e-01,  3.2257e-04],\n",
       "            [ 5.9249e-02,  8.6949e-01,  1.1374e-01,  ...,  2.0287e-01,\n",
       "              6.7119e-01,  3.2265e-04]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.8019, -0.5615,  1.2461,  ..., -0.5659,  0.7897, -0.2388],\n",
       "           [-0.3598,  0.2151,  0.3452,  ..., -0.2341,  0.1651,  0.1718],\n",
       "           [ 0.5472,  0.2347,  0.0327,  ...,  3.0506, -0.6572, -0.3288],\n",
       "           ...,\n",
       "           [-0.5870, -1.4339,  0.3754,  ...,  0.5784, -0.7986, -1.5401],\n",
       "           [-0.5870, -1.4339,  0.3754,  ...,  0.5784, -0.7986, -1.5401],\n",
       "           [-0.5870, -1.4339,  0.3754,  ...,  0.5784, -0.7986, -1.5401]],\n",
       "  \n",
       "          [[-0.7481, -0.9766,  0.4402,  ...,  0.2014,  0.9140, -0.0798],\n",
       "           [-0.9411,  0.5731,  1.2539,  ..., -1.1622,  0.6345, -0.5075],\n",
       "           [ 0.1835,  0.5174,  1.0203,  ...,  0.8035, -0.5828, -0.9901],\n",
       "           ...,\n",
       "           [-0.3223, -1.7316,  0.7146,  ...,  0.3924, -0.9429, -1.4751],\n",
       "           [-0.3223, -1.7316,  0.7146,  ...,  0.3924, -0.9429, -1.4751],\n",
       "           [-0.3223, -1.7316,  0.7146,  ...,  0.3924, -0.9429, -1.4751]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.8.self_attn.k_proj': {'input': (tensor([[[-7.1373e-01,  1.0867e-01,  9.9640e-02,  ...,  2.5961e-01,\n",
       "              1.1202e+00, -4.9499e-02],\n",
       "            [-1.0027e+00,  6.7133e-01,  1.2633e-01,  ...,  2.1643e-01,\n",
       "              6.8043e-01, -3.0562e-01],\n",
       "            [ 3.1815e-01, -3.0233e-02, -3.7133e-02,  ..., -2.5675e-01,\n",
       "             -1.4974e-02, -6.6886e-02],\n",
       "            ...,\n",
       "            [ 3.1451e-01,  9.3486e-01,  1.1317e-02,  ...,  4.0942e-01,\n",
       "              7.8166e-01, -6.3954e-02],\n",
       "            [ 3.1451e-01,  9.3486e-01,  1.1317e-02,  ...,  4.0942e-01,\n",
       "              7.8166e-01, -6.3954e-02],\n",
       "            [ 3.1451e-01,  9.3487e-01,  1.1317e-02,  ...,  4.0942e-01,\n",
       "              7.8166e-01, -6.3954e-02]],\n",
       "   \n",
       "           [[-7.2230e-01,  8.5282e-01, -1.9091e-02,  ..., -9.6538e-02,\n",
       "              4.3290e-01,  4.2199e-01],\n",
       "            [-5.3480e-01,  8.2224e-01,  1.8295e-01,  ..., -1.1094e-01,\n",
       "              3.1845e-01,  2.7443e-01],\n",
       "            [ 2.6077e-01, -7.8678e-01,  3.6883e-01,  ..., -2.1179e-01,\n",
       "              4.3759e-01, -2.0592e-01],\n",
       "            ...,\n",
       "            [ 5.9249e-02,  8.6949e-01,  1.1374e-01,  ...,  2.0287e-01,\n",
       "              6.7119e-01,  3.2257e-04],\n",
       "            [ 5.9249e-02,  8.6949e-01,  1.1374e-01,  ...,  2.0287e-01,\n",
       "              6.7119e-01,  3.2257e-04],\n",
       "            [ 5.9249e-02,  8.6949e-01,  1.1374e-01,  ...,  2.0287e-01,\n",
       "              6.7119e-01,  3.2265e-04]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.3240, -0.0918, -0.1928,  ...,  1.3840, -0.6754,  1.3073],\n",
       "           [-0.0738,  0.5088, -0.0777,  ...,  0.3728, -0.2343,  0.9657],\n",
       "           [ 0.7725,  1.1413, -1.0573,  ...,  0.1667,  0.1039,  0.2722],\n",
       "           ...,\n",
       "           [ 0.6346, -0.7573, -0.9097,  ...,  2.3121,  0.0552,  0.8395],\n",
       "           [ 0.6346, -0.7573, -0.9097,  ...,  2.3121,  0.0552,  0.8395],\n",
       "           [ 0.6346, -0.7573, -0.9097,  ...,  2.3121,  0.0552,  0.8395]],\n",
       "  \n",
       "          [[ 0.5858,  0.8462, -0.6137,  ...,  2.1104, -0.4337,  0.9605],\n",
       "           [ 0.3265,  1.0447,  0.4534,  ...,  1.8008,  0.1164,  0.3213],\n",
       "           [-0.5818,  0.5730, -0.4300,  ...,  0.8122,  0.5740,  1.2300],\n",
       "           ...,\n",
       "           [ 0.4954, -0.8013, -1.0391,  ...,  2.4231, -0.0940,  0.6451],\n",
       "           [ 0.4954, -0.8013, -1.0391,  ...,  2.4231, -0.0940,  0.6451],\n",
       "           [ 0.4954, -0.8013, -1.0391,  ...,  2.4231, -0.0940,  0.6451]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.8.self_attn.v_proj': {'input': (tensor([[[-7.1373e-01,  1.0867e-01,  9.9640e-02,  ...,  2.5961e-01,\n",
       "              1.1202e+00, -4.9499e-02],\n",
       "            [-1.0027e+00,  6.7133e-01,  1.2633e-01,  ...,  2.1643e-01,\n",
       "              6.8043e-01, -3.0562e-01],\n",
       "            [ 3.1815e-01, -3.0233e-02, -3.7133e-02,  ..., -2.5675e-01,\n",
       "             -1.4974e-02, -6.6886e-02],\n",
       "            ...,\n",
       "            [ 3.1451e-01,  9.3486e-01,  1.1317e-02,  ...,  4.0942e-01,\n",
       "              7.8166e-01, -6.3954e-02],\n",
       "            [ 3.1451e-01,  9.3486e-01,  1.1317e-02,  ...,  4.0942e-01,\n",
       "              7.8166e-01, -6.3954e-02],\n",
       "            [ 3.1451e-01,  9.3487e-01,  1.1317e-02,  ...,  4.0942e-01,\n",
       "              7.8166e-01, -6.3954e-02]],\n",
       "   \n",
       "           [[-7.2230e-01,  8.5282e-01, -1.9091e-02,  ..., -9.6538e-02,\n",
       "              4.3290e-01,  4.2199e-01],\n",
       "            [-5.3480e-01,  8.2224e-01,  1.8295e-01,  ..., -1.1094e-01,\n",
       "              3.1845e-01,  2.7443e-01],\n",
       "            [ 2.6077e-01, -7.8678e-01,  3.6883e-01,  ..., -2.1179e-01,\n",
       "              4.3759e-01, -2.0592e-01],\n",
       "            ...,\n",
       "            [ 5.9249e-02,  8.6949e-01,  1.1374e-01,  ...,  2.0287e-01,\n",
       "              6.7119e-01,  3.2257e-04],\n",
       "            [ 5.9249e-02,  8.6949e-01,  1.1374e-01,  ...,  2.0287e-01,\n",
       "              6.7119e-01,  3.2257e-04],\n",
       "            [ 5.9249e-02,  8.6949e-01,  1.1374e-01,  ...,  2.0287e-01,\n",
       "              6.7119e-01,  3.2265e-04]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.7709,  0.8868, -0.7434,  ...,  0.1553, -0.5983,  0.1520],\n",
       "           [-0.0908,  1.2721,  0.1185,  ...,  0.4627,  0.3904, -0.1363],\n",
       "           [-0.3165, -0.0366, -0.2145,  ...,  0.8003,  0.6680,  0.1884],\n",
       "           ...,\n",
       "           [ 0.4288,  0.2461, -1.0453,  ...,  2.7439, -0.5468, -0.1095],\n",
       "           [ 0.4288,  0.2461, -1.0453,  ...,  2.7439, -0.5468, -0.1095],\n",
       "           [ 0.4288,  0.2461, -1.0453,  ...,  2.7439, -0.5468, -0.1095]],\n",
       "  \n",
       "          [[-0.0703,  0.0666, -1.1773,  ...,  0.9319,  0.3949, -1.0358],\n",
       "           [-0.2264,  0.4657, -0.6487,  ...,  1.7537,  0.9155, -0.4542],\n",
       "           [-0.0244,  0.2974, -0.6602,  ...,  0.3889,  0.3250, -1.3550],\n",
       "           ...,\n",
       "           [-0.1016,  0.0114, -1.2133,  ...,  2.7736, -0.6330, -0.4331],\n",
       "           [-0.1016,  0.0114, -1.2133,  ...,  2.7736, -0.6330, -0.4331],\n",
       "           [-0.1016,  0.0114, -1.2133,  ...,  2.7736, -0.6330, -0.4331]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.8.self_attn.out_proj': {'input': (tensor([[[-0.4008, -1.0166,  0.0155,  ...,  0.0369, -0.2001,  0.0180],\n",
       "            [-0.1600, -0.1025,  0.1717,  ...,  0.0997, -0.1692,  0.0460],\n",
       "            [ 0.1310, -0.2095, -0.0849,  ...,  0.8085,  1.4443, -0.7086],\n",
       "            ...,\n",
       "            [ 0.2050, -0.3450, -0.1230,  ..., -0.0260, -0.1863, -0.0115],\n",
       "            [ 0.2050, -0.3450, -0.1230,  ..., -0.0260, -0.1863, -0.0115],\n",
       "            [ 0.2050, -0.3450, -0.1230,  ..., -0.0260, -0.1863, -0.0115]],\n",
       "   \n",
       "           [[-0.1838, -0.4338, -0.1178,  ...,  0.0781, -0.1835,  0.0314],\n",
       "            [-0.0604, -0.3134, -0.1215,  ...,  0.1173, -0.2356, -0.0044],\n",
       "            [ 0.0299, -0.1221, -0.0198,  ...,  0.3111,  0.1730, -0.1434],\n",
       "            ...,\n",
       "            [ 0.1727, -0.2806, -0.1801,  ...,  0.0155, -0.1638,  0.0143],\n",
       "            [ 0.1727, -0.2806, -0.1801,  ...,  0.0155, -0.1638,  0.0143],\n",
       "            [ 0.1727, -0.2806, -0.1801,  ...,  0.0155, -0.1638,  0.0143]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.4034,  0.0077,  0.4646,  ..., -0.3129, -0.0202,  0.1624],\n",
       "           [-0.1655, -0.1256,  0.6427,  ..., -0.3475,  0.0321,  0.1293],\n",
       "           [-0.2392, -0.2589, -0.1319,  ..., -0.5067,  0.1142,  0.3023],\n",
       "           ...,\n",
       "           [-0.1913, -0.2466,  0.1493,  ..., -0.5123, -0.2647,  0.6210],\n",
       "           [-0.1913, -0.2466,  0.1493,  ..., -0.5123, -0.2647,  0.6210],\n",
       "           [-0.1913, -0.2466,  0.1493,  ..., -0.5123, -0.2647,  0.6210]],\n",
       "  \n",
       "          [[-0.3821, -0.6235, -0.5090,  ..., -0.2300, -0.6390,  0.7626],\n",
       "           [-0.0790, -0.4704, -0.9308,  ..., -0.2982, -0.5140,  0.8147],\n",
       "           [-0.1995, -0.3000, -0.5681,  ..., -0.5207, -0.4340,  0.4301],\n",
       "           ...,\n",
       "           [-0.1771, -0.4293, -0.3435,  ..., -0.4418, -0.5622,  0.7773],\n",
       "           [-0.1771, -0.4293, -0.3435,  ..., -0.4418, -0.5622,  0.7773],\n",
       "           [-0.1771, -0.4293, -0.3435,  ..., -0.4418, -0.5622,  0.7773]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.8.mlp.fc1': {'input': (tensor([[[-0.7687, -0.0067,  0.3843,  ...,  0.2037,  1.3587, -0.3874],\n",
       "            [-0.9441,  0.5503,  0.5976,  ...,  0.1166,  0.8917, -0.7320],\n",
       "            [ 0.3240, -0.3719, -0.1771,  ..., -0.4797,  0.1175, -0.2879],\n",
       "            ...,\n",
       "            [ 0.4101,  0.8227,  0.0340,  ...,  0.3469,  0.8547, -0.2239],\n",
       "            [ 0.4101,  0.8227,  0.0340,  ...,  0.3469,  0.8547, -0.2239],\n",
       "            [ 0.4101,  0.8227,  0.0340,  ...,  0.3469,  0.8547, -0.2239]],\n",
       "   \n",
       "           [[-0.7468,  0.3863, -0.4210,  ..., -0.1349,  0.1471,  0.6180],\n",
       "            [-0.4042,  0.4624, -0.5396,  ..., -0.1963,  0.0879,  0.4961],\n",
       "            [ 0.3075, -1.2816, -0.1187,  ..., -0.4569,  0.2600, -0.3689],\n",
       "            ...,\n",
       "            [ 0.1567,  0.6368, -0.1138,  ...,  0.1350,  0.5730, -0.0402],\n",
       "            [ 0.1567,  0.6368, -0.1138,  ...,  0.1350,  0.5730, -0.0402],\n",
       "            [ 0.1567,  0.6368, -0.1138,  ...,  0.1350,  0.5730, -0.0402]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-2.6975, -2.4785, -2.1227,  ..., -1.8343, -3.2953, -1.1205],\n",
       "           [-2.5755, -2.3698, -2.2054,  ..., -2.0032, -3.9261, -0.5542],\n",
       "           [-2.1496, -1.4039, -1.9370,  ..., -2.1758, -3.0806, -2.2653],\n",
       "           ...,\n",
       "           [-2.8592, -2.4655, -0.5663,  ..., -1.6323, -2.1987, -1.2672],\n",
       "           [-2.8592, -2.4655, -0.5663,  ..., -1.6323, -2.1987, -1.2672],\n",
       "           [-2.8592, -2.4655, -0.5663,  ..., -1.6323, -2.1987, -1.2672]],\n",
       "  \n",
       "          [[-1.6226, -0.8790, -0.3486,  ..., -1.1518, -1.8469, -0.6971],\n",
       "           [-2.4199, -1.9831, -0.3626,  ..., -1.2044, -1.6192, -0.5576],\n",
       "           [-2.6736, -1.9279, -0.6521,  ..., -1.9268, -1.8593, -0.9261],\n",
       "           ...,\n",
       "           [-2.0886, -1.7706,  0.0167,  ..., -1.6109, -2.3003, -0.9559],\n",
       "           [-2.0886, -1.7706,  0.0167,  ..., -1.6109, -2.3003, -0.9559],\n",
       "           [-2.0886, -1.7706,  0.0167,  ..., -1.6109, -2.3003, -0.9559]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.8.mlp.fc2': {'input': (tensor([[[-8.9501e-03, -1.5919e-02, -3.5649e-02,  ..., -6.1130e-02,\n",
       "             -1.3335e-03, -1.4726e-01],\n",
       "            [-1.2430e-02, -2.0706e-02, -2.9974e-02,  ..., -4.5127e-02,\n",
       "             -9.9455e-05, -1.6059e-01],\n",
       "            [-3.3726e-02, -1.1279e-01, -5.1042e-02,  ..., -3.1923e-02,\n",
       "             -2.8008e-03, -2.6296e-02],\n",
       "            ...,\n",
       "            [-5.6170e-03, -1.6441e-02, -1.6176e-01,  ..., -8.3915e-02,\n",
       "             -3.0405e-02, -1.3017e-01],\n",
       "            [-5.6170e-03, -1.6441e-02, -1.6176e-01,  ..., -8.3915e-02,\n",
       "             -3.0405e-02, -1.3017e-01],\n",
       "            [-5.6170e-03, -1.6441e-02, -1.6176e-01,  ..., -8.3915e-02,\n",
       "             -3.0405e-02, -1.3017e-01]],\n",
       "   \n",
       "           [[-8.5096e-02, -1.6686e-01, -1.2679e-01,  ..., -1.4384e-01,\n",
       "             -5.9838e-02, -1.6936e-01],\n",
       "            [-1.8376e-02, -4.6871e-02, -1.2999e-01,  ..., -1.3778e-01,\n",
       "             -8.5515e-02, -1.6093e-01],\n",
       "            [-9.5601e-03, -5.1888e-02, -1.6774e-01,  ..., -5.1993e-02,\n",
       "             -5.8577e-02, -1.6423e-01],\n",
       "            ...,\n",
       "            [-3.8198e-02, -6.7922e-02,  8.4566e-03,  ..., -8.6524e-02,\n",
       "             -2.4310e-02, -1.6222e-01],\n",
       "            [-3.8198e-02, -6.7922e-02,  8.4566e-03,  ..., -8.6524e-02,\n",
       "             -2.4310e-02, -1.6222e-01],\n",
       "            [-3.8198e-02, -6.7922e-02,  8.4568e-03,  ..., -8.6524e-02,\n",
       "             -2.4310e-02, -1.6222e-01]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[ 0.1474,  0.1673, -0.3602,  ..., -0.4067, -0.2145, -0.2202],\n",
       "           [ 0.3567, -0.0669, -0.0658,  ..., -0.3335, -0.1549, -0.3860],\n",
       "           [ 0.2804, -0.2203, -0.2283,  ..., -0.1540,  0.2409, -0.6524],\n",
       "           ...,\n",
       "           [ 0.4402,  0.0068, -0.1365,  ...,  0.6191, -0.2310, -0.7358],\n",
       "           [ 0.4402,  0.0068, -0.1365,  ...,  0.6191, -0.2310, -0.7358],\n",
       "           [ 0.4402,  0.0068, -0.1365,  ...,  0.6191, -0.2310, -0.7358]],\n",
       "  \n",
       "          [[-0.0150, -0.2191, -0.2338,  ...,  0.1529,  0.4665,  0.2110],\n",
       "           [-0.5329, -0.0227, -0.6717,  ...,  0.0140,  0.5447, -0.2331],\n",
       "           [ 0.0939, -0.1807, -0.1963,  ..., -0.1162, -0.1313, -0.2315],\n",
       "           ...,\n",
       "           [ 0.7378,  0.0203, -0.2513,  ...,  0.5770, -0.2542, -0.6687],\n",
       "           [ 0.7378,  0.0203, -0.2513,  ...,  0.5770, -0.2542, -0.6687],\n",
       "           [ 0.7378,  0.0203, -0.2513,  ...,  0.5770, -0.2542, -0.6687]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.9.self_attn.q_proj': {'input': (tensor([[[-0.8326,  0.1919,  0.1489,  ..., -0.0367,  0.9569, -0.1575],\n",
       "            [-0.8486,  0.5019,  0.5531,  ..., -0.0907,  0.5967, -0.5547],\n",
       "            [ 0.3301, -0.3504, -0.3040,  ..., -0.4521,  0.2025, -0.3322],\n",
       "            ...,\n",
       "            [ 0.4257,  0.7693, -0.0085,  ...,  0.4638,  0.5712, -0.1826],\n",
       "            [ 0.4257,  0.7693, -0.0085,  ...,  0.4638,  0.5712, -0.1826],\n",
       "            [ 0.4257,  0.7693, -0.0085,  ...,  0.4638,  0.5712, -0.1826]],\n",
       "   \n",
       "           [[-0.9012,  0.2844, -0.5519,  ..., -0.0462,  0.3400,  0.9091],\n",
       "            [-0.8766,  0.4825, -1.0096,  ..., -0.1623,  0.3544,  0.5712],\n",
       "            [ 0.2275, -1.1400, -0.2502,  ..., -0.4510,  0.1250, -0.1631],\n",
       "            ...,\n",
       "            [ 0.3099,  0.6233, -0.2128,  ...,  0.2954,  0.3399, -0.0281],\n",
       "            [ 0.3099,  0.6233, -0.2128,  ...,  0.2954,  0.3399, -0.0281],\n",
       "            [ 0.3099,  0.6233, -0.2128,  ...,  0.2954,  0.3399, -0.0281]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.9144, -0.6574,  0.7282,  ..., -1.0363,  0.0558,  0.1663],\n",
       "           [ 0.7802, -0.3845, -0.0367,  ...,  0.0925,  0.4716, -0.6725],\n",
       "           [-0.3934, -0.5930,  0.4687,  ..., -0.3018,  1.1603, -0.2088],\n",
       "           ...,\n",
       "           [-1.2878,  0.7465, -3.2297,  ...,  0.6692, -0.0749,  0.7107],\n",
       "           [-1.2878,  0.7465, -3.2297,  ...,  0.6692, -0.0749,  0.7107],\n",
       "           [-1.2878,  0.7465, -3.2297,  ...,  0.6692, -0.0749,  0.7107]],\n",
       "  \n",
       "          [[ 0.5003, -0.5948,  0.4612,  ..., -0.7777,  0.7672,  0.5862],\n",
       "           [-0.1998, -0.1730,  0.4467,  ..., -0.2803,  2.4123,  0.0782],\n",
       "           [-0.8960, -0.0698, -1.5146,  ...,  0.8229,  1.3005,  0.6900],\n",
       "           ...,\n",
       "           [-1.7415,  0.7405, -2.6469,  ...,  0.6225,  0.2320,  0.4970],\n",
       "           [-1.7415,  0.7405, -2.6469,  ...,  0.6225,  0.2320,  0.4970],\n",
       "           [-1.7415,  0.7405, -2.6469,  ...,  0.6225,  0.2320,  0.4970]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.9.self_attn.k_proj': {'input': (tensor([[[-0.8326,  0.1919,  0.1489,  ..., -0.0367,  0.9569, -0.1575],\n",
       "            [-0.8486,  0.5019,  0.5531,  ..., -0.0907,  0.5967, -0.5547],\n",
       "            [ 0.3301, -0.3504, -0.3040,  ..., -0.4521,  0.2025, -0.3322],\n",
       "            ...,\n",
       "            [ 0.4257,  0.7693, -0.0085,  ...,  0.4638,  0.5712, -0.1826],\n",
       "            [ 0.4257,  0.7693, -0.0085,  ...,  0.4638,  0.5712, -0.1826],\n",
       "            [ 0.4257,  0.7693, -0.0085,  ...,  0.4638,  0.5712, -0.1826]],\n",
       "   \n",
       "           [[-0.9012,  0.2844, -0.5519,  ..., -0.0462,  0.3400,  0.9091],\n",
       "            [-0.8766,  0.4825, -1.0096,  ..., -0.1623,  0.3544,  0.5712],\n",
       "            [ 0.2275, -1.1400, -0.2502,  ..., -0.4510,  0.1250, -0.1631],\n",
       "            ...,\n",
       "            [ 0.3099,  0.6233, -0.2128,  ...,  0.2954,  0.3399, -0.0281],\n",
       "            [ 0.3099,  0.6233, -0.2128,  ...,  0.2954,  0.3399, -0.0281],\n",
       "            [ 0.3099,  0.6233, -0.2128,  ...,  0.2954,  0.3399, -0.0281]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.0184, -0.9769,  1.1061,  ..., -2.8415,  0.5793,  0.6021],\n",
       "           [ 0.8673, -0.1690,  0.0494,  ..., -2.5717,  0.7150,  0.5654],\n",
       "           [-1.3084,  0.7105, -0.7818,  ..., -3.2112,  2.9871, -2.3810],\n",
       "           ...,\n",
       "           [-0.6871, -1.9724, -0.7953,  ..., -0.5784, -1.2792,  0.2354],\n",
       "           [-0.6871, -1.9724, -0.7953,  ..., -0.5784, -1.2792,  0.2354],\n",
       "           [-0.6871, -1.9724, -0.7953,  ..., -0.5784, -1.2792,  0.2354]],\n",
       "  \n",
       "          [[-1.4586, -0.1801,  1.0784,  ..., -1.5588,  0.0726,  0.0277],\n",
       "           [-0.7917, -0.8425,  1.5308,  ..., -0.7839,  0.7267,  0.4937],\n",
       "           [-1.0093, -1.0002,  0.1755,  ..., -0.8795,  0.8534, -0.7653],\n",
       "           ...,\n",
       "           [-1.1923, -1.8792, -0.4014,  ..., -0.9400, -0.7770, -0.2306],\n",
       "           [-1.1923, -1.8792, -0.4014,  ..., -0.9400, -0.7770, -0.2306],\n",
       "           [-1.1923, -1.8792, -0.4014,  ..., -0.9400, -0.7770, -0.2306]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.9.self_attn.v_proj': {'input': (tensor([[[-0.8326,  0.1919,  0.1489,  ..., -0.0367,  0.9569, -0.1575],\n",
       "            [-0.8486,  0.5019,  0.5531,  ..., -0.0907,  0.5967, -0.5547],\n",
       "            [ 0.3301, -0.3504, -0.3040,  ..., -0.4521,  0.2025, -0.3322],\n",
       "            ...,\n",
       "            [ 0.4257,  0.7693, -0.0085,  ...,  0.4638,  0.5712, -0.1826],\n",
       "            [ 0.4257,  0.7693, -0.0085,  ...,  0.4638,  0.5712, -0.1826],\n",
       "            [ 0.4257,  0.7693, -0.0085,  ...,  0.4638,  0.5712, -0.1826]],\n",
       "   \n",
       "           [[-0.9012,  0.2844, -0.5519,  ..., -0.0462,  0.3400,  0.9091],\n",
       "            [-0.8766,  0.4825, -1.0096,  ..., -0.1623,  0.3544,  0.5712],\n",
       "            [ 0.2275, -1.1400, -0.2502,  ..., -0.4510,  0.1250, -0.1631],\n",
       "            ...,\n",
       "            [ 0.3099,  0.6233, -0.2128,  ...,  0.2954,  0.3399, -0.0281],\n",
       "            [ 0.3099,  0.6233, -0.2128,  ...,  0.2954,  0.3399, -0.0281],\n",
       "            [ 0.3099,  0.6233, -0.2128,  ...,  0.2954,  0.3399, -0.0281]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-2.6896e-01, -6.6935e-01,  5.6185e-01,  ...,  1.5700e+00,\n",
       "             3.9309e-01, -2.3205e-01],\n",
       "           [-6.7886e-01, -3.2924e-01,  2.9049e-01,  ..., -3.9389e-01,\n",
       "             6.3464e-02, -7.4896e-01],\n",
       "           [-2.1577e-01,  2.5306e+00, -1.5739e+00,  ..., -5.2796e-01,\n",
       "             4.4632e-01,  4.1528e-01],\n",
       "           ...,\n",
       "           [ 2.1379e-01, -1.9424e-03, -1.4035e-01,  ..., -1.1443e+00,\n",
       "             3.1563e-01, -3.1390e-01],\n",
       "           [ 2.1379e-01, -1.9424e-03, -1.4035e-01,  ..., -1.1443e+00,\n",
       "             3.1563e-01, -3.1390e-01],\n",
       "           [ 2.1379e-01, -1.9424e-03, -1.4035e-01,  ..., -1.1443e+00,\n",
       "             3.1563e-01, -3.1390e-01]],\n",
       "  \n",
       "          [[-8.9862e-01, -6.5255e-01,  5.5910e-01,  ...,  1.1142e-01,\n",
       "             1.0279e-01, -9.5369e-02],\n",
       "           [-1.6483e+00,  7.3741e-01,  2.7146e-01,  ..., -1.2457e+00,\n",
       "            -3.2654e-01, -3.4321e-01],\n",
       "           [-2.0590e+00,  1.9433e+00,  1.4202e-01,  ..., -2.5189e+00,\n",
       "            -6.3290e-01, -1.6847e-01],\n",
       "           ...,\n",
       "           [-3.5407e-01, -8.1067e-01, -9.4877e-02,  ..., -9.5620e-01,\n",
       "             1.7458e-01,  3.2360e-01],\n",
       "           [-3.5407e-01, -8.1067e-01, -9.4877e-02,  ..., -9.5620e-01,\n",
       "             1.7458e-01,  3.2360e-01],\n",
       "           [-3.5407e-01, -8.1067e-01, -9.4877e-02,  ..., -9.5620e-01,\n",
       "             1.7458e-01,  3.2360e-01]]], grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.9.self_attn.out_proj': {'input': (tensor([[[-1.7820e-01,  2.1172e-01, -7.8945e-02,  ...,  9.5582e-01,\n",
       "              3.8881e-01, -4.5379e-01],\n",
       "            [-3.0132e-01,  1.8922e-01, -1.0192e-01,  ...,  2.6786e-01,\n",
       "              3.5082e-01, -8.9824e-01],\n",
       "            [-5.2804e-02,  1.4951e-01, -1.2203e-01,  ..., -6.4338e-02,\n",
       "              2.9207e-01,  2.7641e-01],\n",
       "            ...,\n",
       "            [-1.6920e-01,  1.0620e+00, -5.3782e-01,  ..., -7.9103e-02,\n",
       "              9.4832e-02, -5.6828e-01],\n",
       "            [-1.6920e-01,  1.0620e+00, -5.3782e-01,  ..., -7.9103e-02,\n",
       "              9.4832e-02, -5.6828e-01],\n",
       "            [-1.6920e-01,  1.0620e+00, -5.3782e-01,  ..., -7.9103e-02,\n",
       "              9.4832e-02, -5.6828e-01]],\n",
       "   \n",
       "           [[ 1.8900e-01, -5.4796e-02, -2.3718e-01,  ..., -2.3366e-01,\n",
       "             -1.8034e-02, -4.6380e-01],\n",
       "            [-3.8674e-01,  5.6944e-01, -6.6530e-01,  ..., -4.3248e-01,\n",
       "              2.7994e-02, -6.9039e-01],\n",
       "            [-1.1427e+00,  1.1823e+00, -6.4129e-01,  ..., -4.7586e-01,\n",
       "              6.9552e-04, -4.4277e-01],\n",
       "            ...,\n",
       "            [-4.4575e-02,  6.5214e-01, -3.5492e-01,  ..., -5.5146e-01,\n",
       "             -1.5909e-02, -8.0230e-01],\n",
       "            [-4.4575e-02,  6.5214e-01, -3.5492e-01,  ..., -5.5146e-01,\n",
       "             -1.5909e-02, -8.0230e-01],\n",
       "            [-4.4575e-02,  6.5214e-01, -3.5492e-01,  ..., -5.5146e-01,\n",
       "             -1.5909e-02, -8.0230e-01]]], grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[ 0.4976,  0.1702,  0.0371,  ...,  0.3219, -0.3336, -0.2054],\n",
       "           [ 0.2631,  0.2132, -0.2225,  ...,  0.5899, -0.3047,  0.0367],\n",
       "           [-0.4712, -0.4908,  0.0094,  ...,  0.2512, -0.1116,  0.0457],\n",
       "           ...,\n",
       "           [ 0.3601,  0.0167,  0.2419,  ...,  0.1247, -0.0147,  0.1162],\n",
       "           [ 0.3601,  0.0167,  0.2419,  ...,  0.1247, -0.0147,  0.1162],\n",
       "           [ 0.3601,  0.0167,  0.2419,  ...,  0.1247, -0.0147,  0.1162]],\n",
       "  \n",
       "          [[ 0.2478, -0.5088,  0.1460,  ..., -0.4450, -0.1437, -0.1403],\n",
       "           [-0.2084, -0.4288, -0.0320,  ..., -0.0167,  0.0719, -0.0899],\n",
       "           [-0.3280, -0.7621, -0.1280,  ...,  0.3074, -0.3405,  0.0587],\n",
       "           ...,\n",
       "           [ 0.0083, -0.6419,  0.2836,  ..., -0.1717, -0.1820,  0.1029],\n",
       "           [ 0.0083, -0.6419,  0.2836,  ..., -0.1717, -0.1820,  0.1029],\n",
       "           [ 0.0083, -0.6419,  0.2836,  ..., -0.1717, -0.1820,  0.1029]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.9.mlp.fc1': {'input': (tensor([[[-0.5418,  0.5772,  0.2435,  ...,  0.0260,  1.0192,  0.0736],\n",
       "            [-0.6750,  1.0324,  0.4756,  ...,  0.1636,  0.5075, -0.2976],\n",
       "            [ 0.1033, -0.6557, -0.2909,  ..., -0.5662,  0.1155,  0.0531],\n",
       "            ...,\n",
       "            [ 0.6210,  1.1666,  0.1514,  ...,  0.5199,  0.6792,  0.2871],\n",
       "            [ 0.6210,  1.1666,  0.1514,  ...,  0.5199,  0.6792,  0.2871],\n",
       "            [ 0.6210,  1.1666,  0.1514,  ...,  0.5199,  0.6792,  0.2871]],\n",
       "   \n",
       "           [[-0.7552,  0.1397, -0.4805,  ..., -0.4784,  0.2897,  1.7090],\n",
       "            [-1.0122,  0.4430, -1.1542,  ..., -0.3694,  0.4643,  1.2573],\n",
       "            [ 0.0641, -1.9349, -0.3498,  ..., -0.5115, -0.1613,  0.3090],\n",
       "            ...,\n",
       "            [ 0.3651,  0.6060, -0.0457,  ...,  0.1702,  0.2906,  0.5070],\n",
       "            [ 0.3651,  0.6060, -0.0457,  ...,  0.1702,  0.2906,  0.5070],\n",
       "            [ 0.3651,  0.6060, -0.0457,  ...,  0.1702,  0.2906,  0.5070]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-2.3257, -2.5010, -1.7753,  ..., -2.5616, -3.4569, -2.0601],\n",
       "           [-2.6615, -3.4651, -1.7116,  ..., -2.0047, -2.7540, -1.5287],\n",
       "           [-2.4249, -2.4474, -1.6249,  ..., -2.3860, -2.3407, -2.0998],\n",
       "           ...,\n",
       "           [-1.6670, -2.6683, -1.0760,  ..., -3.1695, -1.8998, -2.0175],\n",
       "           [-1.6670, -2.6683, -1.0760,  ..., -3.1695, -1.8998, -2.0175],\n",
       "           [-1.6670, -2.6683, -1.0760,  ..., -3.1695, -1.8998, -2.0175]],\n",
       "  \n",
       "          [[-0.9549, -2.5820, -0.7107,  ..., -3.5272, -2.8890, -2.2264],\n",
       "           [-1.8047, -4.1080, -1.8414,  ..., -2.6988, -2.7622, -2.1842],\n",
       "           [-2.1135, -4.8025, -0.9853,  ..., -1.2508, -1.8361, -0.5079],\n",
       "           ...,\n",
       "           [-1.3939, -2.7857, -1.2672,  ..., -3.0298, -1.8386, -2.0385],\n",
       "           [-1.3939, -2.7857, -1.2672,  ..., -3.0298, -1.8386, -2.0385],\n",
       "           [-1.3939, -2.7857, -1.2672,  ..., -3.0298, -1.8386, -2.0385]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.9.mlp.fc2': {'input': (tensor([[[-2.2943e-02, -1.5047e-02, -6.7414e-02,  ..., -1.2888e-02,\n",
       "             -7.2899e-04, -4.0422e-02],\n",
       "            [-9.8793e-03, -7.0624e-04, -7.4552e-02,  ..., -4.5000e-02,\n",
       "             -7.6366e-03, -9.6781e-02],\n",
       "            [-1.8158e-02, -1.7188e-02, -8.4814e-02,  ..., -1.9931e-02,\n",
       "             -2.2164e-02, -3.7345e-02],\n",
       "            ...,\n",
       "            [-7.9757e-02, -9.6995e-03, -1.5185e-01,  ..., -2.0772e-03,\n",
       "             -5.4571e-02, -4.3913e-02],\n",
       "            [-7.9757e-02, -9.6995e-03, -1.5185e-01,  ..., -2.0772e-03,\n",
       "             -5.4571e-02, -4.3913e-02],\n",
       "            [-7.9757e-02, -9.6995e-03, -1.5185e-01,  ..., -2.0772e-03,\n",
       "             -5.4571e-02, -4.3913e-02]],\n",
       "   \n",
       "           [[-1.6229e-01, -1.2221e-02, -1.6966e-01,  ..., -5.5335e-04,\n",
       "             -5.1351e-03, -2.8645e-02],\n",
       "            [-6.4244e-02, -4.1503e-05, -6.0398e-02,  ..., -8.9166e-03,\n",
       "             -7.4594e-03, -3.1358e-02],\n",
       "            [-3.6327e-02, -8.5875e-07, -1.6000e-01,  ..., -1.3218e-01,\n",
       "             -6.0944e-02, -1.5531e-01],\n",
       "            ...,\n",
       "            [-1.1408e-01, -6.9716e-03, -1.3017e-01,  ..., -3.3056e-03,\n",
       "             -6.0687e-02, -4.2166e-02],\n",
       "            [-1.1408e-01, -6.9716e-03, -1.3017e-01,  ..., -3.3056e-03,\n",
       "             -6.0687e-02, -4.2166e-02],\n",
       "            [-1.1408e-01, -6.9715e-03, -1.3017e-01,  ..., -3.3056e-03,\n",
       "             -6.0687e-02, -4.2166e-02]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[ 5.7070e-01,  8.9118e-02,  3.6545e-01,  ..., -4.2889e-01,\n",
       "             6.3306e-02,  3.4036e-01],\n",
       "           [ 4.1324e-01, -2.1147e-01,  3.3701e-01,  ..., -3.0956e-01,\n",
       "            -1.5027e-01,  5.3233e-01],\n",
       "           [ 2.2008e-01,  3.1021e-01,  3.2357e-01,  ...,  2.4684e-03,\n",
       "             8.5420e-02,  4.3487e-02],\n",
       "           ...,\n",
       "           [-4.4582e-01,  4.9265e-01, -3.0349e-01,  ..., -1.8275e-01,\n",
       "             2.1684e-01,  1.0829e+00],\n",
       "           [-4.4582e-01,  4.9265e-01, -3.0349e-01,  ..., -1.8275e-01,\n",
       "             2.1684e-01,  1.0829e+00],\n",
       "           [-4.4581e-01,  4.9265e-01, -3.0349e-01,  ..., -1.8275e-01,\n",
       "             2.1684e-01,  1.0829e+00]],\n",
       "  \n",
       "          [[ 3.5477e-01,  2.7278e-01,  3.5346e-01,  ...,  5.8880e-02,\n",
       "            -9.7347e-02,  5.0851e-01],\n",
       "           [-2.4010e-02, -6.6264e-02,  6.8532e-01,  ...,  1.1266e-03,\n",
       "            -4.5345e-01,  3.3831e-01],\n",
       "           [-5.7197e-02, -5.8457e-02,  5.3105e-01,  ...,  3.4880e-01,\n",
       "             3.4208e-01,  2.3844e-01],\n",
       "           ...,\n",
       "           [-2.0468e-02,  4.4754e-01, -3.6022e-01,  ..., -2.1982e-02,\n",
       "            -5.6031e-02,  1.2299e+00],\n",
       "           [-2.0468e-02,  4.4754e-01, -3.6022e-01,  ..., -2.1982e-02,\n",
       "            -5.6031e-02,  1.2299e+00],\n",
       "           [-2.0468e-02,  4.4754e-01, -3.6022e-01,  ..., -2.1982e-02,\n",
       "            -5.6031e-02,  1.2299e+00]]], grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.10.self_attn.q_proj': {'input': (tensor([[[-0.2195,  0.4360,  0.4586,  ..., -0.0569,  0.9519, -0.0835],\n",
       "            [-0.4260,  0.5913,  0.6697,  ...,  0.0794,  0.3876, -0.1985],\n",
       "            [ 0.1808, -0.4239, -0.0715,  ..., -0.2585,  0.1957, -0.2778],\n",
       "            ...,\n",
       "            [ 0.3865,  1.0291, -0.0402,  ...,  0.4070,  0.6837,  0.2650],\n",
       "            [ 0.3865,  1.0291, -0.0402,  ...,  0.4070,  0.6837,  0.2650],\n",
       "            [ 0.3865,  1.0291, -0.0402,  ...,  0.4070,  0.6837,  0.2650]],\n",
       "   \n",
       "           [[-0.5364,  0.1817, -0.2041,  ..., -0.1702,  0.2331,  1.0789],\n",
       "            [-1.0154,  0.2133, -0.5431,  ..., -0.1294,  0.1675,  0.7311],\n",
       "            [-0.0088, -1.6787,  0.0592,  ..., -0.0656,  0.1341,  0.0210],\n",
       "            ...,\n",
       "            [ 0.3030,  0.5860, -0.2501,  ...,  0.2219,  0.2711,  0.5059],\n",
       "            [ 0.3030,  0.5860, -0.2501,  ...,  0.2219,  0.2711,  0.5059],\n",
       "            [ 0.3030,  0.5860, -0.2501,  ...,  0.2219,  0.2711,  0.5059]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.8008, -0.3525,  1.9069,  ..., -1.5523, -1.0466, -0.2889],\n",
       "           [ 0.7514, -0.7302,  1.0453,  ...,  0.3650, -0.9652, -0.7014],\n",
       "           [-0.8058, -0.3134,  1.0277,  ...,  0.2777, -0.0379,  0.1962],\n",
       "           ...,\n",
       "           [-0.9680,  0.2202,  1.7278,  ...,  1.1166, -1.1878, -0.7639],\n",
       "           [-0.9680,  0.2202,  1.7278,  ...,  1.1166, -1.1878, -0.7639],\n",
       "           [-0.9680,  0.2202,  1.7278,  ...,  1.1166, -1.1878, -0.7639]],\n",
       "  \n",
       "          [[ 0.2103, -0.8832,  2.6607,  ..., -0.1104, -0.5012,  2.0913],\n",
       "           [-0.8808, -1.1568,  3.3957,  ...,  1.0358, -0.2384,  1.4138],\n",
       "           [-0.7004, -0.7725,  2.9024,  ...,  1.1091, -1.4282,  0.6820],\n",
       "           ...,\n",
       "           [-1.3223,  0.5294,  1.8343,  ...,  1.5833, -0.8174, -0.7740],\n",
       "           [-1.3223,  0.5294,  1.8343,  ...,  1.5833, -0.8174, -0.7740],\n",
       "           [-1.3223,  0.5294,  1.8343,  ...,  1.5833, -0.8174, -0.7740]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.10.self_attn.k_proj': {'input': (tensor([[[-0.2195,  0.4360,  0.4586,  ..., -0.0569,  0.9519, -0.0835],\n",
       "            [-0.4260,  0.5913,  0.6697,  ...,  0.0794,  0.3876, -0.1985],\n",
       "            [ 0.1808, -0.4239, -0.0715,  ..., -0.2585,  0.1957, -0.2778],\n",
       "            ...,\n",
       "            [ 0.3865,  1.0291, -0.0402,  ...,  0.4070,  0.6837,  0.2650],\n",
       "            [ 0.3865,  1.0291, -0.0402,  ...,  0.4070,  0.6837,  0.2650],\n",
       "            [ 0.3865,  1.0291, -0.0402,  ...,  0.4070,  0.6837,  0.2650]],\n",
       "   \n",
       "           [[-0.5364,  0.1817, -0.2041,  ..., -0.1702,  0.2331,  1.0789],\n",
       "            [-1.0154,  0.2133, -0.5431,  ..., -0.1294,  0.1675,  0.7311],\n",
       "            [-0.0088, -1.6787,  0.0592,  ..., -0.0656,  0.1341,  0.0210],\n",
       "            ...,\n",
       "            [ 0.3030,  0.5860, -0.2501,  ...,  0.2219,  0.2711,  0.5059],\n",
       "            [ 0.3030,  0.5860, -0.2501,  ...,  0.2219,  0.2711,  0.5059],\n",
       "            [ 0.3030,  0.5860, -0.2501,  ...,  0.2219,  0.2711,  0.5059]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.5267, -1.4852,  0.0359,  ..., -0.3719, -0.6721,  0.9087],\n",
       "           [-1.6594, -1.0126, -0.9084,  ..., -0.3416, -1.0180,  0.6747],\n",
       "           [-0.2097, -0.3819,  1.6183,  ..., -1.2300, -1.1816,  0.5132],\n",
       "           ...,\n",
       "           [-0.5904,  0.2580,  1.7456,  ...,  0.0180,  0.1216,  0.2658],\n",
       "           [-0.5904,  0.2580,  1.7456,  ...,  0.0180,  0.1216,  0.2658],\n",
       "           [-0.5904,  0.2580,  1.7456,  ...,  0.0180,  0.1216,  0.2658]],\n",
       "  \n",
       "          [[-0.2161, -0.9996,  0.4131,  ..., -0.1710, -0.5693, -0.1996],\n",
       "           [-0.3742, -0.2142,  0.8403,  ..., -0.2813, -0.4134, -0.1221],\n",
       "           [-0.8228,  0.3292,  0.9537,  ..., -0.8066, -0.6719, -0.3403],\n",
       "           ...,\n",
       "           [-0.9639,  0.1540,  1.7755,  ...,  0.2204, -0.3691,  0.4699],\n",
       "           [-0.9639,  0.1540,  1.7755,  ...,  0.2204, -0.3691,  0.4699],\n",
       "           [-0.9639,  0.1540,  1.7755,  ...,  0.2204, -0.3691,  0.4699]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.10.self_attn.v_proj': {'input': (tensor([[[-0.2195,  0.4360,  0.4586,  ..., -0.0569,  0.9519, -0.0835],\n",
       "            [-0.4260,  0.5913,  0.6697,  ...,  0.0794,  0.3876, -0.1985],\n",
       "            [ 0.1808, -0.4239, -0.0715,  ..., -0.2585,  0.1957, -0.2778],\n",
       "            ...,\n",
       "            [ 0.3865,  1.0291, -0.0402,  ...,  0.4070,  0.6837,  0.2650],\n",
       "            [ 0.3865,  1.0291, -0.0402,  ...,  0.4070,  0.6837,  0.2650],\n",
       "            [ 0.3865,  1.0291, -0.0402,  ...,  0.4070,  0.6837,  0.2650]],\n",
       "   \n",
       "           [[-0.5364,  0.1817, -0.2041,  ..., -0.1702,  0.2331,  1.0789],\n",
       "            [-1.0154,  0.2133, -0.5431,  ..., -0.1294,  0.1675,  0.7311],\n",
       "            [-0.0088, -1.6787,  0.0592,  ..., -0.0656,  0.1341,  0.0210],\n",
       "            ...,\n",
       "            [ 0.3030,  0.5860, -0.2501,  ...,  0.2219,  0.2711,  0.5059],\n",
       "            [ 0.3030,  0.5860, -0.2501,  ...,  0.2219,  0.2711,  0.5059],\n",
       "            [ 0.3030,  0.5860, -0.2501,  ...,  0.2219,  0.2711,  0.5059]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-7.3882e-01, -4.0714e-02,  8.3999e-01,  ...,  4.0496e-01,\n",
       "            -6.7250e-01, -2.1539e-01],\n",
       "           [-4.9890e-01,  5.7532e-01,  1.2573e+00,  ..., -5.8935e-01,\n",
       "            -9.0929e-02, -1.2058e+00],\n",
       "           [ 6.8031e-04, -2.6991e-02,  2.7022e-01,  ..., -5.0983e-02,\n",
       "             1.8506e-01,  1.4943e-01],\n",
       "           ...,\n",
       "           [ 6.7062e-01, -3.5520e-01,  1.0080e+00,  ...,  7.5559e-01,\n",
       "             8.0564e-01, -1.6833e+00],\n",
       "           [ 6.7062e-01, -3.5520e-01,  1.0080e+00,  ...,  7.5559e-01,\n",
       "             8.0564e-01, -1.6833e+00],\n",
       "           [ 6.7062e-01, -3.5520e-01,  1.0080e+00,  ...,  7.5559e-01,\n",
       "             8.0564e-01, -1.6833e+00]],\n",
       "  \n",
       "          [[ 7.2829e-02, -6.8026e-01,  2.0207e+00,  ..., -1.3411e+00,\n",
       "             7.6729e-01, -6.1914e-01],\n",
       "           [ 1.1626e-01, -7.0821e-01,  2.6331e+00,  ..., -2.0850e+00,\n",
       "             1.6114e+00,  7.5409e-02],\n",
       "           [ 7.8022e-01, -1.5752e+00,  1.2462e+00,  ..., -3.2964e-02,\n",
       "             1.6080e+00,  2.5765e-01],\n",
       "           ...,\n",
       "           [ 4.2591e-01, -1.3919e-01,  1.4954e+00,  ...,  3.1838e-02,\n",
       "             1.3154e+00, -1.2299e+00],\n",
       "           [ 4.2591e-01, -1.3919e-01,  1.4954e+00,  ...,  3.1838e-02,\n",
       "             1.3154e+00, -1.2299e+00],\n",
       "           [ 4.2591e-01, -1.3919e-01,  1.4954e+00,  ...,  3.1839e-02,\n",
       "             1.3154e+00, -1.2299e+00]]], grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.10.self_attn.out_proj': {'input': (tensor([[[-0.0639,  0.1458,  0.1498,  ..., -0.5987,  0.6223, -0.4332],\n",
       "            [-0.0955,  0.1362,  0.2987,  ..., -0.3727,  0.1530, -0.2660],\n",
       "            [ 0.0681,  0.0532,  0.1953,  ...,  0.0109, -0.3475, -0.1580],\n",
       "            ...,\n",
       "            [-0.0231,  0.1198,  0.1017,  ...,  0.7430, -0.0172, -0.0637],\n",
       "            [-0.0231,  0.1198,  0.1017,  ...,  0.7430, -0.0172, -0.0637],\n",
       "            [-0.0231,  0.1198,  0.1017,  ...,  0.7430, -0.0172, -0.0637]],\n",
       "   \n",
       "           [[ 0.2142, -0.2744,  0.4419,  ..., -0.0039,  0.0460, -0.0973],\n",
       "            [ 0.5110, -0.5800,  0.5933,  ...,  0.3395,  0.1583, -0.2171],\n",
       "            [ 0.1244, -0.0267,  0.1754,  ...,  0.5022,  0.3132, -0.3752],\n",
       "            ...,\n",
       "            [ 0.1871, -0.1393,  0.3371,  ...,  0.2344,  0.2608,  0.0226],\n",
       "            [ 0.1871, -0.1393,  0.3371,  ...,  0.2344,  0.2608,  0.0226],\n",
       "            [ 0.1870, -0.1393,  0.3371,  ...,  0.2344,  0.2608,  0.0226]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.3348, -0.2823, -0.2594,  ...,  0.1673, -0.1539,  0.2121],\n",
       "           [-0.3367, -0.0105, -0.3505,  ..., -0.0805, -0.0726,  0.0402],\n",
       "           [ 0.1059, -0.1055,  0.0120,  ..., -0.0308, -0.0506,  0.1059],\n",
       "           ...,\n",
       "           [-0.2978,  0.1211, -0.1591,  ..., -0.3199, -0.0740, -0.2358],\n",
       "           [-0.2978,  0.1211, -0.1591,  ..., -0.3199, -0.0740, -0.2358],\n",
       "           [-0.2978,  0.1211, -0.1591,  ..., -0.3199, -0.0740, -0.2358]],\n",
       "  \n",
       "          [[ 0.2889, -1.0441, -0.1901,  ...,  0.2374, -0.4336, -0.4825],\n",
       "           [ 0.4595, -0.4710, -0.3948,  ...,  0.1950, -0.4004, -0.4538],\n",
       "           [-0.4670,  0.3104, -0.0939,  ..., -0.3559, -0.1470, -0.5127],\n",
       "           ...,\n",
       "           [-0.3444, -0.0174, -0.0289,  ..., -0.2069,  0.1634, -0.3326],\n",
       "           [-0.3444, -0.0174, -0.0289,  ..., -0.2069,  0.1634, -0.3326],\n",
       "           [-0.3444, -0.0174, -0.0289,  ..., -0.2069,  0.1634, -0.3326]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.10.mlp.fc1': {'input': (tensor([[[-0.3708,  0.0191,  0.3250,  ...,  0.1273,  1.0596,  0.0402],\n",
       "            [-0.6092,  0.4566,  0.4654,  ...,  0.1602,  0.4340, -0.2425],\n",
       "            [ 0.2761, -0.8479, -0.0230,  ..., -0.2789,  0.2054, -0.2888],\n",
       "            ...,\n",
       "            [ 0.3207,  1.0309, -0.0804,  ...,  0.5638,  0.8030,  0.2005],\n",
       "            [ 0.3207,  1.0309, -0.0804,  ...,  0.5638,  0.8030,  0.2005],\n",
       "            [ 0.3207,  1.0309, -0.0804,  ...,  0.5638,  0.8030,  0.2005]],\n",
       "   \n",
       "           [[-0.3328, -0.8324, -0.3182,  ..., -0.0126,  0.0060,  1.0148],\n",
       "            [-0.6712, -0.4040, -0.8418,  ...,  0.0405, -0.0795,  0.5435],\n",
       "            [-0.2309, -2.0094,  0.0327,  ..., -0.2202,  0.0629, -0.3891],\n",
       "            ...,\n",
       "            [ 0.2134,  0.4267, -0.2428,  ...,  0.3348,  0.4150,  0.4488],\n",
       "            [ 0.2134,  0.4267, -0.2428,  ...,  0.3348,  0.4150,  0.4488],\n",
       "            [ 0.2134,  0.4267, -0.2428,  ...,  0.3348,  0.4150,  0.4488]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-1.3533, -3.2479, -2.5033,  ..., -1.1490, -2.7525, -1.5078],\n",
       "           [-2.1655, -2.7717, -2.4576,  ..., -0.6220, -1.8323, -1.5842],\n",
       "           [-2.1235, -1.8963, -1.9423,  ..., -1.6983, -2.7516, -2.9354],\n",
       "           ...,\n",
       "           [-0.5054, -3.1841, -2.5229,  ..., -2.9501, -2.5821, -1.3984],\n",
       "           [-0.5054, -3.1841, -2.5229,  ..., -2.9501, -2.5821, -1.3984],\n",
       "           [-0.5054, -3.1841, -2.5229,  ..., -2.9501, -2.5821, -1.3984]],\n",
       "  \n",
       "          [[-0.7592, -3.6520, -2.5403,  ...,  0.1069, -3.5587, -0.5341],\n",
       "           [-2.8712, -3.1057, -2.8651,  ...,  0.5132, -2.2334,  0.2964],\n",
       "           [-3.7950, -4.2131, -2.2199,  ...,  0.1214, -1.0515,  0.0538],\n",
       "           ...,\n",
       "           [-0.9308, -2.7855, -2.4443,  ..., -2.4366, -2.9132, -1.3976],\n",
       "           [-0.9308, -2.7855, -2.4443,  ..., -2.4366, -2.9132, -1.3976],\n",
       "           [-0.9308, -2.7855, -2.4443,  ..., -2.4366, -2.9132, -1.3976]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.10.mlp.fc2': {'input': (tensor([[[-1.1930e-01, -1.5803e-03, -1.4960e-02,  ..., -1.4414e-01,\n",
       "             -7.6683e-03, -9.9429e-02],\n",
       "            [-3.2625e-02, -7.2585e-03, -1.6761e-02,  ..., -1.6609e-01,\n",
       "             -6.1334e-02, -8.9816e-02],\n",
       "            [-3.5590e-02, -5.4912e-02, -5.0548e-02,  ..., -7.6091e-02,\n",
       "             -7.6873e-03, -4.4546e-03],\n",
       "            ...,\n",
       "            [-1.5499e-01, -1.9752e-03, -1.4236e-02,  ..., -4.2565e-03,\n",
       "             -1.2216e-02, -1.1350e-01],\n",
       "            [-1.5499e-01, -1.9752e-03, -1.4236e-02,  ..., -4.2565e-03,\n",
       "             -1.2216e-02, -1.1350e-01],\n",
       "            [-1.5499e-01, -1.9752e-03, -1.4236e-02,  ..., -4.2565e-03,\n",
       "             -1.2217e-02, -1.1349e-01]],\n",
       "   \n",
       "           [[-1.7003e-01, -3.3282e-04, -1.3616e-02,  ...,  5.8019e-02,\n",
       "             -4.8786e-04, -1.5845e-01],\n",
       "            [-5.4195e-03, -2.5769e-03, -5.5191e-03,  ...,  3.5722e-01,\n",
       "             -2.8207e-02,  1.8274e-01],\n",
       "            [-1.8006e-04, -2.4359e-05, -2.9050e-02,  ...,  6.6539e-02,\n",
       "             -1.5423e-01,  2.8043e-02],\n",
       "            ...,\n",
       "            [-1.6393e-01, -6.9759e-03, -1.7319e-02,  ..., -1.7648e-02,\n",
       "             -4.7710e-03, -1.1361e-01],\n",
       "            [-1.6393e-01, -6.9759e-03, -1.7319e-02,  ..., -1.7648e-02,\n",
       "             -4.7710e-03, -1.1361e-01],\n",
       "            [-1.6393e-01, -6.9759e-03, -1.7319e-02,  ..., -1.7648e-02,\n",
       "             -4.7710e-03, -1.1361e-01]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[ 0.2175, -0.7705,  0.5353,  ...,  0.2026, -0.1317,  0.1266],\n",
       "           [ 0.1895, -1.2468,  0.3073,  ...,  0.4951, -0.2109, -0.4009],\n",
       "           [-0.0676, -0.1173,  0.0710,  ..., -0.0199, -0.0676, -0.1625],\n",
       "           ...,\n",
       "           [ 0.3465, -0.9267,  0.8693,  ...,  0.7035, -0.0678, -0.1995],\n",
       "           [ 0.3465, -0.9267,  0.8693,  ...,  0.7035, -0.0678, -0.1995],\n",
       "           [ 0.3465, -0.9267,  0.8693,  ...,  0.7035, -0.0678, -0.1995]],\n",
       "  \n",
       "          [[-0.7527, -0.3310,  0.2051,  ...,  0.3239,  0.0470, -0.4561],\n",
       "           [-0.0924, -0.3617,  0.0533,  ...,  0.8293, -0.0263, -0.7026],\n",
       "           [-0.2096, -0.2210,  0.7178,  ...,  0.8053,  0.1301, -0.0273],\n",
       "           ...,\n",
       "           [ 0.4100, -0.4442,  1.0041,  ...,  0.8588, -0.1003, -0.0793],\n",
       "           [ 0.4100, -0.4442,  1.0041,  ...,  0.8588, -0.1003, -0.0793],\n",
       "           [ 0.4100, -0.4442,  1.0041,  ...,  0.8588, -0.1003, -0.0793]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.11.self_attn.q_proj': {'input': (tensor([[[-0.3001, -0.2600,  0.6278,  ...,  0.0822,  0.8069,  0.1070],\n",
       "            [-0.5593, -0.3240,  0.6137,  ...,  0.2833,  0.2457, -0.4504],\n",
       "            [ 0.2195, -0.4972, -0.0359,  ..., -0.3397,  0.1689, -0.3041],\n",
       "            ...,\n",
       "            [ 0.4152,  0.6269,  0.2359,  ...,  0.4977,  0.6267,  0.0734],\n",
       "            [ 0.4152,  0.6269,  0.2359,  ...,  0.4977,  0.6267,  0.0734],\n",
       "            [ 0.4152,  0.6269,  0.2359,  ...,  0.4977,  0.6267,  0.0734]],\n",
       "   \n",
       "           [[-0.7746, -0.5660, -0.2037,  ...,  0.0139,  0.0641,  0.4885],\n",
       "            [-0.8089, -0.3087, -0.7744,  ...,  0.3129, -0.0405,  0.0026],\n",
       "            [-0.4182, -1.4471,  0.4799,  ...,  0.1116,  0.1622, -0.3078],\n",
       "            ...,\n",
       "            [ 0.3368,  0.3444,  0.1842,  ...,  0.3916,  0.3133,  0.2877],\n",
       "            [ 0.3368,  0.3444,  0.1842,  ...,  0.3916,  0.3133,  0.2877],\n",
       "            [ 0.3368,  0.3444,  0.1842,  ...,  0.3916,  0.3133,  0.2877]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.1830, -1.0874,  0.6284,  ...,  0.4133,  0.4566,  1.0409],\n",
       "           [ 0.4919, -1.2669, -0.2928,  ...,  1.1360,  0.0544,  0.0039],\n",
       "           [-0.7052,  0.3947,  0.0968,  ..., -0.3727, -0.4371, -0.7678],\n",
       "           ...,\n",
       "           [-0.5692,  0.0205,  0.6984,  ..., -1.6034, -0.5202,  2.2746],\n",
       "           [-0.5692,  0.0205,  0.6984,  ..., -1.6034, -0.5202,  2.2746],\n",
       "           [-0.5692,  0.0205,  0.6984,  ..., -1.6034, -0.5202,  2.2746]],\n",
       "  \n",
       "          [[ 0.3026, -1.3176,  0.3054,  ..., -0.2551, -0.4769, -0.0351],\n",
       "           [ 1.1462, -1.2172, -0.2642,  ..., -1.4137, -1.6001,  0.0454],\n",
       "           [ 1.0046, -1.4353, -0.3649,  ..., -2.5666, -0.8177,  0.3961],\n",
       "           ...,\n",
       "           [-0.5867, -0.1341,  0.5181,  ..., -2.1728, -0.0963,  1.0012],\n",
       "           [-0.5867, -0.1341,  0.5181,  ..., -2.1728, -0.0963,  1.0012],\n",
       "           [-0.5867, -0.1341,  0.5181,  ..., -2.1728, -0.0963,  1.0012]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.11.self_attn.k_proj': {'input': (tensor([[[-0.3001, -0.2600,  0.6278,  ...,  0.0822,  0.8069,  0.1070],\n",
       "            [-0.5593, -0.3240,  0.6137,  ...,  0.2833,  0.2457, -0.4504],\n",
       "            [ 0.2195, -0.4972, -0.0359,  ..., -0.3397,  0.1689, -0.3041],\n",
       "            ...,\n",
       "            [ 0.4152,  0.6269,  0.2359,  ...,  0.4977,  0.6267,  0.0734],\n",
       "            [ 0.4152,  0.6269,  0.2359,  ...,  0.4977,  0.6267,  0.0734],\n",
       "            [ 0.4152,  0.6269,  0.2359,  ...,  0.4977,  0.6267,  0.0734]],\n",
       "   \n",
       "           [[-0.7746, -0.5660, -0.2037,  ...,  0.0139,  0.0641,  0.4885],\n",
       "            [-0.8089, -0.3087, -0.7744,  ...,  0.3129, -0.0405,  0.0026],\n",
       "            [-0.4182, -1.4471,  0.4799,  ...,  0.1116,  0.1622, -0.3078],\n",
       "            ...,\n",
       "            [ 0.3368,  0.3444,  0.1842,  ...,  0.3916,  0.3133,  0.2877],\n",
       "            [ 0.3368,  0.3444,  0.1842,  ...,  0.3916,  0.3133,  0.2877],\n",
       "            [ 0.3368,  0.3444,  0.1842,  ...,  0.3916,  0.3133,  0.2877]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.7222,  1.6012,  1.4194,  ...,  0.8142,  0.9974, -0.5514],\n",
       "           [ 0.3049,  1.5364,  0.4161,  ...,  0.6334,  0.3532, -0.1382],\n",
       "           [-0.4260,  1.0297,  0.2650,  ..., -0.1710,  0.5004,  0.1785],\n",
       "           ...,\n",
       "           [-1.5745,  0.9983,  1.3145,  ...,  0.2946,  2.4445,  0.2573],\n",
       "           [-1.5745,  0.9983,  1.3145,  ...,  0.2946,  2.4445,  0.2573],\n",
       "           [-1.5745,  0.9983,  1.3145,  ...,  0.2946,  2.4445,  0.2573]],\n",
       "  \n",
       "          [[-1.0898,  0.9208,  0.7308,  ...,  0.4260,  0.6822,  1.4214],\n",
       "           [-0.3189,  0.4867, -0.0184,  ..., -0.3065,  0.3936,  2.9448],\n",
       "           [ 0.0661, -0.3475, -0.0785,  ..., -2.9877,  0.8175,  1.9304],\n",
       "           ...,\n",
       "           [-1.6607,  0.3650,  1.2195,  ...,  0.0363,  2.1133,  0.1180],\n",
       "           [-1.6607,  0.3650,  1.2195,  ...,  0.0363,  2.1133,  0.1180],\n",
       "           [-1.6607,  0.3650,  1.2195,  ...,  0.0363,  2.1133,  0.1180]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.11.self_attn.v_proj': {'input': (tensor([[[-0.3001, -0.2600,  0.6278,  ...,  0.0822,  0.8069,  0.1070],\n",
       "            [-0.5593, -0.3240,  0.6137,  ...,  0.2833,  0.2457, -0.4504],\n",
       "            [ 0.2195, -0.4972, -0.0359,  ..., -0.3397,  0.1689, -0.3041],\n",
       "            ...,\n",
       "            [ 0.4152,  0.6269,  0.2359,  ...,  0.4977,  0.6267,  0.0734],\n",
       "            [ 0.4152,  0.6269,  0.2359,  ...,  0.4977,  0.6267,  0.0734],\n",
       "            [ 0.4152,  0.6269,  0.2359,  ...,  0.4977,  0.6267,  0.0734]],\n",
       "   \n",
       "           [[-0.7746, -0.5660, -0.2037,  ...,  0.0139,  0.0641,  0.4885],\n",
       "            [-0.8089, -0.3087, -0.7744,  ...,  0.3129, -0.0405,  0.0026],\n",
       "            [-0.4182, -1.4471,  0.4799,  ...,  0.1116,  0.1622, -0.3078],\n",
       "            ...,\n",
       "            [ 0.3368,  0.3444,  0.1842,  ...,  0.3916,  0.3133,  0.2877],\n",
       "            [ 0.3368,  0.3444,  0.1842,  ...,  0.3916,  0.3133,  0.2877],\n",
       "            [ 0.3368,  0.3444,  0.1842,  ...,  0.3916,  0.3133,  0.2877]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.1477,  0.6410,  0.6701,  ...,  0.6870, -1.2152,  0.0865],\n",
       "           [ 0.4529,  0.5167,  0.3476,  ..., -0.0772, -0.7432,  0.2500],\n",
       "           [ 0.0636, -0.9493,  0.0277,  ...,  0.3767, -0.5019,  0.2874],\n",
       "           ...,\n",
       "           [-0.6410,  0.2719,  0.7959,  ..., -0.0161,  1.4244,  0.8435],\n",
       "           [-0.6410,  0.2719,  0.7959,  ..., -0.0161,  1.4244,  0.8435],\n",
       "           [-0.6410,  0.2719,  0.7959,  ..., -0.0161,  1.4244,  0.8435]],\n",
       "  \n",
       "          [[-0.7162,  0.8935,  0.2545,  ...,  0.5581,  0.8882, -0.9292],\n",
       "           [-1.2484,  0.6518,  0.1189,  ...,  0.8892,  0.4980, -0.7202],\n",
       "           [-1.1225,  0.2147,  0.5461,  ..., -0.4978,  0.6179,  0.2644],\n",
       "           ...,\n",
       "           [-0.9623,  0.3195,  0.5131,  ...,  0.1608,  2.0229,  0.5080],\n",
       "           [-0.9623,  0.3195,  0.5131,  ...,  0.1608,  2.0229,  0.5080],\n",
       "           [-0.9623,  0.3195,  0.5131,  ...,  0.1608,  2.0229,  0.5080]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.11.self_attn.out_proj': {'input': (tensor([[[ 0.2055,  0.0736, -0.0123,  ...,  0.0306, -0.0118,  0.1837],\n",
       "            [ 0.0089,  0.0327, -0.0601,  ...,  0.1036, -0.0638,  0.1010],\n",
       "            [-0.3912,  0.1676, -0.0829,  ...,  0.1228, -0.2253,  0.0674],\n",
       "            ...,\n",
       "            [-0.2427,  0.2016, -0.0241,  ..., -0.1704, -0.2357,  0.3446],\n",
       "            [-0.2427,  0.2016, -0.0241,  ..., -0.1704, -0.2357,  0.3446],\n",
       "            [-0.2427,  0.2016, -0.0241,  ..., -0.1704, -0.2357,  0.3446]],\n",
       "   \n",
       "           [[ 0.0104,  0.1537, -0.0447,  ..., -0.3237, -0.1978,  0.1624],\n",
       "            [-0.0689,  0.1863, -0.0535,  ..., -0.1723, -0.2487, -0.0080],\n",
       "            [-0.1921,  0.2300, -0.0218,  ..., -0.0906, -0.2203,  0.0028],\n",
       "            ...,\n",
       "            [-0.3216,  0.3148, -0.0267,  ..., -0.2281,  0.0166,  0.0828],\n",
       "            [-0.3216,  0.3148, -0.0267,  ..., -0.2281,  0.0166,  0.0828],\n",
       "            [-0.3216,  0.3148, -0.0267,  ..., -0.2281,  0.0166,  0.0828]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-5.4545e-02,  5.8452e-02,  4.7313e-02,  ...,  1.5026e-01,\n",
       "             5.0869e-02,  5.5484e-01],\n",
       "           [-3.8951e-01,  2.3813e-01,  5.8404e-02,  ...,  2.1435e-01,\n",
       "             5.5952e-02,  4.8883e-01],\n",
       "           [-2.7204e-01, -2.3127e-01,  2.2194e-02,  ...,  2.6079e-01,\n",
       "            -1.3995e-01,  7.4269e-01],\n",
       "           ...,\n",
       "           [-1.2680e-01,  2.6392e-01,  2.9829e-01,  ...,  9.7114e-01,\n",
       "             7.3919e-01,  1.1251e-01],\n",
       "           [-1.2680e-01,  2.6392e-01,  2.9829e-01,  ...,  9.7114e-01,\n",
       "             7.3919e-01,  1.1251e-01],\n",
       "           [-1.2680e-01,  2.6393e-01,  2.9829e-01,  ...,  9.7114e-01,\n",
       "             7.3919e-01,  1.1251e-01]],\n",
       "  \n",
       "          [[ 4.1592e-01,  2.0207e-01,  6.7742e-01,  ...,  6.3618e-01,\n",
       "             4.5619e-04,  5.3275e-01],\n",
       "           [ 8.0169e-02,  3.4363e-01,  1.0663e+00,  ...,  2.4227e-01,\n",
       "             1.8536e-01,  6.2504e-01],\n",
       "           [-4.2740e-01,  1.3884e-01,  7.2423e-01,  ...,  5.9357e-01,\n",
       "             1.9041e-01,  3.2021e-01],\n",
       "           ...,\n",
       "           [-1.3173e-01,  7.2267e-03,  2.9495e-01,  ...,  7.6677e-01,\n",
       "             6.1266e-01,  1.7756e-01],\n",
       "           [-1.3173e-01,  7.2267e-03,  2.9495e-01,  ...,  7.6677e-01,\n",
       "             6.1266e-01,  1.7756e-01],\n",
       "           [-1.3173e-01,  7.2267e-03,  2.9495e-01,  ...,  7.6677e-01,\n",
       "             6.1266e-01,  1.7756e-01]]], grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.11.mlp.fc1': {'input': (tensor([[[-0.1152,  0.0712,  0.7375,  ...,  0.2594,  1.0632,  0.2288],\n",
       "            [-0.6090,  0.1601,  0.7398,  ...,  0.5940,  0.3732, -0.4888],\n",
       "            [ 0.2762, -0.3961, -0.0905,  ..., -0.2451,  0.1359, -0.2361],\n",
       "            ...,\n",
       "            [ 0.5620,  1.2278,  0.3570,  ...,  1.0626,  1.0811, -0.1977],\n",
       "            [ 0.5620,  1.2278,  0.3570,  ...,  1.0626,  1.0811, -0.1977],\n",
       "            [ 0.5620,  1.2278,  0.3570,  ...,  1.0626,  1.0811, -0.1977]],\n",
       "   \n",
       "           [[-0.3323, -0.2276,  0.1874,  ...,  0.4256,  0.0994,  0.6350],\n",
       "            [-0.4955,  0.2351, -0.0980,  ...,  0.5976,  0.0976,  0.1025],\n",
       "            [-0.4185, -1.2551,  1.0722,  ...,  0.5565,  0.3423, -0.4817],\n",
       "            ...,\n",
       "            [ 0.4829,  0.7698,  0.3050,  ...,  0.8665,  0.6670,  0.1007],\n",
       "            [ 0.4829,  0.7698,  0.3050,  ...,  0.8665,  0.6670,  0.1007],\n",
       "            [ 0.4829,  0.7698,  0.3050,  ...,  0.8665,  0.6670,  0.1007]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-1.0303, -2.1371, -2.8093,  ..., -4.1193, -1.0547, -2.8617],\n",
       "           [-0.8558, -1.1351, -2.9840,  ..., -3.0459, -0.0492, -0.9300],\n",
       "           [-2.1740, -1.9505, -1.5581,  ..., -2.8422, -2.2849, -2.7606],\n",
       "           ...,\n",
       "           [-1.4555, -1.7653, -1.9409,  ..., -3.4387, -1.3890, -4.9801],\n",
       "           [-1.4555, -1.7653, -1.9409,  ..., -3.4387, -1.3890, -4.9801],\n",
       "           [-1.4555, -1.7653, -1.9409,  ..., -3.4387, -1.3890, -4.9801]],\n",
       "  \n",
       "          [[-1.3842, -2.8297, -2.9471,  ..., -2.9895, -0.7553, -3.4010],\n",
       "           [-1.0369, -3.1156, -3.0315,  ..., -2.4502,  3.0296, -2.1686],\n",
       "           [ 0.0285, -2.6787, -1.6490,  ..., -3.2032,  2.0730, -1.2408],\n",
       "           ...,\n",
       "           [-1.5223, -2.0926, -1.8802,  ..., -3.6043, -1.0416, -4.7146],\n",
       "           [-1.5223, -2.0926, -1.8802,  ..., -3.6043, -1.0416, -4.7146],\n",
       "           [-1.5223, -2.0926, -1.8802,  ..., -3.6043, -1.0416, -4.7146]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.11.mlp.fc2': {'input': (tensor([[[-1.5618e-01, -3.4608e-02, -6.5104e-03,  ..., -3.9285e-05,\n",
       "             -1.5392e-01, -5.5749e-03],\n",
       "            [-1.6789e-01, -1.4568e-01, -3.8267e-03,  ..., -3.1378e-03,\n",
       "             -2.3638e-02, -1.6398e-01],\n",
       "            [-3.2045e-02, -4.9799e-02, -9.3067e-02,  ..., -5.9089e-03,\n",
       "             -2.5167e-02, -7.4931e-03],\n",
       "            ...,\n",
       "            [-1.0614e-01, -6.8506e-02, -5.0682e-02,  ..., -7.8183e-04,\n",
       "             -1.1471e-01, -2.9684e-07],\n",
       "            [-1.0614e-01, -6.8506e-02, -5.0682e-02,  ..., -7.8183e-04,\n",
       "             -1.1471e-01, -2.9684e-07],\n",
       "            [-1.0614e-01, -6.8506e-02, -5.0682e-02,  ..., -7.8183e-04,\n",
       "             -1.1471e-01, -2.9684e-07]],\n",
       "   \n",
       "           [[-1.1533e-01, -6.1312e-03, -4.2960e-03,  ..., -3.7608e-03,\n",
       "             -1.7004e-01, -9.0248e-04],\n",
       "            [-1.5559e-01, -2.4932e-03, -3.2881e-03,  ..., -1.7069e-02,\n",
       "              3.0263e+00, -3.2410e-02],\n",
       "            [ 1.4564e-02, -9.4267e-03, -8.1904e-02,  ..., -1.8489e-03,\n",
       "              2.0336e+00, -1.3341e-01],\n",
       "            ...,\n",
       "            [-9.7593e-02, -3.7893e-02, -5.6484e-02,  ..., -4.0539e-04,\n",
       "             -1.5515e-01, -1.4051e-06],\n",
       "            [-9.7593e-02, -3.7893e-02, -5.6484e-02,  ..., -4.0539e-04,\n",
       "             -1.5515e-01, -1.4051e-06],\n",
       "            [-9.7593e-02, -3.7893e-02, -5.6484e-02,  ..., -4.0539e-04,\n",
       "             -1.5515e-01, -1.4051e-06]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[ 0.3285,  0.1614,  0.4836,  ..., -0.2125,  0.5629, -0.4334],\n",
       "           [ 0.3723,  0.0491, -0.4048,  ...,  0.1954,  0.1563, -0.1983],\n",
       "           [ 0.1837,  0.2423, -0.1292,  ..., -0.0646,  0.0878, -0.6131],\n",
       "           ...,\n",
       "           [-0.3657, -0.1131,  0.6909,  ..., -0.4364, -0.1871, -0.1480],\n",
       "           [-0.3657, -0.1131,  0.6909,  ..., -0.4364, -0.1871, -0.1480],\n",
       "           [-0.3657, -0.1131,  0.6909,  ..., -0.4364, -0.1871, -0.1480]],\n",
       "  \n",
       "          [[-0.2025,  0.2479, -0.1028,  ...,  0.0825,  0.7941, -0.4895],\n",
       "           [-0.0270,  0.9736, -0.4141,  ..., -0.0739,  0.6048,  0.1493],\n",
       "           [ 0.6688,  0.9449,  0.1196,  ...,  0.4473,  0.4355,  0.0141],\n",
       "           ...,\n",
       "           [-0.5050,  0.1993,  0.8374,  ..., -0.2694, -0.2773, -0.0756],\n",
       "           [-0.5050,  0.1993,  0.8374,  ..., -0.2694, -0.2773, -0.0756],\n",
       "           [-0.5050,  0.1993,  0.8374,  ..., -0.2694, -0.2773, -0.0756]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.12.self_attn.q_proj': {'input': (tensor([[[-0.1367, -0.1297,  1.2198,  ...,  0.0535,  1.3749,  0.2219],\n",
       "            [-0.7254, -0.1452,  0.4424,  ...,  0.6521,  0.4910, -0.3252],\n",
       "            [ 0.2053, -0.5647, -0.1037,  ..., -0.3052,  0.1717, -0.2774],\n",
       "            ...,\n",
       "            [ 0.2627,  0.7577,  0.7437,  ...,  0.7170,  0.9091,  0.0682],\n",
       "            [ 0.2627,  0.7577,  0.7437,  ...,  0.7170,  0.9091,  0.0682],\n",
       "            [ 0.2627,  0.7577,  0.7437,  ...,  0.7170,  0.9091,  0.0682]],\n",
       "   \n",
       "           [[-0.7936, -0.3861,  0.1956,  ...,  0.3749,  0.5523,  0.6404],\n",
       "            [-0.8727,  0.5672, -0.3486,  ...,  0.4339,  0.4605,  0.5121],\n",
       "            [-0.2890, -0.8304,  1.2014,  ...,  0.6675,  0.5745, -0.1292],\n",
       "            ...,\n",
       "            [ 0.0993,  0.4705,  0.7831,  ...,  0.5918,  0.4848,  0.3656],\n",
       "            [ 0.0993,  0.4705,  0.7831,  ...,  0.5918,  0.4848,  0.3656],\n",
       "            [ 0.0993,  0.4705,  0.7831,  ...,  0.5918,  0.4848,  0.3656]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.4051,  0.7043,  0.7496,  ...,  0.8468,  0.1519, -0.1548],\n",
       "           [-0.0943,  0.4770,  1.3702,  ...,  0.1850, -1.4892,  0.4137],\n",
       "           [ 0.1654, -0.8483, -0.1585,  ..., -1.3381, -1.0313,  0.3290],\n",
       "           ...,\n",
       "           [ 0.3403,  1.2561, -1.7133,  ..., -0.1336,  0.9944, -1.4152],\n",
       "           [ 0.3403,  1.2561, -1.7133,  ..., -0.1336,  0.9944, -1.4152],\n",
       "           [ 0.3403,  1.2561, -1.7133,  ..., -0.1336,  0.9944, -1.4152]],\n",
       "  \n",
       "          [[-1.4101,  0.9729, -0.8398,  ..., -1.0201,  1.2726,  0.7266],\n",
       "           [-1.4202, -0.6323, -0.3061,  ..., -0.7313, -0.1128,  1.6675],\n",
       "           [-2.0441, -0.4065,  0.2081,  ..., -0.6392, -0.7320,  0.2079],\n",
       "           ...,\n",
       "           [-0.2450,  1.8283, -1.7602,  ..., -0.4600,  0.4639, -1.8005],\n",
       "           [-0.2450,  1.8283, -1.7602,  ..., -0.4600,  0.4639, -1.8005],\n",
       "           [-0.2450,  1.8283, -1.7602,  ..., -0.4600,  0.4639, -1.8005]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.12.self_attn.k_proj': {'input': (tensor([[[-0.1367, -0.1297,  1.2198,  ...,  0.0535,  1.3749,  0.2219],\n",
       "            [-0.7254, -0.1452,  0.4424,  ...,  0.6521,  0.4910, -0.3252],\n",
       "            [ 0.2053, -0.5647, -0.1037,  ..., -0.3052,  0.1717, -0.2774],\n",
       "            ...,\n",
       "            [ 0.2627,  0.7577,  0.7437,  ...,  0.7170,  0.9091,  0.0682],\n",
       "            [ 0.2627,  0.7577,  0.7437,  ...,  0.7170,  0.9091,  0.0682],\n",
       "            [ 0.2627,  0.7577,  0.7437,  ...,  0.7170,  0.9091,  0.0682]],\n",
       "   \n",
       "           [[-0.7936, -0.3861,  0.1956,  ...,  0.3749,  0.5523,  0.6404],\n",
       "            [-0.8727,  0.5672, -0.3486,  ...,  0.4339,  0.4605,  0.5121],\n",
       "            [-0.2890, -0.8304,  1.2014,  ...,  0.6675,  0.5745, -0.1292],\n",
       "            ...,\n",
       "            [ 0.0993,  0.4705,  0.7831,  ...,  0.5918,  0.4848,  0.3656],\n",
       "            [ 0.0993,  0.4705,  0.7831,  ...,  0.5918,  0.4848,  0.3656],\n",
       "            [ 0.0993,  0.4705,  0.7831,  ...,  0.5918,  0.4848,  0.3656]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.6088, -0.6980,  0.8144,  ...,  1.0087,  1.1614,  1.3400],\n",
       "           [ 0.4076, -0.6152, -0.1177,  ...,  0.1676, -0.3489,  1.4479],\n",
       "           [ 0.4147, -0.5874, -0.2905,  ..., -0.9881,  0.2787,  1.4636],\n",
       "           ...,\n",
       "           [ 0.0160, -1.0644, -0.6753,  ...,  1.9766,  1.3101, -1.5741],\n",
       "           [ 0.0160, -1.0644, -0.6753,  ...,  1.9766,  1.3101, -1.5741],\n",
       "           [ 0.0160, -1.0644, -0.6753,  ...,  1.9766,  1.3101, -1.5741]],\n",
       "  \n",
       "          [[-0.3408,  0.6030, -1.4726,  ..., -0.4752,  1.5292,  2.1509],\n",
       "           [-0.9665, -0.1187, -1.1197,  ...,  0.1276,  0.8743,  1.5391],\n",
       "           [-1.2482,  0.0894, -1.0515,  ...,  0.1994,  0.1621, -0.7394],\n",
       "           ...,\n",
       "           [-0.5948, -0.7283, -1.2788,  ...,  1.7447,  1.2703, -1.4716],\n",
       "           [-0.5948, -0.7283, -1.2788,  ...,  1.7447,  1.2703, -1.4716],\n",
       "           [-0.5948, -0.7283, -1.2788,  ...,  1.7447,  1.2703, -1.4716]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.12.self_attn.v_proj': {'input': (tensor([[[-0.1367, -0.1297,  1.2198,  ...,  0.0535,  1.3749,  0.2219],\n",
       "            [-0.7254, -0.1452,  0.4424,  ...,  0.6521,  0.4910, -0.3252],\n",
       "            [ 0.2053, -0.5647, -0.1037,  ..., -0.3052,  0.1717, -0.2774],\n",
       "            ...,\n",
       "            [ 0.2627,  0.7577,  0.7437,  ...,  0.7170,  0.9091,  0.0682],\n",
       "            [ 0.2627,  0.7577,  0.7437,  ...,  0.7170,  0.9091,  0.0682],\n",
       "            [ 0.2627,  0.7577,  0.7437,  ...,  0.7170,  0.9091,  0.0682]],\n",
       "   \n",
       "           [[-0.7936, -0.3861,  0.1956,  ...,  0.3749,  0.5523,  0.6404],\n",
       "            [-0.8727,  0.5672, -0.3486,  ...,  0.4339,  0.4605,  0.5121],\n",
       "            [-0.2890, -0.8304,  1.2014,  ...,  0.6675,  0.5745, -0.1292],\n",
       "            ...,\n",
       "            [ 0.0993,  0.4705,  0.7831,  ...,  0.5918,  0.4848,  0.3656],\n",
       "            [ 0.0993,  0.4705,  0.7831,  ...,  0.5918,  0.4848,  0.3656],\n",
       "            [ 0.0993,  0.4705,  0.7831,  ...,  0.5918,  0.4848,  0.3656]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.2640,  0.1289,  1.1141,  ...,  0.4944, -1.0956, -1.4840],\n",
       "           [ 0.4395, -0.0941,  0.9052,  ...,  0.3718, -0.9447, -1.2192],\n",
       "           [-0.4261, -0.1070, -1.0372,  ..., -0.2438,  0.1493, -0.2286],\n",
       "           ...,\n",
       "           [-0.4552,  0.0070,  0.2764,  ...,  1.0387, -0.7462, -0.1319],\n",
       "           [-0.4552,  0.0070,  0.2764,  ...,  1.0387, -0.7462, -0.1319],\n",
       "           [-0.4552,  0.0070,  0.2764,  ...,  1.0387, -0.7462, -0.1319]],\n",
       "  \n",
       "          [[-0.9766, -0.1156,  0.4075,  ...,  0.6985, -1.3263, -1.0489],\n",
       "           [-1.0902,  0.1187,  0.4370,  ...,  0.4131, -0.9958, -0.8800],\n",
       "           [-1.7212,  0.0902,  0.0361,  ...,  0.5457, -0.0563, -0.1076],\n",
       "           ...,\n",
       "           [-1.0918,  0.4774,  0.0753,  ...,  1.2551, -0.5355, -0.2171],\n",
       "           [-1.0918,  0.4774,  0.0753,  ...,  1.2551, -0.5355, -0.2171],\n",
       "           [-1.0918,  0.4774,  0.0753,  ...,  1.2551, -0.5355, -0.2171]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.12.self_attn.out_proj': {'input': (tensor([[[-0.0968,  0.2768, -0.1393,  ..., -0.1215, -0.1594, -0.0210],\n",
       "            [-0.0278,  0.1823,  0.0379,  ..., -0.2323,  0.0700,  0.0586],\n",
       "            [-0.0651,  0.0294,  0.2859,  ..., -0.2114, -0.0928, -0.0760],\n",
       "            ...,\n",
       "            [-0.3070,  0.1581,  0.4191,  ..., -0.2105,  0.0249,  0.1337],\n",
       "            [-0.3070,  0.1581,  0.4191,  ..., -0.2105,  0.0249,  0.1337],\n",
       "            [-0.3070,  0.1581,  0.4191,  ..., -0.2105,  0.0249,  0.1337]],\n",
       "   \n",
       "           [[-0.3771,  0.0566,  0.0404,  ..., -0.1556, -0.2380, -0.0272],\n",
       "            [-0.7742,  0.0072,  0.3118,  ...,  0.0185, -0.2554, -0.2340],\n",
       "            [-0.6872,  0.0380,  0.3949,  ..., -0.0340, -0.1984, -0.1325],\n",
       "            ...,\n",
       "            [-0.5168,  0.0748,  0.2578,  ..., -0.2823, -0.0223,  0.1031],\n",
       "            [-0.5168,  0.0748,  0.2578,  ..., -0.2823, -0.0223,  0.1031],\n",
       "            [-0.5168,  0.0748,  0.2578,  ..., -0.2823, -0.0223,  0.1031]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[ 0.1850,  0.3206,  0.3086,  ..., -0.4763, -0.4409, -0.5888],\n",
       "           [ 0.3834,  0.2579,  0.1024,  ..., -0.4982, -0.2697, -0.4286],\n",
       "           [-0.4938, -0.3311, -0.4309,  ..., -0.4781, -0.4777, -0.6570],\n",
       "           ...,\n",
       "           [-0.1731,  0.2475, -0.0022,  ..., -0.2491, -0.2609, -0.2902],\n",
       "           [-0.1731,  0.2475, -0.0022,  ..., -0.2491, -0.2609, -0.2902],\n",
       "           [-0.1731,  0.2475, -0.0022,  ..., -0.2491, -0.2609, -0.2902]],\n",
       "  \n",
       "          [[ 0.2173,  0.0453,  0.1217,  ..., -0.6463, -0.6348, -0.8479],\n",
       "           [ 0.0474,  0.1580, -0.0198,  ..., -0.8221, -0.5927, -0.4530],\n",
       "           [-0.0012,  0.2263, -0.2729,  ..., -0.8864, -0.4448, -0.1607],\n",
       "           ...,\n",
       "           [-0.0414,  0.5267, -0.3018,  ..., -0.3103, -0.4214, -0.2305],\n",
       "           [-0.0414,  0.5267, -0.3018,  ..., -0.3103, -0.4214, -0.2305],\n",
       "           [-0.0414,  0.5267, -0.3018,  ..., -0.3103, -0.4214, -0.2305]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.12.mlp.fc1': {'input': (tensor([[[ 0.3121, -0.0623,  2.4488,  ..., -0.0316,  1.4639,  0.0220],\n",
       "            [-0.0216, -0.0890,  1.2684,  ...,  0.6722,  0.4064, -0.6470],\n",
       "            [ 0.2112, -1.0792,  0.1011,  ..., -0.4242, -0.0687, -0.5904],\n",
       "            ...,\n",
       "            [ 0.4815,  0.8384,  1.5179,  ...,  1.1461,  1.1224,  0.2351],\n",
       "            [ 0.4815,  0.8384,  1.5179,  ...,  1.1461,  1.1224,  0.2351],\n",
       "            [ 0.4815,  0.8384,  1.5179,  ...,  1.1461,  1.1224,  0.2351]],\n",
       "   \n",
       "           [[-0.2908, -0.6365,  0.9556,  ...,  0.3468,  0.2989,  0.4684],\n",
       "            [-0.4397,  0.6084,  0.1490,  ...,  0.2614,  0.1745,  0.5879],\n",
       "            [ 0.0503, -1.0050,  1.8075,  ...,  0.5381,  0.4475, -0.0208],\n",
       "            ...,\n",
       "            [ 0.3816,  0.6382,  1.3756,  ...,  0.9355,  0.4761,  0.6668],\n",
       "            [ 0.3816,  0.6382,  1.3756,  ...,  0.9355,  0.4761,  0.6668],\n",
       "            [ 0.3816,  0.6382,  1.3756,  ...,  0.9355,  0.4761,  0.6668]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-2.9091,  1.6085, -3.1226,  ..., -2.2736, -2.6979, -1.9589],\n",
       "           [-4.0265,  1.4801, -2.0561,  ..., -1.7749, -2.1461, -1.4853],\n",
       "           [-2.4120, -2.7692, -2.0694,  ..., -2.2771, -2.0611, -2.6937],\n",
       "           ...,\n",
       "           [ 0.6902, -2.5306, -0.9006,  ..., -2.3191, -2.1515, -0.5068],\n",
       "           [ 0.6902, -2.5306, -0.9006,  ..., -2.3191, -2.1515, -0.5068],\n",
       "           [ 0.6902, -2.5306, -0.9006,  ..., -2.3191, -2.1515, -0.5068]],\n",
       "  \n",
       "          [[-2.8532, -0.0582, -2.6718,  ..., -1.1211, -2.6028, -0.6549],\n",
       "           [-2.2141,  0.2088, -2.6869,  ..., -1.5658, -2.2977, -0.8040],\n",
       "           [-1.4679, -0.6768, -2.5176,  ..., -1.2262, -2.1821, -0.9990],\n",
       "           ...,\n",
       "           [ 0.6028, -2.7740, -1.2013,  ..., -2.1179, -2.1760, -0.7021],\n",
       "           [ 0.6028, -2.7740, -1.2013,  ..., -2.1179, -2.1760, -0.7021],\n",
       "           [ 0.6028, -2.7740, -1.2013,  ..., -2.1179, -2.1760, -0.7021]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.12.mlp.fc2': {'input': (tensor([[[-4.8299e-03,  1.5217e+00, -2.4355e-03,  ..., -2.5815e-02,\n",
       "             -8.9390e-03, -4.9036e-02],\n",
       "            [-6.1919e-05,  1.3771e+00, -4.0741e-02,  ..., -6.7458e-02,\n",
       "             -3.3968e-02, -1.0231e-01],\n",
       "            [-1.8730e-02, -7.3122e-03, -3.9684e-02,  ..., -2.5616e-02,\n",
       "             -4.0343e-02, -9.0432e-03],\n",
       "            ...,\n",
       "            [ 5.2103e-01, -1.3959e-02, -1.6574e-01,  ..., -2.3291e-02,\n",
       "             -3.3588e-02, -1.5518e-01],\n",
       "            [ 5.2103e-01, -1.3959e-02, -1.6574e-01,  ..., -2.3291e-02,\n",
       "             -3.3588e-02, -1.5518e-01],\n",
       "            [ 5.2103e-01, -1.3959e-02, -1.6574e-01,  ..., -2.3291e-02,\n",
       "             -3.3588e-02, -1.5518e-01]],\n",
       "   \n",
       "           [[-5.7196e-03, -2.7767e-02, -9.6057e-03,  ..., -1.4720e-01,\n",
       "             -1.1569e-02, -1.6787e-01],\n",
       "            [-2.9417e-02,  1.2169e-01, -9.2165e-03,  ..., -9.2105e-02,\n",
       "             -2.4452e-02, -1.6949e-01],\n",
       "            [-1.0454e-01, -1.6875e-01, -1.4430e-02,  ..., -1.3518e-01,\n",
       "             -3.1500e-02, -1.5889e-01],\n",
       "            ...,\n",
       "            [ 4.3805e-01, -7.2104e-03, -1.3815e-01,  ..., -3.5998e-02,\n",
       "             -3.1907e-02, -1.6948e-01],\n",
       "            [ 4.3805e-01, -7.2104e-03, -1.3815e-01,  ..., -3.5998e-02,\n",
       "             -3.1907e-02, -1.6948e-01],\n",
       "            [ 4.3805e-01, -7.2104e-03, -1.3815e-01,  ..., -3.5998e-02,\n",
       "             -3.1907e-02, -1.6948e-01]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[ 0.0518, -0.1120, -0.5501,  ...,  0.5964, -0.1012,  0.1865],\n",
       "           [ 0.4268, -0.2452,  0.0931,  ...,  0.1001,  0.4269,  0.4363],\n",
       "           [ 0.2890,  0.2462,  0.1353,  ...,  0.1566,  0.2484,  0.3568],\n",
       "           ...,\n",
       "           [-0.1278,  0.0213,  0.2902,  ...,  0.2977,  0.0493, -0.1777],\n",
       "           [-0.1278,  0.0213,  0.2902,  ...,  0.2977,  0.0493, -0.1777],\n",
       "           [-0.1278,  0.0213,  0.2902,  ...,  0.2977,  0.0493, -0.1777]],\n",
       "  \n",
       "          [[-0.9406,  0.2308,  0.3530,  ..., -0.1063,  0.1086,  0.1318],\n",
       "           [ 0.6284,  0.5415, -0.3356,  ...,  0.4876,  0.6735, -0.0244],\n",
       "           [ 0.8777,  0.3257,  0.4239,  ..., -0.2712,  0.4698,  0.5410],\n",
       "           ...,\n",
       "           [-0.0535,  0.3839,  0.3738,  ...,  0.4318, -0.2350, -0.2581],\n",
       "           [-0.0535,  0.3839,  0.3738,  ...,  0.4318, -0.2350, -0.2581],\n",
       "           [-0.0535,  0.3839,  0.3738,  ...,  0.4318, -0.2350, -0.2581]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.13.self_attn.q_proj': {'input': (tensor([[[ 0.1363,  0.0628,  1.1707,  ...,  0.1764,  0.9729, -0.1141],\n",
       "            [ 0.1123, -0.0965,  0.7987,  ...,  0.4095,  0.5972, -0.3751],\n",
       "            [ 0.1668, -0.5923, -0.2102,  ..., -0.4089,  0.0066, -0.5360],\n",
       "            ...,\n",
       "            [ 0.2327,  0.9272,  1.0723,  ...,  0.7949,  0.8347, -0.1683],\n",
       "            [ 0.2327,  0.9272,  1.0723,  ...,  0.7949,  0.8347, -0.1683],\n",
       "            [ 0.2327,  0.9272,  1.0723,  ...,  0.7949,  0.8347, -0.1683]],\n",
       "   \n",
       "           [[-1.2027, -0.1889,  0.7340,  ...,  0.0513,  0.2265,  0.1833],\n",
       "            [-0.2914,  1.0729, -0.5169,  ...,  0.2988,  0.4844,  0.1592],\n",
       "            [ 0.4024, -0.4299,  1.5406,  ...,  0.1059,  0.5846,  0.0850],\n",
       "            ...,\n",
       "            [ 0.1515,  0.9124,  0.9959,  ...,  0.6854,  0.1994,  0.1302],\n",
       "            [ 0.1515,  0.9124,  0.9959,  ...,  0.6854,  0.1994,  0.1302],\n",
       "            [ 0.1515,  0.9124,  0.9959,  ...,  0.6854,  0.1994,  0.1302]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-1.4555, -0.0670, -1.1594,  ..., -0.5197,  0.7338,  1.1439],\n",
       "           [-0.3231, -0.2566,  0.0492,  ..., -1.2660,  0.7076,  0.9399],\n",
       "           [ 0.5700,  1.1013, -0.0406,  ...,  1.0042, -0.0069, -0.3339],\n",
       "           ...,\n",
       "           [-0.9470,  0.5755, -0.5022,  ...,  1.2015, -0.8561,  2.0478],\n",
       "           [-0.9470,  0.5755, -0.5022,  ...,  1.2015, -0.8561,  2.0478],\n",
       "           [-0.9470,  0.5755, -0.5022,  ...,  1.2015, -0.8561,  2.0478]],\n",
       "  \n",
       "          [[-2.1701, -0.5767, -2.9423,  ..., -1.2618,  0.6473,  0.5997],\n",
       "           [-1.4683, -0.3763, -2.3714,  ..., -0.4589,  0.4255, -0.4460],\n",
       "           [-0.2989, -0.8122, -1.5946,  ...,  1.8498,  0.4472,  0.1276],\n",
       "           ...,\n",
       "           [-0.1380,  0.0707, -0.2194,  ...,  1.7997, -0.9397,  2.1536],\n",
       "           [-0.1380,  0.0707, -0.2194,  ...,  1.7997, -0.9397,  2.1536],\n",
       "           [-0.1380,  0.0707, -0.2194,  ...,  1.7997, -0.9397,  2.1536]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.13.self_attn.k_proj': {'input': (tensor([[[ 0.1363,  0.0628,  1.1707,  ...,  0.1764,  0.9729, -0.1141],\n",
       "            [ 0.1123, -0.0965,  0.7987,  ...,  0.4095,  0.5972, -0.3751],\n",
       "            [ 0.1668, -0.5923, -0.2102,  ..., -0.4089,  0.0066, -0.5360],\n",
       "            ...,\n",
       "            [ 0.2327,  0.9272,  1.0723,  ...,  0.7949,  0.8347, -0.1683],\n",
       "            [ 0.2327,  0.9272,  1.0723,  ...,  0.7949,  0.8347, -0.1683],\n",
       "            [ 0.2327,  0.9272,  1.0723,  ...,  0.7949,  0.8347, -0.1683]],\n",
       "   \n",
       "           [[-1.2027, -0.1889,  0.7340,  ...,  0.0513,  0.2265,  0.1833],\n",
       "            [-0.2914,  1.0729, -0.5169,  ...,  0.2988,  0.4844,  0.1592],\n",
       "            [ 0.4024, -0.4299,  1.5406,  ...,  0.1059,  0.5846,  0.0850],\n",
       "            ...,\n",
       "            [ 0.1515,  0.9124,  0.9959,  ...,  0.6854,  0.1994,  0.1302],\n",
       "            [ 0.1515,  0.9124,  0.9959,  ...,  0.6854,  0.1994,  0.1302],\n",
       "            [ 0.1515,  0.9124,  0.9959,  ...,  0.6854,  0.1994,  0.1302]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-2.5261e+00, -5.0574e-01,  7.3800e-01,  ..., -2.0416e+00,\n",
       "             1.7092e-04,  1.0431e+00],\n",
       "           [-1.2188e+00, -1.7386e+00,  1.2333e+00,  ..., -1.8109e+00,\n",
       "             1.0609e+00,  1.0413e+00],\n",
       "           [ 7.8120e-01, -3.4399e-01, -1.9655e-01,  ..., -3.8174e-01,\n",
       "             3.5932e-01,  7.7679e-01],\n",
       "           ...,\n",
       "           [-3.8708e-01, -1.9519e-02,  1.0518e+00,  ...,  9.3791e-01,\n",
       "             4.9448e-01,  1.1432e+00],\n",
       "           [-3.8708e-01, -1.9519e-02,  1.0518e+00,  ...,  9.3791e-01,\n",
       "             4.9448e-01,  1.1432e+00],\n",
       "           [-3.8708e-01, -1.9519e-02,  1.0518e+00,  ...,  9.3791e-01,\n",
       "             4.9447e-01,  1.1432e+00]],\n",
       "  \n",
       "          [[-3.2099e+00, -1.7024e+00, -1.4473e+00,  ..., -3.1446e-01,\n",
       "            -1.0451e+00,  1.0086e-01],\n",
       "           [-2.4581e+00, -1.1855e+00, -1.4792e+00,  ...,  1.3739e-01,\n",
       "            -4.9600e-01,  6.7163e-02],\n",
       "           [-6.4885e-01, -2.5526e+00, -1.2026e+00,  ...,  1.8582e+00,\n",
       "             1.5747e+00, -6.8730e-01],\n",
       "           ...,\n",
       "           [ 2.7739e-01, -2.9021e-01,  1.6161e+00,  ...,  1.6133e+00,\n",
       "             1.8909e-01,  9.2973e-01],\n",
       "           [ 2.7739e-01, -2.9021e-01,  1.6161e+00,  ...,  1.6133e+00,\n",
       "             1.8909e-01,  9.2973e-01],\n",
       "           [ 2.7739e-01, -2.9021e-01,  1.6161e+00,  ...,  1.6133e+00,\n",
       "             1.8909e-01,  9.2973e-01]]], grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.13.self_attn.v_proj': {'input': (tensor([[[ 0.1363,  0.0628,  1.1707,  ...,  0.1764,  0.9729, -0.1141],\n",
       "            [ 0.1123, -0.0965,  0.7987,  ...,  0.4095,  0.5972, -0.3751],\n",
       "            [ 0.1668, -0.5923, -0.2102,  ..., -0.4089,  0.0066, -0.5360],\n",
       "            ...,\n",
       "            [ 0.2327,  0.9272,  1.0723,  ...,  0.7949,  0.8347, -0.1683],\n",
       "            [ 0.2327,  0.9272,  1.0723,  ...,  0.7949,  0.8347, -0.1683],\n",
       "            [ 0.2327,  0.9272,  1.0723,  ...,  0.7949,  0.8347, -0.1683]],\n",
       "   \n",
       "           [[-1.2027, -0.1889,  0.7340,  ...,  0.0513,  0.2265,  0.1833],\n",
       "            [-0.2914,  1.0729, -0.5169,  ...,  0.2988,  0.4844,  0.1592],\n",
       "            [ 0.4024, -0.4299,  1.5406,  ...,  0.1059,  0.5846,  0.0850],\n",
       "            ...,\n",
       "            [ 0.1515,  0.9124,  0.9959,  ...,  0.6854,  0.1994,  0.1302],\n",
       "            [ 0.1515,  0.9124,  0.9959,  ...,  0.6854,  0.1994,  0.1302],\n",
       "            [ 0.1515,  0.9124,  0.9959,  ...,  0.6854,  0.1994,  0.1302]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.8422, -0.1537, -0.0514,  ...,  0.8596, -0.7423,  0.7181],\n",
       "           [ 1.4290, -0.1005, -0.1334,  ...,  0.1624, -0.9094, -0.2482],\n",
       "           [ 0.8160,  0.9242,  0.2723,  ..., -0.4988,  0.0401, -0.6024],\n",
       "           ...,\n",
       "           [ 1.6246, -0.9785,  0.8742,  ..., -1.3254,  1.1260,  0.4760],\n",
       "           [ 1.6246, -0.9785,  0.8742,  ..., -1.3254,  1.1260,  0.4760],\n",
       "           [ 1.6246, -0.9785,  0.8742,  ..., -1.3254,  1.1260,  0.4760]],\n",
       "  \n",
       "          [[ 0.2799, -0.9079,  0.2280,  ..., -0.1764,  0.1616,  1.3448],\n",
       "           [-0.3561, -0.0221,  1.2648,  ..., -0.8366,  1.8846,  1.0944],\n",
       "           [ 0.6103,  1.1147,  0.3968,  ..., -1.3571,  1.7224,  0.5064],\n",
       "           ...,\n",
       "           [ 1.6194, -0.8149,  1.1403,  ..., -1.6499,  1.2805,  0.5511],\n",
       "           [ 1.6194, -0.8149,  1.1403,  ..., -1.6499,  1.2805,  0.5511],\n",
       "           [ 1.6194, -0.8149,  1.1403,  ..., -1.6499,  1.2805,  0.5511]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.13.self_attn.out_proj': {'input': (tensor([[[ 0.5000, -0.1652, -0.1345,  ...,  0.0275, -0.4154, -0.0408],\n",
       "            [ 0.6162,  0.2593,  0.0891,  ..., -0.0489, -0.4134, -0.3339],\n",
       "            [ 0.1945, -0.0568,  0.2509,  ..., -0.0869,  0.1634,  0.3114],\n",
       "            ...,\n",
       "            [ 0.1512, -0.0431,  0.1433,  ..., -0.3483, -0.0827, -0.2056],\n",
       "            [ 0.1512, -0.0431,  0.1433,  ..., -0.3483, -0.0827, -0.2056],\n",
       "            [ 0.1512, -0.0431,  0.1433,  ..., -0.3483, -0.0827, -0.2056]],\n",
       "   \n",
       "           [[ 0.0319, -0.4585,  0.0585,  ...,  0.0184, -0.1534,  0.3070],\n",
       "            [-0.2745, -0.2289,  0.3205,  ...,  0.0391, -0.3074,  0.2558],\n",
       "            [ 0.0950,  0.0281, -0.2778,  ...,  0.0585,  0.1964,  0.2756],\n",
       "            ...,\n",
       "            [ 0.1763,  0.0216, -0.1189,  ..., -0.5149, -0.0715, -0.1673],\n",
       "            [ 0.1763,  0.0216, -0.1189,  ..., -0.5149, -0.0715, -0.1673],\n",
       "            [ 0.1763,  0.0216, -0.1189,  ..., -0.5149, -0.0715, -0.1673]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.2042, -0.2902, -0.1240,  ...,  0.5315, -0.7017, -0.0597],\n",
       "           [ 0.0807, -0.2640, -0.1185,  ...,  0.2512, -0.1654,  0.1496],\n",
       "           [ 0.4615, -0.0902, -0.3632,  ...,  0.4641, -0.1104,  0.0859],\n",
       "           ...,\n",
       "           [-0.0742, -0.4243,  0.0232,  ...,  0.2478, -0.2049,  0.2156],\n",
       "           [-0.0742, -0.4243,  0.0232,  ...,  0.2478, -0.2049,  0.2156],\n",
       "           [-0.0742, -0.4243,  0.0232,  ...,  0.2478, -0.2049,  0.2156]],\n",
       "  \n",
       "          [[ 0.5242, -0.9347,  0.4154,  ...,  0.5094, -0.8019,  0.0303],\n",
       "           [ 0.6722, -0.6585,  0.4171,  ..., -0.0184, -0.8417, -0.0582],\n",
       "           [ 0.4124, -0.3551, -0.2116,  ...,  0.2372, -0.1407,  0.0418],\n",
       "           ...,\n",
       "           [ 0.2146, -0.6813, -0.0046,  ...,  0.4157, -0.2176,  0.2176],\n",
       "           [ 0.2146, -0.6813, -0.0046,  ...,  0.4157, -0.2176,  0.2176],\n",
       "           [ 0.2146, -0.6813, -0.0046,  ...,  0.4157, -0.2176,  0.2176]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.13.mlp.fc1': {'input': (tensor([[[ 0.0858, -0.0100,  0.9854,  ...,  1.0173,  0.4970, -0.4407],\n",
       "            [ 0.2567, -0.2627,  0.6010,  ...,  1.1920,  0.4778, -0.5509],\n",
       "            [ 0.4824, -0.6261, -0.5859,  ...,  0.0790, -0.1685, -0.8413],\n",
       "            ...,\n",
       "            [ 0.2677,  1.1619,  0.9935,  ...,  1.5719,  0.8105, -0.3367],\n",
       "            [ 0.2677,  1.1619,  0.9935,  ...,  1.5719,  0.8105, -0.3367],\n",
       "            [ 0.2677,  1.1619,  0.9935,  ...,  1.5719,  0.8105, -0.3367]],\n",
       "   \n",
       "           [[-0.6386, -0.8897,  0.9866,  ...,  0.7762, -0.4453,  0.0305],\n",
       "            [ 0.2263,  0.9980, -0.2819,  ...,  0.7303, -0.1618, -0.0814],\n",
       "            [ 0.6859, -0.6996,  1.2974,  ...,  0.6579,  0.5059, -0.0807],\n",
       "            ...,\n",
       "            [ 0.3013,  0.9807,  0.8959,  ...,  1.4935,  0.0315,  0.0538],\n",
       "            [ 0.3013,  0.9807,  0.8959,  ...,  1.4935,  0.0315,  0.0538],\n",
       "            [ 0.3013,  0.9807,  0.8959,  ...,  1.4935,  0.0315,  0.0538]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.9480, -2.6778, -1.2435,  ..., -1.6841, -2.6627, -2.7126],\n",
       "           [-1.9553, -2.4295, -0.6886,  ..., -1.8038, -1.9210, -2.3788],\n",
       "           [-3.1387, -2.9598, -1.8745,  ..., -1.5286, -2.2343, -3.0314],\n",
       "           ...,\n",
       "           [-2.5544, -2.3974,  1.1050,  ..., -1.2723, -2.9875,  0.5821],\n",
       "           [-2.5544, -2.3974,  1.1050,  ..., -1.2723, -2.9875,  0.5821],\n",
       "           [-2.5544, -2.3974,  1.1050,  ..., -1.2723, -2.9875,  0.5821]],\n",
       "  \n",
       "          [[-2.7051, -4.3429, -1.4486,  ..., -1.7782, -2.5968, -1.7196],\n",
       "           [-3.0528, -3.4612, -1.7334,  ..., -1.7805, -1.0122, -1.5844],\n",
       "           [-3.5660, -3.7808, -1.8297,  ..., -1.1420, -1.4375, -2.8203],\n",
       "           ...,\n",
       "           [-2.0361, -2.5362,  0.9245,  ..., -1.4274, -2.9685,  1.0335],\n",
       "           [-2.0361, -2.5362,  0.9245,  ..., -1.4274, -2.9685,  1.0335],\n",
       "           [-2.0361, -2.5362,  0.9245,  ..., -1.4274, -2.9685,  1.0335]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.13.mlp.fc2': {'input': (tensor([[[-1.6278e-01, -9.4494e-03, -1.3307e-01,  ..., -7.7752e-02,\n",
       "             -9.8483e-03, -8.5807e-03],\n",
       "            [-4.9362e-02, -1.7957e-02, -1.6913e-01,  ..., -6.4336e-02,\n",
       "             -5.2543e-02, -2.0271e-02],\n",
       "            [-2.3067e-03, -4.1290e-03, -5.7048e-02,  ..., -9.6794e-02,\n",
       "             -2.8153e-02, -3.2886e-03],\n",
       "            ...,\n",
       "            [-1.3129e-02, -1.9395e-02,  9.5608e-01,  ..., -1.2954e-01,\n",
       "             -3.7848e-03,  4.1893e-01],\n",
       "            [-1.3129e-02, -1.9395e-02,  9.5608e-01,  ..., -1.2954e-01,\n",
       "             -3.7848e-03,  4.1893e-01],\n",
       "            [-1.3130e-02, -1.9395e-02,  9.5608e-01,  ..., -1.2954e-01,\n",
       "             -3.7848e-03,  4.1893e-01]],\n",
       "   \n",
       "           [[-8.7637e-03, -1.2296e-05, -1.0703e-01,  ..., -6.7090e-02,\n",
       "             -1.1754e-02, -7.3642e-02],\n",
       "            [-3.0682e-03, -7.1690e-04, -7.2072e-02,  ..., -6.6842e-02,\n",
       "             -1.5778e-01, -8.9791e-02],\n",
       "            [-4.7377e-04, -1.9167e-04, -6.1607e-02,  ..., -1.4492e-01,\n",
       "             -1.0846e-01, -6.3027e-03],\n",
       "            ...,\n",
       "            [-4.2366e-02, -1.3760e-02,  7.6013e-01,  ..., -1.0976e-01,\n",
       "             -4.0178e-03,  8.7763e-01],\n",
       "            [-4.2366e-02, -1.3760e-02,  7.6013e-01,  ..., -1.0976e-01,\n",
       "             -4.0178e-03,  8.7763e-01],\n",
       "            [-4.2366e-02, -1.3760e-02,  7.6013e-01,  ..., -1.0976e-01,\n",
       "             -4.0178e-03,  8.7763e-01]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[ 0.0326,  0.3712, -0.3772,  ...,  0.1033,  0.2280, -0.0757],\n",
       "           [ 0.0339,  0.4295, -0.0968,  ..., -0.0095,  0.0330,  0.0661],\n",
       "           [-0.1620, -0.0152,  0.0271,  ...,  0.0392,  0.2361, -0.2468],\n",
       "           ...,\n",
       "           [-0.3389, -0.0968, -0.6409,  ..., -0.5614,  0.4159, -0.4627],\n",
       "           [-0.3389, -0.0968, -0.6409,  ..., -0.5614,  0.4159, -0.4627],\n",
       "           [-0.3389, -0.0968, -0.6409,  ..., -0.5614,  0.4159, -0.4627]],\n",
       "  \n",
       "          [[-0.2599,  0.5179,  0.4350,  ..., -0.2536,  0.3205, -0.1400],\n",
       "           [-0.7334, -0.1937,  0.4052,  ..., -0.4157,  0.3740, -0.2759],\n",
       "           [ 0.2528, -0.0583,  0.3272,  ..., -0.2937, -0.2159, -0.2821],\n",
       "           ...,\n",
       "           [-0.2057, -0.5663, -0.4184,  ..., -0.4601,  0.7131, -0.7682],\n",
       "           [-0.2057, -0.5663, -0.4184,  ..., -0.4601,  0.7131, -0.7682],\n",
       "           [-0.2057, -0.5663, -0.4184,  ..., -0.4601,  0.7131, -0.7682]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.14.self_attn.q_proj': {'input': (tensor([[[ 2.0774e-02,  4.7471e-02,  6.7689e-01,  ...,  6.6072e-01,\n",
       "              5.7911e-01, -1.3903e-01],\n",
       "            [ 2.5920e-01, -2.6179e-02,  5.3987e-01,  ...,  7.3238e-01,\n",
       "              4.3111e-01, -1.0243e-01],\n",
       "            [ 4.1413e-01, -7.9960e-01, -4.7118e-01,  ..., -1.7927e-01,\n",
       "             -1.0231e-02, -5.7037e-01],\n",
       "            ...,\n",
       "            [ 7.8803e-02,  6.1773e-01,  6.4992e-01,  ...,  7.6662e-01,\n",
       "              7.7503e-01, -1.8809e-01],\n",
       "            [ 7.8803e-02,  6.1773e-01,  6.4992e-01,  ...,  7.6662e-01,\n",
       "              7.7503e-01, -1.8809e-01],\n",
       "            [ 7.8803e-02,  6.1773e-01,  6.4992e-01,  ...,  7.6662e-01,\n",
       "              7.7503e-01, -1.8809e-01]],\n",
       "   \n",
       "           [[-1.0962e+00, -5.7424e-01,  1.2710e+00,  ...,  2.1311e-01,\n",
       "             -1.7677e-01,  1.7835e-01],\n",
       "            [-3.4985e-01,  4.2298e-01,  1.2291e-01,  ...,  8.4093e-02,\n",
       "              8.7995e-02,  8.7822e-03],\n",
       "            [ 1.0026e+00, -8.8903e-01,  1.5780e+00,  ...,  9.6190e-02,\n",
       "              2.7781e-01, -1.0388e-03],\n",
       "            ...,\n",
       "            [ 1.7316e-01,  2.6688e-01,  6.4858e-01,  ...,  7.2539e-01,\n",
       "              2.7617e-01, -3.7482e-02],\n",
       "            [ 1.7316e-01,  2.6688e-01,  6.4858e-01,  ...,  7.2539e-01,\n",
       "              2.7617e-01, -3.7482e-02],\n",
       "            [ 1.7316e-01,  2.6688e-01,  6.4858e-01,  ...,  7.2539e-01,\n",
       "              2.7617e-01, -3.7482e-02]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-1.8552, -0.6661, -0.7695,  ...,  1.1101, -1.0345, -0.6959],\n",
       "           [ 0.0625,  0.9845, -1.4053,  ...,  1.8600, -1.8827, -0.3826],\n",
       "           [ 0.3433, -0.8771, -0.6248,  ...,  0.1940, -0.8458, -0.1933],\n",
       "           ...,\n",
       "           [-0.6800, -0.6060,  0.6073,  ..., -0.6823, -0.3054, -0.7815],\n",
       "           [-0.6800, -0.6060,  0.6073,  ..., -0.6823, -0.3054, -0.7815],\n",
       "           [-0.6800, -0.6060,  0.6073,  ..., -0.6823, -0.3054, -0.7815]],\n",
       "  \n",
       "          [[ 0.1488, -0.6853,  0.0893,  ..., -1.4452, -0.6302, -0.7146],\n",
       "           [-0.1474,  0.1880,  0.2631,  ..., -1.4386,  0.2472,  0.9197],\n",
       "           [-0.0260,  0.4052, -1.2385,  ..., -2.3831, -0.6661, -1.0926],\n",
       "           ...,\n",
       "           [-0.5187, -0.6258,  0.7344,  ..., -1.0486,  0.2609, -0.7157],\n",
       "           [-0.5187, -0.6258,  0.7344,  ..., -1.0486,  0.2609, -0.7157],\n",
       "           [-0.5187, -0.6258,  0.7344,  ..., -1.0486,  0.2609, -0.7157]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.14.self_attn.k_proj': {'input': (tensor([[[ 2.0774e-02,  4.7471e-02,  6.7689e-01,  ...,  6.6072e-01,\n",
       "              5.7911e-01, -1.3903e-01],\n",
       "            [ 2.5920e-01, -2.6179e-02,  5.3987e-01,  ...,  7.3238e-01,\n",
       "              4.3111e-01, -1.0243e-01],\n",
       "            [ 4.1413e-01, -7.9960e-01, -4.7118e-01,  ..., -1.7927e-01,\n",
       "             -1.0231e-02, -5.7037e-01],\n",
       "            ...,\n",
       "            [ 7.8803e-02,  6.1773e-01,  6.4992e-01,  ...,  7.6662e-01,\n",
       "              7.7503e-01, -1.8809e-01],\n",
       "            [ 7.8803e-02,  6.1773e-01,  6.4992e-01,  ...,  7.6662e-01,\n",
       "              7.7503e-01, -1.8809e-01],\n",
       "            [ 7.8803e-02,  6.1773e-01,  6.4992e-01,  ...,  7.6662e-01,\n",
       "              7.7503e-01, -1.8809e-01]],\n",
       "   \n",
       "           [[-1.0962e+00, -5.7424e-01,  1.2710e+00,  ...,  2.1311e-01,\n",
       "             -1.7677e-01,  1.7835e-01],\n",
       "            [-3.4985e-01,  4.2298e-01,  1.2291e-01,  ...,  8.4093e-02,\n",
       "              8.7995e-02,  8.7822e-03],\n",
       "            [ 1.0026e+00, -8.8903e-01,  1.5780e+00,  ...,  9.6190e-02,\n",
       "              2.7781e-01, -1.0388e-03],\n",
       "            ...,\n",
       "            [ 1.7316e-01,  2.6688e-01,  6.4858e-01,  ...,  7.2539e-01,\n",
       "              2.7617e-01, -3.7482e-02],\n",
       "            [ 1.7316e-01,  2.6688e-01,  6.4858e-01,  ...,  7.2539e-01,\n",
       "              2.7617e-01, -3.7482e-02],\n",
       "            [ 1.7316e-01,  2.6688e-01,  6.4858e-01,  ...,  7.2539e-01,\n",
       "              2.7617e-01, -3.7482e-02]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.3426,  0.4249, -0.6970,  ..., -0.9556, -1.9423,  0.3907],\n",
       "           [ 0.5766,  1.1295, -2.0693,  ...,  0.5803, -2.4390, -0.0564],\n",
       "           [ 1.6085,  0.6349,  0.6789,  ..., -1.2587, -1.6491,  0.5346],\n",
       "           ...,\n",
       "           [-0.1214,  0.6320, -0.3265,  ..., -2.0444, -1.5260, -1.5070],\n",
       "           [-0.1214,  0.6320, -0.3265,  ..., -2.0444, -1.5260, -1.5070],\n",
       "           [-0.1214,  0.6320, -0.3265,  ..., -2.0444, -1.5260, -1.5070]],\n",
       "  \n",
       "          [[ 0.7761, -0.1646, -0.6483,  ..., -2.9594, -1.7563, -0.1513],\n",
       "           [ 1.7869,  1.2097, -0.5575,  ..., -2.6706, -0.2294,  0.5272],\n",
       "           [ 1.8948,  0.5290, -0.5163,  ..., -2.4198, -0.5620, -0.5584],\n",
       "           ...,\n",
       "           [-0.5828, -0.0729, -0.3076,  ..., -1.9660, -0.7894, -1.4479],\n",
       "           [-0.5828, -0.0729, -0.3076,  ..., -1.9660, -0.7894, -1.4479],\n",
       "           [-0.5828, -0.0729, -0.3076,  ..., -1.9659, -0.7894, -1.4479]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.14.self_attn.v_proj': {'input': (tensor([[[ 2.0774e-02,  4.7471e-02,  6.7689e-01,  ...,  6.6072e-01,\n",
       "              5.7911e-01, -1.3903e-01],\n",
       "            [ 2.5920e-01, -2.6179e-02,  5.3987e-01,  ...,  7.3238e-01,\n",
       "              4.3111e-01, -1.0243e-01],\n",
       "            [ 4.1413e-01, -7.9960e-01, -4.7118e-01,  ..., -1.7927e-01,\n",
       "             -1.0231e-02, -5.7037e-01],\n",
       "            ...,\n",
       "            [ 7.8803e-02,  6.1773e-01,  6.4992e-01,  ...,  7.6662e-01,\n",
       "              7.7503e-01, -1.8809e-01],\n",
       "            [ 7.8803e-02,  6.1773e-01,  6.4992e-01,  ...,  7.6662e-01,\n",
       "              7.7503e-01, -1.8809e-01],\n",
       "            [ 7.8803e-02,  6.1773e-01,  6.4992e-01,  ...,  7.6662e-01,\n",
       "              7.7503e-01, -1.8809e-01]],\n",
       "   \n",
       "           [[-1.0962e+00, -5.7424e-01,  1.2710e+00,  ...,  2.1311e-01,\n",
       "             -1.7677e-01,  1.7835e-01],\n",
       "            [-3.4985e-01,  4.2298e-01,  1.2291e-01,  ...,  8.4093e-02,\n",
       "              8.7995e-02,  8.7822e-03],\n",
       "            [ 1.0026e+00, -8.8903e-01,  1.5780e+00,  ...,  9.6190e-02,\n",
       "              2.7781e-01, -1.0388e-03],\n",
       "            ...,\n",
       "            [ 1.7316e-01,  2.6688e-01,  6.4858e-01,  ...,  7.2539e-01,\n",
       "              2.7617e-01, -3.7482e-02],\n",
       "            [ 1.7316e-01,  2.6688e-01,  6.4858e-01,  ...,  7.2539e-01,\n",
       "              2.7617e-01, -3.7482e-02],\n",
       "            [ 1.7316e-01,  2.6688e-01,  6.4858e-01,  ...,  7.2539e-01,\n",
       "              2.7617e-01, -3.7482e-02]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.6955, -0.1086,  0.3857,  ..., -0.4997,  0.4926, -0.0804],\n",
       "           [ 1.0669,  1.3401,  1.7042,  ..., -0.2518,  0.3331, -0.4333],\n",
       "           [-0.2083,  0.0409,  0.6465,  ...,  0.3641, -0.1198,  0.2970],\n",
       "           ...,\n",
       "           [-0.2069,  0.0185, -0.1290,  ...,  0.4593, -0.6399, -0.5271],\n",
       "           [-0.2069,  0.0185, -0.1290,  ...,  0.4593, -0.6399, -0.5271],\n",
       "           [-0.2069,  0.0185, -0.1290,  ...,  0.4593, -0.6399, -0.5271]],\n",
       "  \n",
       "          [[ 0.8507, -0.0602,  0.2110,  ..., -1.6911,  0.8574, -1.1383],\n",
       "           [-0.0971,  0.6035,  1.0217,  ..., -1.4596, -0.4208, -0.3489],\n",
       "           [-0.0411,  0.1832, -0.1637,  ..., -1.5370, -1.4802, -0.7131],\n",
       "           ...,\n",
       "           [ 0.1499, -0.5984, -0.5867,  ...,  0.5939, -0.1331, -0.3601],\n",
       "           [ 0.1499, -0.5984, -0.5867,  ...,  0.5939, -0.1331, -0.3601],\n",
       "           [ 0.1499, -0.5984, -0.5867,  ...,  0.5939, -0.1331, -0.3601]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.14.self_attn.out_proj': {'input': (tensor([[[ 0.1278, -0.0241,  0.3087,  ..., -0.4661,  0.3958, -0.2412],\n",
       "            [ 0.1063,  0.0131,  0.2470,  ..., -0.2903,  0.4791, -0.0986],\n",
       "            [ 0.8789,  0.3042,  0.8836,  ..., -0.2415, -0.1247, -0.0537],\n",
       "            ...,\n",
       "            [-0.0865,  0.0528,  0.3798,  ..., -0.3007,  0.2603,  0.0704],\n",
       "            [-0.0865,  0.0528,  0.3798,  ..., -0.3007,  0.2603,  0.0704],\n",
       "            [-0.0865,  0.0528,  0.3798,  ..., -0.3007,  0.2603,  0.0704]],\n",
       "   \n",
       "           [[ 0.0110,  0.2128,  0.2641,  ..., -0.2573,  0.1506, -0.1016],\n",
       "            [-0.0850,  0.0221,  0.4826,  ..., -0.1499, -0.1201,  0.0077],\n",
       "            [-0.0144, -0.0159,  0.0996,  ..., -0.0056, -0.3319,  0.0215],\n",
       "            ...,\n",
       "            [-0.1166,  0.1168,  0.3261,  ..., -0.4140,  0.1859, -0.1546],\n",
       "            [-0.1166,  0.1168,  0.3261,  ..., -0.4140,  0.1859, -0.1546],\n",
       "            [-0.1166,  0.1168,  0.3261,  ..., -0.4140,  0.1859, -0.1546]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.1819, -0.0181, -0.0141,  ..., -0.2829,  0.0325, -0.3076],\n",
       "           [-0.4626, -0.0839, -0.2094,  ..., -0.2016,  0.1984,  0.2114],\n",
       "           [ 0.0704,  0.2658, -0.3053,  ..., -0.3985,  0.4385,  0.2195],\n",
       "           ...,\n",
       "           [ 0.1190,  0.3323, -0.0245,  ..., -0.1819,  0.4142,  0.0394],\n",
       "           [ 0.1190,  0.3323, -0.0245,  ..., -0.1819,  0.4142,  0.0394],\n",
       "           [ 0.1190,  0.3323, -0.0245,  ..., -0.1819,  0.4142,  0.0394]],\n",
       "  \n",
       "          [[ 0.1460,  0.1684, -0.1757,  ...,  0.2206, -0.0299, -0.0945],\n",
       "           [ 0.6524,  0.0433, -0.1367,  ...,  0.0893,  0.3621, -0.1838],\n",
       "           [ 0.2457, -0.2061, -0.1805,  ..., -0.1311,  0.2531, -0.0361],\n",
       "           ...,\n",
       "           [-0.2594,  0.0834, -0.4204,  ..., -0.2570,  0.5137, -0.0423],\n",
       "           [-0.2594,  0.0834, -0.4204,  ..., -0.2570,  0.5137, -0.0423],\n",
       "           [-0.2594,  0.0834, -0.4204,  ..., -0.2570,  0.5137, -0.0423]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.14.mlp.fc1': {'input': (tensor([[[-0.0833, -0.1217,  1.0327,  ...,  0.7774,  0.9449, -0.5024],\n",
       "            [-0.1750, -0.3054,  0.6203,  ...,  0.9116,  0.9896,  0.1203],\n",
       "            [ 0.4231, -0.8971, -0.5645,  ..., -0.2746,  0.4734, -0.5418],\n",
       "            ...,\n",
       "            [ 0.1419,  0.7667,  0.9883,  ...,  1.0505,  1.3601, -0.2401],\n",
       "            [ 0.1419,  0.7667,  0.9883,  ...,  1.0505,  1.3601, -0.2401],\n",
       "            [ 0.1419,  0.7667,  0.9883,  ...,  1.0505,  1.3601, -0.2401]],\n",
       "   \n",
       "           [[-0.8487, -0.7400,  1.6231,  ...,  0.6400, -0.1295,  0.1333],\n",
       "            [ 0.1369,  0.4007,  0.2622,  ...,  0.3821,  0.5563, -0.1631],\n",
       "            [ 1.0837, -1.4833,  1.9649,  ...,  0.2273,  0.7501, -0.0524],\n",
       "            ...,\n",
       "            [ 0.0819,  0.2040,  0.7733,  ...,  0.9631,  0.7513, -0.0912],\n",
       "            [ 0.0819,  0.2040,  0.7733,  ...,  0.9631,  0.7513, -0.0912],\n",
       "            [ 0.0819,  0.2040,  0.7733,  ...,  0.9631,  0.7513, -0.0912]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.0586, -0.3727, -3.5883,  ..., -1.9830, -0.3035, -0.4131],\n",
       "           [ 0.1908, -0.5483, -2.9683,  ..., -2.6577,  0.2210, -0.6141],\n",
       "           [-2.8463, -2.1119, -3.1842,  ..., -2.1712, -1.0498, -1.2775],\n",
       "           ...,\n",
       "           [-0.1013, -1.5494, -2.5491,  ..., -0.1621, -1.4608, -0.4641],\n",
       "           [-0.1013, -1.5494, -2.5491,  ..., -0.1621, -1.4608, -0.4641],\n",
       "           [-0.1013, -1.5494, -2.5491,  ..., -0.1621, -1.4608, -0.4641]],\n",
       "  \n",
       "          [[-0.0468, -0.2849, -2.1680,  ..., -1.0699, -2.7038,  0.0822],\n",
       "           [-1.2959, -1.9512, -2.1176,  ..., -1.7365, -1.8043, -0.4958],\n",
       "           [ 0.1316,  0.3772, -1.7251,  ..., -1.5080, -0.3150, -3.1468],\n",
       "           ...,\n",
       "           [ 0.1463, -1.3961, -3.3831,  ..., -0.4354, -1.5529, -0.9998],\n",
       "           [ 0.1463, -1.3961, -3.3831,  ..., -0.4354, -1.5529, -0.9998],\n",
       "           [ 0.1463, -1.3961, -3.3831,  ..., -0.4354, -1.5529, -0.9998]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.14.mlp.fc2': {'input': (tensor([[[ 0.0307, -0.1322, -0.0004,  ..., -0.0469, -0.1156, -0.1404],\n",
       "            [ 0.1099, -0.1600, -0.0040,  ..., -0.0100,  0.1298, -0.1656],\n",
       "            [-0.0058, -0.0364, -0.0020,  ..., -0.0322, -0.1544, -0.1289],\n",
       "            ...,\n",
       "            [-0.0466, -0.0942, -0.0133,  ..., -0.0706, -0.1055, -0.1491],\n",
       "            [-0.0466, -0.0942, -0.0133,  ..., -0.0706, -0.1055, -0.1491],\n",
       "            [-0.0466, -0.0942, -0.0133,  ..., -0.0706, -0.1055, -0.1491]],\n",
       "   \n",
       "           [[-0.0225, -0.1105, -0.0324,  ..., -0.1525, -0.0088,  0.0438],\n",
       "            [-0.1266, -0.0497, -0.0360,  ..., -0.0717, -0.0643, -0.1537],\n",
       "            [ 0.0727,  0.2440, -0.0730,  ..., -0.0994, -0.1186, -0.0022],\n",
       "            ...,\n",
       "            [ 0.0816, -0.1138, -0.0010,  ..., -0.1444, -0.0937, -0.1588],\n",
       "            [ 0.0816, -0.1138, -0.0010,  ..., -0.1444, -0.0937, -0.1588],\n",
       "            [ 0.0816, -0.1138, -0.0010,  ..., -0.1444, -0.0937, -0.1588]]],\n",
       "          grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[-0.3700, -0.0450, -0.4994,  ...,  0.1378, -0.3785,  0.0339],\n",
       "           [-0.3591, -0.2609,  0.1393,  ...,  0.1769, -0.7209, -0.3475],\n",
       "           [-0.1791, -0.1422, -0.0683,  ...,  0.1883, -0.1341, -0.2733],\n",
       "           ...,\n",
       "           [-1.1449,  0.7704,  0.7079,  ..., -0.9584,  1.2026, -0.6032],\n",
       "           [-1.1449,  0.7704,  0.7079,  ..., -0.9584,  1.2026, -0.6032],\n",
       "           [-1.1449,  0.7704,  0.7079,  ..., -0.9584,  1.2026, -0.6032]],\n",
       "  \n",
       "          [[ 0.1768, -0.4473,  0.6122,  ..., -0.2041, -1.0959,  0.0590],\n",
       "           [ 0.4731, -0.1519,  0.8486,  ...,  0.4049, -0.9774, -0.3536],\n",
       "           [ 0.1691,  0.2593, -0.3165,  ..., -0.3544, -0.0220, -1.0427],\n",
       "           ...,\n",
       "           [-0.9748,  0.2341,  0.6266,  ..., -0.5993,  1.5923, -0.2964],\n",
       "           [-0.9748,  0.2341,  0.6266,  ..., -0.5993,  1.5923, -0.2964],\n",
       "           [-0.9748,  0.2341,  0.6266,  ..., -0.5993,  1.5923, -0.2964]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.15.self_attn.q_proj': {'input': (tensor([[[-0.3486,  0.0898,  0.2776,  ...,  0.5249,  0.3157, -0.3388],\n",
       "            [-0.5573, -0.2866,  0.5177,  ...,  0.7387, -0.0498, -0.2364],\n",
       "            [ 0.3645, -0.5421, -0.5514,  ..., -0.2789,  0.1579, -0.5616],\n",
       "            ...,\n",
       "            [-0.2474,  1.0235,  0.8460,  ...,  0.3043,  1.2579, -0.3765],\n",
       "            [-0.2474,  1.0235,  0.8460,  ...,  0.3043,  1.2579, -0.3765],\n",
       "            [-0.2474,  1.0235,  0.8460,  ...,  0.3043,  1.2579, -0.3765]],\n",
       "   \n",
       "           [[-0.8129, -0.6765,  1.5732,  ...,  0.2241, -0.9540,  0.1731],\n",
       "            [ 0.5046,  0.4314,  0.6602,  ...,  0.3699, -0.3125, -0.3390],\n",
       "            [ 1.3633, -0.7279,  1.1521,  ..., -0.2217,  0.4577, -0.7980],\n",
       "            ...,\n",
       "            [-0.2358,  0.4171,  0.6515,  ...,  0.3367,  0.9332, -0.1580],\n",
       "            [-0.2358,  0.4171,  0.6515,  ...,  0.3367,  0.9332, -0.1580],\n",
       "            [-0.2358,  0.4171,  0.6515,  ...,  0.3367,  0.9332, -0.1580]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.3676,  1.8804,  0.6031,  ..., -0.3541,  0.8232,  0.8185],\n",
       "           [ 1.8956,  1.6456, -0.2265,  ...,  0.1063,  0.5209,  0.5382],\n",
       "           [-0.5227, -1.4103, -0.4372,  ...,  1.8334, -0.3965,  1.1212],\n",
       "           ...,\n",
       "           [-2.6289,  0.6501,  0.5196,  ...,  1.2218, -0.3577,  2.6549],\n",
       "           [-2.6289,  0.6501,  0.5196,  ...,  1.2218, -0.3577,  2.6549],\n",
       "           [-2.6289,  0.6501,  0.5196,  ...,  1.2218, -0.3577,  2.6549]],\n",
       "  \n",
       "          [[ 1.5325,  0.7024,  0.4010,  ..., -0.0983,  0.6730, -1.4913],\n",
       "           [-0.4477,  0.6733,  1.8562,  ...,  0.0305, -0.8170,  0.0467],\n",
       "           [-2.1261, -0.1724,  1.0954,  ...,  1.9161, -0.5305, -0.1366],\n",
       "           ...,\n",
       "           [-1.8823,  0.4859,  0.8666,  ...,  0.7787,  0.0373,  1.9815],\n",
       "           [-1.8823,  0.4859,  0.8666,  ...,  0.7787,  0.0373,  1.9815],\n",
       "           [-1.8823,  0.4859,  0.8666,  ...,  0.7787,  0.0373,  1.9815]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.15.self_attn.k_proj': {'input': (tensor([[[-0.3486,  0.0898,  0.2776,  ...,  0.5249,  0.3157, -0.3388],\n",
       "            [-0.5573, -0.2866,  0.5177,  ...,  0.7387, -0.0498, -0.2364],\n",
       "            [ 0.3645, -0.5421, -0.5514,  ..., -0.2789,  0.1579, -0.5616],\n",
       "            ...,\n",
       "            [-0.2474,  1.0235,  0.8460,  ...,  0.3043,  1.2579, -0.3765],\n",
       "            [-0.2474,  1.0235,  0.8460,  ...,  0.3043,  1.2579, -0.3765],\n",
       "            [-0.2474,  1.0235,  0.8460,  ...,  0.3043,  1.2579, -0.3765]],\n",
       "   \n",
       "           [[-0.8129, -0.6765,  1.5732,  ...,  0.2241, -0.9540,  0.1731],\n",
       "            [ 0.5046,  0.4314,  0.6602,  ...,  0.3699, -0.3125, -0.3390],\n",
       "            [ 1.3633, -0.7279,  1.1521,  ..., -0.2217,  0.4577, -0.7980],\n",
       "            ...,\n",
       "            [-0.2358,  0.4171,  0.6515,  ...,  0.3367,  0.9332, -0.1580],\n",
       "            [-0.2358,  0.4171,  0.6515,  ...,  0.3367,  0.9332, -0.1580],\n",
       "            [-0.2358,  0.4171,  0.6515,  ...,  0.3367,  0.9332, -0.1580]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-1.4603e+00,  4.3029e-01, -8.0358e-01,  ..., -3.9758e-01,\n",
       "            -1.8090e+00,  1.4969e+00],\n",
       "           [-8.3325e-03,  1.4933e+00, -1.6652e+00,  ..., -2.2242e-01,\n",
       "            -1.5547e+00,  8.3297e-01],\n",
       "           [-2.1029e+00,  2.7603e-01, -4.6500e-01,  ..., -1.7830e-01,\n",
       "            -1.7752e+00,  5.3387e-01],\n",
       "           ...,\n",
       "           [-1.8448e+00,  6.3608e-01,  2.2432e-01,  ...,  2.0762e+00,\n",
       "            -2.1319e+00,  2.3256e+00],\n",
       "           [-1.8448e+00,  6.3608e-01,  2.2432e-01,  ...,  2.0762e+00,\n",
       "            -2.1319e+00,  2.3256e+00],\n",
       "           [-1.8448e+00,  6.3608e-01,  2.2432e-01,  ...,  2.0762e+00,\n",
       "            -2.1319e+00,  2.3256e+00]],\n",
       "  \n",
       "          [[-1.6872e+00,  4.9206e-01, -1.4137e-01,  ...,  7.1422e-04,\n",
       "            -1.3752e+00, -1.3355e+00],\n",
       "           [-1.8994e+00,  1.4509e+00, -6.7687e-02,  ...,  6.1873e-01,\n",
       "            -1.8221e+00,  1.2265e+00],\n",
       "           [-2.4602e+00,  7.9008e-01,  5.7811e-01,  ...,  2.5376e+00,\n",
       "            -1.7768e+00,  3.0348e+00],\n",
       "           ...,\n",
       "           [-1.9499e+00,  7.5235e-01,  3.4415e-01,  ...,  1.5840e+00,\n",
       "            -2.1817e+00,  1.9673e+00],\n",
       "           [-1.9499e+00,  7.5235e-01,  3.4415e-01,  ...,  1.5840e+00,\n",
       "            -2.1817e+00,  1.9673e+00],\n",
       "           [-1.9499e+00,  7.5234e-01,  3.4415e-01,  ...,  1.5840e+00,\n",
       "            -2.1817e+00,  1.9673e+00]]], grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.15.self_attn.v_proj': {'input': (tensor([[[-0.3486,  0.0898,  0.2776,  ...,  0.5249,  0.3157, -0.3388],\n",
       "            [-0.5573, -0.2866,  0.5177,  ...,  0.7387, -0.0498, -0.2364],\n",
       "            [ 0.3645, -0.5421, -0.5514,  ..., -0.2789,  0.1579, -0.5616],\n",
       "            ...,\n",
       "            [-0.2474,  1.0235,  0.8460,  ...,  0.3043,  1.2579, -0.3765],\n",
       "            [-0.2474,  1.0235,  0.8460,  ...,  0.3043,  1.2579, -0.3765],\n",
       "            [-0.2474,  1.0235,  0.8460,  ...,  0.3043,  1.2579, -0.3765]],\n",
       "   \n",
       "           [[-0.8129, -0.6765,  1.5732,  ...,  0.2241, -0.9540,  0.1731],\n",
       "            [ 0.5046,  0.4314,  0.6602,  ...,  0.3699, -0.3125, -0.3390],\n",
       "            [ 1.3633, -0.7279,  1.1521,  ..., -0.2217,  0.4577, -0.7980],\n",
       "            ...,\n",
       "            [-0.2358,  0.4171,  0.6515,  ...,  0.3367,  0.9332, -0.1580],\n",
       "            [-0.2358,  0.4171,  0.6515,  ...,  0.3367,  0.9332, -0.1580],\n",
       "            [-0.2358,  0.4171,  0.6515,  ...,  0.3367,  0.9332, -0.1580]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.3912, -1.3807,  0.0044,  ...,  0.3786, -0.5772, -0.1045],\n",
       "           [ 0.4293, -0.3640,  0.5697,  ...,  0.9481,  0.4456, -0.1658],\n",
       "           [-0.2251, -0.3453,  0.1511,  ..., -0.5143,  0.0713, -0.1883],\n",
       "           ...,\n",
       "           [-0.7320,  1.2823,  0.0146,  ...,  0.4098,  0.3476, -1.0802],\n",
       "           [-0.7320,  1.2823,  0.0146,  ...,  0.4098,  0.3476, -1.0802],\n",
       "           [-0.7320,  1.2823,  0.0146,  ...,  0.4098,  0.3476, -1.0802]],\n",
       "  \n",
       "          [[-0.5167, -0.7030, -0.3790,  ...,  0.8093, -0.9684, -1.0877],\n",
       "           [ 0.4817, -1.1608,  0.6128,  ...,  1.5401, -1.2197, -0.4868],\n",
       "           [ 0.2466, -1.0289,  1.2611,  ...,  1.4151, -0.6130,  0.1264],\n",
       "           ...,\n",
       "           [-0.8822,  0.8728, -0.4038,  ...,  0.8605,  0.3002, -1.1061],\n",
       "           [-0.8822,  0.8728, -0.4038,  ...,  0.8605,  0.3002, -1.1061],\n",
       "           [-0.8822,  0.8728, -0.4038,  ...,  0.8605,  0.3002, -1.1061]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.15.self_attn.out_proj': {'input': (tensor([[[-0.0637, -0.1002, -0.0351,  ...,  0.1533, -0.0975, -0.0358],\n",
       "            [ 0.1006, -0.0376,  0.0810,  ...,  0.2774, -0.0787, -0.0159],\n",
       "            [ 0.3756, -0.0848, -0.0568,  ...,  0.2324, -0.0471,  0.0650],\n",
       "            ...,\n",
       "            [-0.0942, -0.2030, -0.0424,  ...,  0.1077, -0.0430, -0.0940],\n",
       "            [-0.0942, -0.2030, -0.0424,  ...,  0.1077, -0.0430, -0.0940],\n",
       "            [-0.0942, -0.2030, -0.0424,  ...,  0.1077, -0.0430, -0.0940]],\n",
       "   \n",
       "           [[-0.0393, -0.0789,  0.0055,  ...,  0.0201, -0.0447, -0.1025],\n",
       "            [ 0.0488, -0.1486,  0.1143,  ..., -0.0609, -0.0196, -0.0575],\n",
       "            [ 0.1388, -0.1857,  0.0672,  ...,  0.0525, -0.0626, -0.0781],\n",
       "            ...,\n",
       "            [ 0.0084, -0.1950,  0.0666,  ...,  0.0016, -0.0667, -0.1301],\n",
       "            [ 0.0084, -0.1950,  0.0666,  ...,  0.0016, -0.0667, -0.1301],\n",
       "            [ 0.0084, -0.1950,  0.0666,  ...,  0.0016, -0.0667, -0.1301]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[ 0.1038,  0.0575, -0.0376,  ..., -0.1342, -0.1845, -0.1173],\n",
       "           [ 0.0012,  0.0028,  0.1176,  ..., -0.2473, -0.1235, -0.2222],\n",
       "           [-0.1740, -0.1117, -0.2994,  ..., -0.1339,  0.0088,  0.2462],\n",
       "           ...,\n",
       "           [-0.2942, -0.1017, -0.1567,  ...,  0.5358,  0.0089, -0.0396],\n",
       "           [-0.2942, -0.1017, -0.1567,  ...,  0.5358,  0.0089, -0.0396],\n",
       "           [-0.2942, -0.1017, -0.1567,  ...,  0.5358,  0.0089, -0.0396]],\n",
       "  \n",
       "          [[ 0.3263, -0.0732,  0.2111,  ..., -0.1796, -0.2260, -0.0793],\n",
       "           [-0.0433, -0.0057,  0.0488,  ..., -0.1320, -0.4394, -0.3892],\n",
       "           [-0.3251,  0.1987,  0.0245,  ...,  0.1161, -0.2460, -0.0917],\n",
       "           ...,\n",
       "           [-0.4175,  0.0614, -0.3410,  ...,  0.6024, -0.0441,  0.2001],\n",
       "           [-0.4175,  0.0614, -0.3410,  ...,  0.6024, -0.0441,  0.2001],\n",
       "           [-0.4175,  0.0614, -0.3410,  ...,  0.6024, -0.0441,  0.2001]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.15.mlp.fc1': {'input': (tensor([[[-0.2157, -0.1814, -0.1247,  ...,  0.9654,  0.7975,  0.0524],\n",
       "            [-0.4978, -0.7786,  0.4876,  ...,  1.0882,  0.2379, -0.0291],\n",
       "            [ 0.2751, -1.2176, -1.5870,  ..., -0.0981,  0.7615,  0.1057],\n",
       "            ...,\n",
       "            [-0.3053,  1.0447,  0.7056,  ...,  1.0223,  2.5335,  0.1102],\n",
       "            [-0.3053,  1.0447,  0.7056,  ...,  1.0223,  2.5335,  0.1102],\n",
       "            [-0.3053,  1.0447,  0.7056,  ...,  1.0223,  2.5335,  0.1102]],\n",
       "   \n",
       "           [[-0.5117, -1.4093,  2.1164,  ...,  0.5277, -1.2464,  1.0311],\n",
       "            [ 0.4874,  0.2375,  0.5494,  ...,  0.7713, -0.4291, -0.2495],\n",
       "            [ 1.1057, -1.2013,  1.3020,  ...,  0.1701,  0.9672, -0.7546],\n",
       "            ...,\n",
       "            [-0.3338,  0.2574,  0.3092,  ...,  1.0889,  1.9820,  0.6591],\n",
       "            [-0.3338,  0.2574,  0.3092,  ...,  1.0889,  1.9820,  0.6591],\n",
       "            [-0.3338,  0.2574,  0.3092,  ...,  1.0889,  1.9820,  0.6591]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-4.9293, -5.1522, -0.8805,  ..., -4.1134, -3.7853, -3.1033],\n",
       "           [-2.5957, -3.9504,  0.1524,  ..., -3.0055, -2.6359, -1.2628],\n",
       "           [-3.1313, -4.7080, -2.2315,  ..., -2.9171, -2.0273, -2.4325],\n",
       "           ...,\n",
       "           [-2.1861, -6.0143, -2.8969,  ..., -2.0517, -3.3295, -1.2014],\n",
       "           [-2.1861, -6.0143, -2.8969,  ..., -2.0517, -3.3295, -1.2014],\n",
       "           [-2.1861, -6.0143, -2.8969,  ..., -2.0517, -3.3295, -1.2014]],\n",
       "  \n",
       "          [[-3.5471, -2.2691, -0.4760,  ..., -3.8636, -3.3042, -2.8631],\n",
       "           [-4.1173, -3.5550, -2.0280,  ..., -3.5082, -3.5763, -1.6896],\n",
       "           [-2.9379, -4.4661, -1.9519,  ..., -2.3926, -1.9141, -1.7048],\n",
       "           ...,\n",
       "           [-2.2774, -5.9882, -2.8169,  ..., -2.1430, -2.6618, -0.6498],\n",
       "           [-2.2774, -5.9882, -2.8169,  ..., -2.1430, -2.6618, -0.6498],\n",
       "           [-2.2774, -5.9882, -2.8169,  ..., -2.1430, -2.6618, -0.6498]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.15.mlp.fc2': {'input': (tensor([[[-2.9381e-07, -0.0000e+00, -1.6679e-01,  ..., -4.0454e-05,\n",
       "             -1.8794e-04, -2.5979e-03],\n",
       "            [-1.1787e-02, -8.8769e-05,  8.5449e-02,  ..., -3.5744e-03,\n",
       "             -1.0588e-02, -1.3071e-01],\n",
       "            [-2.3651e-03, -1.5434e-06, -2.8327e-02,  ..., -4.7136e-03,\n",
       "             -4.3089e-02, -1.7825e-02],\n",
       "            ...,\n",
       "            [-3.1234e-02, -0.0000e+00, -5.0135e-03,  ..., -4.1095e-02,\n",
       "             -1.1774e-03, -1.3814e-01],\n",
       "            [-3.1234e-02, -0.0000e+00, -5.0135e-03,  ..., -4.1095e-02,\n",
       "             -1.1774e-03, -1.3814e-01],\n",
       "            [-3.1234e-02, -0.0000e+00, -5.0135e-03,  ..., -4.1095e-02,\n",
       "             -1.1774e-03, -1.3814e-01]],\n",
       "   \n",
       "           [[-5.1111e-04, -2.6077e-02, -1.5092e-01,  ..., -1.3253e-04,\n",
       "             -1.2916e-03, -5.5520e-03],\n",
       "            [-3.9634e-05, -4.9520e-04, -4.3034e-02,  ..., -5.9668e-04,\n",
       "             -4.5436e-04, -7.7101e-02],\n",
       "            [-4.4204e-03, -6.2557e-06, -4.9665e-02,  ..., -1.9620e-02,\n",
       "             -5.3192e-02, -7.5341e-02],\n",
       "            ...,\n",
       "            [-2.5596e-02, -0.0000e+00, -6.3666e-03,  ..., -3.4185e-02,\n",
       "             -9.8730e-03, -1.6763e-01],\n",
       "            [-2.5596e-02, -0.0000e+00, -6.3666e-03,  ..., -3.4185e-02,\n",
       "             -9.8730e-03, -1.6763e-01],\n",
       "            [-2.5596e-02, -0.0000e+00, -6.3667e-03,  ..., -3.4185e-02,\n",
       "             -9.8730e-03, -1.6763e-01]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[-0.0430,  0.2121, -0.2541,  ..., -0.1707, -0.4207, -0.0873],\n",
       "           [-0.0461,  0.4509,  0.0640,  ..., -0.2561, -0.4418,  0.0381],\n",
       "           [ 0.0037,  0.3620,  0.1339,  ...,  0.1513, -0.4360,  0.1007],\n",
       "           ...,\n",
       "           [-0.5548,  0.1660, -0.2322,  ..., -0.3550,  0.0236, -0.1057],\n",
       "           [-0.5548,  0.1660, -0.2322,  ..., -0.3550,  0.0236, -0.1057],\n",
       "           [-0.5548,  0.1660, -0.2322,  ..., -0.3550,  0.0236, -0.1057]],\n",
       "  \n",
       "          [[-0.4086,  0.4333, -0.1012,  ..., -0.3164, -0.4424, -0.0265],\n",
       "           [-0.1365, -0.0274,  0.1003,  ..., -0.4227, -0.0274,  0.1245],\n",
       "           [ 0.2124,  0.0378,  0.2934,  ..., -0.2804, -0.2326, -0.1854],\n",
       "           ...,\n",
       "           [-0.6244, -0.0285, -0.0853,  ..., -0.2162, -0.0132,  0.1498],\n",
       "           [-0.6244, -0.0285, -0.0853,  ..., -0.2162, -0.0132,  0.1498],\n",
       "           [-0.6244, -0.0285, -0.0853,  ..., -0.2162, -0.0132,  0.1498]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.16.self_attn.q_proj': {'input': (tensor([[[-0.3222,  0.2047, -0.0646,  ...,  0.3759,  0.0052, -0.5100],\n",
       "            [-0.6230,  0.1168,  0.6211,  ...,  0.3469, -0.4885, -0.4425],\n",
       "            [ 0.2247, -0.4599, -0.7939,  ..., -0.1963,  0.0330, -0.3637],\n",
       "            ...,\n",
       "            [-0.5980,  0.9570,  0.6095,  ...,  0.4168,  1.4340, -0.4592],\n",
       "            [-0.5980,  0.9570,  0.6095,  ...,  0.4168,  1.4340, -0.4592],\n",
       "            [-0.5980,  0.9570,  0.6095,  ...,  0.4168,  1.4340, -0.4592]],\n",
       "   \n",
       "           [[-0.8752, -0.4939,  1.5588,  ..., -0.0106, -1.2683,  0.0723],\n",
       "            [ 0.3497,  0.3157,  0.6551,  ...,  0.1113, -0.4766, -0.5253],\n",
       "            [ 1.2914, -0.6480,  1.3778,  ..., -0.2685,  0.2593, -1.0614],\n",
       "            ...,\n",
       "            [-0.6456,  0.3363,  0.3930,  ...,  0.5072,  1.0635, -0.0641],\n",
       "            [-0.6456,  0.3363,  0.3930,  ...,  0.5072,  1.0635, -0.0641],\n",
       "            [-0.6456,  0.3363,  0.3930,  ...,  0.5072,  1.0635, -0.0641]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 2.4308,  0.1808,  1.2949,  ...,  0.1313, -1.0282,  0.5253],\n",
       "           [ 2.2116,  0.3252, -0.0060,  ...,  1.0625, -0.1381,  0.2907],\n",
       "           [-0.2449, -0.8389, -1.2885,  ..., -1.0048, -0.0785,  1.3389],\n",
       "           ...,\n",
       "           [-1.0851,  1.6205,  0.2849,  ...,  0.0861,  1.8352, -0.8421],\n",
       "           [-1.0851,  1.6205,  0.2849,  ...,  0.0861,  1.8352, -0.8421],\n",
       "           [-1.0851,  1.6205,  0.2849,  ...,  0.0861,  1.8352, -0.8421]],\n",
       "  \n",
       "          [[ 1.5063,  1.0887,  0.8382,  ...,  0.1847, -1.0244,  0.1325],\n",
       "           [ 3.5554,  1.8816,  0.2743,  ..., -0.6250, -0.1894, -1.3482],\n",
       "           [ 0.4125,  1.3584, -0.9362,  ...,  0.9275,  0.1024, -1.9536],\n",
       "           ...,\n",
       "           [-1.4889,  1.2787,  0.8963,  ..., -0.2221,  1.0909, -0.5213],\n",
       "           [-1.4889,  1.2787,  0.8963,  ..., -0.2221,  1.0909, -0.5213],\n",
       "           [-1.4889,  1.2787,  0.8963,  ..., -0.2221,  1.0909, -0.5213]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.16.self_attn.k_proj': {'input': (tensor([[[-0.3222,  0.2047, -0.0646,  ...,  0.3759,  0.0052, -0.5100],\n",
       "            [-0.6230,  0.1168,  0.6211,  ...,  0.3469, -0.4885, -0.4425],\n",
       "            [ 0.2247, -0.4599, -0.7939,  ..., -0.1963,  0.0330, -0.3637],\n",
       "            ...,\n",
       "            [-0.5980,  0.9570,  0.6095,  ...,  0.4168,  1.4340, -0.4592],\n",
       "            [-0.5980,  0.9570,  0.6095,  ...,  0.4168,  1.4340, -0.4592],\n",
       "            [-0.5980,  0.9570,  0.6095,  ...,  0.4168,  1.4340, -0.4592]],\n",
       "   \n",
       "           [[-0.8752, -0.4939,  1.5588,  ..., -0.0106, -1.2683,  0.0723],\n",
       "            [ 0.3497,  0.3157,  0.6551,  ...,  0.1113, -0.4766, -0.5253],\n",
       "            [ 1.2914, -0.6480,  1.3778,  ..., -0.2685,  0.2593, -1.0614],\n",
       "            ...,\n",
       "            [-0.6456,  0.3363,  0.3930,  ...,  0.5072,  1.0635, -0.0641],\n",
       "            [-0.6456,  0.3363,  0.3930,  ...,  0.5072,  1.0635, -0.0641],\n",
       "            [-0.6456,  0.3363,  0.3930,  ...,  0.5072,  1.0635, -0.0641]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 2.0667, -0.4892, -1.2714,  ..., -1.7188,  0.9544,  1.0097],\n",
       "           [ 2.3553, -0.0374, -0.7525,  ..., -0.6284, -0.4421,  1.3773],\n",
       "           [ 1.4853, -1.0476, -1.9027,  ..., -0.0675, -0.5812,  1.7141],\n",
       "           ...,\n",
       "           [-1.3056, -0.5914, -2.6923,  ..., -0.5171, -0.3219,  0.0122],\n",
       "           [-1.3056, -0.5914, -2.6923,  ..., -0.5171, -0.3219,  0.0122],\n",
       "           [-1.3056, -0.5914, -2.6923,  ..., -0.5171, -0.3219,  0.0122]],\n",
       "  \n",
       "          [[ 1.7823, -0.1930, -1.6127,  ..., -0.1409, -0.0129,  1.0968],\n",
       "           [ 2.5578, -0.0289, -0.7281,  ...,  0.6502, -0.8584,  0.6294],\n",
       "           [ 0.0832, -2.2651, -1.9614,  ..., -1.4148, -1.0692,  0.5556],\n",
       "           ...,\n",
       "           [-1.0527, -0.7299, -2.2525,  ..., -0.4957,  0.0541,  0.4376],\n",
       "           [-1.0527, -0.7299, -2.2525,  ..., -0.4957,  0.0541,  0.4376],\n",
       "           [-1.0527, -0.7299, -2.2525,  ..., -0.4957,  0.0541,  0.4376]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.16.self_attn.v_proj': {'input': (tensor([[[-0.3222,  0.2047, -0.0646,  ...,  0.3759,  0.0052, -0.5100],\n",
       "            [-0.6230,  0.1168,  0.6211,  ...,  0.3469, -0.4885, -0.4425],\n",
       "            [ 0.2247, -0.4599, -0.7939,  ..., -0.1963,  0.0330, -0.3637],\n",
       "            ...,\n",
       "            [-0.5980,  0.9570,  0.6095,  ...,  0.4168,  1.4340, -0.4592],\n",
       "            [-0.5980,  0.9570,  0.6095,  ...,  0.4168,  1.4340, -0.4592],\n",
       "            [-0.5980,  0.9570,  0.6095,  ...,  0.4168,  1.4340, -0.4592]],\n",
       "   \n",
       "           [[-0.8752, -0.4939,  1.5588,  ..., -0.0106, -1.2683,  0.0723],\n",
       "            [ 0.3497,  0.3157,  0.6551,  ...,  0.1113, -0.4766, -0.5253],\n",
       "            [ 1.2914, -0.6480,  1.3778,  ..., -0.2685,  0.2593, -1.0614],\n",
       "            ...,\n",
       "            [-0.6456,  0.3363,  0.3930,  ...,  0.5072,  1.0635, -0.0641],\n",
       "            [-0.6456,  0.3363,  0.3930,  ...,  0.5072,  1.0635, -0.0641],\n",
       "            [-0.6456,  0.3363,  0.3930,  ...,  0.5072,  1.0635, -0.0641]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.0616,  1.0102, -1.2307,  ..., -0.4810,  0.4849, -0.3172],\n",
       "           [-0.5168,  0.0632, -0.2479,  ...,  0.4780, -0.2746, -0.0287],\n",
       "           [-0.1778, -0.6334, -0.7334,  ...,  0.0778,  0.5346, -0.1594],\n",
       "           ...,\n",
       "           [-0.5893, -0.4495,  0.2161,  ..., -0.2094,  1.5129, -0.8476],\n",
       "           [-0.5893, -0.4495,  0.2161,  ..., -0.2094,  1.5129, -0.8476],\n",
       "           [-0.5893, -0.4495,  0.2161,  ..., -0.2094,  1.5129, -0.8476]],\n",
       "  \n",
       "          [[ 0.1218,  0.9855, -1.3919,  ...,  0.1479, -1.2412,  1.1753],\n",
       "           [ 0.2628,  0.4941,  0.0462,  ...,  0.9267, -1.5765,  0.7858],\n",
       "           [-0.5422,  0.6258,  1.6645,  ...,  0.8321, -1.4587,  0.8316],\n",
       "           ...,\n",
       "           [-0.7664, -0.6234, -0.3939,  ...,  0.3469,  1.3065, -0.3435],\n",
       "           [-0.7664, -0.6234, -0.3939,  ...,  0.3469,  1.3065, -0.3435],\n",
       "           [-0.7664, -0.6234, -0.3939,  ...,  0.3469,  1.3065, -0.3435]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.16.self_attn.out_proj': {'input': (tensor([[[ 0.4653, -0.4518,  0.5590,  ..., -0.1866,  0.0867,  0.2183],\n",
       "            [ 0.1736,  0.0230,  0.1208,  ..., -0.1096,  0.1008,  0.1409],\n",
       "            [ 0.1611, -0.1030,  0.0204,  ...,  0.1424,  0.0946,  0.1540],\n",
       "            ...,\n",
       "            [ 0.3335,  0.0478,  0.2487,  ...,  0.0017,  0.0942,  0.1279],\n",
       "            [ 0.3335,  0.0478,  0.2487,  ...,  0.0017,  0.0942,  0.1279],\n",
       "            [ 0.3335,  0.0478,  0.2487,  ...,  0.0017,  0.0942,  0.1279]],\n",
       "   \n",
       "           [[ 0.3192, -0.3470,  0.2474,  ..., -0.1226, -0.0550,  0.3494],\n",
       "            [ 0.3522,  0.0345,  0.2310,  ..., -0.0144, -0.1432,  0.4172],\n",
       "            [ 0.0114, -0.1073,  0.2246,  ..., -0.1040, -0.1463,  0.3096],\n",
       "            ...,\n",
       "            [ 0.3469, -0.0501,  0.2157,  ..., -0.0771, -0.0140,  0.2571],\n",
       "            [ 0.3469, -0.0501,  0.2157,  ..., -0.0771, -0.0140,  0.2571],\n",
       "            [ 0.3469, -0.0501,  0.2157,  ..., -0.0771, -0.0140,  0.2571]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.1693, -0.2133, -0.1400,  ..., -0.3434,  0.4129, -0.1950],\n",
       "           [-0.3075,  0.0104,  0.3916,  ...,  0.1369,  0.1700,  0.0595],\n",
       "           [-0.0857,  0.0503,  0.1057,  ...,  0.2401,  0.0069,  0.0181],\n",
       "           ...,\n",
       "           [-0.5633, -0.1645,  0.5118,  ...,  0.2587, -0.2903,  0.5543],\n",
       "           [-0.5633, -0.1645,  0.5118,  ...,  0.2587, -0.2903,  0.5543],\n",
       "           [-0.5633, -0.1645,  0.5118,  ...,  0.2587, -0.2903,  0.5543]],\n",
       "  \n",
       "          [[-0.0700, -0.1195,  0.0767,  ..., -0.0461,  0.3394,  0.2028],\n",
       "           [-0.2820, -0.4045,  0.4317,  ...,  0.2695,  0.2889,  0.3505],\n",
       "           [-0.1648, -0.0613,  0.2817,  ...,  0.3811,  0.2923,  0.2622],\n",
       "           ...,\n",
       "           [-0.6932, -0.1386,  0.3472,  ...,  0.3232, -0.2185,  0.5583],\n",
       "           [-0.6932, -0.1386,  0.3472,  ...,  0.3232, -0.2185,  0.5583],\n",
       "           [-0.6932, -0.1386,  0.3472,  ...,  0.3232, -0.2185,  0.5583]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.16.mlp.fc1': {'input': (tensor([[[-0.6082, -1.2124, -0.9639,  ...,  1.5019,  0.7449, -0.8192],\n",
       "            [-1.1788, -1.0193,  1.0574,  ...,  2.1371, -0.2229, -0.3544],\n",
       "            [ 0.1149, -2.3243, -1.8836,  ...,  1.0346,  0.3179, -0.3136],\n",
       "            ...,\n",
       "            [-1.0238,  0.7788,  0.6562,  ...,  2.1930,  2.3352, -0.1614],\n",
       "            [-1.0238,  0.7788,  0.6562,  ...,  2.1930,  2.3352, -0.1614],\n",
       "            [-1.0238,  0.7788,  0.6562,  ...,  2.1930,  2.3352, -0.1614]],\n",
       "   \n",
       "           [[-1.1641, -2.6782,  2.0281,  ...,  1.1004, -1.3733,  0.6285],\n",
       "            [ 0.1044, -1.2079,  0.8718,  ...,  1.7074, -0.1818, -0.2124],\n",
       "            [ 1.2765, -2.9646,  2.0231,  ...,  1.1498,  1.0243, -1.1657],\n",
       "            ...,\n",
       "            [-1.1223, -0.6547,  0.1831,  ...,  2.4095,  1.7945,  0.4892],\n",
       "            [-1.1223, -0.6547,  0.1831,  ...,  2.4095,  1.7945,  0.4892],\n",
       "            [-1.1223, -0.6547,  0.1831,  ...,  2.4095,  1.7945,  0.4892]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-3.9390, -1.3068, -2.3617,  ..., -2.0814, -0.9867, -3.0002],\n",
       "           [-3.3235, -1.9659, -0.1282,  ..., -0.3883, -2.1363, -3.4691],\n",
       "           [-2.2340, -2.5287, -2.2563,  ..., -2.3956, -3.1861, -3.5595],\n",
       "           ...,\n",
       "           [-4.8563, -0.8744, -2.9399,  ..., -3.5371, -1.0597, -1.3845],\n",
       "           [-4.8563, -0.8744, -2.9399,  ..., -3.5371, -1.0597, -1.3845],\n",
       "           [-4.8563, -0.8744, -2.9399,  ..., -3.5371, -1.0597, -1.3845]],\n",
       "  \n",
       "          [[-4.4147, -1.8116, -2.3161,  ..., -2.6717, -0.9920, -2.9074],\n",
       "           [-3.6935, -0.9876, -1.2635,  ..., -2.5609, -1.7750, -3.6684],\n",
       "           [-2.6571, -2.2484, -2.2241,  ..., -3.4695, -1.5912, -2.3969],\n",
       "           ...,\n",
       "           [-5.3146, -1.5636, -3.4604,  ..., -3.4035, -0.8541, -1.2456],\n",
       "           [-5.3146, -1.5636, -3.4604,  ..., -3.4035, -0.8541, -1.2456],\n",
       "           [-5.3146, -1.5636, -3.4604,  ..., -3.4035, -0.8541, -1.2456]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.16.mlp.fc2': {'input': (tensor([[[-9.3678e-05, -1.2522e-01, -2.1103e-02,  ..., -3.8753e-02,\n",
       "             -1.5989e-01, -3.6347e-03],\n",
       "            [-1.2035e-03, -4.8400e-02, -5.7564e-02,  ..., -1.3549e-01,\n",
       "             -3.4663e-02, -6.9528e-04],\n",
       "            [-2.8174e-02, -1.4025e-02, -2.6824e-02,  ..., -1.9479e-02,\n",
       "             -1.9617e-03, -4.8618e-04],\n",
       "            ...,\n",
       "            [-5.7891e-07, -1.6708e-01, -4.3929e-03,  ..., -5.3192e-04,\n",
       "             -1.5344e-01, -1.1529e-01],\n",
       "            [-5.7891e-07, -1.6708e-01, -4.3929e-03,  ..., -5.3192e-04,\n",
       "             -1.5344e-01, -1.1529e-01],\n",
       "            [-5.7891e-07, -1.6708e-01, -4.3929e-03,  ..., -5.3192e-04,\n",
       "             -1.5344e-01, -1.1529e-01]],\n",
       "   \n",
       "           [[-8.2888e-06, -6.3506e-02, -2.3454e-02,  ..., -9.6092e-03,\n",
       "             -1.5946e-01, -4.8563e-03],\n",
       "            [-2.7937e-04, -1.5982e-01, -1.3063e-01,  ..., -1.2912e-02,\n",
       "             -6.7439e-02, -3.1060e-04],\n",
       "            [-9.9996e-03, -2.7295e-02, -2.8785e-02,  ..., -6.9412e-04,\n",
       "             -8.8943e-02, -1.9419e-02],\n",
       "            ...,\n",
       "            [-0.0000e+00, -9.2380e-02, -7.1911e-04,  ..., -8.9402e-04,\n",
       "             -1.6795e-01, -1.3283e-01],\n",
       "            [-0.0000e+00, -9.2380e-02, -7.1911e-04,  ..., -8.9402e-04,\n",
       "             -1.6795e-01, -1.3283e-01],\n",
       "            [-0.0000e+00, -9.2380e-02, -7.1911e-04,  ..., -8.9402e-04,\n",
       "             -1.6795e-01, -1.3283e-01]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[-0.0423, -0.2458,  0.0968,  ..., -0.4165,  0.1148,  0.0310],\n",
       "           [ 0.0057, -0.1850,  0.0571,  ..., -0.4018,  0.0417, -0.1949],\n",
       "           [-0.0894,  0.1451,  0.1309,  ..., -0.1393,  0.0703, -0.1095],\n",
       "           ...,\n",
       "           [ 0.1209, -0.0545,  0.0847,  ...,  0.4199,  0.2526, -0.5048],\n",
       "           [ 0.1209, -0.0545,  0.0847,  ...,  0.4199,  0.2526, -0.5048],\n",
       "           [ 0.1209, -0.0545,  0.0847,  ...,  0.4199,  0.2526, -0.5048]],\n",
       "  \n",
       "          [[-0.2918,  0.5967, -0.1866,  ..., -0.6363, -0.0621, -0.0530],\n",
       "           [-0.1272,  0.4420,  0.5283,  ...,  0.2236,  0.2652, -0.9596],\n",
       "           [-0.0094, -0.0718,  0.1874,  ..., -0.2367, -0.1373, -0.2461],\n",
       "           ...,\n",
       "           [ 0.1962,  0.0148,  0.1300,  ...,  0.5212,  0.4482, -0.8210],\n",
       "           [ 0.1962,  0.0148,  0.1300,  ...,  0.5212,  0.4482, -0.8210],\n",
       "           [ 0.1962,  0.0148,  0.1300,  ...,  0.5212,  0.4482, -0.8210]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.17.self_attn.q_proj': {'input': (tensor([[[-0.4441, -0.0828, -0.1743,  ..., -0.0532,  0.3011, -0.6614],\n",
       "            [-0.9093, -0.0091,  0.9604,  ...,  0.1858, -0.3020, -0.6125],\n",
       "            [ 0.1464, -0.2732, -0.6774,  ..., -0.0910,  0.0136, -0.4421],\n",
       "            ...,\n",
       "            [-0.7625,  0.9248,  0.7197,  ...,  0.7587,  1.2878, -0.4852],\n",
       "            [-0.7625,  0.9248,  0.7197,  ...,  0.7587,  1.2878, -0.4852],\n",
       "            [-0.7625,  0.9248,  0.7197,  ...,  0.7587,  1.2878, -0.4852]],\n",
       "   \n",
       "           [[-1.0857, -0.1064,  1.2961,  ..., -0.3766, -1.0432,  0.1733],\n",
       "            [ 0.1142,  0.3400,  1.0150,  ...,  0.4439, -0.1540, -0.8787],\n",
       "            [ 1.2247, -0.6979,  1.6221,  ..., -0.1311,  0.3036, -1.1358],\n",
       "            ...,\n",
       "            [-0.8030,  0.3316,  0.4473,  ...,  0.8885,  0.9967, -0.1765],\n",
       "            [-0.8030,  0.3316,  0.4473,  ...,  0.8885,  0.9967, -0.1765],\n",
       "            [-0.8030,  0.3316,  0.4473,  ...,  0.8885,  0.9967, -0.1765]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.4878,  0.2763, -0.3204,  ..., -0.1623, -0.2803, -0.2599],\n",
       "           [ 0.0565,  0.0840,  0.5825,  ..., -0.7391, -0.2142,  0.0594],\n",
       "           [ 0.6295, -0.1658, -0.6788,  ...,  0.7362, -0.3331,  0.0915],\n",
       "           ...,\n",
       "           [ 1.2854, -0.4725, -0.7831,  ..., -0.3816, -0.1942,  0.4562],\n",
       "           [ 1.2854, -0.4725, -0.7831,  ..., -0.3816, -0.1942,  0.4562],\n",
       "           [ 1.2854, -0.4725, -0.7831,  ..., -0.3816, -0.1942,  0.4562]],\n",
       "  \n",
       "          [[ 0.2734, -0.4389, -0.4360,  ...,  0.0090, -0.4477,  0.7248],\n",
       "           [ 0.9030,  0.4223, -0.8475,  ...,  1.1533,  0.0064,  0.7452],\n",
       "           [-1.2726, -0.2768, -1.5578,  ...,  1.0305,  0.7062, -0.1759],\n",
       "           ...,\n",
       "           [ 1.8526, -0.4265, -0.2957,  ..., -0.2315, -0.6624,  0.7639],\n",
       "           [ 1.8526, -0.4265, -0.2957,  ..., -0.2315, -0.6624,  0.7639],\n",
       "           [ 1.8526, -0.4265, -0.2957,  ..., -0.2315, -0.6624,  0.7639]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.17.self_attn.k_proj': {'input': (tensor([[[-0.4441, -0.0828, -0.1743,  ..., -0.0532,  0.3011, -0.6614],\n",
       "            [-0.9093, -0.0091,  0.9604,  ...,  0.1858, -0.3020, -0.6125],\n",
       "            [ 0.1464, -0.2732, -0.6774,  ..., -0.0910,  0.0136, -0.4421],\n",
       "            ...,\n",
       "            [-0.7625,  0.9248,  0.7197,  ...,  0.7587,  1.2878, -0.4852],\n",
       "            [-0.7625,  0.9248,  0.7197,  ...,  0.7587,  1.2878, -0.4852],\n",
       "            [-0.7625,  0.9248,  0.7197,  ...,  0.7587,  1.2878, -0.4852]],\n",
       "   \n",
       "           [[-1.0857, -0.1064,  1.2961,  ..., -0.3766, -1.0432,  0.1733],\n",
       "            [ 0.1142,  0.3400,  1.0150,  ...,  0.4439, -0.1540, -0.8787],\n",
       "            [ 1.2247, -0.6979,  1.6221,  ..., -0.1311,  0.3036, -1.1358],\n",
       "            ...,\n",
       "            [-0.8030,  0.3316,  0.4473,  ...,  0.8885,  0.9967, -0.1765],\n",
       "            [-0.8030,  0.3316,  0.4473,  ...,  0.8885,  0.9967, -0.1765],\n",
       "            [-0.8030,  0.3316,  0.4473,  ...,  0.8885,  0.9967, -0.1765]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.5870,  0.5765, -1.4252,  ...,  0.1515, -0.9451,  0.1247],\n",
       "           [-0.0065, -0.6822, -0.1164,  ..., -0.5325,  0.1576,  0.5097],\n",
       "           [ 1.0839, -0.5855, -0.6135,  ...,  1.5452,  0.1879, -0.7283],\n",
       "           ...,\n",
       "           [ 0.4283,  1.0612, -1.5797,  ..., -1.4978, -0.4319,  1.9303],\n",
       "           [ 0.4283,  1.0612, -1.5797,  ..., -1.4978, -0.4319,  1.9303],\n",
       "           [ 0.4283,  1.0612, -1.5797,  ..., -1.4978, -0.4319,  1.9303]],\n",
       "  \n",
       "          [[ 0.3864,  0.1045, -1.8682,  ...,  0.7528, -1.0578,  0.2631],\n",
       "           [ 0.3634,  0.3488, -2.4146,  ...,  0.9277, -0.9881, -0.8253],\n",
       "           [-0.2370,  0.9402, -1.4834,  ...,  0.3179, -0.5519,  0.4885],\n",
       "           ...,\n",
       "           [ 0.5114,  1.0217, -1.2129,  ..., -0.5343, -0.5105,  1.5662],\n",
       "           [ 0.5114,  1.0217, -1.2129,  ..., -0.5343, -0.5105,  1.5662],\n",
       "           [ 0.5114,  1.0217, -1.2129,  ..., -0.5343, -0.5105,  1.5662]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.17.self_attn.v_proj': {'input': (tensor([[[-0.4441, -0.0828, -0.1743,  ..., -0.0532,  0.3011, -0.6614],\n",
       "            [-0.9093, -0.0091,  0.9604,  ...,  0.1858, -0.3020, -0.6125],\n",
       "            [ 0.1464, -0.2732, -0.6774,  ..., -0.0910,  0.0136, -0.4421],\n",
       "            ...,\n",
       "            [-0.7625,  0.9248,  0.7197,  ...,  0.7587,  1.2878, -0.4852],\n",
       "            [-0.7625,  0.9248,  0.7197,  ...,  0.7587,  1.2878, -0.4852],\n",
       "            [-0.7625,  0.9248,  0.7197,  ...,  0.7587,  1.2878, -0.4852]],\n",
       "   \n",
       "           [[-1.0857, -0.1064,  1.2961,  ..., -0.3766, -1.0432,  0.1733],\n",
       "            [ 0.1142,  0.3400,  1.0150,  ...,  0.4439, -0.1540, -0.8787],\n",
       "            [ 1.2247, -0.6979,  1.6221,  ..., -0.1311,  0.3036, -1.1358],\n",
       "            ...,\n",
       "            [-0.8030,  0.3316,  0.4473,  ...,  0.8885,  0.9967, -0.1765],\n",
       "            [-0.8030,  0.3316,  0.4473,  ...,  0.8885,  0.9967, -0.1765],\n",
       "            [-0.8030,  0.3316,  0.4473,  ...,  0.8885,  0.9967, -0.1765]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 2.7939, -0.6296,  1.6531,  ..., -0.2121,  0.2115, -1.1852],\n",
       "           [ 0.9532, -0.3059,  0.3584,  ..., -0.4871, -0.4316, -0.6477],\n",
       "           [-0.0601,  0.1530, -0.2480,  ...,  0.1936,  0.7940, -0.2825],\n",
       "           ...,\n",
       "           [-0.5428,  0.8574,  1.0138,  ..., -2.2212, -1.3071,  1.1080],\n",
       "           [-0.5428,  0.8574,  1.0138,  ..., -2.2212, -1.3071,  1.1080],\n",
       "           [-0.5428,  0.8574,  1.0138,  ..., -2.2212, -1.3071,  1.1080]],\n",
       "  \n",
       "          [[ 1.5335, -0.9785,  0.0237,  ...,  0.4459, -0.3884, -0.7946],\n",
       "           [ 0.3834,  0.3964,  1.3202,  ...,  1.0237, -1.2528,  0.6907],\n",
       "           [ 0.3884, -0.8594,  0.9431,  ..., -0.6587, -0.4050,  1.4072],\n",
       "           ...,\n",
       "           [-0.7827,  1.4270,  0.8312,  ..., -1.6736, -1.3779,  1.5495],\n",
       "           [-0.7827,  1.4270,  0.8312,  ..., -1.6736, -1.3779,  1.5495],\n",
       "           [-0.7827,  1.4270,  0.8312,  ..., -1.6736, -1.3779,  1.5495]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.17.self_attn.out_proj': {'input': (tensor([[[ 1.1637, -0.4709,  0.1636,  ...,  0.1218, -0.0625,  0.1059],\n",
       "            [ 0.4100, -0.6228,  0.0267,  ...,  0.0526, -0.0083,  0.0641],\n",
       "            [-0.0277, -0.0713, -0.0944,  ...,  0.2680, -0.1725,  0.3510],\n",
       "            ...,\n",
       "            [ 0.6479, -1.1624, -0.0043,  ...,  0.0221, -0.0976,  0.2169],\n",
       "            [ 0.6479, -1.1624, -0.0043,  ...,  0.0221, -0.0976,  0.2169],\n",
       "            [ 0.6479, -1.1624, -0.0043,  ...,  0.0221, -0.0976,  0.2169]],\n",
       "   \n",
       "           [[ 0.6246, -0.4530,  0.2079,  ...,  0.0838, -0.0592,  0.1740],\n",
       "            [ 0.3249, -0.7598,  0.2044,  ...,  0.0472,  0.0140,  0.1532],\n",
       "            [ 0.1109, -0.6072,  0.2431,  ...,  0.0100,  0.0886,  0.1270],\n",
       "            ...,\n",
       "            [ 0.2925, -1.2574, -0.0798,  ...,  0.0120, -0.1221,  0.3027],\n",
       "            [ 0.2925, -1.2574, -0.0798,  ...,  0.0120, -0.1221,  0.3027],\n",
       "            [ 0.2925, -1.2574, -0.0798,  ...,  0.0120, -0.1221,  0.3027]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.7186, -0.5168, -0.0416,  ..., -0.0560,  0.1486, -0.2482],\n",
       "           [-0.4799, -0.3389, -0.1945,  ..., -0.0800, -0.0534,  0.0046],\n",
       "           [-0.2897, -0.6363,  0.1805,  ...,  0.4615,  0.2967, -0.3018],\n",
       "           ...,\n",
       "           [-0.8219, -0.5392,  0.0246,  ..., -0.1491, -0.0844, -0.1097],\n",
       "           [-0.8219, -0.5392,  0.0246,  ..., -0.1491, -0.0844, -0.1097],\n",
       "           [-0.8219, -0.5392,  0.0246,  ..., -0.1491, -0.0844, -0.1097]],\n",
       "  \n",
       "          [[-0.6591, -0.5793, -0.0298,  ...,  0.0314,  0.0790, -0.1807],\n",
       "           [-0.6444, -0.3131, -0.0478,  ..., -0.0486,  0.0124, -0.2331],\n",
       "           [-0.6350, -0.4346,  0.1016,  ...,  0.0690,  0.0947, -0.0587],\n",
       "           ...,\n",
       "           [-0.6116, -0.4736,  0.1928,  ...,  0.1103, -0.0952,  0.0337],\n",
       "           [-0.6116, -0.4736,  0.1928,  ...,  0.1103, -0.0952,  0.0337],\n",
       "           [-0.6116, -0.4736,  0.1928,  ...,  0.1103, -0.0952,  0.0337]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.17.mlp.fc1': {'input': (tensor([[[-4.9917, -5.6126, -0.3550,  ...,  1.8848,  1.5520, -1.8222],\n",
       "            [-6.0976, -5.2246,  1.4701,  ...,  2.4557, -0.1235, -1.4052],\n",
       "            [-2.6642, -6.4028, -1.0447,  ...,  2.6240,  1.0539, -1.4421],\n",
       "            ...,\n",
       "            [-5.2178, -0.8551,  1.3884,  ...,  4.1641,  3.4534, -1.2597],\n",
       "            [-5.2178, -0.8551,  1.3884,  ...,  4.1641,  3.4534, -1.2597],\n",
       "            [-5.2178, -0.8551,  1.3884,  ...,  4.1641,  3.4534, -1.2597]],\n",
       "   \n",
       "           [[-6.3493, -5.7563,  2.4104,  ...,  1.1242, -1.5399, -0.2572],\n",
       "            [-3.2445, -3.1416,  1.8640,  ...,  3.3325,  0.3361, -2.1129],\n",
       "            [-0.8898, -7.9357,  3.1770,  ...,  1.9218,  1.4798, -2.3877],\n",
       "            ...,\n",
       "            [-5.0733, -3.1159,  0.9805,  ...,  4.7746,  2.7915, -0.6234],\n",
       "            [-5.0733, -3.1159,  0.9805,  ...,  4.7746,  2.7915, -0.6234],\n",
       "            [-5.0733, -3.1159,  0.9805,  ...,  4.7746,  2.7915, -0.6234]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ -8.2373,  -8.3900,  -6.4644,  ...,  -4.6565,  -4.3889,  -2.2509],\n",
       "           [ -5.6892,  -7.2787,  -4.9842,  ...,  -5.8220,  -4.2282,  -2.1258],\n",
       "           [ -6.3374,  -7.1891,  -4.2646,  ...,  -3.8591,  -4.4546,  -3.1213],\n",
       "           ...,\n",
       "           [ -5.6848,  -6.6256,  -3.5019,  ...,  -2.1496,  -2.1857,  -3.2535],\n",
       "           [ -5.6848,  -6.6256,  -3.5019,  ...,  -2.1496,  -2.1857,  -3.2535],\n",
       "           [ -5.6848,  -6.6256,  -3.5019,  ...,  -2.1496,  -2.1857,  -3.2535]],\n",
       "  \n",
       "          [[ -9.9067,  -7.6908,  -5.7804,  ...,  -1.9396,  -2.6085,  -5.3157],\n",
       "           [-10.5095,  -7.6423,  -4.6757,  ...,  -2.7149,  -3.1166,  -5.1688],\n",
       "           [ -6.9636,  -7.5914,  -5.7034,  ...,  -2.4344,  -3.3689,  -3.5226],\n",
       "           ...,\n",
       "           [ -6.4440,  -6.9335,  -4.0863,  ...,  -3.1503,  -0.5703,  -4.0154],\n",
       "           [ -6.4440,  -6.9335,  -4.0863,  ...,  -3.1503,  -0.5703,  -4.0154],\n",
       "           [ -6.4440,  -6.9335,  -4.0863,  ...,  -3.1503,  -0.5703,  -4.0154]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.17.mlp.fc2': {'input': (tensor([[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -2.0816e-06,\n",
       "             -9.5484e-06, -2.7147e-02],\n",
       "            [-0.0000e+00, -0.0000e+00, -2.9708e-07,  ..., -0.0000e+00,\n",
       "             -2.2556e-05, -3.5425e-02],\n",
       "            [-0.0000e+00, -0.0000e+00, -1.8683e-05,  ..., -1.3514e-04,\n",
       "             -6.6379e-06, -2.4463e-03],\n",
       "            ...,\n",
       "            [-0.0000e+00, -0.0000e+00, -6.1158e-04,  ..., -3.3725e-02,\n",
       "             -3.1263e-02, -1.5494e-03],\n",
       "            [-0.0000e+00, -0.0000e+00, -6.1158e-04,  ..., -3.3725e-02,\n",
       "             -3.1263e-02, -1.5494e-03],\n",
       "            [-0.0000e+00, -0.0000e+00, -6.1158e-04,  ..., -3.3725e-02,\n",
       "             -3.1263e-02, -1.5494e-03]],\n",
       "   \n",
       "           [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -5.0797e-02,\n",
       "             -1.1394e-02, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -1.8115e-06,  ..., -8.5263e-03,\n",
       "             -2.4847e-03, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -1.7744e-02,\n",
       "             -1.0181e-03, -5.6344e-04],\n",
       "            ...,\n",
       "            [-0.0000e+00, -0.0000e+00, -4.6277e-05,  ..., -2.2179e-03,\n",
       "             -1.6213e-01, -6.5219e-05],\n",
       "            [-0.0000e+00, -0.0000e+00, -4.6277e-05,  ..., -2.2179e-03,\n",
       "             -1.6213e-01, -6.5219e-05],\n",
       "            [-0.0000e+00, -0.0000e+00, -4.6277e-05,  ..., -2.2179e-03,\n",
       "             -1.6213e-01, -6.5219e-05]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[-0.0528,  0.2534, -0.1723,  ..., -0.2376,  0.0164,  0.2474],\n",
       "           [ 0.0738,  0.4623,  0.1714,  ..., -0.3396,  0.1175,  0.0168],\n",
       "           [ 0.2998,  0.3274, -0.1257,  ..., -0.1750, -0.1112,  0.1365],\n",
       "           ...,\n",
       "           [-0.1103,  0.1244, -0.7621,  ..., -0.2189,  0.0628, -0.0433],\n",
       "           [-0.1103,  0.1244, -0.7621,  ..., -0.2189,  0.0628, -0.0433],\n",
       "           [-0.1103,  0.1244, -0.7621,  ..., -0.2189,  0.0628, -0.0433]],\n",
       "  \n",
       "          [[ 0.1432,  0.2243, -0.2310,  ..., -0.6254, -0.2904,  0.1344],\n",
       "           [ 0.3071, -0.0927, -0.2373,  ..., -0.8071,  0.5401,  0.4614],\n",
       "           [ 0.2612,  0.4890,  0.0065,  ..., -0.4435, -0.1950,  0.0533],\n",
       "           ...,\n",
       "           [-0.0505,  0.1515, -0.8137,  ..., -0.4281, -0.0930, -0.1455],\n",
       "           [-0.0505,  0.1515, -0.8137,  ..., -0.4281, -0.0930, -0.1455],\n",
       "           [-0.0505,  0.1515, -0.8137,  ..., -0.4281, -0.0930, -0.1455]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.18.self_attn.q_proj': {'input': (tensor([[[-0.9583, -0.2281, -0.2400,  ..., -0.3026,  0.3672, -0.6085],\n",
       "            [-1.1257,  0.1337,  0.8843,  ..., -0.2386, -0.2710, -0.5000],\n",
       "            [ 0.1217, -0.4153, -0.5581,  ...,  0.0182,  0.0721, -0.5134],\n",
       "            ...,\n",
       "            [-1.0426,  0.7651,  0.4989,  ...,  0.5746,  1.2330, -0.5027],\n",
       "            [-1.0426,  0.7651,  0.4989,  ...,  0.5746,  1.2330, -0.5027],\n",
       "            [-1.0426,  0.7651,  0.4989,  ...,  0.5746,  1.2330, -0.5027]],\n",
       "   \n",
       "           [[-1.3181, -0.2911,  1.1274,  ..., -0.7855, -1.2110,  0.1262],\n",
       "            [-0.0879,  0.1258,  0.8314,  ..., -0.0641,  0.0785, -0.6470],\n",
       "            [ 0.8206, -0.5954,  1.7077,  ..., -0.4609,  0.1846, -1.0513],\n",
       "            ...,\n",
       "            [-0.9639,  0.2365,  0.2893,  ...,  0.7216,  0.8803, -0.2011],\n",
       "            [-0.9639,  0.2365,  0.2893,  ...,  0.7216,  0.8803, -0.2011],\n",
       "            [-0.9639,  0.2365,  0.2893,  ...,  0.7216,  0.8803, -0.2011]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.2321, -1.1067,  3.2107,  ...,  1.3571, -0.6966,  3.7223],\n",
       "           [ 0.7056, -1.3878,  2.6313,  ...,  1.4096,  0.9866,  0.4813],\n",
       "           [-0.5596,  1.0119,  1.2738,  ..., -1.1215, -0.7888,  2.3100],\n",
       "           ...,\n",
       "           [-1.8021, -0.5865,  2.3665,  ...,  0.9499,  0.1404,  2.6618],\n",
       "           [-1.8021, -0.5865,  2.3665,  ...,  0.9499,  0.1404,  2.6618],\n",
       "           [-1.8021, -0.5865,  2.3665,  ...,  0.9499,  0.1404,  2.6618]],\n",
       "  \n",
       "          [[ 0.4587, -0.9253,  1.8567,  ..., -0.2252,  1.6835,  3.7632],\n",
       "           [ 1.0619, -2.0779, -0.9654,  ..., -0.4831,  1.0648,  4.6452],\n",
       "           [ 1.2288, -0.7696, -0.4025,  ..., -0.2555,  0.8905, -0.8597],\n",
       "           ...,\n",
       "           [-2.1868,  0.0376,  1.2847,  ...,  0.4492,  0.2377,  3.3347],\n",
       "           [-2.1868,  0.0376,  1.2847,  ...,  0.4492,  0.2377,  3.3347],\n",
       "           [-2.1869,  0.0376,  1.2847,  ...,  0.4492,  0.2377,  3.3347]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.18.self_attn.k_proj': {'input': (tensor([[[-0.9583, -0.2281, -0.2400,  ..., -0.3026,  0.3672, -0.6085],\n",
       "            [-1.1257,  0.1337,  0.8843,  ..., -0.2386, -0.2710, -0.5000],\n",
       "            [ 0.1217, -0.4153, -0.5581,  ...,  0.0182,  0.0721, -0.5134],\n",
       "            ...,\n",
       "            [-1.0426,  0.7651,  0.4989,  ...,  0.5746,  1.2330, -0.5027],\n",
       "            [-1.0426,  0.7651,  0.4989,  ...,  0.5746,  1.2330, -0.5027],\n",
       "            [-1.0426,  0.7651,  0.4989,  ...,  0.5746,  1.2330, -0.5027]],\n",
       "   \n",
       "           [[-1.3181, -0.2911,  1.1274,  ..., -0.7855, -1.2110,  0.1262],\n",
       "            [-0.0879,  0.1258,  0.8314,  ..., -0.0641,  0.0785, -0.6470],\n",
       "            [ 0.8206, -0.5954,  1.7077,  ..., -0.4609,  0.1846, -1.0513],\n",
       "            ...,\n",
       "            [-0.9639,  0.2365,  0.2893,  ...,  0.7216,  0.8803, -0.2011],\n",
       "            [-0.9639,  0.2365,  0.2893,  ...,  0.7216,  0.8803, -0.2011],\n",
       "            [-0.9639,  0.2365,  0.2893,  ...,  0.7216,  0.8803, -0.2011]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.3653, -1.2184, -0.0035,  ...,  2.6831, -0.6485,  1.0615],\n",
       "           [ 1.2808, -1.3749, -0.7324,  ...,  2.4356, -0.2546, -0.2674],\n",
       "           [ 0.2145, -0.4414,  0.7971,  ...,  1.4882,  1.6015,  0.4897],\n",
       "           ...,\n",
       "           [ 1.2061, -1.0643,  0.9058,  ...,  1.4605,  0.3125, -1.3594],\n",
       "           [ 1.2061, -1.0643,  0.9058,  ...,  1.4605,  0.3125, -1.3594],\n",
       "           [ 1.2061, -1.0643,  0.9058,  ...,  1.4605,  0.3125, -1.3594]],\n",
       "  \n",
       "          [[ 0.3689, -0.5092, -0.5185,  ...,  2.0787, -0.0873,  1.2394],\n",
       "           [ 2.3323, -1.4812, -0.9965,  ...,  1.8301, -0.3838,  2.0375],\n",
       "           [ 1.1723, -1.5917,  0.9478,  ...,  0.5397,  0.3643, -2.5127],\n",
       "           ...,\n",
       "           [ 1.0548, -0.8823,  1.1095,  ...,  1.5212,  1.0580, -1.5939],\n",
       "           [ 1.0548, -0.8823,  1.1095,  ...,  1.5212,  1.0580, -1.5939],\n",
       "           [ 1.0548, -0.8823,  1.1095,  ...,  1.5212,  1.0580, -1.5939]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.18.self_attn.v_proj': {'input': (tensor([[[-0.9583, -0.2281, -0.2400,  ..., -0.3026,  0.3672, -0.6085],\n",
       "            [-1.1257,  0.1337,  0.8843,  ..., -0.2386, -0.2710, -0.5000],\n",
       "            [ 0.1217, -0.4153, -0.5581,  ...,  0.0182,  0.0721, -0.5134],\n",
       "            ...,\n",
       "            [-1.0426,  0.7651,  0.4989,  ...,  0.5746,  1.2330, -0.5027],\n",
       "            [-1.0426,  0.7651,  0.4989,  ...,  0.5746,  1.2330, -0.5027],\n",
       "            [-1.0426,  0.7651,  0.4989,  ...,  0.5746,  1.2330, -0.5027]],\n",
       "   \n",
       "           [[-1.3181, -0.2911,  1.1274,  ..., -0.7855, -1.2110,  0.1262],\n",
       "            [-0.0879,  0.1258,  0.8314,  ..., -0.0641,  0.0785, -0.6470],\n",
       "            [ 0.8206, -0.5954,  1.7077,  ..., -0.4609,  0.1846, -1.0513],\n",
       "            ...,\n",
       "            [-0.9639,  0.2365,  0.2893,  ...,  0.7216,  0.8803, -0.2011],\n",
       "            [-0.9639,  0.2365,  0.2893,  ...,  0.7216,  0.8803, -0.2011],\n",
       "            [-0.9639,  0.2365,  0.2893,  ...,  0.7216,  0.8803, -0.2011]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.0982, -1.4506, -1.1812,  ..., -0.1141,  0.2387,  1.9641],\n",
       "           [-0.2776,  0.2275, -0.5394,  ...,  0.4601, -0.6944,  1.9249],\n",
       "           [-0.5217, -0.2084, -0.4025,  ..., -0.3391, -0.2230, -0.5810],\n",
       "           ...,\n",
       "           [ 0.3536, -0.5124,  0.6633,  ...,  1.2266, -1.1606,  1.1701],\n",
       "           [ 0.3536, -0.5124,  0.6633,  ...,  1.2266, -1.1606,  1.1701],\n",
       "           [ 0.3536, -0.5124,  0.6633,  ...,  1.2266, -1.1606,  1.1701]],\n",
       "  \n",
       "          [[ 0.8092, -1.0682, -1.5672,  ..., -0.2586, -1.6734,  2.5818],\n",
       "           [-0.7794, -0.2565, -0.1359,  ...,  0.2015, -0.6198,  2.2065],\n",
       "           [-1.0412,  1.0632, -0.8297,  ..., -0.3557,  0.0688,  1.4677],\n",
       "           ...,\n",
       "           [ 0.1162, -0.9259,  0.4220,  ...,  1.1798, -1.2560,  1.2632],\n",
       "           [ 0.1162, -0.9259,  0.4220,  ...,  1.1798, -1.2560,  1.2632],\n",
       "           [ 0.1162, -0.9259,  0.4220,  ...,  1.1798, -1.2560,  1.2632]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.18.self_attn.out_proj': {'input': (tensor([[[ 1.6719e-01, -8.9860e-02, -4.0156e-01,  ..., -1.1870e-01,\n",
       "             -2.7411e-01, -2.7470e-01],\n",
       "            [-2.2359e-01, -1.1493e-01, -1.8941e-01,  ..., -1.1471e-01,\n",
       "             -1.8707e-01, -1.9758e-01],\n",
       "            [-1.1272e-01, -4.3313e-01, -3.5441e-01,  ...,  5.5334e-02,\n",
       "             -1.6668e-01,  2.4336e-01],\n",
       "            ...,\n",
       "            [-4.6414e-01, -4.3920e-02, -2.8040e-01,  ...,  5.9603e-02,\n",
       "             -1.1240e-01,  2.5859e-01],\n",
       "            [-4.6414e-01, -4.3920e-02, -2.8040e-01,  ...,  5.9603e-02,\n",
       "             -1.1240e-01,  2.5859e-01],\n",
       "            [-4.6414e-01, -4.3920e-02, -2.8040e-01,  ...,  5.9603e-02,\n",
       "             -1.1240e-01,  2.5859e-01]],\n",
       "   \n",
       "           [[ 4.3911e-01, -4.7260e-01, -6.7650e-01,  ..., -2.1593e-04,\n",
       "             -3.1087e-01,  1.6592e-01],\n",
       "            [ 1.9532e-01, -3.7283e-01, -4.4110e-01,  ...,  1.0583e-01,\n",
       "             -3.6256e-01,  6.7236e-01],\n",
       "            [-1.6802e-01,  3.9164e-02, -4.6508e-04,  ..., -4.6999e-02,\n",
       "             -8.8862e-02,  1.2735e-01],\n",
       "            ...,\n",
       "            [-2.5892e-01, -8.5553e-02, -3.1450e-01,  ...,  1.3249e-01,\n",
       "             -2.4595e-01,  6.3046e-01],\n",
       "            [-2.5892e-01, -8.5553e-02, -3.1450e-01,  ...,  1.3249e-01,\n",
       "             -2.4595e-01,  6.3046e-01],\n",
       "            [-2.5892e-01, -8.5553e-02, -3.1450e-01,  ...,  1.3249e-01,\n",
       "             -2.4595e-01,  6.3046e-01]]], grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.2831,  0.5500,  0.3185,  ..., -0.4277, -0.4357, -0.1909],\n",
       "           [-0.3544, -0.0722,  0.1091,  ..., -0.2920, -0.2233, -0.1894],\n",
       "           [-0.0727, -0.1798, -0.0881,  ...,  0.2495,  0.1146, -0.9880],\n",
       "           ...,\n",
       "           [-0.4351,  0.0147,  0.2296,  ..., -0.2235, -0.0867,  0.0260],\n",
       "           [-0.4351,  0.0147,  0.2296,  ..., -0.2235, -0.0867,  0.0260],\n",
       "           [-0.4351,  0.0147,  0.2296,  ..., -0.2235, -0.0867,  0.0260]],\n",
       "  \n",
       "          [[-0.2270,  0.1665,  0.0873,  ...,  0.3188, -0.0982, -0.0332],\n",
       "           [-0.3247,  0.1375,  0.1400,  ...,  0.1273,  0.2896, -0.2297],\n",
       "           [-0.3351, -0.0932,  0.2809,  ..., -0.0401, -0.4152, -0.0843],\n",
       "           ...,\n",
       "           [-0.5220,  0.1223,  0.3564,  ..., -0.2376, -0.2949,  0.0946],\n",
       "           [-0.5220,  0.1223,  0.3564,  ..., -0.2376, -0.2949,  0.0946],\n",
       "           [-0.5220,  0.1223,  0.3564,  ..., -0.2376, -0.2949,  0.0946]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.18.mlp.fc1': {'input': (tensor([[[-4.4490, -1.9688,  1.2944,  ...,  0.4496,  1.1951, -1.7803],\n",
       "            [-5.2414, -2.2388,  4.2036,  ...,  0.7120, -0.0816, -1.6685],\n",
       "            [-1.5299, -4.3270, -0.3158,  ...,  2.7995,  1.3295, -2.4567],\n",
       "            ...,\n",
       "            [-4.5580,  0.2638,  2.9799,  ...,  3.8567,  3.8393, -1.2796],\n",
       "            [-4.5580,  0.2638,  2.9799,  ...,  3.8567,  3.8393, -1.2796],\n",
       "            [-4.5580,  0.2638,  2.9799,  ...,  3.8567,  3.8393, -1.2796]],\n",
       "   \n",
       "           [[-5.1832, -3.1802,  4.6798,  ...,  0.4632, -1.9492, -0.0653],\n",
       "            [-2.3487, -1.7765,  3.9749,  ...,  2.3030,  1.5722, -1.8722],\n",
       "            [-0.3574, -4.9348,  6.8363,  ...,  0.7461,  0.7413, -2.5672],\n",
       "            ...,\n",
       "            [-4.4250, -1.5110,  2.4967,  ...,  4.3225,  2.8680, -0.6234],\n",
       "            [-4.4250, -1.5110,  2.4967,  ...,  4.3225,  2.8680, -0.6234],\n",
       "            [-4.4250, -1.5110,  2.4967,  ...,  4.3225,  2.8680, -0.6234]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ -8.3389,  -7.4701,  -6.7769,  ...,  -6.1700,  -7.4790,  -5.2485],\n",
       "           [ -6.4218,  -9.6297,  -3.9640,  ...,  -5.0735,  -7.1597,  -5.7128],\n",
       "           [ -5.8129,  -6.6575,  -6.7758,  ...,  -5.0393,  -9.9182,  -5.5083],\n",
       "           ...,\n",
       "           [ -4.6799,  -8.0907,  -5.7025,  ...,  -6.0097,  -1.8397,  -9.9994],\n",
       "           [ -4.6799,  -8.0907,  -5.7025,  ...,  -6.0097,  -1.8397,  -9.9994],\n",
       "           [ -4.6799,  -8.0907,  -5.7025,  ...,  -6.0097,  -1.8397,  -9.9994]],\n",
       "  \n",
       "          [[ -4.7016, -10.7661,  -5.0515,  ...,  -3.5798,  -6.1889,  -7.1655],\n",
       "           [ -6.7731,  -9.0569,  -4.0939,  ...,  -4.3631,  -5.6671,  -7.1313],\n",
       "           [ -6.3023,  -7.5127,  -4.4505,  ...,  -3.8139,  -4.7582,  -8.3988],\n",
       "           ...,\n",
       "           [ -4.0858,  -7.9849,  -7.2692,  ...,  -7.4972,  -1.4237,  -8.8558],\n",
       "           [ -4.0858,  -7.9849,  -7.2692,  ...,  -7.4972,  -1.4237,  -8.8558],\n",
       "           [ -4.0858,  -7.9849,  -7.2692,  ...,  -7.4972,  -1.4237,  -8.8558]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.18.mlp.fc2': {'input': (tensor([[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -8.3287e-05,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -1.5018e-07,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            ...,\n",
       "            [-1.8131e-06, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -6.0569e-02, -0.0000e+00],\n",
       "            [-1.8131e-06, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -6.0569e-02, -0.0000e+00],\n",
       "            [-1.8131e-06, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -6.0570e-02, -0.0000e+00]],\n",
       "   \n",
       "           [[-1.5413e-06, -0.0000e+00, -1.5055e-07,  ..., -4.4808e-04,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -4.4533e-05,  ..., -1.1053e-05,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -6.7644e-06,  ..., -1.6561e-04,\n",
       "             -1.1344e-06, -0.0000e+00],\n",
       "            ...,\n",
       "            [-4.6393e-05, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -1.1024e-01, -0.0000e+00],\n",
       "            [-4.6393e-05, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -1.1024e-01, -0.0000e+00],\n",
       "            [-4.6394e-05, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -1.1024e-01, -0.0000e+00]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[ 0.2961,  0.1853, -0.0808,  ...,  0.0500,  0.4153,  0.4540],\n",
       "           [-0.2347, -0.4951, -0.3315,  ..., -0.8028, -0.5110,  0.1076],\n",
       "           [ 0.2040,  0.1055, -0.4723,  ..., -0.2202, -0.0378,  0.0626],\n",
       "           ...,\n",
       "           [ 0.7348, -1.0495, -1.3050,  ...,  1.1194,  0.6365, -1.6617],\n",
       "           [ 0.7348, -1.0495, -1.3050,  ...,  1.1194,  0.6365, -1.6617],\n",
       "           [ 0.7348, -1.0495, -1.3050,  ...,  1.1194,  0.6365, -1.6617]],\n",
       "  \n",
       "          [[ 0.6507,  0.3976, -0.4499,  ..., -0.1858, -0.3392, -0.1799],\n",
       "           [ 0.7406, -1.3123, -0.4522,  ..., -0.0151, -0.8792, -1.2768],\n",
       "           [ 0.2847, -0.0137, -0.5254,  ...,  0.0501, -0.2540,  0.6755],\n",
       "           ...,\n",
       "           [ 1.4256, -1.4998, -1.6170,  ...,  2.3411,  1.3354, -2.4213],\n",
       "           [ 1.4256, -1.4998, -1.6170,  ...,  2.3411,  1.3354, -2.4213],\n",
       "           [ 1.4256, -1.4998, -1.6170,  ...,  2.3411,  1.3354, -2.4213]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.19.self_attn.q_proj': {'input': (tensor([[[-0.9957,  0.4638,  0.0848,  ..., -0.6393,  0.3484, -0.3930],\n",
       "            [-1.7302, -0.1888,  0.9129,  ..., -1.2974, -0.9175, -0.5413],\n",
       "            [ 0.2159, -0.2970, -0.7937,  ..., -0.0236,  0.1106, -1.0735],\n",
       "            ...,\n",
       "            [-0.9064,  0.5616,  0.2866,  ...,  0.7921,  1.3105, -1.0100],\n",
       "            [-0.9064,  0.5616,  0.2866,  ...,  0.7921,  1.3105, -1.0100],\n",
       "            [-0.9064,  0.5616,  0.2866,  ...,  0.7921,  1.3105, -1.0100]],\n",
       "   \n",
       "           [[-1.0716,  0.2411,  1.1083,  ..., -0.7656, -1.4626,  0.0316],\n",
       "            [ 0.1402, -0.2785,  0.8487,  ..., -0.0609, -0.2198, -1.3709],\n",
       "            [ 0.8255, -0.4935,  1.7749,  ..., -0.5141, -0.3117, -0.5546],\n",
       "            ...,\n",
       "            [-0.6175, -0.0186,  0.0438,  ...,  1.2490,  1.0757, -0.8833],\n",
       "            [-0.6175, -0.0186,  0.0438,  ...,  1.2490,  1.0757, -0.8833],\n",
       "            [-0.6175, -0.0186,  0.0438,  ...,  1.2490,  1.0757, -0.8833]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.1577,  0.2816,  0.8740,  ..., -0.2134,  1.3509, -1.7474],\n",
       "           [-0.5116, -1.2237, -0.7137,  ..., -0.8972,  0.9380, -0.6718],\n",
       "           [-0.9730,  0.0512,  1.1697,  ..., -0.1800,  0.0110,  0.7312],\n",
       "           ...,\n",
       "           [ 0.1599, -0.0852, -2.2673,  ..., -2.2124,  0.0307,  2.1001],\n",
       "           [ 0.1599, -0.0852, -2.2673,  ..., -2.2124,  0.0307,  2.1001],\n",
       "           [ 0.1599, -0.0852, -2.2673,  ..., -2.2124,  0.0307,  2.1001]],\n",
       "  \n",
       "          [[ 0.1971,  0.2787,  0.2275,  ...,  0.3504, -0.2545, -2.4514],\n",
       "           [ 0.2247,  0.2058,  0.4662,  ..., -0.3338, -1.2110, -1.9969],\n",
       "           [ 0.3074, -1.4261, -0.0942,  ..., -0.4910,  0.8649, -1.9786],\n",
       "           ...,\n",
       "           [ 0.2494,  0.0697, -2.0266,  ..., -1.7694, -0.6141,  2.6575],\n",
       "           [ 0.2494,  0.0697, -2.0266,  ..., -1.7694, -0.6141,  2.6575],\n",
       "           [ 0.2494,  0.0697, -2.0266,  ..., -1.7694, -0.6141,  2.6575]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.19.self_attn.k_proj': {'input': (tensor([[[-0.9957,  0.4638,  0.0848,  ..., -0.6393,  0.3484, -0.3930],\n",
       "            [-1.7302, -0.1888,  0.9129,  ..., -1.2974, -0.9175, -0.5413],\n",
       "            [ 0.2159, -0.2970, -0.7937,  ..., -0.0236,  0.1106, -1.0735],\n",
       "            ...,\n",
       "            [-0.9064,  0.5616,  0.2866,  ...,  0.7921,  1.3105, -1.0100],\n",
       "            [-0.9064,  0.5616,  0.2866,  ...,  0.7921,  1.3105, -1.0100],\n",
       "            [-0.9064,  0.5616,  0.2866,  ...,  0.7921,  1.3105, -1.0100]],\n",
       "   \n",
       "           [[-1.0716,  0.2411,  1.1083,  ..., -0.7656, -1.4626,  0.0316],\n",
       "            [ 0.1402, -0.2785,  0.8487,  ..., -0.0609, -0.2198, -1.3709],\n",
       "            [ 0.8255, -0.4935,  1.7749,  ..., -0.5141, -0.3117, -0.5546],\n",
       "            ...,\n",
       "            [-0.6175, -0.0186,  0.0438,  ...,  1.2490,  1.0757, -0.8833],\n",
       "            [-0.6175, -0.0186,  0.0438,  ...,  1.2490,  1.0757, -0.8833],\n",
       "            [-0.6175, -0.0186,  0.0438,  ...,  1.2490,  1.0757, -0.8833]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.7283,  0.6823,  1.8388,  ...,  0.5387, -0.7343, -3.1956],\n",
       "           [-0.3568, -1.9797,  0.2813,  ..., -1.5197,  0.1558, -1.4671],\n",
       "           [-0.5986,  0.3234,  0.3139,  ...,  0.0497, -0.0737, -1.3005],\n",
       "           ...,\n",
       "           [-0.6869, -1.1591, -1.6700,  ..., -1.1338,  0.6425,  0.2757],\n",
       "           [-0.6869, -1.1591, -1.6700,  ..., -1.1338,  0.6425,  0.2757],\n",
       "           [-0.6869, -1.1591, -1.6700,  ..., -1.1338,  0.6425,  0.2757]],\n",
       "  \n",
       "          [[-0.2778,  0.0248,  0.0091,  ..., -0.3705, -0.0903, -1.6648],\n",
       "           [ 0.6692, -0.4393,  0.0864,  ..., -1.8150, -0.6328, -1.1578],\n",
       "           [-0.3308, -1.4604,  0.0051,  ..., -0.6965,  0.1619, -0.6007],\n",
       "           ...,\n",
       "           [-0.3978, -1.2259, -1.5463,  ..., -1.2582, -0.0493, -0.1259],\n",
       "           [-0.3978, -1.2259, -1.5463,  ..., -1.2582, -0.0493, -0.1259],\n",
       "           [-0.3978, -1.2259, -1.5463,  ..., -1.2582, -0.0493, -0.1259]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.19.self_attn.v_proj': {'input': (tensor([[[-0.9957,  0.4638,  0.0848,  ..., -0.6393,  0.3484, -0.3930],\n",
       "            [-1.7302, -0.1888,  0.9129,  ..., -1.2974, -0.9175, -0.5413],\n",
       "            [ 0.2159, -0.2970, -0.7937,  ..., -0.0236,  0.1106, -1.0735],\n",
       "            ...,\n",
       "            [-0.9064,  0.5616,  0.2866,  ...,  0.7921,  1.3105, -1.0100],\n",
       "            [-0.9064,  0.5616,  0.2866,  ...,  0.7921,  1.3105, -1.0100],\n",
       "            [-0.9064,  0.5616,  0.2866,  ...,  0.7921,  1.3105, -1.0100]],\n",
       "   \n",
       "           [[-1.0716,  0.2411,  1.1083,  ..., -0.7656, -1.4626,  0.0316],\n",
       "            [ 0.1402, -0.2785,  0.8487,  ..., -0.0609, -0.2198, -1.3709],\n",
       "            [ 0.8255, -0.4935,  1.7749,  ..., -0.5141, -0.3117, -0.5546],\n",
       "            ...,\n",
       "            [-0.6175, -0.0186,  0.0438,  ...,  1.2490,  1.0757, -0.8833],\n",
       "            [-0.6175, -0.0186,  0.0438,  ...,  1.2490,  1.0757, -0.8833],\n",
       "            [-0.6175, -0.0186,  0.0438,  ...,  1.2490,  1.0757, -0.8833]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.5968, -0.2866, -0.7893,  ..., -0.5451,  1.1797, -2.4847],\n",
       "           [-0.3368, -0.2379, -0.7194,  ...,  0.1451,  1.1719,  0.0773],\n",
       "           [-0.2169,  0.9632,  0.1735,  ...,  0.8968,  0.0894, -0.9374],\n",
       "           ...,\n",
       "           [ 0.6473, -0.1545, -0.9749,  ..., -2.1724, -1.6974,  0.7274],\n",
       "           [ 0.6473, -0.1545, -0.9749,  ..., -2.1724, -1.6974,  0.7274],\n",
       "           [ 0.6473, -0.1545, -0.9749,  ..., -2.1724, -1.6974,  0.7274]],\n",
       "  \n",
       "          [[ 0.2663, -0.5939, -0.8250,  ..., -1.4265,  0.3662, -2.5786],\n",
       "           [ 0.3446, -1.5726, -1.0386,  ..., -0.9592,  0.5046, -1.8480],\n",
       "           [-0.8790,  0.1539, -0.3280,  ...,  0.0807,  1.6489, -0.0418],\n",
       "           ...,\n",
       "           [ 0.4892, -0.6551, -1.2765,  ..., -2.1878, -2.6710,  0.2592],\n",
       "           [ 0.4892, -0.6551, -1.2765,  ..., -2.1878, -2.6710,  0.2592],\n",
       "           [ 0.4892, -0.6551, -1.2765,  ..., -2.1878, -2.6710,  0.2592]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.19.self_attn.out_proj': {'input': (tensor([[[ 7.8619e-02,  3.7888e-01, -1.9232e-01,  ..., -1.0355e+00,\n",
       "              8.4795e-01, -1.7273e+00],\n",
       "            [-6.4654e-02,  5.8273e-01, -1.0913e-01,  ...,  5.3383e-02,\n",
       "              2.5591e-01, -1.5577e-01],\n",
       "            [ 4.7889e-02,  4.5218e-01, -1.1796e-01,  ..., -1.5809e+00,\n",
       "              5.0218e-01, -2.5708e-01],\n",
       "            ...,\n",
       "            [ 1.1807e-01, -1.1474e-01, -9.6838e-02,  ..., -2.6229e-01,\n",
       "             -4.8218e-01, -2.5754e-01],\n",
       "            [ 1.1807e-01, -1.1474e-01, -9.6838e-02,  ..., -2.6229e-01,\n",
       "             -4.8218e-01, -2.5754e-01],\n",
       "            [ 1.1807e-01, -1.1474e-01, -9.6838e-02,  ..., -2.6229e-01,\n",
       "             -4.8218e-01, -2.5754e-01]],\n",
       "   \n",
       "           [[ 2.0162e-01,  5.3496e-01, -1.1861e-01,  ..., -8.7072e-01,\n",
       "              1.0476e+00, -1.6237e+00],\n",
       "            [ 2.5137e-01, -4.2251e-01, -1.3779e-01,  ..., -1.5557e-02,\n",
       "              2.6515e-01, -2.0712e-01],\n",
       "            [ 7.7311e-02,  2.1426e-01, -1.3691e-01,  ...,  2.1950e-01,\n",
       "              1.8203e-01, -1.3692e-01],\n",
       "            ...,\n",
       "            [-2.7105e-02, -4.8792e-04,  1.2899e-01,  ..., -1.2208e+00,\n",
       "             -1.5945e+00, -6.5387e-01],\n",
       "            [-2.7105e-02, -4.8792e-04,  1.2899e-01,  ..., -1.2208e+00,\n",
       "             -1.5945e+00, -6.5387e-01],\n",
       "            [-2.7105e-02, -4.8782e-04,  1.2899e-01,  ..., -1.2208e+00,\n",
       "             -1.5945e+00, -6.5387e-01]]], grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.1398, -0.2305, -0.7126,  ..., -0.2652,  0.0760,  0.3036],\n",
       "           [-0.3899, -0.1421, -0.1377,  ...,  0.0516, -0.1045,  0.3756],\n",
       "           [ 0.0365, -0.2033, -0.4459,  ...,  0.1869,  0.1989,  0.1480],\n",
       "           ...,\n",
       "           [-0.4127, -0.0458, -0.0932,  ...,  0.6432,  0.4859, -0.3580],\n",
       "           [-0.4127, -0.0458, -0.0932,  ...,  0.6432,  0.4859, -0.3580],\n",
       "           [-0.4127, -0.0458, -0.0932,  ...,  0.6432,  0.4859, -0.3580]],\n",
       "  \n",
       "          [[-0.3846, -0.1207, -0.4276,  ..., -0.4942,  0.1937,  0.1068],\n",
       "           [-1.3693, -0.4626, -0.0904,  ..., -0.4784,  0.3237,  0.0220],\n",
       "           [-0.4318, -0.5368,  0.0299,  ..., -0.4689, -0.3244,  0.2192],\n",
       "           ...,\n",
       "           [-0.7963, -0.1933,  0.1336,  ...,  0.4205,  0.2209,  0.1026],\n",
       "           [-0.7963, -0.1933,  0.1336,  ...,  0.4205,  0.2209,  0.1026],\n",
       "           [-0.7963, -0.1933,  0.1336,  ...,  0.4205,  0.2209,  0.1026]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.19.mlp.fc1': {'input': (tensor([[[-3.8472, -2.3677, -2.7398,  ..., -0.0868,  2.1391, -0.2059],\n",
       "            [-6.4226, -5.1231,  1.0950,  ..., -1.7345, -1.8574, -0.2445],\n",
       "            [-0.4800, -5.6001, -4.7864,  ...,  3.3838,  1.6642, -2.2541],\n",
       "            ...,\n",
       "            [-3.7991, -1.2342, -0.5603,  ...,  6.9591,  5.2726, -2.6591],\n",
       "            [-3.7991, -1.2342, -0.5603,  ...,  6.9591,  5.2726, -2.6591],\n",
       "            [-3.7991, -1.2342, -0.5603,  ...,  6.9591,  5.2726, -2.6591]],\n",
       "   \n",
       "           [[-4.4115, -2.9651,  1.2283,  ..., -1.0275, -2.7916,  0.4674],\n",
       "            [-2.5452, -6.0079,  1.1793,  ...,  1.8858,  0.8345, -3.2209],\n",
       "            [ 0.1563, -7.7686,  4.3454,  ..., -0.3191, -0.5587, -0.7426],\n",
       "            ...,\n",
       "            [-3.3449, -4.0713, -1.1027,  ...,  8.4357,  4.3539, -1.9558],\n",
       "            [-3.3449, -4.0713, -1.1027,  ...,  8.4357,  4.3539, -1.9558],\n",
       "            [-3.3449, -4.0713, -1.1027,  ...,  8.4357,  4.3539, -1.9558]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ -4.2614,  -7.5437,  -8.7095,  ..., -21.7615, -11.5076,  -4.5474],\n",
       "           [ -2.3730,  -7.4946,  -9.5477,  ..., -15.4307,  -9.8818,  -5.1342],\n",
       "           [ -4.8046, -11.8272, -10.1074,  ..., -12.5988,  -5.8194,  -9.6764],\n",
       "           ...,\n",
       "           [ -2.4573,  -6.5121,  -8.9468,  ..., -10.5885,  -7.3245,  -5.6473],\n",
       "           [ -2.4573,  -6.5121,  -8.9468,  ..., -10.5885,  -7.3245,  -5.6473],\n",
       "           [ -2.4573,  -6.5121,  -8.9468,  ..., -10.5885,  -7.3245,  -5.6473]],\n",
       "  \n",
       "          [[ -5.4325,  -4.1978,  -8.3418,  ..., -19.9725,  -9.9031,  -9.4213],\n",
       "           [-11.3544,  -9.4037,  -6.5442,  ..., -16.0556, -11.0379, -11.3865],\n",
       "           [-11.2852, -11.2601, -11.5231,  ..., -20.5756,  -4.2465, -12.1163],\n",
       "           ...,\n",
       "           [ -2.1114,  -7.1023,  -8.0920,  ..., -13.1267,  -5.2449,  -5.8085],\n",
       "           [ -2.1114,  -7.1023,  -8.0920,  ..., -13.1267,  -5.2449,  -5.8085],\n",
       "           [ -2.1114,  -7.1023,  -8.0920,  ..., -13.1267,  -5.2449,  -5.8085]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.19.mlp.fc2': {'input': (tensor([[[-1.8923e-05, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -3.9302e-06],\n",
       "            [-2.0553e-02, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-8.5913e-07, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            ...,\n",
       "            [-1.6776e-02, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-1.6776e-02, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-1.6776e-02, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00]],\n",
       "   \n",
       "           [[-0.0000e+00, -2.6397e-05, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -2.0502e-05, -0.0000e+00],\n",
       "            ...,\n",
       "            [-3.6476e-02, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-3.6476e-02, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-3.6476e-02, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[-0.3125, -0.6627,  0.1583,  ..., -0.1462,  0.4852,  0.0917],\n",
       "           [-0.7662,  0.1124, -0.3839,  ..., -0.3802,  0.1486, -0.9804],\n",
       "           [-0.0682,  0.3320,  0.0693,  ..., -0.2496, -0.0843, -0.3665],\n",
       "           ...,\n",
       "           [-1.6714, -0.2455, -0.2816,  ...,  0.2767,  0.9151, -0.0467],\n",
       "           [-1.6714, -0.2455, -0.2816,  ...,  0.2767,  0.9151, -0.0467],\n",
       "           [-1.6714, -0.2455, -0.2816,  ...,  0.2767,  0.9151, -0.0467]],\n",
       "  \n",
       "          [[ 0.2784,  0.0883,  0.1902,  ..., -0.0869, -0.4962, -0.3865],\n",
       "           [-1.1112, -0.7474,  0.4636,  ..., -1.4403,  0.6420, -2.8010],\n",
       "           [-0.1586,  0.3578,  0.3800,  ..., -0.3740, -0.2248, -0.4618],\n",
       "           ...,\n",
       "           [-1.7895, -0.1136, -0.7246,  ..., -0.0787,  1.4080,  0.0469],\n",
       "           [-1.7895, -0.1136, -0.7246,  ..., -0.0787,  1.4080,  0.0469],\n",
       "           [-1.7895, -0.1136, -0.7246,  ..., -0.0787,  1.4080,  0.0469]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.20.self_attn.q_proj': {'input': (tensor([[[-1.1197e+00, -5.3150e-02, -1.5969e-01,  ..., -9.6576e-01,\n",
       "              6.5754e-01,  3.6734e-02],\n",
       "            [-2.5722e+00, -9.9121e-02,  5.6190e-01,  ..., -1.6083e+00,\n",
       "             -8.5274e-01, -9.4833e-01],\n",
       "            [ 3.7106e-01, -1.1849e-01, -8.8098e-01,  ..., -1.2107e-01,\n",
       "              1.4825e-01, -1.0841e+00],\n",
       "            ...,\n",
       "            [-1.3764e+00,  5.5931e-01,  3.0735e-01,  ...,  9.8725e-01,\n",
       "              1.5954e+00, -9.6928e-01],\n",
       "            [-1.3764e+00,  5.5931e-01,  3.0735e-01,  ...,  9.8725e-01,\n",
       "              1.5954e+00, -9.6928e-01],\n",
       "            [-1.3764e+00,  5.5931e-01,  3.0735e-01,  ...,  9.8725e-01,\n",
       "              1.5954e+00, -9.6928e-01]],\n",
       "   \n",
       "           [[-9.3485e-01,  3.2312e-01,  1.0766e+00,  ..., -1.1548e+00,\n",
       "             -1.5742e+00, -7.3080e-03],\n",
       "            [-8.7442e-01, -6.9850e-01,  1.1205e+00,  ..., -1.0007e+00,\n",
       "              1.8196e-01, -2.4771e+00],\n",
       "            [ 5.4352e-01, -5.5215e-01,  2.3042e+00,  ..., -1.2488e+00,\n",
       "             -7.3220e-01, -6.2994e-01],\n",
       "            ...,\n",
       "            [-1.2011e+00,  2.5286e-03,  1.9733e-02,  ...,  1.2424e+00,\n",
       "              1.4159e+00, -6.7553e-01],\n",
       "            [-1.2011e+00,  2.5286e-03,  1.9733e-02,  ...,  1.2424e+00,\n",
       "              1.4159e+00, -6.7553e-01],\n",
       "            [-1.2011e+00,  2.5283e-03,  1.9732e-02,  ...,  1.2424e+00,\n",
       "              1.4159e+00, -6.7553e-01]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-3.0416,  0.8435, -0.8827,  ...,  0.1751, -1.7380, -0.7395],\n",
       "           [-5.6788, -0.1460,  0.5068,  ..., -1.3376, -0.8037, -0.3620],\n",
       "           [ 1.0574,  0.0711,  0.6345,  ..., -0.3252, -0.8914,  1.0519],\n",
       "           ...,\n",
       "           [-0.5435, -1.1833, -1.4175,  ...,  2.0517,  0.8798, -0.7185],\n",
       "           [-0.5435, -1.1833, -1.4175,  ...,  2.0517,  0.8798, -0.7185],\n",
       "           [-0.5435, -1.1833, -1.4175,  ...,  2.0517,  0.8798, -0.7185]],\n",
       "  \n",
       "          [[-1.8536,  1.7708,  0.7455,  ...,  1.3073,  0.0331,  0.0285],\n",
       "           [-1.1930,  1.3979,  0.6869,  ...,  1.4770, -1.3724,  0.0185],\n",
       "           [-3.1328,  1.3082,  0.2890,  ..., -0.6846, -1.6062, -1.0954],\n",
       "           ...,\n",
       "           [ 0.3646, -1.6455, -1.1903,  ...,  1.9015,  1.0197, -0.4164],\n",
       "           [ 0.3646, -1.6455, -1.1903,  ...,  1.9015,  1.0197, -0.4164],\n",
       "           [ 0.3646, -1.6455, -1.1903,  ...,  1.9015,  1.0197, -0.4164]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.20.self_attn.k_proj': {'input': (tensor([[[-1.1197e+00, -5.3150e-02, -1.5969e-01,  ..., -9.6576e-01,\n",
       "              6.5754e-01,  3.6734e-02],\n",
       "            [-2.5722e+00, -9.9121e-02,  5.6190e-01,  ..., -1.6083e+00,\n",
       "             -8.5274e-01, -9.4833e-01],\n",
       "            [ 3.7106e-01, -1.1849e-01, -8.8098e-01,  ..., -1.2107e-01,\n",
       "              1.4825e-01, -1.0841e+00],\n",
       "            ...,\n",
       "            [-1.3764e+00,  5.5931e-01,  3.0735e-01,  ...,  9.8725e-01,\n",
       "              1.5954e+00, -9.6928e-01],\n",
       "            [-1.3764e+00,  5.5931e-01,  3.0735e-01,  ...,  9.8725e-01,\n",
       "              1.5954e+00, -9.6928e-01],\n",
       "            [-1.3764e+00,  5.5931e-01,  3.0735e-01,  ...,  9.8725e-01,\n",
       "              1.5954e+00, -9.6928e-01]],\n",
       "   \n",
       "           [[-9.3485e-01,  3.2312e-01,  1.0766e+00,  ..., -1.1548e+00,\n",
       "             -1.5742e+00, -7.3080e-03],\n",
       "            [-8.7442e-01, -6.9850e-01,  1.1205e+00,  ..., -1.0007e+00,\n",
       "              1.8196e-01, -2.4771e+00],\n",
       "            [ 5.4352e-01, -5.5215e-01,  2.3042e+00,  ..., -1.2488e+00,\n",
       "             -7.3220e-01, -6.2994e-01],\n",
       "            ...,\n",
       "            [-1.2011e+00,  2.5286e-03,  1.9733e-02,  ...,  1.2424e+00,\n",
       "              1.4159e+00, -6.7553e-01],\n",
       "            [-1.2011e+00,  2.5286e-03,  1.9733e-02,  ...,  1.2424e+00,\n",
       "              1.4159e+00, -6.7553e-01],\n",
       "            [-1.2011e+00,  2.5283e-03,  1.9732e-02,  ...,  1.2424e+00,\n",
       "              1.4159e+00, -6.7553e-01]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-1.8842,  0.7461, -0.4474,  ...,  0.8673, -2.8822, -0.4857],\n",
       "           [-2.8649,  0.3761,  0.8446,  ...,  2.0649, -0.1735, -0.0154],\n",
       "           [-2.7700, -0.6708,  0.3131,  ...,  1.6593, -0.1029,  0.2014],\n",
       "           ...,\n",
       "           [-1.3967, -0.6387, -0.7430,  ...,  2.0034, -0.5916, -1.7592],\n",
       "           [-1.3967, -0.6387, -0.7430,  ...,  2.0034, -0.5916, -1.7592],\n",
       "           [-1.3967, -0.6387, -0.7430,  ...,  2.0034, -0.5916, -1.7592]],\n",
       "  \n",
       "          [[ 0.4378,  1.0581, -1.6146,  ...,  0.8577, -2.1580, -0.3076],\n",
       "           [ 1.6193,  0.4773, -0.4547,  ...,  1.2258, -1.7035, -1.1577],\n",
       "           [-1.1763, -0.8138,  0.2128,  ...,  1.6059,  1.4219, -1.5971],\n",
       "           ...,\n",
       "           [-0.3456, -1.1387, -0.9154,  ...,  1.5660, -0.3627, -1.6394],\n",
       "           [-0.3456, -1.1387, -0.9154,  ...,  1.5660, -0.3627, -1.6394],\n",
       "           [-0.3456, -1.1387, -0.9154,  ...,  1.5660, -0.3627, -1.6394]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.20.self_attn.v_proj': {'input': (tensor([[[-1.1197e+00, -5.3150e-02, -1.5969e-01,  ..., -9.6576e-01,\n",
       "              6.5754e-01,  3.6734e-02],\n",
       "            [-2.5722e+00, -9.9121e-02,  5.6190e-01,  ..., -1.6083e+00,\n",
       "             -8.5274e-01, -9.4833e-01],\n",
       "            [ 3.7106e-01, -1.1849e-01, -8.8098e-01,  ..., -1.2107e-01,\n",
       "              1.4825e-01, -1.0841e+00],\n",
       "            ...,\n",
       "            [-1.3764e+00,  5.5931e-01,  3.0735e-01,  ...,  9.8725e-01,\n",
       "              1.5954e+00, -9.6928e-01],\n",
       "            [-1.3764e+00,  5.5931e-01,  3.0735e-01,  ...,  9.8725e-01,\n",
       "              1.5954e+00, -9.6928e-01],\n",
       "            [-1.3764e+00,  5.5931e-01,  3.0735e-01,  ...,  9.8725e-01,\n",
       "              1.5954e+00, -9.6928e-01]],\n",
       "   \n",
       "           [[-9.3485e-01,  3.2312e-01,  1.0766e+00,  ..., -1.1548e+00,\n",
       "             -1.5742e+00, -7.3080e-03],\n",
       "            [-8.7442e-01, -6.9850e-01,  1.1205e+00,  ..., -1.0007e+00,\n",
       "              1.8196e-01, -2.4771e+00],\n",
       "            [ 5.4352e-01, -5.5215e-01,  2.3042e+00,  ..., -1.2488e+00,\n",
       "             -7.3220e-01, -6.2994e-01],\n",
       "            ...,\n",
       "            [-1.2011e+00,  2.5286e-03,  1.9733e-02,  ...,  1.2424e+00,\n",
       "              1.4159e+00, -6.7553e-01],\n",
       "            [-1.2011e+00,  2.5286e-03,  1.9733e-02,  ...,  1.2424e+00,\n",
       "              1.4159e+00, -6.7553e-01],\n",
       "            [-1.2011e+00,  2.5283e-03,  1.9732e-02,  ...,  1.2424e+00,\n",
       "              1.4159e+00, -6.7553e-01]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-1.4674, -0.3906,  0.2297,  ..., -0.5343, -0.6753,  0.8565],\n",
       "           [-2.5813,  0.2637,  1.4770,  ...,  0.7966, -0.2414,  0.3813],\n",
       "           [-0.3219, -0.4694, -0.5353,  ...,  0.3332,  0.6588, -0.0532],\n",
       "           ...,\n",
       "           [-0.7997,  0.1038, -0.4471,  ..., -2.9764,  0.2643, -1.0499],\n",
       "           [-0.7997,  0.1038, -0.4471,  ..., -2.9764,  0.2643, -1.0499],\n",
       "           [-0.7997,  0.1038, -0.4471,  ..., -2.9764,  0.2643, -1.0499]],\n",
       "  \n",
       "          [[-0.8746, -0.8747,  1.4030,  ...,  0.6386,  0.3777,  0.4779],\n",
       "           [-0.0607, -0.1627,  0.4755,  ...,  0.0085,  0.6533, -1.4174],\n",
       "           [ 0.0348,  0.0214, -0.3097,  ...,  0.6318,  2.3654,  0.3419],\n",
       "           ...,\n",
       "           [-0.8796,  0.7654, -0.3316,  ..., -2.6136, -0.1205, -1.1792],\n",
       "           [-0.8796,  0.7654, -0.3316,  ..., -2.6136, -0.1205, -1.1792],\n",
       "           [-0.8796,  0.7654, -0.3316,  ..., -2.6136, -0.1205, -1.1792]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.20.self_attn.out_proj': {'input': (tensor([[[-1.1757e+00, -4.4452e-01,  3.1525e-01,  ..., -1.3896e-02,\n",
       "              1.4816e-01,  1.1573e-01],\n",
       "            [-5.2264e-01, -2.8055e-01, -9.6085e-02,  ...,  1.8556e-01,\n",
       "              5.4663e-02,  1.2706e-01],\n",
       "            [-3.9112e-01, -1.0502e-01, -1.2631e-01,  ...,  4.5826e-01,\n",
       "             -1.7508e-01,  1.8411e-01],\n",
       "            ...,\n",
       "            [-1.6279e-01,  1.0507e-01, -1.8597e-01,  ..., -3.2992e-01,\n",
       "             -6.4759e-01,  3.3683e-02],\n",
       "            [-1.6279e-01,  1.0507e-01, -1.8597e-01,  ..., -3.2992e-01,\n",
       "             -6.4759e-01,  3.3683e-02],\n",
       "            [-1.6279e-01,  1.0507e-01, -1.8597e-01,  ..., -3.2992e-01,\n",
       "             -6.4759e-01,  3.3683e-02]],\n",
       "   \n",
       "           [[-4.2878e-01, -8.4840e-01,  4.2270e-01,  ...,  4.3499e-01,\n",
       "             -3.0657e-01,  6.1472e-01],\n",
       "            [-1.6848e-01, -4.2518e-01,  2.1962e-02,  ..., -2.0146e-02,\n",
       "             -1.7119e-01,  4.1269e-01],\n",
       "            [-1.1065e-01, -1.7904e-01, -8.0073e-02,  ...,  6.9185e-02,\n",
       "              2.3136e-01, -4.2151e-04],\n",
       "            ...,\n",
       "            [-1.4720e-01,  3.3356e-01, -4.1445e-01,  ..., -4.2209e-01,\n",
       "             -8.3258e-01,  1.9678e-01],\n",
       "            [-1.4720e-01,  3.3356e-01, -4.1445e-01,  ..., -4.2209e-01,\n",
       "             -8.3258e-01,  1.9678e-01],\n",
       "            [-1.4720e-01,  3.3356e-01, -4.1445e-01,  ..., -4.2209e-01,\n",
       "             -8.3258e-01,  1.9678e-01]]], grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.0799, -0.6792, -0.1741,  ...,  0.6121,  0.1877, -0.3433],\n",
       "           [-0.0969, -0.6810, -0.0770,  ...,  0.5717,  0.1142, -0.0376],\n",
       "           [-0.0396, -0.6559, -0.5866,  ...,  0.4890,  0.2765, -0.0558],\n",
       "           ...,\n",
       "           [-0.1873, -0.0241, -0.2860,  ...,  0.4351, -0.1930,  0.0393],\n",
       "           [-0.1873, -0.0241, -0.2860,  ...,  0.4351, -0.1930,  0.0393],\n",
       "           [-0.1873, -0.0241, -0.2860,  ...,  0.4351, -0.1930,  0.0393]],\n",
       "  \n",
       "          [[-0.2352, -0.7905, -0.3513,  ...,  0.6182,  0.3516, -0.5863],\n",
       "           [-0.1623, -0.7748, -0.3312,  ...,  0.5046,  0.5044,  0.1360],\n",
       "           [-0.0805, -0.2357, -0.4100,  ...,  0.3489, -0.1394, -0.1640],\n",
       "           ...,\n",
       "           [-0.4702, -0.0629, -0.3389,  ...,  0.2218, -0.1295, -0.1466],\n",
       "           [-0.4702, -0.0629, -0.3389,  ...,  0.2218, -0.1295, -0.1466],\n",
       "           [-0.4702, -0.0629, -0.3389,  ...,  0.2218, -0.1295, -0.1466]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.20.mlp.fc1': {'input': (tensor([[[ -7.7216,  -5.9769,  -0.4899,  ...,  -0.2708,   2.7735,  -2.3874],\n",
       "            [-13.2105,  -6.8478,   1.6757,  ...,  -2.3233,  -1.8484,  -4.9246],\n",
       "            [ -2.1437,  -5.8548,  -3.0723,  ...,   2.3358,   1.3133,  -5.2398],\n",
       "            ...,\n",
       "            [ -8.6181,  -1.0243,   0.9114,  ...,   5.8054,   5.0214,  -4.7829],\n",
       "            [ -8.6181,  -1.0243,   0.9114,  ...,   5.8054,   5.0214,  -4.7829],\n",
       "            [ -8.6181,  -1.0243,   0.9114,  ...,   5.8054,   5.0214,  -4.7829]],\n",
       "   \n",
       "           [[ -7.3306,  -4.2668,   2.6911,  ...,  -1.1124,  -3.6591,  -2.9184],\n",
       "            [ -6.9090,  -8.4647,   3.0075,  ...,  -1.0943,   1.6406,  -9.5834],\n",
       "            [ -1.6623,  -6.9430,   5.7779,  ...,  -1.8185,  -2.0483,  -4.1737],\n",
       "            ...,\n",
       "            [ -8.2832,  -3.6275,   0.0873,  ...,   6.4712,   4.5587,  -4.0250],\n",
       "            [ -8.2832,  -3.6275,   0.0873,  ...,   6.4712,   4.5587,  -4.0250],\n",
       "            [ -8.2832,  -3.6275,   0.0873,  ...,   6.4712,   4.5587,  -4.0250]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ -6.2620,  -4.3044,  -2.5417,  ..., -13.1836, -15.7589, -14.9116],\n",
       "           [ -4.8623, -11.8582,  -5.7303,  ..., -18.3672, -14.2796,  -6.2217],\n",
       "           [-10.0627, -12.2744, -13.2128,  ..., -16.3459, -10.2509, -20.0208],\n",
       "           ...,\n",
       "           [ -3.6917,  -7.7607,  -9.2342,  ..., -15.7769, -10.6539, -10.6614],\n",
       "           [ -3.6917,  -7.7607,  -9.2342,  ..., -15.7769, -10.6539, -10.6614],\n",
       "           [ -3.6917,  -7.7607,  -9.2342,  ..., -15.7769, -10.6539, -10.6614]],\n",
       "  \n",
       "          [[ -7.9500, -16.7276,  -7.5882,  ..., -18.0735, -16.0906, -10.9414],\n",
       "           [ -6.1518, -17.6857,  -2.6183,  ...,  -9.1840, -16.8121,  -6.3192],\n",
       "           [ -9.9273, -18.1615,  -6.5731,  ...,  -3.5676,  -8.8052,  -9.4288],\n",
       "           ...,\n",
       "           [ -4.7374,  -7.7956,  -5.7793,  ..., -19.9670,  -9.8295, -14.4312],\n",
       "           [ -4.7374,  -7.7956,  -5.7793,  ..., -19.9670,  -9.8295, -14.4312],\n",
       "           [ -4.7374,  -7.7956,  -5.7793,  ..., -19.9670,  -9.8295, -14.4312]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.20.mlp.fc2': {'input': (tensor([[[-0.0000e+00, -1.5137e-05, -1.3566e-02,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-5.7963e-07, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            ...,\n",
       "            [-2.8154e-04, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-2.8154e-04, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-2.8154e-04, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00]],\n",
       "   \n",
       "           [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -1.1100e-02,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -4.7069e-04,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            ...,\n",
       "            [-1.2707e-06, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-1.2707e-06, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-1.2707e-06, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[-1.8053e-01, -1.1025e+00, -2.1371e-01,  ...,  2.8039e-03,\n",
       "             6.2048e-01,  8.2921e-01],\n",
       "           [-1.4158e-01, -5.6511e-01,  5.8906e-01,  ..., -3.3419e-01,\n",
       "            -3.9714e-01,  3.1288e-01],\n",
       "           [ 2.4912e-01,  2.9228e-01,  7.3490e-02,  ..., -7.9664e-02,\n",
       "             7.0316e-02,  1.4881e-01],\n",
       "           ...,\n",
       "           [ 3.6106e-02,  1.0452e-01, -2.1384e-01,  ..., -2.4246e-01,\n",
       "             2.2075e-01,  1.1890e-01],\n",
       "           [ 3.6106e-02,  1.0452e-01, -2.1384e-01,  ..., -2.4246e-01,\n",
       "             2.2075e-01,  1.1890e-01],\n",
       "           [ 3.6106e-02,  1.0452e-01, -2.1384e-01,  ..., -2.4246e-01,\n",
       "             2.2074e-01,  1.1890e-01]],\n",
       "  \n",
       "          [[-2.0349e-01, -2.9334e-01, -6.0329e-01,  ...,  3.6250e-01,\n",
       "             3.5717e-01,  2.7226e-01],\n",
       "           [ 2.5015e-01, -5.1963e-01, -3.6500e-01,  ...,  1.6296e-01,\n",
       "             4.1252e-01, -2.5369e-01],\n",
       "           [ 2.1729e-01,  2.7605e-01,  9.7131e-02,  ...,  1.0001e-01,\n",
       "            -1.5994e-01,  4.3364e-01],\n",
       "           ...,\n",
       "           [-3.0078e-01, -4.3761e-01,  2.1328e-01,  ...,  2.2670e-01,\n",
       "            -2.9592e-04,  5.9510e-01],\n",
       "           [-3.0078e-01, -4.3761e-01,  2.1328e-01,  ...,  2.2670e-01,\n",
       "            -2.9592e-04,  5.9510e-01],\n",
       "           [-3.0078e-01, -4.3761e-01,  2.1328e-01,  ...,  2.2670e-01,\n",
       "            -2.9617e-04,  5.9510e-01]]], grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.21.self_attn.q_proj': {'input': (tensor([[[-1.1441e+00, -1.0257e+00, -3.9907e-01,  ..., -5.3775e-01,\n",
       "              1.1626e+00,  3.9484e-01],\n",
       "            [-2.4783e+00, -9.4334e-01,  1.0236e+00,  ..., -1.3325e+00,\n",
       "             -1.1437e+00, -6.0495e-01],\n",
       "            [ 5.5533e-01, -2.7697e-01, -1.2307e+00,  ...,  1.1521e-01,\n",
       "              3.3900e-01, -9.8483e-01],\n",
       "            ...,\n",
       "            [-1.3327e+00,  5.2968e-01,  1.6287e-01,  ...,  1.0394e+00,\n",
       "              1.6954e+00, -8.7005e-01],\n",
       "            [-1.3327e+00,  5.2968e-01,  1.6287e-01,  ...,  1.0394e+00,\n",
       "              1.6954e+00, -8.7005e-01],\n",
       "            [-1.3327e+00,  5.2968e-01,  1.6287e-01,  ...,  1.0394e+00,\n",
       "              1.6954e+00, -8.7005e-01]],\n",
       "   \n",
       "           [[-1.0798e+00, -2.5093e-01,  5.0052e-01,  ..., -5.6395e-01,\n",
       "             -1.2713e+00, -1.6263e-01],\n",
       "            [-7.0345e-01, -1.0392e+00,  7.9748e-01,  ..., -6.8210e-01,\n",
       "              5.4571e-01, -2.3436e+00],\n",
       "            [ 6.9529e-01, -4.1782e-01,  2.0830e+00,  ..., -8.9807e-01,\n",
       "             -1.0446e+00, -3.6255e-01],\n",
       "            ...,\n",
       "            [-1.3297e+00, -9.9927e-02, -7.8677e-04,  ...,  1.3553e+00,\n",
       "              1.4443e+00, -4.9481e-01],\n",
       "            [-1.3297e+00, -9.9927e-02, -7.8677e-04,  ...,  1.3553e+00,\n",
       "              1.4443e+00, -4.9481e-01],\n",
       "            [-1.3297e+00, -9.9928e-02, -7.8794e-04,  ...,  1.3553e+00,\n",
       "              1.4443e+00, -4.9481e-01]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.2915, -0.6389,  1.3953,  ...,  0.4307, -0.5921, -0.5441],\n",
       "           [ 0.7890, -0.2205, -1.1770,  ..., -0.6146,  1.1885,  0.3514],\n",
       "           [-0.2097, -0.1669,  0.7739,  ..., -0.4656, -0.3747, -0.1807],\n",
       "           ...,\n",
       "           [-1.3257, -1.7368,  1.2571,  ..., -0.9603,  0.1354,  1.4373],\n",
       "           [-1.3257, -1.7368,  1.2571,  ..., -0.9603,  0.1354,  1.4373],\n",
       "           [-1.3257, -1.7368,  1.2571,  ..., -0.9603,  0.1354,  1.4373]],\n",
       "  \n",
       "          [[-0.3544,  0.1615,  1.7236,  ...,  0.3763,  1.2983, -1.1872],\n",
       "           [ 0.8831,  1.5160,  0.4907,  ..., -0.5263, -0.0147, -1.7162],\n",
       "           [ 1.1667,  1.0167,  3.0479,  ...,  1.3740, -0.8787,  0.1625],\n",
       "           ...,\n",
       "           [-1.5588, -2.2071,  1.3235,  ..., -1.0439,  0.4085,  1.2465],\n",
       "           [-1.5588, -2.2071,  1.3235,  ..., -1.0439,  0.4085,  1.2465],\n",
       "           [-1.5588, -2.2071,  1.3235,  ..., -1.0439,  0.4085,  1.2465]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.21.self_attn.k_proj': {'input': (tensor([[[-1.1441e+00, -1.0257e+00, -3.9907e-01,  ..., -5.3775e-01,\n",
       "              1.1626e+00,  3.9484e-01],\n",
       "            [-2.4783e+00, -9.4334e-01,  1.0236e+00,  ..., -1.3325e+00,\n",
       "             -1.1437e+00, -6.0495e-01],\n",
       "            [ 5.5533e-01, -2.7697e-01, -1.2307e+00,  ...,  1.1521e-01,\n",
       "              3.3900e-01, -9.8483e-01],\n",
       "            ...,\n",
       "            [-1.3327e+00,  5.2968e-01,  1.6287e-01,  ...,  1.0394e+00,\n",
       "              1.6954e+00, -8.7005e-01],\n",
       "            [-1.3327e+00,  5.2968e-01,  1.6287e-01,  ...,  1.0394e+00,\n",
       "              1.6954e+00, -8.7005e-01],\n",
       "            [-1.3327e+00,  5.2968e-01,  1.6287e-01,  ...,  1.0394e+00,\n",
       "              1.6954e+00, -8.7005e-01]],\n",
       "   \n",
       "           [[-1.0798e+00, -2.5093e-01,  5.0052e-01,  ..., -5.6395e-01,\n",
       "             -1.2713e+00, -1.6263e-01],\n",
       "            [-7.0345e-01, -1.0392e+00,  7.9748e-01,  ..., -6.8210e-01,\n",
       "              5.4571e-01, -2.3436e+00],\n",
       "            [ 6.9529e-01, -4.1782e-01,  2.0830e+00,  ..., -8.9807e-01,\n",
       "             -1.0446e+00, -3.6255e-01],\n",
       "            ...,\n",
       "            [-1.3297e+00, -9.9927e-02, -7.8677e-04,  ...,  1.3553e+00,\n",
       "              1.4443e+00, -4.9481e-01],\n",
       "            [-1.3297e+00, -9.9927e-02, -7.8677e-04,  ...,  1.3553e+00,\n",
       "              1.4443e+00, -4.9481e-01],\n",
       "            [-1.3297e+00, -9.9928e-02, -7.8794e-04,  ...,  1.3553e+00,\n",
       "              1.4443e+00, -4.9481e-01]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.3345,  0.8854, -0.1447,  ..., -0.2695, -2.6148,  0.9648],\n",
       "           [-1.5131,  0.8275,  0.3371,  ..., -0.8317, -0.3113,  1.9430],\n",
       "           [ 1.3167,  0.4501, -0.0241,  ..., -0.5033,  0.0148,  1.5959],\n",
       "           ...,\n",
       "           [-1.5283,  0.5753, -1.0074,  ..., -2.9807, -0.1059,  2.8175],\n",
       "           [-1.5283,  0.5753, -1.0074,  ..., -2.9807, -0.1059,  2.8175],\n",
       "           [-1.5283,  0.5753, -1.0074,  ..., -2.9807, -0.1059,  2.8175]],\n",
       "  \n",
       "          [[ 0.4093,  2.4068,  1.6744,  ..., -0.8623, -2.2031,  1.5692],\n",
       "           [-1.8321,  1.4576,  0.9827,  ..., -0.2050,  0.9231,  1.0874],\n",
       "           [-1.0507,  0.4668,  2.5677,  ...,  0.6052, -0.1020,  2.3615],\n",
       "           ...,\n",
       "           [-1.2709,  0.2000, -1.5170,  ..., -2.9184, -0.0448,  2.4905],\n",
       "           [-1.2709,  0.2000, -1.5170,  ..., -2.9184, -0.0448,  2.4905],\n",
       "           [-1.2709,  0.2000, -1.5170,  ..., -2.9184, -0.0448,  2.4905]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.21.self_attn.v_proj': {'input': (tensor([[[-1.1441e+00, -1.0257e+00, -3.9907e-01,  ..., -5.3775e-01,\n",
       "              1.1626e+00,  3.9484e-01],\n",
       "            [-2.4783e+00, -9.4334e-01,  1.0236e+00,  ..., -1.3325e+00,\n",
       "             -1.1437e+00, -6.0495e-01],\n",
       "            [ 5.5533e-01, -2.7697e-01, -1.2307e+00,  ...,  1.1521e-01,\n",
       "              3.3900e-01, -9.8483e-01],\n",
       "            ...,\n",
       "            [-1.3327e+00,  5.2968e-01,  1.6287e-01,  ...,  1.0394e+00,\n",
       "              1.6954e+00, -8.7005e-01],\n",
       "            [-1.3327e+00,  5.2968e-01,  1.6287e-01,  ...,  1.0394e+00,\n",
       "              1.6954e+00, -8.7005e-01],\n",
       "            [-1.3327e+00,  5.2968e-01,  1.6287e-01,  ...,  1.0394e+00,\n",
       "              1.6954e+00, -8.7005e-01]],\n",
       "   \n",
       "           [[-1.0798e+00, -2.5093e-01,  5.0052e-01,  ..., -5.6395e-01,\n",
       "             -1.2713e+00, -1.6263e-01],\n",
       "            [-7.0345e-01, -1.0392e+00,  7.9748e-01,  ..., -6.8210e-01,\n",
       "              5.4571e-01, -2.3436e+00],\n",
       "            [ 6.9529e-01, -4.1782e-01,  2.0830e+00,  ..., -8.9807e-01,\n",
       "             -1.0446e+00, -3.6255e-01],\n",
       "            ...,\n",
       "            [-1.3297e+00, -9.9927e-02, -7.8677e-04,  ...,  1.3553e+00,\n",
       "              1.4443e+00, -4.9481e-01],\n",
       "            [-1.3297e+00, -9.9927e-02, -7.8677e-04,  ...,  1.3553e+00,\n",
       "              1.4443e+00, -4.9481e-01],\n",
       "            [-1.3297e+00, -9.9928e-02, -7.8794e-04,  ...,  1.3553e+00,\n",
       "              1.4443e+00, -4.9481e-01]]], grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.4138,  2.1815,  0.2728,  ..., -0.0735, -1.1130,  0.4008],\n",
       "           [-1.0756, -0.0941,  0.1875,  ...,  0.9720, -0.1346,  0.7403],\n",
       "           [ 0.7780, -0.5296, -0.0089,  ..., -0.0726, -0.1536, -0.6877],\n",
       "           ...,\n",
       "           [ 1.4678, -0.4296,  0.4381,  ..., -0.0297, -0.9876,  0.2458],\n",
       "           [ 1.4678, -0.4296,  0.4381,  ..., -0.0297, -0.9876,  0.2458],\n",
       "           [ 1.4678, -0.4296,  0.4381,  ..., -0.0297, -0.9876,  0.2458]],\n",
       "  \n",
       "          [[ 0.6814,  0.4760, -0.5788,  ..., -0.5091, -0.7731, -1.1466],\n",
       "           [-2.6753, -1.2534, -0.2590,  ..., -0.0132,  0.3968,  0.6319],\n",
       "           [ 0.7315, -0.9492,  0.4918,  ...,  1.4078,  0.6423,  1.4716],\n",
       "           ...,\n",
       "           [ 1.6610, -0.8115, -0.0878,  ..., -0.2685, -0.8781, -0.0150],\n",
       "           [ 1.6610, -0.8115, -0.0878,  ..., -0.2685, -0.8781, -0.0150],\n",
       "           [ 1.6610, -0.8115, -0.0878,  ..., -0.2685, -0.8781, -0.0150]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.21.self_attn.out_proj': {'input': (tensor([[[ 2.1074e-01, -2.9682e-01,  5.1518e-02,  ...,  6.2943e-02,\n",
       "              8.9621e-02, -7.7738e-02],\n",
       "            [ 2.5137e-01, -4.1149e-02, -2.5593e-04,  ..., -1.5005e-03,\n",
       "              1.7484e-01, -1.2122e-01],\n",
       "            [ 6.2566e-02, -2.5600e-01, -8.8782e-02,  ...,  1.2991e-01,\n",
       "              3.4466e-01, -1.6702e-01],\n",
       "            ...,\n",
       "            [-2.9701e-02, -1.7352e-01,  1.0700e-01,  ...,  5.8098e-02,\n",
       "              2.5701e-01,  1.1468e-01],\n",
       "            [-2.9701e-02, -1.7352e-01,  1.0700e-01,  ...,  5.8098e-02,\n",
       "              2.5701e-01,  1.1468e-01],\n",
       "            [-2.9701e-02, -1.7352e-01,  1.0700e-01,  ...,  5.8098e-02,\n",
       "              2.5701e-01,  1.1468e-01]],\n",
       "   \n",
       "           [[-5.7305e-02,  8.7201e-02,  5.8997e-02,  ..., -2.2392e-01,\n",
       "              2.0258e-01, -2.7873e-01],\n",
       "            [-7.5708e-01, -6.8681e-01, -9.1605e-02,  ...,  7.9152e-02,\n",
       "              3.6407e-01,  4.9672e-01],\n",
       "            [ 2.1424e-01, -1.2802e-01,  1.3459e-01,  ...,  1.1301e-01,\n",
       "              2.5793e-01, -1.6201e-01],\n",
       "            ...,\n",
       "            [-5.9793e-02, -2.4607e-01,  1.4182e-01,  ..., -3.4157e-02,\n",
       "              3.6026e-01,  1.3461e-01],\n",
       "            [-5.9793e-02, -2.4607e-01,  1.4182e-01,  ..., -3.4157e-02,\n",
       "              3.6026e-01,  1.3461e-01],\n",
       "            [-5.9793e-02, -2.4607e-01,  1.4182e-01,  ..., -3.4157e-02,\n",
       "              3.6026e-01,  1.3461e-01]]], grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.0560, -0.1199, -0.3028,  ...,  0.3323, -0.3815,  0.0391],\n",
       "           [-0.1810, -0.5441, -0.1370,  ...,  0.1268, -0.4941,  0.2537],\n",
       "           [ 0.2636, -0.3688, -0.6066,  ..., -0.4234,  0.5295,  0.0732],\n",
       "           ...,\n",
       "           [-0.4231,  0.1399, -0.6501,  ..., -0.3231,  0.3458,  0.3534],\n",
       "           [-0.4231,  0.1399, -0.6501,  ..., -0.3231,  0.3458,  0.3534],\n",
       "           [-0.4231,  0.1399, -0.6501,  ..., -0.3231,  0.3458,  0.3534]],\n",
       "  \n",
       "          [[-0.0504, -0.3907,  0.0236,  ...,  0.0402, -0.1332,  0.4145],\n",
       "           [-0.1571, -0.0939, -0.0681,  ..., -0.7298,  0.6696,  1.0256],\n",
       "           [-0.2326, -0.6328, -0.2110,  ...,  0.1898, -0.4517,  0.1809],\n",
       "           ...,\n",
       "           [ 0.1752,  0.3275, -0.8958,  ..., -0.6649,  0.2678,  0.3220],\n",
       "           [ 0.1752,  0.3275, -0.8958,  ..., -0.6649,  0.2678,  0.3220],\n",
       "           [ 0.1752,  0.3275, -0.8958,  ..., -0.6649,  0.2678,  0.3220]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.21.mlp.fc1': {'input': (tensor([[[ -8.1999,  -7.7982,  -1.9959,  ...,  -0.0242,   4.2774,  -1.2815],\n",
       "            [-15.0203,  -9.2692,   3.7398,  ...,  -3.9468,  -4.9700,  -5.2337],\n",
       "            [  0.2363,  -4.4227,  -5.5433,  ...,   0.7174,   3.2443,  -7.6283],\n",
       "            ...,\n",
       "            [ -9.3270,   0.8369,   0.1220,  ...,   5.1651,   7.4550,  -6.8403],\n",
       "            [ -9.3270,   0.8369,   0.1220,  ...,   5.1651,   7.4550,  -6.8403],\n",
       "            [ -9.3270,   0.8369,   0.1220,  ...,   5.1651,   7.4550,  -6.8403]],\n",
       "   \n",
       "           [[ -7.9038,  -4.4593,   2.2373,  ...,  -0.9561,  -4.0955,  -2.8849],\n",
       "            [ -6.2470,  -7.5238,   3.1468,  ...,  -2.7847,   3.9332, -12.0999],\n",
       "            [ -0.6167,  -6.5325,   7.5242,  ...,  -1.8594,  -4.2819,  -4.3738],\n",
       "            ...,\n",
       "            [ -8.4675,  -2.1519,  -0.6811,  ...,   6.0801,   6.4094,  -5.1545],\n",
       "            [ -8.4675,  -2.1519,  -0.6811,  ...,   6.0801,   6.4094,  -5.1545],\n",
       "            [ -8.4675,  -2.1519,  -0.6812,  ...,   6.0801,   6.4094,  -5.1545]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-15.0233,  -4.6309, -13.9351,  ..., -20.2072, -22.7953, -10.7747],\n",
       "           [ -9.9096, -10.3285,  -9.0797,  ..., -21.0028, -14.5023, -19.7740],\n",
       "           [-17.8281,  -7.4055, -30.4977,  ..., -17.7765, -15.0107, -17.3293],\n",
       "           ...,\n",
       "           [-18.1722,  -6.3956, -21.8989,  ..., -10.4109, -17.7280, -17.0715],\n",
       "           [-18.1722,  -6.3956, -21.8989,  ..., -10.4109, -17.7280, -17.0715],\n",
       "           [-18.1722,  -6.3956, -21.8989,  ..., -10.4109, -17.7280, -17.0715]],\n",
       "  \n",
       "          [[-13.4565, -10.9595, -25.1615,  ..., -19.7192, -17.7272,  -9.0894],\n",
       "           [-11.5664, -15.0254, -13.4082,  ...,  -7.1686, -25.6546, -14.6882],\n",
       "           [-17.3044, -17.8876,  -4.8324,  ..., -13.9316, -24.7792, -24.5620],\n",
       "           ...,\n",
       "           [-15.2089,  -6.8818, -24.2621,  ...,  -7.4490, -13.1725, -18.3053],\n",
       "           [-15.2089,  -6.8818, -24.2621,  ...,  -7.4490, -13.1725, -18.3053],\n",
       "           [-15.2089,  -6.8818, -24.2621,  ...,  -7.4490, -13.1725, -18.3053]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.21.mlp.fc2': {'input': (tensor([[[-0.0000e+00, -2.3462e-06, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            ...,\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00]],\n",
       "   \n",
       "           [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -7.2009e-07,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            ...,\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[ 0.9122,  0.6476,  0.3438,  ..., -0.1924,  0.4977,  0.7671],\n",
       "           [-0.9160,  0.0361, -0.7806,  ..., -0.5873,  0.0480,  1.0827],\n",
       "           [ 0.4648,  0.6639,  0.1161,  ..., -0.2012,  0.5537,  0.4560],\n",
       "           ...,\n",
       "           [-0.1715, -0.1828, -0.4769,  ..., -0.3548,  0.7868,  0.5341],\n",
       "           [-0.1715, -0.1828, -0.4769,  ..., -0.3548,  0.7868,  0.5341],\n",
       "           [-0.1715, -0.1828, -0.4769,  ..., -0.3548,  0.7868,  0.5341]],\n",
       "  \n",
       "          [[-0.0857,  0.1422, -0.3018,  ..., -0.2422, -0.1801,  0.6381],\n",
       "           [ 0.4085, -0.4786, -0.8682,  ...,  0.6272, -0.8780,  0.1212],\n",
       "           [ 0.3785,  0.5701,  0.0947,  ..., -0.2249,  0.4582,  0.3188],\n",
       "           ...,\n",
       "           [-0.3438, -0.0632, -0.6869,  ..., -0.1053,  0.8036,  0.4819],\n",
       "           [-0.3438, -0.0632, -0.6869,  ..., -0.1053,  0.8036,  0.4819],\n",
       "           [-0.3438, -0.0632, -0.6869,  ..., -0.1053,  0.8036,  0.4819]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.22.self_attn.q_proj': {'input': (tensor([[[-0.6568, -0.9687, -0.2117,  ..., -0.4289,  1.0545,  0.8241],\n",
       "            [-3.1837, -1.4711,  0.3159,  ..., -1.4931, -1.4846,  0.3963],\n",
       "            [ 0.9060, -0.3129, -1.2536,  ..., -0.2622,  0.8199, -0.7272],\n",
       "            ...,\n",
       "            [-1.5399,  0.3639, -0.0588,  ...,  0.7218,  1.7691, -0.6417],\n",
       "            [-1.5399,  0.3639, -0.0588,  ...,  0.7218,  1.7691, -0.6417],\n",
       "            [-1.5399,  0.3639, -0.0588,  ...,  0.7218,  1.7691, -0.6417]],\n",
       "   \n",
       "           [[-1.2286, -0.5892,  0.4117,  ..., -0.6452, -1.5137,  0.3646],\n",
       "            [-0.6188, -1.4461,  0.4438,  ..., -0.6530,  0.2731, -1.7743],\n",
       "            [ 0.7136, -0.6789,  1.8940,  ..., -0.8697, -1.1203, -0.0615],\n",
       "            ...,\n",
       "            [-1.3880, -0.2095, -0.2877,  ...,  0.9902,  1.4768, -0.3307],\n",
       "            [-1.3880, -0.2095, -0.2877,  ...,  0.9902,  1.4768, -0.3307],\n",
       "            [-1.3880, -0.2095, -0.2877,  ...,  0.9902,  1.4768, -0.3307]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.0848, -0.5841,  0.9886,  ..., -1.6867,  0.7372,  0.2231],\n",
       "           [ 0.2523, -1.4347,  0.1847,  ..., -1.5117,  0.9760,  0.9094],\n",
       "           [ 1.5032,  0.1570, -0.3296,  ..., -0.8007,  0.9666,  0.0692],\n",
       "           ...,\n",
       "           [-1.1159, -1.0489, -0.6079,  ...,  0.0604,  0.5042,  0.9549],\n",
       "           [-1.1159, -1.0489, -0.6079,  ...,  0.0604,  0.5042,  0.9549],\n",
       "           [-1.1159, -1.0489, -0.6079,  ...,  0.0604,  0.5042,  0.9549]],\n",
       "  \n",
       "          [[ 0.5152, -1.3657,  0.2183,  ..., -0.9906,  0.3600, -1.1968],\n",
       "           [ 1.5059, -1.4024,  0.8153,  ..., -0.5760,  0.6611,  0.2534],\n",
       "           [-1.6581, -1.8472,  0.8629,  ..., -0.2739,  1.1191, -0.9482],\n",
       "           ...,\n",
       "           [-1.2398, -0.8880, -1.4492,  ...,  0.3452,  0.4940,  0.6453],\n",
       "           [-1.2398, -0.8880, -1.4492,  ...,  0.3452,  0.4940,  0.6453],\n",
       "           [-1.2398, -0.8880, -1.4492,  ...,  0.3452,  0.4940,  0.6453]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.22.self_attn.k_proj': {'input': (tensor([[[-0.6568, -0.9687, -0.2117,  ..., -0.4289,  1.0545,  0.8241],\n",
       "            [-3.1837, -1.4711,  0.3159,  ..., -1.4931, -1.4846,  0.3963],\n",
       "            [ 0.9060, -0.3129, -1.2536,  ..., -0.2622,  0.8199, -0.7272],\n",
       "            ...,\n",
       "            [-1.5399,  0.3639, -0.0588,  ...,  0.7218,  1.7691, -0.6417],\n",
       "            [-1.5399,  0.3639, -0.0588,  ...,  0.7218,  1.7691, -0.6417],\n",
       "            [-1.5399,  0.3639, -0.0588,  ...,  0.7218,  1.7691, -0.6417]],\n",
       "   \n",
       "           [[-1.2286, -0.5892,  0.4117,  ..., -0.6452, -1.5137,  0.3646],\n",
       "            [-0.6188, -1.4461,  0.4438,  ..., -0.6530,  0.2731, -1.7743],\n",
       "            [ 0.7136, -0.6789,  1.8940,  ..., -0.8697, -1.1203, -0.0615],\n",
       "            ...,\n",
       "            [-1.3880, -0.2095, -0.2877,  ...,  0.9902,  1.4768, -0.3307],\n",
       "            [-1.3880, -0.2095, -0.2877,  ...,  0.9902,  1.4768, -0.3307],\n",
       "            [-1.3880, -0.2095, -0.2877,  ...,  0.9902,  1.4768, -0.3307]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.3351, -2.0908,  0.6961,  ...,  0.2493, -1.7063, -0.7994],\n",
       "           [ 1.3636, -1.9647, -0.5347,  ..., -0.3030, -0.7967, -0.5456],\n",
       "           [ 0.8410,  0.2032,  1.0228,  ...,  0.5478, -0.5023, -0.0177],\n",
       "           ...,\n",
       "           [ 0.4144, -1.0964, -0.5893,  ...,  1.2758, -1.6340, -0.5450],\n",
       "           [ 0.4144, -1.0964, -0.5893,  ...,  1.2758, -1.6340, -0.5450],\n",
       "           [ 0.4144, -1.0964, -0.5893,  ...,  1.2758, -1.6340, -0.5450]],\n",
       "  \n",
       "          [[ 1.6975, -2.3087, -0.0924,  ..., -0.0439, -2.2084,  0.1493],\n",
       "           [ 1.2444, -3.0349,  0.3711,  ..., -1.0692, -3.1878, -0.1433],\n",
       "           [-1.6090, -2.2046, -0.7176,  ..., -0.1345, -2.6262, -0.9729],\n",
       "           ...,\n",
       "           [ 0.3646, -0.8623, -0.7529,  ...,  1.5898, -1.3650, -0.3090],\n",
       "           [ 0.3646, -0.8623, -0.7529,  ...,  1.5898, -1.3650, -0.3090],\n",
       "           [ 0.3646, -0.8623, -0.7529,  ...,  1.5898, -1.3650, -0.3090]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.22.self_attn.v_proj': {'input': (tensor([[[-0.6568, -0.9687, -0.2117,  ..., -0.4289,  1.0545,  0.8241],\n",
       "            [-3.1837, -1.4711,  0.3159,  ..., -1.4931, -1.4846,  0.3963],\n",
       "            [ 0.9060, -0.3129, -1.2536,  ..., -0.2622,  0.8199, -0.7272],\n",
       "            ...,\n",
       "            [-1.5399,  0.3639, -0.0588,  ...,  0.7218,  1.7691, -0.6417],\n",
       "            [-1.5399,  0.3639, -0.0588,  ...,  0.7218,  1.7691, -0.6417],\n",
       "            [-1.5399,  0.3639, -0.0588,  ...,  0.7218,  1.7691, -0.6417]],\n",
       "   \n",
       "           [[-1.2286, -0.5892,  0.4117,  ..., -0.6452, -1.5137,  0.3646],\n",
       "            [-0.6188, -1.4461,  0.4438,  ..., -0.6530,  0.2731, -1.7743],\n",
       "            [ 0.7136, -0.6789,  1.8940,  ..., -0.8697, -1.1203, -0.0615],\n",
       "            ...,\n",
       "            [-1.3880, -0.2095, -0.2877,  ...,  0.9902,  1.4768, -0.3307],\n",
       "            [-1.3880, -0.2095, -0.2877,  ...,  0.9902,  1.4768, -0.3307],\n",
       "            [-1.3880, -0.2095, -0.2877,  ...,  0.9902,  1.4768, -0.3307]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-1.7578,  0.1877, -0.3214,  ...,  2.0622, -1.3077, -1.8950],\n",
       "           [-1.3717, -0.9032,  0.0806,  ...,  1.0982, -2.1417, -2.5006],\n",
       "           [ 3.0231, -0.4696,  0.7851,  ..., -0.6868,  0.0652, -0.4302],\n",
       "           ...,\n",
       "           [-0.1780,  0.2342,  0.2306,  ...,  0.7456, -0.9786, -0.9922],\n",
       "           [-0.1780,  0.2342,  0.2306,  ...,  0.7456, -0.9786, -0.9922],\n",
       "           [-0.1780,  0.2342,  0.2306,  ...,  0.7456, -0.9786, -0.9922]],\n",
       "  \n",
       "          [[-2.1464, -0.7631, -0.2884,  ...,  1.3738, -0.3338, -2.0454],\n",
       "           [-2.7015,  0.8155,  0.8064,  ..., -0.5372,  0.3158, -1.6295],\n",
       "           [-1.6440,  2.1290,  1.2880,  ...,  2.0256, -1.2269, -1.8780],\n",
       "           ...,\n",
       "           [ 0.0987, -0.6440,  0.8589,  ...,  0.8595, -1.2646, -0.7021],\n",
       "           [ 0.0987, -0.6440,  0.8589,  ...,  0.8595, -1.2646, -0.7021],\n",
       "           [ 0.0987, -0.6440,  0.8589,  ...,  0.8595, -1.2646, -0.7021]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.22.self_attn.out_proj': {'input': (tensor([[[-0.3556, -0.3369, -0.1037,  ...,  0.4927, -0.5307, -1.4287],\n",
       "            [-0.5015, -0.1862, -0.1886,  ...,  0.4713, -0.4267, -1.3535],\n",
       "            [ 2.0304, -0.5487,  0.2863,  ..., -0.0610, -0.0029, -0.2243],\n",
       "            ...,\n",
       "            [-0.2491, -0.4930, -0.4533,  ...,  0.0234, -0.7303, -1.0195],\n",
       "            [-0.2491, -0.4930, -0.4533,  ...,  0.0234, -0.7303, -1.0195],\n",
       "            [-0.2491, -0.4930, -0.4533,  ...,  0.0234, -0.7303, -1.0195]],\n",
       "   \n",
       "           [[-1.3399, -0.5493, -0.3539,  ...,  0.3444, -0.3978, -1.4581],\n",
       "            [-1.5314, -0.0208, -0.3116,  ...,  0.0958, -0.1058, -0.8501],\n",
       "            [-0.2527,  0.0997, -0.2663,  ...,  0.2193, -0.3254, -1.6359],\n",
       "            ...,\n",
       "            [-0.2745, -0.6673, -0.4893,  ...,  0.0534, -0.9450, -0.9033],\n",
       "            [-0.2745, -0.6673, -0.4893,  ...,  0.0534, -0.9450, -0.9033],\n",
       "            [-0.2745, -0.6673, -0.4893,  ...,  0.0534, -0.9450, -0.9033]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.8543,  0.4134, -1.4902,  ...,  0.2909,  0.5625,  0.3492],\n",
       "           [-0.8789, -0.0270, -1.1077,  ...,  0.1983,  0.0146,  0.4214],\n",
       "           [-0.2282,  0.2784, -0.4225,  ..., -0.2347, -0.1623, -0.1968],\n",
       "           ...,\n",
       "           [-0.4214,  0.6863, -1.0196,  ..., -0.6601,  0.5116,  0.4042],\n",
       "           [-0.4214,  0.6863, -1.0196,  ..., -0.6601,  0.5116,  0.4042],\n",
       "           [-0.4214,  0.6863, -1.0196,  ..., -0.6601,  0.5116,  0.4042]],\n",
       "  \n",
       "          [[-0.1866,  0.4122, -2.0293,  ..., -0.0417,  0.5518,  0.5168],\n",
       "           [-0.7592,  0.3652, -1.7575,  ..., -0.4335,  0.5636, -0.2788],\n",
       "           [-1.1044,  0.2615, -0.4895,  ...,  0.0191, -0.2435,  0.2263],\n",
       "           ...,\n",
       "           [-0.4862,  1.0831, -1.1819,  ..., -0.4561,  0.3509,  0.4746],\n",
       "           [-0.4862,  1.0831, -1.1819,  ..., -0.4561,  0.3509,  0.4746],\n",
       "           [-0.4862,  1.0831, -1.1819,  ..., -0.4561,  0.3509,  0.4746]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.22.mlp.fc1': {'input': (tensor([[[ -4.5608,  -2.6601, -10.2992,  ...,  -1.4124,  18.1002,   6.6124],\n",
       "            [-15.6232,  -7.1823,  -6.4779,  ...,  -6.4060,  -3.3819,   4.9552],\n",
       "            [  3.5196,   0.5868, -11.9703,  ...,  -1.9976,  12.7247,  -2.9190],\n",
       "            ...,\n",
       "            [ -6.3622,   4.6000,  -5.5547,  ...,   2.0961,  21.8209,  -1.3849],\n",
       "            [ -6.3622,   4.6000,  -5.5547,  ...,   2.0961,  21.8209,  -1.3849],\n",
       "            [ -6.3622,   4.6000,  -5.5547,  ...,   2.0961,  21.8209,  -1.3849]],\n",
       "   \n",
       "           [[ -5.1744,  -0.7148,  -7.8663,  ...,  -3.3251,  -1.4167,   4.7102],\n",
       "            [ -3.4659,  -6.1483,  -4.9966,  ...,  -4.0026,  11.4811,  -8.5833],\n",
       "            [ -0.1219,  -1.3999,   5.8232,  ...,  -4.1355,  -2.0212,   1.8182],\n",
       "            ...,\n",
       "            [ -5.7814,   1.8272,  -7.0229,  ...,   3.6130,  19.2672,   0.2289],\n",
       "            [ -5.7814,   1.8272,  -7.0229,  ...,   3.6130,  19.2672,   0.2289],\n",
       "            [ -5.7814,   1.8272,  -7.0230,  ...,   3.6130,  19.2672,   0.2289]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-28.6819, -33.8948, -14.5615,  ..., -23.5177, -15.8335, -25.8370],\n",
       "           [-19.5132, -25.8296, -27.2633,  ..., -26.6632, -18.0635, -19.3736],\n",
       "           [-33.2049, -33.2097, -21.8713,  ..., -21.0840, -24.8923, -26.2084],\n",
       "           ...,\n",
       "           [-18.4380, -28.5897, -23.2689,  ..., -21.4588,  -7.5853, -28.8000],\n",
       "           [-18.4380, -28.5897, -23.2689,  ..., -21.4588,  -7.5853, -28.8000],\n",
       "           [-18.4380, -28.5897, -23.2689,  ..., -21.4588,  -7.5853, -28.8000]],\n",
       "  \n",
       "          [[-29.7778, -26.0208, -33.2268,  ..., -29.2747,  -6.7244, -22.4740],\n",
       "           [-31.1064, -21.3115, -33.0156,  ..., -32.2520, -11.1850, -26.4121],\n",
       "           [-26.3676, -14.0106,  -8.4283,  ..., -18.9419, -17.2754, -20.3323],\n",
       "           ...,\n",
       "           [-18.0398, -30.2198, -20.5036,  ..., -22.2190, -14.2629, -29.0682],\n",
       "           [-18.0398, -30.2198, -20.5036,  ..., -22.2190, -14.2629, -29.0682],\n",
       "           [-18.0398, -30.2198, -20.5036,  ..., -22.2190, -14.2629, -29.0682]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.22.mlp.fc2': {'input': (tensor([[[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            ...,\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.]],\n",
       "   \n",
       "           [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            ...,\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[ 0.3163,  0.4839,  0.0604,  ..., -0.8355, -0.2023, -0.0742],\n",
       "           [ 0.6159,  1.2198,  0.8344,  ..., -0.2706, -0.4004, -0.9301],\n",
       "           [ 0.2386,  0.3027,  0.9690,  ..., -0.1997,  0.0674, -0.1478],\n",
       "           ...,\n",
       "           [-0.0203, -0.2168,  1.7079,  ..., -1.6244,  0.9192,  0.9746],\n",
       "           [-0.0203, -0.2168,  1.7079,  ..., -1.6244,  0.9192,  0.9746],\n",
       "           [-0.0203, -0.2168,  1.7079,  ..., -1.6244,  0.9192,  0.9746]],\n",
       "  \n",
       "          [[ 0.5802, -0.5181,  0.6290,  ..., -0.2116, -0.1430,  0.2695],\n",
       "           [-0.2217, -0.2580, -0.0310,  ..., -0.3363,  0.5306,  1.0133],\n",
       "           [-0.4456, -0.8586,  0.1019,  ..., -1.0896, -0.6446, -0.0956],\n",
       "           ...,\n",
       "           [-0.1479, -0.4638,  1.9573,  ..., -1.9065,  1.0516,  1.2427],\n",
       "           [-0.1479, -0.4638,  1.9573,  ..., -1.9065,  1.0516,  1.2427],\n",
       "           [-0.1479, -0.4638,  1.9573,  ..., -1.9065,  1.0516,  1.2427]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.23.self_attn.q_proj': {'input': (tensor([[[-0.8574, -0.6068, -1.0627,  ..., -0.5454,  0.9679,  1.0472],\n",
       "            [-2.8501, -0.7572,  0.1196,  ..., -1.1979, -1.7427,  0.1132],\n",
       "            [ 0.8920, -0.2103, -0.9279,  ..., -0.3221,  0.5179, -0.7764],\n",
       "            ...,\n",
       "            [-1.4990,  0.2426,  0.1293,  ...,  0.2146,  1.8519, -0.0917],\n",
       "            [-1.4990,  0.2426,  0.1293,  ...,  0.2146,  1.8519, -0.0917],\n",
       "            [-1.4990,  0.2426,  0.1293,  ...,  0.2146,  1.8519, -0.0917]],\n",
       "   \n",
       "           [[-0.9038, -0.8515, -0.3925,  ..., -0.5981, -1.4105,  0.9198],\n",
       "            [-0.8574, -1.4909, -0.2467,  ..., -0.7075,  0.4330, -1.2357],\n",
       "            [-0.3463, -1.1826,  1.4948,  ..., -1.2712, -1.7437,  0.1518],\n",
       "            ...,\n",
       "            [-1.3954, -0.2706, -0.0921,  ...,  0.5028,  1.5333,  0.2555],\n",
       "            [-1.3954, -0.2706, -0.0921,  ...,  0.5028,  1.5333,  0.2555],\n",
       "            [-1.3954, -0.2706, -0.0921,  ...,  0.5028,  1.5333,  0.2555]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 2.2855,  0.0711, -0.9190,  ..., -1.5638, -0.4676,  0.8839],\n",
       "           [ 2.6341,  0.0172, -0.7176,  ..., -2.3214, -1.7793, -0.2351],\n",
       "           [ 1.7811, -1.7314,  0.4582,  ...,  0.7766, -0.0328, -0.8551],\n",
       "           ...,\n",
       "           [-4.1578,  0.4315, -1.7803,  ..., -1.4743,  0.2278,  2.0937],\n",
       "           [-4.1578,  0.4315, -1.7803,  ..., -1.4743,  0.2278,  2.0937],\n",
       "           [-4.1578,  0.4315, -1.7803,  ..., -1.4743,  0.2278,  2.0937]],\n",
       "  \n",
       "          [[-0.4841, -0.7769, -0.8837,  ..., -0.9277,  0.3216,  2.8728],\n",
       "           [-1.5020, -0.3722,  0.7069,  ..., -0.8435, -1.2135,  2.5431],\n",
       "           [ 1.8377,  0.1763,  0.6360,  ..., -0.3829, -0.9056, -0.1960],\n",
       "           ...,\n",
       "           [-4.0143,  0.8872, -1.5866,  ..., -1.1629,  0.6276,  2.1977],\n",
       "           [-4.0143,  0.8872, -1.5866,  ..., -1.1629,  0.6276,  2.1977],\n",
       "           [-4.0143,  0.8872, -1.5866,  ..., -1.1629,  0.6276,  2.1977]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.23.self_attn.k_proj': {'input': (tensor([[[-0.8574, -0.6068, -1.0627,  ..., -0.5454,  0.9679,  1.0472],\n",
       "            [-2.8501, -0.7572,  0.1196,  ..., -1.1979, -1.7427,  0.1132],\n",
       "            [ 0.8920, -0.2103, -0.9279,  ..., -0.3221,  0.5179, -0.7764],\n",
       "            ...,\n",
       "            [-1.4990,  0.2426,  0.1293,  ...,  0.2146,  1.8519, -0.0917],\n",
       "            [-1.4990,  0.2426,  0.1293,  ...,  0.2146,  1.8519, -0.0917],\n",
       "            [-1.4990,  0.2426,  0.1293,  ...,  0.2146,  1.8519, -0.0917]],\n",
       "   \n",
       "           [[-0.9038, -0.8515, -0.3925,  ..., -0.5981, -1.4105,  0.9198],\n",
       "            [-0.8574, -1.4909, -0.2467,  ..., -0.7075,  0.4330, -1.2357],\n",
       "            [-0.3463, -1.1826,  1.4948,  ..., -1.2712, -1.7437,  0.1518],\n",
       "            ...,\n",
       "            [-1.3954, -0.2706, -0.0921,  ...,  0.5028,  1.5333,  0.2555],\n",
       "            [-1.3954, -0.2706, -0.0921,  ...,  0.5028,  1.5333,  0.2555],\n",
       "            [-1.3954, -0.2706, -0.0921,  ...,  0.5028,  1.5333,  0.2555]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 3.3703,  0.4176, -0.1784,  ..., -2.8216, -1.2302, -0.6696],\n",
       "           [ 3.6277,  0.0960, -0.0970,  ..., -3.2997,  0.4648, -0.1132],\n",
       "           [ 1.8619, -0.5194,  0.7335,  ...,  2.3140,  1.2188, -0.2197],\n",
       "           ...,\n",
       "           [-0.1746,  0.7528, -0.4620,  ..., -3.8788,  3.0263,  0.3490],\n",
       "           [-0.1746,  0.7528, -0.4620,  ..., -3.8788,  3.0263,  0.3490],\n",
       "           [-0.1746,  0.7528, -0.4620,  ..., -3.8788,  3.0263,  0.3490]],\n",
       "  \n",
       "          [[ 1.3773,  0.4384,  0.2389,  ..., -0.8780,  0.6691, -1.1399],\n",
       "           [ 0.5812, -0.0233,  2.2377,  ..., -0.8135, -0.2112, -0.9581],\n",
       "           [ 3.3210, -0.7938,  0.5789,  ..., -0.4624,  1.2770, -1.1139],\n",
       "           ...,\n",
       "           [ 0.0463,  0.9467, -0.0341,  ..., -3.9755,  2.9927,  0.6803],\n",
       "           [ 0.0463,  0.9467, -0.0341,  ..., -3.9755,  2.9927,  0.6803],\n",
       "           [ 0.0463,  0.9467, -0.0341,  ..., -3.9755,  2.9927,  0.6803]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.23.self_attn.v_proj': {'input': (tensor([[[-0.8574, -0.6068, -1.0627,  ..., -0.5454,  0.9679,  1.0472],\n",
       "            [-2.8501, -0.7572,  0.1196,  ..., -1.1979, -1.7427,  0.1132],\n",
       "            [ 0.8920, -0.2103, -0.9279,  ..., -0.3221,  0.5179, -0.7764],\n",
       "            ...,\n",
       "            [-1.4990,  0.2426,  0.1293,  ...,  0.2146,  1.8519, -0.0917],\n",
       "            [-1.4990,  0.2426,  0.1293,  ...,  0.2146,  1.8519, -0.0917],\n",
       "            [-1.4990,  0.2426,  0.1293,  ...,  0.2146,  1.8519, -0.0917]],\n",
       "   \n",
       "           [[-0.9038, -0.8515, -0.3925,  ..., -0.5981, -1.4105,  0.9198],\n",
       "            [-0.8574, -1.4909, -0.2467,  ..., -0.7075,  0.4330, -1.2357],\n",
       "            [-0.3463, -1.1826,  1.4948,  ..., -1.2712, -1.7437,  0.1518],\n",
       "            ...,\n",
       "            [-1.3954, -0.2706, -0.0921,  ...,  0.5028,  1.5333,  0.2555],\n",
       "            [-1.3954, -0.2706, -0.0921,  ...,  0.5028,  1.5333,  0.2555],\n",
       "            [-1.3954, -0.2706, -0.0921,  ...,  0.5028,  1.5333,  0.2555]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 0.9633,  1.1809, -1.8762,  ...,  0.8906, -0.2336,  0.5189],\n",
       "           [ 1.1931,  0.9739,  0.3869,  ...,  0.5622, -0.0019,  0.8777],\n",
       "           [ 0.0390,  0.8329,  0.0629,  ..., -0.3823, -0.9999,  0.6336],\n",
       "           ...,\n",
       "           [-0.9308, -0.6239, -0.1088,  ..., -0.3826,  1.2329, -1.1323],\n",
       "           [-0.9308, -0.6239, -0.1088,  ..., -0.3826,  1.2329, -1.1323],\n",
       "           [-0.9308, -0.6239, -0.1088,  ..., -0.3826,  1.2329, -1.1323]],\n",
       "  \n",
       "          [[ 1.0226,  1.4373, -0.3343,  ..., -0.3232,  0.4131, -0.5020],\n",
       "           [ 0.7235, -1.6324, -0.1348,  ..., -0.3915, -1.0062, -0.4580],\n",
       "           [-1.2030, -0.6475,  0.0882,  ...,  0.3760, -0.7055,  0.0490],\n",
       "           ...,\n",
       "           [-0.2268, -0.5148, -0.3135,  ..., -0.2446,  0.1920, -0.8706],\n",
       "           [-0.2268, -0.5148, -0.3135,  ..., -0.2446,  0.1920, -0.8706],\n",
       "           [-0.2268, -0.5148, -0.3135,  ..., -0.2446,  0.1920, -0.8706]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.23.self_attn.out_proj': {'input': (tensor([[[ 0.1713,  0.2187, -0.1051,  ..., -0.1071, -0.2169, -0.0259],\n",
       "            [ 0.3388,  0.1962, -0.1370,  ...,  0.0058, -0.1858, -0.0476],\n",
       "            [ 0.2828,  0.0929,  0.1227,  ..., -0.1020, -0.1819, -0.0037],\n",
       "            ...,\n",
       "            [-0.2580,  1.3811, -0.5559,  ..., -0.0428, -0.1740, -0.0836],\n",
       "            [-0.2580,  1.3811, -0.5559,  ..., -0.0428, -0.1740, -0.0836],\n",
       "            [-0.2580,  1.3811, -0.5559,  ..., -0.0428, -0.1740, -0.0836]],\n",
       "   \n",
       "           [[ 0.1130, -0.1158, -0.2697,  ..., -0.0324,  0.1527,  0.1189],\n",
       "            [ 0.9196,  0.0215, -0.4889,  ..., -0.2407, -0.3150, -0.0104],\n",
       "            [ 0.1892,  0.1465, -0.0061,  ..., -0.2946, -0.2149,  0.1084],\n",
       "            ...,\n",
       "            [-0.2016,  1.4657, -0.9236,  ..., -0.2578, -0.2908,  0.0461],\n",
       "            [-0.2016,  1.4657, -0.9236,  ..., -0.2578, -0.2908,  0.0461],\n",
       "            [-0.2016,  1.4657, -0.9236,  ..., -0.2578, -0.2908,  0.0461]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.3146, -0.1946, -0.7929,  ...,  0.5908,  0.1135, -0.2277],\n",
       "           [ 0.1579,  0.0696, -0.5438,  ...,  0.8026, -0.0806, -0.7707],\n",
       "           [ 0.0876,  0.6605,  0.3859,  ...,  1.1270, -0.1467, -0.4977],\n",
       "           ...,\n",
       "           [-0.1082,  0.8753,  0.2543,  ...,  0.1640, -0.4479,  0.0225],\n",
       "           [-0.1082,  0.8753,  0.2543,  ...,  0.1640, -0.4479,  0.0225],\n",
       "           [-0.1082,  0.8753,  0.2543,  ...,  0.1640, -0.4479,  0.0225]],\n",
       "  \n",
       "          [[-0.0726, -0.1719, -0.8310,  ...,  0.0056, -0.1799,  0.0363],\n",
       "           [-0.8924,  1.1637,  0.0272,  ...,  1.1558, -0.3697,  0.0997],\n",
       "           [-0.3770, -0.4522, -0.4940,  ...,  0.9078, -0.4203, -0.2635],\n",
       "           ...,\n",
       "           [-0.0530,  0.1243,  0.1514,  ..., -0.1360, -0.3581, -0.3787],\n",
       "           [-0.0530,  0.1243,  0.1514,  ..., -0.1360, -0.3581, -0.3787],\n",
       "           [-0.0530,  0.1243,  0.1514,  ..., -0.1360, -0.3581, -0.3787]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.23.mlp.fc1': {'input': (tensor([[[ -4.8513,  -1.4909,  -8.1287,  ...,   0.2039,  16.9544,   3.4806],\n",
       "            [-14.5011,  -1.6387,  -0.6736,  ...,  -2.8264,  -6.6238,  -3.8516],\n",
       "            [  5.7247,   5.1642,  -3.1879,  ...,   3.0565,  11.9699,  -7.2118],\n",
       "            ...,\n",
       "            [ -7.6352,   7.9671,   2.0692,  ...,   3.0657,  23.0534,  -1.9568],\n",
       "            [ -7.6352,   7.9671,   2.0692,  ...,   3.0657,  23.0534,  -1.9568],\n",
       "            [ -7.6352,   7.9671,   2.0692,  ...,   3.0657,  23.0534,  -1.9568]],\n",
       "   \n",
       "           [[ -4.4117,  -3.3471,  -4.2521,  ...,  -2.0836,  -4.0139,   3.7563],\n",
       "            [ -5.8644,  -4.6068,  -0.6067,  ...,  -0.3993,  11.1532,  -8.2463],\n",
       "            [ -2.7268,  -7.9524,   7.9515,  ...,  -2.9752,  -8.7456,  -1.6968],\n",
       "            ...,\n",
       "            [ -7.0317,   2.2194,   0.5242,  ...,   4.3620,  20.7630,  -0.6206],\n",
       "            [ -7.0317,   2.2194,   0.5242,  ...,   4.3620,  20.7630,  -0.6206],\n",
       "            [ -7.0317,   2.2194,   0.5242,  ...,   4.3620,  20.7629,  -0.6206]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-35.2599, -20.2841, -16.2293,  ..., -33.7603, -13.5003, -16.9300],\n",
       "           [-28.7738, -12.6048, -11.8912,  ..., -37.4745, -19.6001, -15.1504],\n",
       "           [-33.6947, -31.2088, -29.3051,  ..., -35.5231, -23.5351, -45.3109],\n",
       "           ...,\n",
       "           [-30.7476, -14.5290, -30.7627,  ..., -23.6533, -22.7118, -32.8846],\n",
       "           [-30.7476, -14.5290, -30.7627,  ..., -23.6533, -22.7118, -32.8846],\n",
       "           [-30.7476, -14.5290, -30.7627,  ..., -23.6533, -22.7118, -32.8846]],\n",
       "  \n",
       "          [[-29.1554, -10.5308, -13.2405,  ..., -32.8495, -21.8441, -28.0508],\n",
       "           [-39.4758, -22.4964, -27.0751,  ..., -40.4345, -30.4238, -18.9351],\n",
       "           [-22.1836, -17.2390, -29.3832,  ..., -37.2261, -23.2835, -25.4791],\n",
       "           ...,\n",
       "           [-29.0939, -15.7087, -30.1110,  ..., -20.7880, -30.1556, -28.8630],\n",
       "           [-29.0939, -15.7087, -30.1110,  ..., -20.7880, -30.1556, -28.8630],\n",
       "           [-29.0939, -15.7087, -30.1110,  ..., -20.7879, -30.1556, -28.8630]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.23.mlp.fc2': {'input': (tensor([[[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            ...,\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.]],\n",
       "   \n",
       "           [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            ...,\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[-1.1115,  0.2079,  1.0240,  ..., -0.3817,  0.8453,  1.2342],\n",
       "           [ 2.0843,  0.6159,  0.3827,  ...,  1.0020, -0.0390, -0.9149],\n",
       "           [ 0.8497,  0.5246,  1.0679,  ..., -0.2215,  0.3769, -0.1074],\n",
       "           ...,\n",
       "           [ 0.3512, -0.2223,  0.7235,  ..., -0.6967,  0.0843,  0.0938],\n",
       "           [ 0.3512, -0.2223,  0.7235,  ..., -0.6967,  0.0843,  0.0938],\n",
       "           [ 0.3512, -0.2223,  0.7235,  ..., -0.6967,  0.0843,  0.0938]],\n",
       "  \n",
       "          [[-0.6284, -0.6247,  0.4326,  ...,  0.8327,  1.1506,  0.6074],\n",
       "           [ 1.3498,  1.1160, -0.3441,  ..., -3.1944, -0.3523,  3.1378],\n",
       "           [ 0.1964, -0.2421,  0.6354,  ..., -0.3851,  0.4515, -0.2372],\n",
       "           ...,\n",
       "           [ 0.1183,  0.0282,  0.7263,  ..., -0.6877,  0.2137,  0.0743],\n",
       "           [ 0.1183,  0.0282,  0.7263,  ..., -0.6877,  0.2137,  0.0743],\n",
       "           [ 0.1183,  0.0282,  0.7263,  ..., -0.6877,  0.2137,  0.0743]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.24.self_attn.q_proj': {'input': (tensor([[[-1.4649, -0.4672, -0.9967,  ..., -0.2718,  1.0952,  1.3275],\n",
       "            [-1.2429, -0.2685, -0.2607,  ..., -0.0061, -1.6974, -0.9304],\n",
       "            [ 1.1456,  0.3555, -0.3962,  ...,  0.1903,  0.5064, -1.1007],\n",
       "            ...,\n",
       "            [-1.6016,  0.4105,  0.0774,  ...,  0.1240,  1.7817, -0.1192],\n",
       "            [-1.6016,  0.4105,  0.0774,  ...,  0.1240,  1.7817, -0.1192],\n",
       "            [-1.6016,  0.4105,  0.0774,  ...,  0.1240,  1.7817, -0.1192]],\n",
       "   \n",
       "           [[-1.3206, -1.0961, -0.8672,  ..., -0.0896, -0.9363,  1.1649],\n",
       "            [-0.7377, -0.5842, -0.6145,  ..., -1.1564,  0.0514, -0.0454],\n",
       "            [-0.5412, -1.3515,  1.1172,  ..., -0.7564, -1.6814, -0.2192],\n",
       "            ...,\n",
       "            [-1.5554, -0.2109, -0.2014,  ...,  0.3518,  1.5207,  0.1208],\n",
       "            [-1.5554, -0.2109, -0.2014,  ...,  0.3518,  1.5207,  0.1208],\n",
       "            [-1.5554, -0.2109, -0.2014,  ...,  0.3518,  1.5207,  0.1208]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.3035, -0.9969, -0.6260,  ...,  0.6847,  0.1275, -0.4510],\n",
       "           [ 2.2765,  3.4545, -1.7293,  ...,  1.8841,  1.5942,  1.6163],\n",
       "           [ 0.3123, -0.8756,  0.6831,  ...,  0.6802, -0.6530,  0.0548],\n",
       "           ...,\n",
       "           [ 0.0783, -0.0279, -1.0396,  ..., -1.4918,  0.9578,  0.4262],\n",
       "           [ 0.0783, -0.0279, -1.0396,  ..., -1.4918,  0.9578,  0.4262],\n",
       "           [ 0.0783, -0.0279, -1.0396,  ..., -1.4918,  0.9578,  0.4262]],\n",
       "  \n",
       "          [[ 0.1287, -1.2050,  1.1632,  ...,  1.3864,  0.2643, -0.1359],\n",
       "           [ 0.6726,  1.2805, -0.7168,  ...,  1.1291, -0.3290,  0.2480],\n",
       "           [ 0.7705,  1.1445,  0.1410,  ...,  0.7030, -0.5564,  0.0246],\n",
       "           ...,\n",
       "           [ 0.3948, -0.1359, -0.7750,  ..., -1.4971,  0.9070, -0.3229],\n",
       "           [ 0.3948, -0.1359, -0.7750,  ..., -1.4971,  0.9070, -0.3229],\n",
       "           [ 0.3948, -0.1359, -0.7750,  ..., -1.4971,  0.9070, -0.3229]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.24.self_attn.k_proj': {'input': (tensor([[[-1.4649, -0.4672, -0.9967,  ..., -0.2718,  1.0952,  1.3275],\n",
       "            [-1.2429, -0.2685, -0.2607,  ..., -0.0061, -1.6974, -0.9304],\n",
       "            [ 1.1456,  0.3555, -0.3962,  ...,  0.1903,  0.5064, -1.1007],\n",
       "            ...,\n",
       "            [-1.6016,  0.4105,  0.0774,  ...,  0.1240,  1.7817, -0.1192],\n",
       "            [-1.6016,  0.4105,  0.0774,  ...,  0.1240,  1.7817, -0.1192],\n",
       "            [-1.6016,  0.4105,  0.0774,  ...,  0.1240,  1.7817, -0.1192]],\n",
       "   \n",
       "           [[-1.3206, -1.0961, -0.8672,  ..., -0.0896, -0.9363,  1.1649],\n",
       "            [-0.7377, -0.5842, -0.6145,  ..., -1.1564,  0.0514, -0.0454],\n",
       "            [-0.5412, -1.3515,  1.1172,  ..., -0.7564, -1.6814, -0.2192],\n",
       "            ...,\n",
       "            [-1.5554, -0.2109, -0.2014,  ...,  0.3518,  1.5207,  0.1208],\n",
       "            [-1.5554, -0.2109, -0.2014,  ...,  0.3518,  1.5207,  0.1208],\n",
       "            [-1.5554, -0.2109, -0.2014,  ...,  0.3518,  1.5207,  0.1208]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.7512,  1.4101, -3.7577,  ...,  0.7803,  0.3539,  0.4058],\n",
       "           [ 1.1592,  1.1196, -0.6010,  ..., -0.7370, -0.9136, -0.8834],\n",
       "           [-1.4186,  2.0451, -2.5110,  ..., -1.4415,  0.8239, -0.5037],\n",
       "           ...,\n",
       "           [-1.0323,  2.2187, -1.3984,  ..., -0.6793, -0.5627, -1.1064],\n",
       "           [-1.0323,  2.2187, -1.3984,  ..., -0.6793, -0.5627, -1.1064],\n",
       "           [-1.0323,  2.2187, -1.3984,  ..., -0.6793, -0.5627, -1.1064]],\n",
       "  \n",
       "          [[ 1.7417,  2.1997,  0.0209,  ..., -0.1837, -0.7257,  0.3122],\n",
       "           [-0.0746,  2.4235, -0.0733,  ...,  0.4327, -0.2485, -0.6667],\n",
       "           [ 0.9132,  1.0244,  0.6103,  ..., -0.7721, -0.2850, -1.3654],\n",
       "           ...,\n",
       "           [-1.8568,  1.8046, -1.5820,  ..., -0.3895, -0.0698, -1.3036],\n",
       "           [-1.8568,  1.8046, -1.5820,  ..., -0.3895, -0.0698, -1.3036],\n",
       "           [-1.8568,  1.8046, -1.5820,  ..., -0.3895, -0.0698, -1.3036]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.24.self_attn.v_proj': {'input': (tensor([[[-1.4649, -0.4672, -0.9967,  ..., -0.2718,  1.0952,  1.3275],\n",
       "            [-1.2429, -0.2685, -0.2607,  ..., -0.0061, -1.6974, -0.9304],\n",
       "            [ 1.1456,  0.3555, -0.3962,  ...,  0.1903,  0.5064, -1.1007],\n",
       "            ...,\n",
       "            [-1.6016,  0.4105,  0.0774,  ...,  0.1240,  1.7817, -0.1192],\n",
       "            [-1.6016,  0.4105,  0.0774,  ...,  0.1240,  1.7817, -0.1192],\n",
       "            [-1.6016,  0.4105,  0.0774,  ...,  0.1240,  1.7817, -0.1192]],\n",
       "   \n",
       "           [[-1.3206, -1.0961, -0.8672,  ..., -0.0896, -0.9363,  1.1649],\n",
       "            [-0.7377, -0.5842, -0.6145,  ..., -1.1564,  0.0514, -0.0454],\n",
       "            [-0.5412, -1.3515,  1.1172,  ..., -0.7564, -1.6814, -0.2192],\n",
       "            ...,\n",
       "            [-1.5554, -0.2109, -0.2014,  ...,  0.3518,  1.5207,  0.1208],\n",
       "            [-1.5554, -0.2109, -0.2014,  ...,  0.3518,  1.5207,  0.1208],\n",
       "            [-1.5554, -0.2109, -0.2014,  ...,  0.3518,  1.5207,  0.1208]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.0633,  2.4038, -1.2274,  ..., -1.1624,  0.6801,  0.4677],\n",
       "           [ 0.8169,  2.1334,  0.6665,  ..., -1.1811, -1.5843,  0.5746],\n",
       "           [-1.7954,  0.5655,  0.2942,  ...,  1.4917, -0.3066,  0.4288],\n",
       "           ...,\n",
       "           [-0.6828,  1.2028, -0.7934,  ...,  0.6670,  0.2547, -1.7269],\n",
       "           [-0.6828,  1.2028, -0.7934,  ...,  0.6670,  0.2547, -1.7269],\n",
       "           [-0.6828,  1.2028, -0.7934,  ...,  0.6670,  0.2547, -1.7269]],\n",
       "  \n",
       "          [[-1.1834,  1.2630, -0.9148,  ..., -1.9986,  1.4515, -2.6986],\n",
       "           [ 2.1390, -2.0200, -1.5940,  ..., -0.0293, -0.3677, -2.6504],\n",
       "           [-1.5395, -0.3654,  1.1013,  ..., -0.0163, -1.6142, -1.3042],\n",
       "           ...,\n",
       "           [-1.1784,  0.1924, -1.4570,  ...,  1.0772,  0.1168, -1.4592],\n",
       "           [-1.1784,  0.1924, -1.4570,  ...,  1.0772,  0.1168, -1.4592],\n",
       "           [-1.1784,  0.1924, -1.4570,  ...,  1.0772,  0.1168, -1.4592]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.24.self_attn.out_proj': {'input': (tensor([[[ 0.4671,  0.1109, -0.1929,  ..., -0.1156,  0.6213, -0.1507],\n",
       "            [-0.6373,  0.0400, -0.0873,  ...,  0.6006,  0.2612, -0.1500],\n",
       "            [-0.0777, -0.0189, -0.1262,  ...,  0.2732,  0.4228, -0.1692],\n",
       "            ...,\n",
       "            [-0.3976,  0.0357, -0.0453,  ...,  0.6279,  0.2249, -0.0910],\n",
       "            [-0.3976,  0.0357, -0.0453,  ...,  0.6279,  0.2249, -0.0910],\n",
       "            [-0.3976,  0.0357, -0.0453,  ...,  0.6279,  0.2249, -0.0910]],\n",
       "   \n",
       "           [[ 0.5773,  0.0715, -0.2939,  ..., -0.8020,  0.2915, -0.7819],\n",
       "            [ 0.5839,  0.0769, -0.5270,  ...,  0.3823,  0.1827, -0.0667],\n",
       "            [-0.2515,  0.1452,  0.0611,  ...,  0.5980,  0.1599, -0.0174],\n",
       "            ...,\n",
       "            [-0.2381, -0.1954,  0.0049,  ...,  0.7277,  0.1458,  0.0566],\n",
       "            [-0.2381, -0.1954,  0.0049,  ...,  0.7277,  0.1458,  0.0566],\n",
       "            [-0.2381, -0.1954,  0.0049,  ...,  0.7277,  0.1458,  0.0566]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-0.0889,  0.9351, -0.2160,  ..., -0.1093, -0.1639,  0.7790],\n",
       "           [ 0.6001,  1.6660, -1.1005,  ...,  0.5100,  0.1060,  0.6764],\n",
       "           [-0.7746, -0.1854, -0.7432,  ...,  0.5872,  1.1099, -0.0388],\n",
       "           ...,\n",
       "           [ 0.3635,  1.0823, -0.6288,  ...,  0.9826,  0.2035, -0.3631],\n",
       "           [ 0.3635,  1.0823, -0.6288,  ...,  0.9826,  0.2035, -0.3631],\n",
       "           [ 0.3635,  1.0823, -0.6288,  ...,  0.9826,  0.2035, -0.3631]],\n",
       "  \n",
       "          [[ 0.0347,  1.1625, -0.8195,  ..., -0.0868, -0.0123,  0.9482],\n",
       "           [-0.0470,  0.5402,  0.3359,  ...,  0.2419,  0.6083,  0.4319],\n",
       "           [ 0.6366,  1.3472, -0.6324,  ...,  0.0391, -0.0454, -0.0360],\n",
       "           ...,\n",
       "           [ 0.6482,  0.7179, -0.9796,  ...,  1.7958, -0.5358,  0.3639],\n",
       "           [ 0.6482,  0.7179, -0.9796,  ...,  1.7958, -0.5358,  0.3639],\n",
       "           [ 0.6482,  0.7179, -0.9796,  ...,  1.7958, -0.5358,  0.3639]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.24.mlp.fc1': {'input': (tensor([[[-8.0641,  1.4406, -4.6138,  ..., -2.0773, 22.1175, 11.5835],\n",
       "            [-3.9370,  6.3035, -3.0918,  ...,  1.5365, -5.0276, -3.2318],\n",
       "            [ 7.9009,  4.0495, -2.2001,  ...,  2.8383, 22.3076, -6.8673],\n",
       "            ...,\n",
       "            [-7.6763,  7.1506,  2.5004,  ...,  2.3226, 29.8224, -1.1277],\n",
       "            [-7.6763,  7.1506,  2.5004,  ...,  2.3226, 29.8224, -1.1277],\n",
       "            [-7.6763,  7.1506,  2.5004,  ...,  2.3226, 29.8223, -1.1277]],\n",
       "   \n",
       "           [[-6.6508, -2.3954, -5.9874,  ..., -0.8733,  2.1974, 11.3720],\n",
       "            [-2.8259, -1.3251, -0.4383,  ..., -7.0906, 14.5035,  1.0294],\n",
       "            [ 1.4500, -2.9963,  8.5098,  ..., -4.7273, -5.5883, -1.2579],\n",
       "            ...,\n",
       "            [-6.9261,  1.6518,  0.0412,  ...,  4.7691, 25.3037,  1.7476],\n",
       "            [-6.9261,  1.6518,  0.0412,  ...,  4.7691, 25.3037,  1.7476],\n",
       "            [-6.9261,  1.6518,  0.0412,  ...,  4.7691, 25.3037,  1.7476]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-23.3601, -26.2977, -21.5298,  ..., -21.3240, -37.0478, -24.3697],\n",
       "           [-23.2808, -28.9570,  -9.8298,  ..., -23.2095, -35.7748, -32.9785],\n",
       "           [-36.1472, -46.6299, -30.3469,  ..., -26.6341, -32.3567, -47.6547],\n",
       "           ...,\n",
       "           [-17.6634, -26.7226, -28.9399,  ...,  -7.1883, -19.3876, -35.5355],\n",
       "           [-17.6634, -26.7226, -28.9399,  ...,  -7.1883, -19.3876, -35.5355],\n",
       "           [-17.6634, -26.7226, -28.9399,  ...,  -7.1883, -19.3876, -35.5355]],\n",
       "  \n",
       "          [[-24.0062, -40.5707, -33.7951,  ..., -11.5327, -18.5481, -37.9249],\n",
       "           [-23.2380, -34.7845, -10.3172,  ..., -18.9747, -46.7722, -44.4401],\n",
       "           [-19.9191, -40.9955, -24.0919,  ..., -22.9705, -35.4072, -19.5263],\n",
       "           ...,\n",
       "           [-24.7006, -28.4312, -25.3588,  ...,  -6.3252, -23.1534, -39.5869],\n",
       "           [-24.7006, -28.4312, -25.3588,  ...,  -6.3252, -23.1534, -39.5869],\n",
       "           [-24.7006, -28.4312, -25.3588,  ...,  -6.3252, -23.1534, -39.5869]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.24.mlp.fc2': {'input': (tensor([[[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            ...,\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.]],\n",
       "   \n",
       "           [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            ...,\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "            [-0., -0., -0.,  ..., -0., -0., -0.]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[ 2.0744e-01, -1.4012e+00, -6.0114e-01,  ..., -7.2522e-02,\n",
       "            -1.2057e+00, -1.9677e+00],\n",
       "           [-4.9018e-01, -1.0617e+00,  6.9020e-01,  ...,  1.3274e+00,\n",
       "            -3.0292e+00, -2.0557e-01],\n",
       "           [ 5.2217e-01, -1.0345e+00,  5.4318e-01,  ..., -3.0928e-01,\n",
       "            -2.0516e-01, -7.1570e-01],\n",
       "           ...,\n",
       "           [ 1.5788e+00, -1.2318e+00,  1.4131e+00,  ..., -1.0845e+00,\n",
       "            -1.2843e+00,  2.2882e-01],\n",
       "           [ 1.5788e+00, -1.2318e+00,  1.4131e+00,  ..., -1.0845e+00,\n",
       "            -1.2843e+00,  2.2882e-01],\n",
       "           [ 1.5788e+00, -1.2318e+00,  1.4131e+00,  ..., -1.0845e+00,\n",
       "            -1.2843e+00,  2.2882e-01]],\n",
       "  \n",
       "          [[ 1.1296e+00, -4.0752e-01, -1.3095e+00,  ..., -1.5937e-01,\n",
       "            -3.0518e+00, -2.0130e+00],\n",
       "           [-6.3434e-04, -2.3604e+00,  6.7595e-01,  ...,  1.4026e+00,\n",
       "            -1.2967e+00, -4.6299e-01],\n",
       "           [-8.0315e-01, -1.7465e+00,  2.5445e-01,  ..., -5.4366e-01,\n",
       "            -8.3446e-01,  1.0911e+00],\n",
       "           ...,\n",
       "           [ 1.5925e+00, -1.7387e+00,  2.2965e+00,  ..., -1.5100e+00,\n",
       "            -1.3068e+00,  1.0716e+00],\n",
       "           [ 1.5925e+00, -1.7387e+00,  2.2965e+00,  ..., -1.5100e+00,\n",
       "            -1.3068e+00,  1.0716e+00],\n",
       "           [ 1.5925e+00, -1.7387e+00,  2.2965e+00,  ..., -1.5100e+00,\n",
       "            -1.3068e+00,  1.0716e+00]]], grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.25.self_attn.q_proj': {'input': (tensor([[[-1.1853, -0.4047, -0.9781,  ..., -0.1679,  0.1857,  0.4577],\n",
       "            [-0.9251,  0.0759, -0.3190,  ...,  0.6559, -2.1004, -0.3517],\n",
       "            [ 0.5853, -0.0779, -0.4089,  ...,  0.3200,  0.6436, -1.0417],\n",
       "            ...,\n",
       "            [-0.9989,  0.3661,  0.2047,  ...,  0.1495,  0.9924, -0.1029],\n",
       "            [-0.9989,  0.3661,  0.2047,  ...,  0.1495,  0.9924, -0.1029],\n",
       "            [-0.9989,  0.3661,  0.2047,  ...,  0.1495,  0.9924, -0.1029]],\n",
       "   \n",
       "           [[-0.7037, -0.4638, -1.3592,  ..., -0.0617, -1.7528,  0.3696],\n",
       "            [-0.7512, -0.7532, -0.2248,  ..., -0.4126, -0.2138, -0.0305],\n",
       "            [-0.5766, -0.9039,  0.5123,  ..., -0.5349, -1.4259,  0.2471],\n",
       "            ...,\n",
       "            [-0.9530, -0.2717,  0.0856,  ...,  0.3982,  0.6926,  0.3605],\n",
       "            [-0.9530, -0.2717,  0.0856,  ...,  0.3982,  0.6926,  0.3605],\n",
       "            [-0.9530, -0.2717,  0.0856,  ...,  0.3982,  0.6926,  0.3605]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.3396,  0.2126,  1.1027,  ...,  1.7364, -0.1796, -0.4336],\n",
       "           [-0.4840,  0.5157, -0.0382,  ...,  1.0620,  0.0753, -0.8968],\n",
       "           [ 1.1683,  0.0327, -0.9633,  ..., -2.0543, -0.3276,  0.2033],\n",
       "           ...,\n",
       "           [-0.1314, -0.0671,  0.1420,  ...,  1.6905,  0.6231, -0.6032],\n",
       "           [-0.1314, -0.0671,  0.1420,  ...,  1.6905,  0.6231, -0.6032],\n",
       "           [-0.1314, -0.0671,  0.1420,  ...,  1.6905,  0.6231, -0.6032]],\n",
       "  \n",
       "          [[-0.1915, -0.1173,  0.9316,  ...,  1.0643, -0.4473, -0.4370],\n",
       "           [ 0.4590,  0.9472, -0.2131,  ...,  1.4422,  0.0295,  0.3023],\n",
       "           [-0.6532, -0.3419,  0.2329,  ..., -0.0668, -1.5214, -0.3827],\n",
       "           ...,\n",
       "           [ 0.0362, -0.2101, -0.1081,  ...,  2.0395,  0.9304, -0.5660],\n",
       "           [ 0.0362, -0.2101, -0.1081,  ...,  2.0395,  0.9304, -0.5660],\n",
       "           [ 0.0362, -0.2101, -0.1081,  ...,  2.0395,  0.9304, -0.5660]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.25.self_attn.k_proj': {'input': (tensor([[[-1.1853, -0.4047, -0.9781,  ..., -0.1679,  0.1857,  0.4577],\n",
       "            [-0.9251,  0.0759, -0.3190,  ...,  0.6559, -2.1004, -0.3517],\n",
       "            [ 0.5853, -0.0779, -0.4089,  ...,  0.3200,  0.6436, -1.0417],\n",
       "            ...,\n",
       "            [-0.9989,  0.3661,  0.2047,  ...,  0.1495,  0.9924, -0.1029],\n",
       "            [-0.9989,  0.3661,  0.2047,  ...,  0.1495,  0.9924, -0.1029],\n",
       "            [-0.9989,  0.3661,  0.2047,  ...,  0.1495,  0.9924, -0.1029]],\n",
       "   \n",
       "           [[-0.7037, -0.4638, -1.3592,  ..., -0.0617, -1.7528,  0.3696],\n",
       "            [-0.7512, -0.7532, -0.2248,  ..., -0.4126, -0.2138, -0.0305],\n",
       "            [-0.5766, -0.9039,  0.5123,  ..., -0.5349, -1.4259,  0.2471],\n",
       "            ...,\n",
       "            [-0.9530, -0.2717,  0.0856,  ...,  0.3982,  0.6926,  0.3605],\n",
       "            [-0.9530, -0.2717,  0.0856,  ...,  0.3982,  0.6926,  0.3605],\n",
       "            [-0.9530, -0.2717,  0.0856,  ...,  0.3982,  0.6926,  0.3605]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-0.6607,  0.0672,  0.2689,  ..., -0.2693,  0.3641,  1.0471],\n",
       "           [-0.1094,  0.4855,  0.6731,  ..., -1.2589, -0.3054, -0.5907],\n",
       "           [ 0.8676,  0.6632,  0.6941,  ...,  0.4634, -0.7724, -0.0404],\n",
       "           ...,\n",
       "           [-0.3962,  0.9324,  1.1480,  ...,  0.2745, -0.4364, -0.4017],\n",
       "           [-0.3962,  0.9324,  1.1480,  ...,  0.2745, -0.4364, -0.4017],\n",
       "           [-0.3962,  0.9324,  1.1480,  ...,  0.2745, -0.4364, -0.4017]],\n",
       "  \n",
       "          [[ 0.6990,  2.0353,  0.2567,  ..., -0.8214, -0.7478, -0.1584],\n",
       "           [ 0.8563,  0.1445,  0.6662,  ...,  1.0113, -0.4965, -0.2547],\n",
       "           [-0.7398,  1.3759,  0.7394,  ...,  1.9875,  0.0319, -1.5648],\n",
       "           ...,\n",
       "           [-0.4980,  0.5897,  1.4638,  ..., -0.0825,  0.1601, -0.5321],\n",
       "           [-0.4980,  0.5897,  1.4638,  ..., -0.0825,  0.1601, -0.5321],\n",
       "           [-0.4980,  0.5897,  1.4638,  ..., -0.0825,  0.1601, -0.5321]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.25.self_attn.v_proj': {'input': (tensor([[[-1.1853, -0.4047, -0.9781,  ..., -0.1679,  0.1857,  0.4577],\n",
       "            [-0.9251,  0.0759, -0.3190,  ...,  0.6559, -2.1004, -0.3517],\n",
       "            [ 0.5853, -0.0779, -0.4089,  ...,  0.3200,  0.6436, -1.0417],\n",
       "            ...,\n",
       "            [-0.9989,  0.3661,  0.2047,  ...,  0.1495,  0.9924, -0.1029],\n",
       "            [-0.9989,  0.3661,  0.2047,  ...,  0.1495,  0.9924, -0.1029],\n",
       "            [-0.9989,  0.3661,  0.2047,  ...,  0.1495,  0.9924, -0.1029]],\n",
       "   \n",
       "           [[-0.7037, -0.4638, -1.3592,  ..., -0.0617, -1.7528,  0.3696],\n",
       "            [-0.7512, -0.7532, -0.2248,  ..., -0.4126, -0.2138, -0.0305],\n",
       "            [-0.5766, -0.9039,  0.5123,  ..., -0.5349, -1.4259,  0.2471],\n",
       "            ...,\n",
       "            [-0.9530, -0.2717,  0.0856,  ...,  0.3982,  0.6926,  0.3605],\n",
       "            [-0.9530, -0.2717,  0.0856,  ...,  0.3982,  0.6926,  0.3605],\n",
       "            [-0.9530, -0.2717,  0.0856,  ...,  0.3982,  0.6926,  0.3605]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.5533,  1.2939, -0.2217,  ...,  1.4301, -0.4299, -2.0518],\n",
       "           [ 2.9227,  1.1086, -2.7397,  ..., -0.3886, -1.8233, -1.1611],\n",
       "           [-1.2627, -1.8328, -0.0842,  ...,  0.5104, -0.9588,  0.0952],\n",
       "           ...,\n",
       "           [ 1.4097, -2.5780,  0.3481,  ...,  0.8345,  1.4266,  0.6321],\n",
       "           [ 1.4097, -2.5780,  0.3481,  ...,  0.8345,  1.4266,  0.6321],\n",
       "           [ 1.4097, -2.5780,  0.3481,  ...,  0.8345,  1.4266,  0.6321]],\n",
       "  \n",
       "          [[ 2.4624,  1.4143, -1.3027,  ...,  1.2718,  0.1159, -0.5684],\n",
       "           [-1.8750,  0.5197, -0.7096,  ...,  1.2078, -0.0777, -1.4322],\n",
       "           [ 2.9313,  2.7389,  0.1536,  ..., -1.5408,  0.4085, -2.9189],\n",
       "           ...,\n",
       "           [ 1.8296, -2.5572,  0.3910,  ...,  0.2680,  1.5515, -0.0098],\n",
       "           [ 1.8296, -2.5572,  0.3910,  ...,  0.2680,  1.5515, -0.0098],\n",
       "           [ 1.8296, -2.5572,  0.3910,  ...,  0.2680,  1.5515, -0.0098]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.25.self_attn.out_proj': {'input': (tensor([[[ 0.9928,  0.2337,  0.1013,  ...,  0.2217, -0.0998,  0.4170],\n",
       "            [ 1.0970,  0.8672, -1.0013,  ...,  0.3703, -0.2380, -0.3587],\n",
       "            [-0.9379, -1.4216, -0.1475,  ...,  0.0282, -0.4235, -0.8121],\n",
       "            ...,\n",
       "            [-0.7475, -0.3381,  0.2955,  ..., -0.1912, -0.4304,  0.1493],\n",
       "            [-0.7475, -0.3381,  0.2955,  ..., -0.1912, -0.4304,  0.1493],\n",
       "            [-0.7475, -0.3381,  0.2955,  ..., -0.1912, -0.4304,  0.1493]],\n",
       "   \n",
       "           [[ 1.7036,  0.9651, -0.8212,  ...,  0.0418, -0.1407, -0.1113],\n",
       "            [ 0.8447,  1.5151, -0.7199,  ..., -0.4280,  0.8015, -1.1744],\n",
       "            [ 0.3596,  0.9644, -0.6227,  ...,  0.0976, -0.4012, -0.4120],\n",
       "            ...,\n",
       "            [-0.5955, -0.4017,  0.0932,  ...,  0.0412, -0.3341,  0.0545],\n",
       "            [-0.5955, -0.4017,  0.0932,  ...,  0.0412, -0.3341,  0.0545],\n",
       "            [-0.5955, -0.4017,  0.0932,  ...,  0.0412, -0.3341,  0.0545]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[ 1.2389, -0.0534, -1.0924,  ...,  1.1169, -0.2987,  0.3985],\n",
       "           [-0.0330,  0.2524, -1.0388,  ...,  1.3414,  1.0698, -0.6781],\n",
       "           [ 0.6857, -0.3200,  0.6702,  ...,  0.5406,  0.5204, -0.0269],\n",
       "           ...,\n",
       "           [-1.0201,  0.9774,  0.4298,  ..., -0.1764,  0.4541,  0.5112],\n",
       "           [-1.0201,  0.9774,  0.4298,  ..., -0.1764,  0.4541,  0.5112],\n",
       "           [-1.0201,  0.9774,  0.4298,  ..., -0.1764,  0.4541,  0.5112]],\n",
       "  \n",
       "          [[ 0.4785,  0.0482, -1.3038,  ...,  2.0462,  0.5374,  0.2873],\n",
       "           [-0.4587, -0.3578,  0.6451,  ...,  2.1637,  0.0097,  0.3191],\n",
       "           [-0.2746,  1.0419, -1.6974,  ...,  2.4362,  1.7889, -0.4125],\n",
       "           ...,\n",
       "           [-0.7574, -0.0194,  0.4509,  ...,  0.0348,  0.7278,  1.5480],\n",
       "           [-0.7574, -0.0194,  0.4509,  ...,  0.0348,  0.7278,  1.5480],\n",
       "           [-0.7574, -0.0194,  0.4509,  ...,  0.0348,  0.7278,  1.5480]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.25.mlp.fc1': {'input': (tensor([[[ -1.5694,  -2.8184,  -7.2477,  ...,  -1.8329,   9.5189,   3.6169],\n",
       "            [ -2.9465,   1.2553,  -2.8034,  ...,   5.0588,  -7.2540,  -4.2131],\n",
       "            [ 12.6061,  -1.1643,   0.7038,  ...,   0.7695,  16.0725,  -7.3449],\n",
       "            ...,\n",
       "            [ -5.3678,   4.0945,   3.5978,  ...,  -2.2511,  18.3694,  -0.3275],\n",
       "            [ -5.3678,   4.0945,   3.5978,  ...,  -2.2511,  18.3694,  -0.3275],\n",
       "            [ -5.3678,   4.0945,   3.5978,  ...,  -2.2511,  18.3694,  -0.3275]],\n",
       "   \n",
       "           [[  0.5759,  -2.9912, -10.3418,  ...,   1.4168,  -5.5942,   2.8022],\n",
       "            [ -2.1372,  -5.5758,   1.1881,  ...,  -2.6552,   6.6911,  -0.0576],\n",
       "            [ -0.5093,  -3.7353,   1.0754,  ...,  -1.2221,   1.2621,   0.3586],\n",
       "            ...,\n",
       "            [ -4.2949,  -1.7525,   2.7836,  ...,  -0.0698,  15.9927,   4.0136],\n",
       "            [ -4.2949,  -1.7525,   2.7836,  ...,  -0.0698,  15.9927,   4.0136],\n",
       "            [ -4.2949,  -1.7525,   2.7836,  ...,  -0.0698,  15.9927,   4.0136]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ -8.0710, -20.0199,  -4.4190,  ..., -24.4461,  -7.9040,  -9.0205],\n",
       "           [-13.9922, -14.4109, -24.4183,  ..., -29.2008,  -8.4298, -24.5068],\n",
       "           [ -9.7858, -32.7692, -23.0458,  ..., -33.2004, -16.2210, -24.5893],\n",
       "           ...,\n",
       "           [-22.3819, -26.4124, -31.0821,  ..., -27.0665, -10.1063, -12.3352],\n",
       "           [-22.3819, -26.4124, -31.0821,  ..., -27.0665, -10.1063, -12.3352],\n",
       "           [-22.3819, -26.4124, -31.0821,  ..., -27.0665, -10.1063, -12.3352]],\n",
       "  \n",
       "          [[-14.6172, -12.9233, -15.0761,  ..., -19.9028,  -8.4619,  -5.7409],\n",
       "           [  1.8138, -14.1043, -31.1548,  ..., -18.0244, -16.8299, -30.6471],\n",
       "           [ -9.3124, -11.7085, -24.5511,  ..., -39.7311,  -9.8286,   0.4558],\n",
       "           ...,\n",
       "           [-23.7961, -30.9352, -36.1991,  ..., -22.3200, -13.3067, -11.9273],\n",
       "           [-23.7961, -30.9352, -36.1991,  ..., -22.3200, -13.3067, -11.9273],\n",
       "           [-23.7961, -30.9352, -36.1991,  ..., -22.3200, -13.3067, -11.9273]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.25.mlp.fc2': {'input': (tensor([[[-0.0000e+00, -0.0000e+00, -8.1653e-06,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            ...,\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00]],\n",
       "   \n",
       "           [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [ 1.7505e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00,  3.0800e-01],\n",
       "            ...,\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -0.0000e+00, -0.0000e+00]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[-0.4515,  7.1397, -0.3377,  ..., -0.8507, -1.1259, -3.0240],\n",
       "           [ 1.5398, -0.5763, -1.6662,  ..., -1.6482, -1.2746,  0.8082],\n",
       "           [-1.4632, -0.4315, -0.2202,  ...,  0.5970, -1.6281,  0.3701],\n",
       "           ...,\n",
       "           [ 0.3405,  1.9835,  1.6827,  ..., -0.8111, -3.1389,  0.5905],\n",
       "           [ 0.3405,  1.9835,  1.6827,  ..., -0.8111, -3.1389,  0.5905],\n",
       "           [ 0.3405,  1.9835,  1.6827,  ..., -0.8111, -3.1389,  0.5905]],\n",
       "  \n",
       "          [[ 1.6036,  2.8050,  3.8384,  ..., -4.4766, -0.0667, -2.4420],\n",
       "           [-0.6863, -2.4560,  0.2030,  ..., -1.0643, -0.3741,  0.6632],\n",
       "           [ 0.8906,  0.2706,  3.3155,  ..., -0.8233, -0.8379,  1.9172],\n",
       "           ...,\n",
       "           [ 0.6976,  3.1114,  1.6075,  ..., -0.2174, -3.8698,  0.6409],\n",
       "           [ 0.6976,  3.1114,  1.6075,  ..., -0.2174, -3.8698,  0.6409],\n",
       "           [ 0.6976,  3.1114,  1.6075,  ..., -0.2174, -3.8698,  0.6409]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.26.self_attn.q_proj': {'input': (tensor([[[-0.5578,  1.0587, -0.8102,  ...,  0.0245,  0.0210, -0.2632],\n",
       "            [-0.3176,  0.0649, -0.8640,  ...,  0.4080, -1.1913, -0.2807],\n",
       "            [ 0.1876, -0.2098, -0.2318,  ...,  0.5748,  0.4436, -0.8008],\n",
       "            ...,\n",
       "            [-0.8751,  0.8178,  0.4201,  ...,  0.0068,  0.5487,  0.0146],\n",
       "            [-0.8751,  0.8178,  0.4201,  ...,  0.0068,  0.5487,  0.0146],\n",
       "            [-0.8751,  0.8178,  0.4201,  ...,  0.0068,  0.5487,  0.0146]],\n",
       "   \n",
       "           [[-0.0297,  0.3711, -0.3284,  ..., -0.4241, -0.7490, -0.2805],\n",
       "            [-0.7501, -0.9211, -0.0654,  ..., -0.0660,  0.0306,  0.0607],\n",
       "            [-0.2690, -0.2335,  0.5990,  ...,  0.0517, -0.4639,  0.3928],\n",
       "            ...,\n",
       "            [-0.7432,  0.3031,  0.2993,  ...,  0.2903,  0.3090,  0.4895],\n",
       "            [-0.7432,  0.3031,  0.2993,  ...,  0.2903,  0.3090,  0.4895],\n",
       "            [-0.7432,  0.3031,  0.2993,  ...,  0.2903,  0.3090,  0.4895]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 1.0858e+00, -1.2004e-01, -2.2532e-03,  ...,  1.4006e-01,\n",
       "            -7.1064e-01,  5.3423e+00],\n",
       "           [ 7.1767e-01, -5.0531e-01,  2.7054e-01,  ...,  1.4409e+00,\n",
       "             1.3071e+00,  4.6915e-01],\n",
       "           [ 1.0577e+00,  1.2917e+00, -1.1508e+00,  ..., -1.8874e+00,\n",
       "             2.2032e+00,  1.6169e+00],\n",
       "           ...,\n",
       "           [-1.4523e+00,  1.0808e+00,  1.2552e+00,  ...,  1.5375e+00,\n",
       "             1.4657e+00,  3.3212e+00],\n",
       "           [-1.4523e+00,  1.0808e+00,  1.2552e+00,  ...,  1.5375e+00,\n",
       "             1.4657e+00,  3.3212e+00],\n",
       "           [-1.4523e+00,  1.0808e+00,  1.2552e+00,  ...,  1.5375e+00,\n",
       "             1.4657e+00,  3.3212e+00]],\n",
       "  \n",
       "          [[ 1.2208e+00, -1.0294e+00,  9.1757e-01,  ...,  2.9627e-01,\n",
       "            -3.5104e-01,  3.8311e+00],\n",
       "           [-9.5606e-04,  3.1705e-01,  2.9933e-01,  ..., -3.9514e-01,\n",
       "            -6.8915e-01, -1.3862e+00],\n",
       "           [-6.5877e-02,  3.5668e-01,  1.1512e+00,  ...,  9.2670e-01,\n",
       "             5.4158e-01,  6.8982e-01],\n",
       "           ...,\n",
       "           [-4.0077e-01,  7.9221e-01,  1.7113e+00,  ...,  1.9733e+00,\n",
       "             1.6658e+00,  3.4125e+00],\n",
       "           [-4.0077e-01,  7.9221e-01,  1.7113e+00,  ...,  1.9733e+00,\n",
       "             1.6658e+00,  3.4125e+00],\n",
       "           [-4.0077e-01,  7.9221e-01,  1.7113e+00,  ...,  1.9733e+00,\n",
       "             1.6658e+00,  3.4125e+00]]], grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.26.self_attn.k_proj': {'input': (tensor([[[-0.5578,  1.0587, -0.8102,  ...,  0.0245,  0.0210, -0.2632],\n",
       "            [-0.3176,  0.0649, -0.8640,  ...,  0.4080, -1.1913, -0.2807],\n",
       "            [ 0.1876, -0.2098, -0.2318,  ...,  0.5748,  0.4436, -0.8008],\n",
       "            ...,\n",
       "            [-0.8751,  0.8178,  0.4201,  ...,  0.0068,  0.5487,  0.0146],\n",
       "            [-0.8751,  0.8178,  0.4201,  ...,  0.0068,  0.5487,  0.0146],\n",
       "            [-0.8751,  0.8178,  0.4201,  ...,  0.0068,  0.5487,  0.0146]],\n",
       "   \n",
       "           [[-0.0297,  0.3711, -0.3284,  ..., -0.4241, -0.7490, -0.2805],\n",
       "            [-0.7501, -0.9211, -0.0654,  ..., -0.0660,  0.0306,  0.0607],\n",
       "            [-0.2690, -0.2335,  0.5990,  ...,  0.0517, -0.4639,  0.3928],\n",
       "            ...,\n",
       "            [-0.7432,  0.3031,  0.2993,  ...,  0.2903,  0.3090,  0.4895],\n",
       "            [-0.7432,  0.3031,  0.2993,  ...,  0.2903,  0.3090,  0.4895],\n",
       "            [-0.7432,  0.3031,  0.2993,  ...,  0.2903,  0.3090,  0.4895]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 2.5819,  1.4796, -0.3583,  ..., -2.8028,  0.2693,  2.8965],\n",
       "           [ 1.0578,  0.3963, -0.6418,  ..., -0.9804, -0.6252,  0.4989],\n",
       "           [ 1.1107,  0.6292, -0.8883,  ..., -0.4440,  0.2352,  2.7206],\n",
       "           ...,\n",
       "           [-0.7088,  1.4021,  1.4777,  ...,  0.9587, -0.6606,  0.0813],\n",
       "           [-0.7088,  1.4021,  1.4777,  ...,  0.9587, -0.6606,  0.0813],\n",
       "           [-0.7088,  1.4021,  1.4777,  ...,  0.9587, -0.6606,  0.0813]],\n",
       "  \n",
       "          [[ 1.1808,  1.3136, -0.4120,  ..., -0.6303, -0.8372,  2.6101],\n",
       "           [-0.6860,  1.5575, -0.5122,  ..., -1.4868, -1.4782, -1.1950],\n",
       "           [ 0.5840,  2.4235, -0.1721,  ..., -0.3999, -1.8003,  1.1227],\n",
       "           ...,\n",
       "           [ 0.2377,  1.1306,  1.3147,  ...,  1.5099, -0.2489,  0.5236],\n",
       "           [ 0.2377,  1.1306,  1.3147,  ...,  1.5099, -0.2489,  0.5236],\n",
       "           [ 0.2377,  1.1306,  1.3147,  ...,  1.5099, -0.2489,  0.5236]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.26.self_attn.v_proj': {'input': (tensor([[[-0.5578,  1.0587, -0.8102,  ...,  0.0245,  0.0210, -0.2632],\n",
       "            [-0.3176,  0.0649, -0.8640,  ...,  0.4080, -1.1913, -0.2807],\n",
       "            [ 0.1876, -0.2098, -0.2318,  ...,  0.5748,  0.4436, -0.8008],\n",
       "            ...,\n",
       "            [-0.8751,  0.8178,  0.4201,  ...,  0.0068,  0.5487,  0.0146],\n",
       "            [-0.8751,  0.8178,  0.4201,  ...,  0.0068,  0.5487,  0.0146],\n",
       "            [-0.8751,  0.8178,  0.4201,  ...,  0.0068,  0.5487,  0.0146]],\n",
       "   \n",
       "           [[-0.0297,  0.3711, -0.3284,  ..., -0.4241, -0.7490, -0.2805],\n",
       "            [-0.7501, -0.9211, -0.0654,  ..., -0.0660,  0.0306,  0.0607],\n",
       "            [-0.2690, -0.2335,  0.5990,  ...,  0.0517, -0.4639,  0.3928],\n",
       "            ...,\n",
       "            [-0.7432,  0.3031,  0.2993,  ...,  0.2903,  0.3090,  0.4895],\n",
       "            [-0.7432,  0.3031,  0.2993,  ...,  0.2903,  0.3090,  0.4895],\n",
       "            [-0.7432,  0.3031,  0.2993,  ...,  0.2903,  0.3090,  0.4895]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[ 3.5034e-01, -8.9153e-01, -9.3606e-02,  ...,  1.4405e-01,\n",
       "             9.7562e-01, -9.6577e-01],\n",
       "           [ 1.1801e+00,  3.8225e-01,  2.2963e+00,  ...,  1.5892e+00,\n",
       "            -5.3086e-01,  1.3562e-01],\n",
       "           [ 1.5809e+00,  9.4351e-01,  1.5726e+00,  ..., -8.4711e-02,\n",
       "            -3.5858e-01, -9.2462e-01],\n",
       "           ...,\n",
       "           [-3.8014e+00,  7.2373e+00,  6.0171e+00,  ..., -1.8613e+00,\n",
       "             3.8392e-01,  4.0428e-01],\n",
       "           [-3.8014e+00,  7.2373e+00,  6.0171e+00,  ..., -1.8613e+00,\n",
       "             3.8392e-01,  4.0428e-01],\n",
       "           [-3.8014e+00,  7.2373e+00,  6.0171e+00,  ..., -1.8613e+00,\n",
       "             3.8392e-01,  4.0428e-01]],\n",
       "  \n",
       "          [[ 1.1249e+00,  3.0424e-03,  3.9805e-01,  ..., -1.4181e+00,\n",
       "             5.6649e-01, -1.4184e+00],\n",
       "           [-1.1063e+00,  7.1914e-01,  1.5093e-01,  ...,  2.1078e+00,\n",
       "             1.5802e+00, -5.5818e-01],\n",
       "           [-1.0888e+00, -1.8164e+00, -1.6294e+00,  ...,  1.0788e-01,\n",
       "            -7.7869e-01, -1.6048e+00],\n",
       "           ...,\n",
       "           [-4.1316e+00,  8.1976e+00,  6.2802e+00,  ..., -1.7586e+00,\n",
       "             7.7965e-01,  4.3884e-01],\n",
       "           [-4.1316e+00,  8.1976e+00,  6.2802e+00,  ..., -1.7586e+00,\n",
       "             7.7965e-01,  4.3884e-01],\n",
       "           [-4.1316e+00,  8.1976e+00,  6.2802e+00,  ..., -1.7586e+00,\n",
       "             7.7965e-01,  4.3884e-01]]], grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.26.self_attn.out_proj': {'input': (tensor([[[ 0.2325,  0.4401,  1.1660,  ...,  0.1528, -0.1480, -0.2351],\n",
       "            [ 0.2817, -0.3563,  0.8539,  ..., -0.0244, -0.2380, -0.3255],\n",
       "            [ 1.3064,  1.0838,  1.4286,  ...,  0.0147,  0.0247, -0.4703],\n",
       "            ...,\n",
       "            [-1.5072,  2.6987,  1.7988,  ...,  0.2132,  0.0158, -0.6904],\n",
       "            [-1.5072,  2.6987,  1.7988,  ...,  0.2132,  0.0158, -0.6904],\n",
       "            [-1.5072,  2.6987,  1.7988,  ...,  0.2132,  0.0158, -0.6904]],\n",
       "   \n",
       "           [[ 0.3014,  0.7846,  1.4614,  ..., -0.4410, -0.8121, -0.4189],\n",
       "            [-0.0986,  0.3958,  0.6188,  ...,  0.6723,  0.2619, -0.3608],\n",
       "            [-0.7131, -2.3612, -1.7242,  ..., -0.3073, -0.5017, -0.5926],\n",
       "            ...,\n",
       "            [-0.2355,  1.6466,  0.7998,  ..., -0.3561, -0.4701, -0.8913],\n",
       "            [-0.2355,  1.6466,  0.7998,  ..., -0.3561, -0.4701, -0.8913],\n",
       "            [-0.2355,  1.6466,  0.7998,  ..., -0.3561, -0.4701, -0.8913]]],\n",
       "          grad_fn=<ViewBackward0>),),\n",
       "  'output': tensor([[[-2.1585e-01,  6.1096e-01, -7.8206e-01,  ..., -7.9996e-01,\n",
       "             5.9985e-01,  9.7640e-02],\n",
       "           [-3.2030e+00, -5.1436e-01, -6.0195e-01,  ..., -9.1783e-01,\n",
       "             6.0411e-02, -6.2041e-01],\n",
       "           [-4.5887e-02, -1.2908e+00,  1.1489e+00,  ..., -8.1229e-01,\n",
       "            -1.4480e-01, -6.2743e-02],\n",
       "           ...,\n",
       "           [-1.9035e-01,  4.7059e-02, -1.1158e-01,  ...,  7.7019e-01,\n",
       "             1.7206e-01, -2.2490e+00],\n",
       "           [-1.9035e-01,  4.7059e-02, -1.1158e-01,  ...,  7.7019e-01,\n",
       "             1.7206e-01, -2.2490e+00],\n",
       "           [-1.9035e-01,  4.7058e-02, -1.1158e-01,  ...,  7.7019e-01,\n",
       "             1.7205e-01, -2.2490e+00]],\n",
       "  \n",
       "          [[-1.1333e+00,  1.2666e+00, -1.0234e+00,  ..., -4.4319e-01,\n",
       "            -1.0019e-01, -2.5159e-01],\n",
       "           [-5.0480e-01, -5.2138e-01, -7.6901e-01,  ...,  1.2194e+00,\n",
       "            -1.1164e+00, -1.4197e+00],\n",
       "           [-3.0178e+00,  3.4139e+00, -1.8879e+00,  ...,  7.2266e-01,\n",
       "             5.4926e-01, -3.9326e-01],\n",
       "           ...,\n",
       "           [-1.6860e+00,  7.2307e-01, -2.0525e-03,  ...,  3.4982e-01,\n",
       "            -2.8361e-03, -6.3968e-01],\n",
       "           [-1.6860e+00,  7.2307e-01, -2.0525e-03,  ...,  3.4982e-01,\n",
       "            -2.8361e-03, -6.3968e-01],\n",
       "           [-1.6860e+00,  7.2307e-01, -2.0519e-03,  ...,  3.4982e-01,\n",
       "            -2.8356e-03, -6.3968e-01]]], grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.26.mlp.fc1': {'input': (tensor([[[-0.6714,  6.4618, -3.2705,  ..., -3.1252,  2.5484, -2.0270],\n",
       "            [-3.4369, -1.0181, -3.5028,  ..., -0.8035, -4.5344, -2.9641],\n",
       "            [ 4.2577, -4.6568,  3.9102,  ...,  0.2363,  4.2880, -5.5986],\n",
       "            ...,\n",
       "            [-2.6189,  4.3700,  5.7668,  ..., -1.6164,  5.0756, -2.4944],\n",
       "            [-2.6189,  4.3700,  5.7668,  ..., -1.6164,  5.0756, -2.4944],\n",
       "            [-2.6189,  4.3700,  5.7668,  ..., -1.6164,  5.0756, -2.4944]],\n",
       "   \n",
       "           [[ 1.3580,  3.1310, -0.5232,  ..., -5.9986, -2.3286, -2.4996],\n",
       "            [-1.9831, -6.6932,  1.8575,  ..., -1.6133,  1.1491, -1.4951],\n",
       "            [-2.7161,  2.2304,  4.4016,  ..., -1.0766, -0.0723,  1.1955],\n",
       "            ...,\n",
       "            [-3.0704,  1.7462,  5.0457,  ..., -0.0333,  3.5771,  1.6908],\n",
       "            [-3.0704,  1.7462,  5.0457,  ..., -0.0333,  3.5771,  1.6908],\n",
       "            [-3.0704,  1.7462,  5.0457,  ..., -0.0333,  3.5771,  1.6908]]],\n",
       "          grad_fn=<NativeLayerNormBackward0>),),\n",
       "  'output': tensor([[[-11.8682,   0.0304,  -3.8174,  ...,  -1.7952,  -4.6899, -10.1004],\n",
       "           [-10.6816, -15.8358, -18.0509,  ...,   2.0800,  -5.9950, -17.3151],\n",
       "           [  4.4779,  -5.3672, -15.2680,  ...,   4.0223,  12.6982, -15.3643],\n",
       "           ...,\n",
       "           [-23.8290, -10.6085,  -8.8977,  ..., -10.5095,   1.0851, -17.8699],\n",
       "           [-23.8290, -10.6085,  -8.8977,  ..., -10.5095,   1.0851, -17.8699],\n",
       "           [-23.8290, -10.6085,  -8.8977,  ..., -10.5095,   1.0851, -17.8699]],\n",
       "  \n",
       "          [[-11.6818,  -6.0206, -17.7421,  ..., -14.4538,  -4.3300,  -9.8386],\n",
       "           [ -9.9427, -17.2294, -11.6727,  ...,  -9.8351,  -1.6088, -14.1650],\n",
       "           [-17.6335,  -4.6091, -16.4925,  ...,  -5.4579,   1.5419, -17.6931],\n",
       "           ...,\n",
       "           [-24.1579, -12.0136,  -9.7518,  ...,  -6.6828,   2.0933, -16.8199],\n",
       "           [-24.1579, -12.0136,  -9.7518,  ...,  -6.6828,   2.0933, -16.8199],\n",
       "           [-24.1579, -12.0136,  -9.7518,  ...,  -6.6828,   2.0933, -16.8199]]],\n",
       "         grad_fn=<ViewBackward0>)},\n",
       " 'vpm.encoder.layers.26.mlp.fc2': {'input': (tensor([[[-0.0000e+00,  1.5555e-02, -1.6303e-04,  ..., -6.5260e-02,\n",
       "             -1.6772e-06, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  2.0412e+00,\n",
       "             -0.0000e+00, -0.0000e+00],\n",
       "            [ 4.4779e+00, -0.0000e+00, -0.0000e+00,  ...,  4.0223e+00,\n",
       "              1.2698e+01, -0.0000e+00],\n",
       "            ...,\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "              9.3419e-01, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "              9.3419e-01, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "              9.3420e-01, -0.0000e+00]],\n",
       "   \n",
       "           [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -1.3163e-05, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "             -8.6779e-02, -0.0000e+00],\n",
       "            [-0.0000e+00, -2.7472e-06, -0.0000e+00,  ..., -0.0000e+00,\n",
       "              1.4468e+00, -0.0000e+00],\n",
       "            ...,\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "              2.0555e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "              2.0555e+00, -0.0000e+00],\n",
       "            [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "              2.0555e+00, -0.0000e+00]]], grad_fn=<GeluBackward0>),),\n",
       "  'output': tensor([[[ -5.3913,   0.6256,  -2.0927,  ...,  -1.9053,  -1.2957,  -4.8068],\n",
       "           [ -1.0700,  -3.7706,  -1.1812,  ...,  -2.0173,  -2.8801,  -0.0583],\n",
       "           [-10.7068,  -4.3393,  -2.2511,  ...,   0.5814,   1.6678,  -4.3061],\n",
       "           ...,\n",
       "           [ -2.3112,  -3.2177,  -2.7403,  ...,  -0.5292,  -2.2634,  -0.9870],\n",
       "           [ -2.3112,  -3.2177,  -2.7403,  ...,  -0.5292,  -2.2634,  -0.9870],\n",
       "           [ -2.3112,  -3.2177,  -2.7403,  ...,  -0.5292,  -2.2634,  -0.9870]],\n",
       "  \n",
       "          [[ -2.3198,   5.2981,  -0.2800,  ...,  -5.3562,  -0.7046,  -2.6289],\n",
       "           [ -4.2932,  -0.9067,   1.3951,  ...,  -5.3564,  -3.3278,   0.8724],\n",
       "           [  2.1837,  -1.0593,  -2.8652,  ...,   0.8120,   6.3111,   4.5543],\n",
       "           ...,\n",
       "           [ -2.6133,  -4.0927,  -3.4306,  ...,  -1.4801,  -3.1337,  -1.0471],\n",
       "           [ -2.6133,  -4.0927,  -3.4306,  ...,  -1.4801,  -3.1337,  -1.0471],\n",
       "           [ -2.6133,  -4.0927,  -3.4306,  ...,  -1.4801,  -3.1337,  -1.0471]]],\n",
       "         grad_fn=<ViewBackward0>)}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vpm.encoder.layers.0.self_attn.q_proj.input  scale:  10.097468376159668\n",
      "vpm.encoder.layers.0.self_attn.q_proj.output  scale:  7.09360408782959\n",
      "vpm.encoder.layers.0.self_attn.k_proj.input  scale:  10.097468376159668\n",
      "vpm.encoder.layers.0.self_attn.k_proj.output  scale:  7.006409168243408\n",
      "vpm.encoder.layers.0.self_attn.v_proj.input  scale:  10.097468376159668\n",
      "vpm.encoder.layers.0.self_attn.v_proj.output  scale:  7.136604309082031\n",
      "vpm.encoder.layers.0.self_attn.out_proj.input  scale:  7.132835865020752\n",
      "vpm.encoder.layers.0.self_attn.out_proj.output  scale:  9.77147388458252\n",
      "vpm.encoder.layers.0.mlp.fc1.input  scale:  22.65842056274414\n",
      "vpm.encoder.layers.0.mlp.fc1.output  scale:  12.893155097961426\n",
      "vpm.encoder.layers.0.mlp.fc2.input  scale:  12.079593658447266\n",
      "vpm.encoder.layers.0.mlp.fc2.output  scale:  12.842710494995117\n",
      "vpm.encoder.layers.1.self_attn.q_proj.input  scale:  9.794299125671387\n",
      "vpm.encoder.layers.1.self_attn.q_proj.output  scale:  6.926030158996582\n",
      "vpm.encoder.layers.1.self_attn.k_proj.input  scale:  9.794299125671387\n",
      "vpm.encoder.layers.1.self_attn.k_proj.output  scale:  5.062451362609863\n",
      "vpm.encoder.layers.1.self_attn.v_proj.input  scale:  9.794299125671387\n",
      "vpm.encoder.layers.1.self_attn.v_proj.output  scale:  4.046346664428711\n",
      "vpm.encoder.layers.1.self_attn.out_proj.input  scale:  1.9816601276397705\n",
      "vpm.encoder.layers.1.self_attn.out_proj.output  scale:  11.243560791015625\n",
      "vpm.encoder.layers.1.mlp.fc1.input  scale:  14.040548324584961\n",
      "vpm.encoder.layers.1.mlp.fc1.output  scale:  9.451053619384766\n",
      "vpm.encoder.layers.1.mlp.fc2.input  scale:  6.249223709106445\n",
      "vpm.encoder.layers.1.mlp.fc2.output  scale:  18.22589111328125\n",
      "vpm.encoder.layers.2.self_attn.q_proj.input  scale:  12.713516235351562\n",
      "vpm.encoder.layers.2.self_attn.q_proj.output  scale:  8.051827430725098\n",
      "vpm.encoder.layers.2.self_attn.k_proj.input  scale:  12.713516235351562\n",
      "vpm.encoder.layers.2.self_attn.k_proj.output  scale:  8.681768417358398\n",
      "vpm.encoder.layers.2.self_attn.v_proj.input  scale:  12.713516235351562\n",
      "vpm.encoder.layers.2.self_attn.v_proj.output  scale:  3.7417068481445312\n",
      "vpm.encoder.layers.2.self_attn.out_proj.input  scale:  3.298668622970581\n",
      "vpm.encoder.layers.2.self_attn.out_proj.output  scale:  9.388376235961914\n",
      "vpm.encoder.layers.2.mlp.fc1.input  scale:  13.694029808044434\n",
      "vpm.encoder.layers.2.mlp.fc1.output  scale:  9.991759300231934\n",
      "vpm.encoder.layers.2.mlp.fc2.input  scale:  6.486385822296143\n",
      "vpm.encoder.layers.2.mlp.fc2.output  scale:  10.139246940612793\n",
      "vpm.encoder.layers.3.self_attn.q_proj.input  scale:  16.866121292114258\n",
      "vpm.encoder.layers.3.self_attn.q_proj.output  scale:  10.41799259185791\n",
      "vpm.encoder.layers.3.self_attn.k_proj.input  scale:  16.866121292114258\n",
      "vpm.encoder.layers.3.self_attn.k_proj.output  scale:  8.52966022491455\n",
      "vpm.encoder.layers.3.self_attn.v_proj.input  scale:  16.866121292114258\n",
      "vpm.encoder.layers.3.self_attn.v_proj.output  scale:  9.713088989257812\n",
      "vpm.encoder.layers.3.self_attn.out_proj.input  scale:  7.5250325202941895\n",
      "vpm.encoder.layers.3.self_attn.out_proj.output  scale:  77.11209106445312\n",
      "vpm.encoder.layers.3.mlp.fc1.input  scale:  13.80133056640625\n",
      "vpm.encoder.layers.3.mlp.fc1.output  scale:  12.261015892028809\n",
      "vpm.encoder.layers.3.mlp.fc2.input  scale:  5.946786880493164\n",
      "vpm.encoder.layers.3.mlp.fc2.output  scale:  7.20151424407959\n",
      "vpm.encoder.layers.4.self_attn.q_proj.input  scale:  21.365928649902344\n",
      "vpm.encoder.layers.4.self_attn.q_proj.output  scale:  11.067889213562012\n",
      "vpm.encoder.layers.4.self_attn.k_proj.input  scale:  21.365928649902344\n",
      "vpm.encoder.layers.4.self_attn.k_proj.output  scale:  6.664567470550537\n",
      "vpm.encoder.layers.4.self_attn.v_proj.input  scale:  21.365928649902344\n",
      "vpm.encoder.layers.4.self_attn.v_proj.output  scale:  4.9474005699157715\n",
      "vpm.encoder.layers.4.self_attn.out_proj.input  scale:  3.9875173568725586\n",
      "vpm.encoder.layers.4.self_attn.out_proj.output  scale:  5.744137287139893\n",
      "vpm.encoder.layers.4.mlp.fc1.input  scale:  13.73019027709961\n",
      "vpm.encoder.layers.4.mlp.fc1.output  scale:  7.699523448944092\n",
      "vpm.encoder.layers.4.mlp.fc2.input  scale:  5.022085189819336\n",
      "vpm.encoder.layers.4.mlp.fc2.output  scale:  8.5152006149292\n",
      "vpm.encoder.layers.5.self_attn.q_proj.input  scale:  20.457550048828125\n",
      "vpm.encoder.layers.5.self_attn.q_proj.output  scale:  9.60555362701416\n",
      "vpm.encoder.layers.5.self_attn.k_proj.input  scale:  20.457550048828125\n",
      "vpm.encoder.layers.5.self_attn.k_proj.output  scale:  9.98354721069336\n",
      "vpm.encoder.layers.5.self_attn.v_proj.input  scale:  20.457550048828125\n",
      "vpm.encoder.layers.5.self_attn.v_proj.output  scale:  4.661391258239746\n",
      "vpm.encoder.layers.5.self_attn.out_proj.input  scale:  4.060399532318115\n",
      "vpm.encoder.layers.5.self_attn.out_proj.output  scale:  4.152318000793457\n",
      "vpm.encoder.layers.5.mlp.fc1.input  scale:  12.765827178955078\n",
      "vpm.encoder.layers.5.mlp.fc1.output  scale:  7.846220970153809\n",
      "vpm.encoder.layers.5.mlp.fc2.input  scale:  6.075899600982666\n",
      "vpm.encoder.layers.5.mlp.fc2.output  scale:  4.445439338684082\n",
      "vpm.encoder.layers.6.self_attn.q_proj.input  scale:  22.354211807250977\n",
      "vpm.encoder.layers.6.self_attn.q_proj.output  scale:  10.204248428344727\n",
      "vpm.encoder.layers.6.self_attn.k_proj.input  scale:  22.354211807250977\n",
      "vpm.encoder.layers.6.self_attn.k_proj.output  scale:  8.75912094116211\n",
      "vpm.encoder.layers.6.self_attn.v_proj.input  scale:  22.354211807250977\n",
      "vpm.encoder.layers.6.self_attn.v_proj.output  scale:  4.974418640136719\n",
      "vpm.encoder.layers.6.self_attn.out_proj.input  scale:  4.2906670570373535\n",
      "vpm.encoder.layers.6.self_attn.out_proj.output  scale:  5.961665630340576\n",
      "vpm.encoder.layers.6.mlp.fc1.input  scale:  15.250578880310059\n",
      "vpm.encoder.layers.6.mlp.fc1.output  scale:  7.766264915466309\n",
      "vpm.encoder.layers.6.mlp.fc2.input  scale:  5.611362457275391\n",
      "vpm.encoder.layers.6.mlp.fc2.output  scale:  4.972400665283203\n",
      "vpm.encoder.layers.7.self_attn.q_proj.input  scale:  28.664335250854492\n",
      "vpm.encoder.layers.7.self_attn.q_proj.output  scale:  10.501871109008789\n",
      "vpm.encoder.layers.7.self_attn.k_proj.input  scale:  28.664335250854492\n",
      "vpm.encoder.layers.7.self_attn.k_proj.output  scale:  8.359672546386719\n",
      "vpm.encoder.layers.7.self_attn.v_proj.input  scale:  28.664335250854492\n",
      "vpm.encoder.layers.7.self_attn.v_proj.output  scale:  4.5869293212890625\n",
      "vpm.encoder.layers.7.self_attn.out_proj.input  scale:  3.544221878051758\n",
      "vpm.encoder.layers.7.self_attn.out_proj.output  scale:  3.6758627891540527\n",
      "vpm.encoder.layers.7.mlp.fc1.input  scale:  11.399003028869629\n",
      "vpm.encoder.layers.7.mlp.fc1.output  scale:  7.40150260925293\n",
      "vpm.encoder.layers.7.mlp.fc2.input  scale:  5.666521072387695\n",
      "vpm.encoder.layers.7.mlp.fc2.output  scale:  5.253983497619629\n",
      "vpm.encoder.layers.8.self_attn.q_proj.input  scale:  21.997997283935547\n",
      "vpm.encoder.layers.8.self_attn.q_proj.output  scale:  11.024928092956543\n",
      "vpm.encoder.layers.8.self_attn.k_proj.input  scale:  21.997997283935547\n",
      "vpm.encoder.layers.8.self_attn.k_proj.output  scale:  8.944297790527344\n",
      "vpm.encoder.layers.8.self_attn.v_proj.input  scale:  21.997997283935547\n",
      "vpm.encoder.layers.8.self_attn.v_proj.output  scale:  5.698811054229736\n",
      "vpm.encoder.layers.8.self_attn.out_proj.input  scale:  3.6269218921661377\n",
      "vpm.encoder.layers.8.self_attn.out_proj.output  scale:  8.265068054199219\n",
      "vpm.encoder.layers.8.mlp.fc1.input  scale:  20.99497413635254\n",
      "vpm.encoder.layers.8.mlp.fc1.output  scale:  8.001457214355469\n",
      "vpm.encoder.layers.8.mlp.fc2.input  scale:  5.762652397155762\n",
      "vpm.encoder.layers.8.mlp.fc2.output  scale:  5.199799537658691\n",
      "vpm.encoder.layers.9.self_attn.q_proj.input  scale:  22.479778289794922\n",
      "vpm.encoder.layers.9.self_attn.q_proj.output  scale:  10.63400936126709\n",
      "vpm.encoder.layers.9.self_attn.k_proj.input  scale:  22.479778289794922\n",
      "vpm.encoder.layers.9.self_attn.k_proj.output  scale:  7.641982078552246\n",
      "vpm.encoder.layers.9.self_attn.v_proj.input  scale:  22.479778289794922\n",
      "vpm.encoder.layers.9.self_attn.v_proj.output  scale:  5.59074592590332\n",
      "vpm.encoder.layers.9.self_attn.out_proj.input  scale:  5.300651550292969\n",
      "vpm.encoder.layers.9.self_attn.out_proj.output  scale:  5.863957405090332\n",
      "vpm.encoder.layers.9.mlp.fc1.input  scale:  23.450841903686523\n",
      "vpm.encoder.layers.9.mlp.fc1.output  scale:  8.674960136413574\n",
      "vpm.encoder.layers.9.mlp.fc2.input  scale:  6.964848041534424\n",
      "vpm.encoder.layers.9.mlp.fc2.output  scale:  9.810750961303711\n",
      "vpm.encoder.layers.10.self_attn.q_proj.input  scale:  21.279529571533203\n",
      "vpm.encoder.layers.10.self_attn.q_proj.output  scale:  10.016227722167969\n",
      "vpm.encoder.layers.10.self_attn.k_proj.input  scale:  21.279529571533203\n",
      "vpm.encoder.layers.10.self_attn.k_proj.output  scale:  7.1827898025512695\n",
      "vpm.encoder.layers.10.self_attn.v_proj.input  scale:  21.279529571533203\n",
      "vpm.encoder.layers.10.self_attn.v_proj.output  scale:  6.613838195800781\n",
      "vpm.encoder.layers.10.self_attn.out_proj.input  scale:  5.742992401123047\n",
      "vpm.encoder.layers.10.self_attn.out_proj.output  scale:  4.094354629516602\n",
      "vpm.encoder.layers.10.mlp.fc1.input  scale:  33.891807556152344\n",
      "vpm.encoder.layers.10.mlp.fc1.output  scale:  7.883004188537598\n",
      "vpm.encoder.layers.10.mlp.fc2.input  scale:  7.707489013671875\n",
      "vpm.encoder.layers.10.mlp.fc2.output  scale:  7.592639923095703\n",
      "vpm.encoder.layers.11.self_attn.q_proj.input  scale:  20.379962921142578\n",
      "vpm.encoder.layers.11.self_attn.q_proj.output  scale:  11.208513259887695\n",
      "vpm.encoder.layers.11.self_attn.k_proj.input  scale:  20.379962921142578\n",
      "vpm.encoder.layers.11.self_attn.k_proj.output  scale:  7.108658790588379\n",
      "vpm.encoder.layers.11.self_attn.v_proj.input  scale:  20.379962921142578\n",
      "vpm.encoder.layers.11.self_attn.v_proj.output  scale:  7.140175819396973\n",
      "vpm.encoder.layers.11.self_attn.out_proj.input  scale:  5.312326431274414\n",
      "vpm.encoder.layers.11.self_attn.out_proj.output  scale:  4.638376235961914\n",
      "vpm.encoder.layers.11.mlp.fc1.input  scale:  43.70060729980469\n",
      "vpm.encoder.layers.11.mlp.fc1.output  scale:  9.504144668579102\n",
      "vpm.encoder.layers.11.mlp.fc2.input  scale:  7.491249084472656\n",
      "vpm.encoder.layers.11.mlp.fc2.output  scale:  5.976984977722168\n",
      "vpm.encoder.layers.12.self_attn.q_proj.input  scale:  23.78822898864746\n",
      "vpm.encoder.layers.12.self_attn.q_proj.output  scale:  11.87839126586914\n",
      "vpm.encoder.layers.12.self_attn.k_proj.input  scale:  23.78822898864746\n",
      "vpm.encoder.layers.12.self_attn.k_proj.output  scale:  9.44593620300293\n",
      "vpm.encoder.layers.12.self_attn.v_proj.input  scale:  23.78822898864746\n",
      "vpm.encoder.layers.12.self_attn.v_proj.output  scale:  6.660285472869873\n",
      "vpm.encoder.layers.12.self_attn.out_proj.input  scale:  5.014573097229004\n",
      "vpm.encoder.layers.12.self_attn.out_proj.output  scale:  4.314451694488525\n",
      "vpm.encoder.layers.12.mlp.fc1.input  scale:  24.963457107543945\n",
      "vpm.encoder.layers.12.mlp.fc1.output  scale:  8.834508895874023\n",
      "vpm.encoder.layers.12.mlp.fc2.input  scale:  8.347285270690918\n",
      "vpm.encoder.layers.12.mlp.fc2.output  scale:  8.051888465881348\n",
      "vpm.encoder.layers.13.self_attn.q_proj.input  scale:  24.08555793762207\n",
      "vpm.encoder.layers.13.self_attn.q_proj.output  scale:  11.832084655761719\n",
      "vpm.encoder.layers.13.self_attn.k_proj.input  scale:  24.08555793762207\n",
      "vpm.encoder.layers.13.self_attn.k_proj.output  scale:  8.779234886169434\n",
      "vpm.encoder.layers.13.self_attn.v_proj.input  scale:  24.08555793762207\n",
      "vpm.encoder.layers.13.self_attn.v_proj.output  scale:  11.66130256652832\n",
      "vpm.encoder.layers.13.self_attn.out_proj.input  scale:  9.878283500671387\n",
      "vpm.encoder.layers.13.self_attn.out_proj.output  scale:  3.194638967514038\n",
      "vpm.encoder.layers.13.mlp.fc1.input  scale:  68.14678955078125\n",
      "vpm.encoder.layers.13.mlp.fc1.output  scale:  8.762812614440918\n",
      "vpm.encoder.layers.13.mlp.fc2.input  scale:  8.762812614440918\n",
      "vpm.encoder.layers.13.mlp.fc2.output  scale:  7.157195091247559\n",
      "vpm.encoder.layers.14.self_attn.q_proj.input  scale:  22.824161529541016\n",
      "vpm.encoder.layers.14.self_attn.q_proj.output  scale:  13.063549995422363\n",
      "vpm.encoder.layers.14.self_attn.k_proj.input  scale:  22.824161529541016\n",
      "vpm.encoder.layers.14.self_attn.k_proj.output  scale:  8.439029693603516\n",
      "vpm.encoder.layers.14.self_attn.v_proj.input  scale:  22.824161529541016\n",
      "vpm.encoder.layers.14.self_attn.v_proj.output  scale:  8.536093711853027\n",
      "vpm.encoder.layers.14.self_attn.out_proj.input  scale:  8.53134822845459\n",
      "vpm.encoder.layers.14.self_attn.out_proj.output  scale:  4.319625377655029\n",
      "vpm.encoder.layers.14.mlp.fc1.input  scale:  65.69261932373047\n",
      "vpm.encoder.layers.14.mlp.fc1.output  scale:  8.931795120239258\n",
      "vpm.encoder.layers.14.mlp.fc2.input  scale:  8.85251235961914\n",
      "vpm.encoder.layers.14.mlp.fc2.output  scale:  11.250736236572266\n",
      "vpm.encoder.layers.15.self_attn.q_proj.input  scale:  23.1822566986084\n",
      "vpm.encoder.layers.15.self_attn.q_proj.output  scale:  12.544840812683105\n",
      "vpm.encoder.layers.15.self_attn.k_proj.input  scale:  23.1822566986084\n",
      "vpm.encoder.layers.15.self_attn.k_proj.output  scale:  7.64193058013916\n",
      "vpm.encoder.layers.15.self_attn.v_proj.input  scale:  23.1822566986084\n",
      "vpm.encoder.layers.15.self_attn.v_proj.output  scale:  6.376547336578369\n",
      "vpm.encoder.layers.15.self_attn.out_proj.input  scale:  3.7161006927490234\n",
      "vpm.encoder.layers.15.self_attn.out_proj.output  scale:  2.581176280975342\n",
      "vpm.encoder.layers.15.mlp.fc1.input  scale:  27.348480224609375\n",
      "vpm.encoder.layers.15.mlp.fc1.output  scale:  19.539430618286133\n",
      "vpm.encoder.layers.15.mlp.fc2.input  scale:  11.888547897338867\n",
      "vpm.encoder.layers.15.mlp.fc2.output  scale:  29.272945404052734\n",
      "vpm.encoder.layers.16.self_attn.q_proj.input  scale:  23.120676040649414\n",
      "vpm.encoder.layers.16.self_attn.q_proj.output  scale:  12.205976486206055\n",
      "vpm.encoder.layers.16.self_attn.k_proj.input  scale:  23.120676040649414\n",
      "vpm.encoder.layers.16.self_attn.k_proj.output  scale:  7.2290472984313965\n",
      "vpm.encoder.layers.16.self_attn.v_proj.input  scale:  23.120676040649414\n",
      "vpm.encoder.layers.16.self_attn.v_proj.output  scale:  5.250640869140625\n",
      "vpm.encoder.layers.16.self_attn.out_proj.input  scale:  3.7428066730499268\n",
      "vpm.encoder.layers.16.self_attn.out_proj.output  scale:  3.003042697906494\n",
      "vpm.encoder.layers.16.mlp.fc1.input  scale:  43.19096755981445\n",
      "vpm.encoder.layers.16.mlp.fc1.output  scale:  26.466289520263672\n",
      "vpm.encoder.layers.16.mlp.fc2.input  scale:  26.466289520263672\n",
      "vpm.encoder.layers.16.mlp.fc2.output  scale:  64.9435043334961\n",
      "vpm.encoder.layers.17.self_attn.q_proj.input  scale:  28.374406814575195\n",
      "vpm.encoder.layers.17.self_attn.q_proj.output  scale:  12.033463478088379\n",
      "vpm.encoder.layers.17.self_attn.k_proj.input  scale:  28.374406814575195\n",
      "vpm.encoder.layers.17.self_attn.k_proj.output  scale:  8.343409538269043\n",
      "vpm.encoder.layers.17.self_attn.v_proj.input  scale:  28.374406814575195\n",
      "vpm.encoder.layers.17.self_attn.v_proj.output  scale:  6.469033241271973\n",
      "vpm.encoder.layers.17.self_attn.out_proj.input  scale:  5.339149475097656\n",
      "vpm.encoder.layers.17.self_attn.out_proj.output  scale:  3.5320098400115967\n",
      "vpm.encoder.layers.17.mlp.fc1.input  scale:  55.655189514160156\n",
      "vpm.encoder.layers.17.mlp.fc1.output  scale:  80.8602294921875\n",
      "vpm.encoder.layers.17.mlp.fc2.input  scale:  80.8602294921875\n",
      "vpm.encoder.layers.17.mlp.fc2.output  scale:  79.24030303955078\n",
      "vpm.encoder.layers.18.self_attn.q_proj.input  scale:  31.277362823486328\n",
      "vpm.encoder.layers.18.self_attn.q_proj.output  scale:  12.268763542175293\n",
      "vpm.encoder.layers.18.self_attn.k_proj.input  scale:  31.277362823486328\n",
      "vpm.encoder.layers.18.self_attn.k_proj.output  scale:  8.729034423828125\n",
      "vpm.encoder.layers.18.self_attn.v_proj.input  scale:  31.277362823486328\n",
      "vpm.encoder.layers.18.self_attn.v_proj.output  scale:  9.527307510375977\n",
      "vpm.encoder.layers.18.self_attn.out_proj.input  scale:  4.898108005523682\n",
      "vpm.encoder.layers.18.self_attn.out_proj.output  scale:  12.117831230163574\n",
      "vpm.encoder.layers.18.mlp.fc1.input  scale:  50.074119567871094\n",
      "vpm.encoder.layers.18.mlp.fc1.output  scale:  26.233173370361328\n",
      "vpm.encoder.layers.18.mlp.fc2.input  scale:  20.703285217285156\n",
      "vpm.encoder.layers.18.mlp.fc2.output  scale:  11.641801834106445\n",
      "vpm.encoder.layers.19.self_attn.q_proj.input  scale:  29.066043853759766\n",
      "vpm.encoder.layers.19.self_attn.q_proj.output  scale:  12.9135103225708\n",
      "vpm.encoder.layers.19.self_attn.k_proj.input  scale:  29.066043853759766\n",
      "vpm.encoder.layers.19.self_attn.k_proj.output  scale:  9.319238662719727\n",
      "vpm.encoder.layers.19.self_attn.v_proj.input  scale:  29.066043853759766\n",
      "vpm.encoder.layers.19.self_attn.v_proj.output  scale:  6.442079544067383\n",
      "vpm.encoder.layers.19.self_attn.out_proj.input  scale:  4.611564636230469\n",
      "vpm.encoder.layers.19.self_attn.out_proj.output  scale:  4.678833961486816\n",
      "vpm.encoder.layers.19.mlp.fc1.input  scale:  60.522220611572266\n",
      "vpm.encoder.layers.19.mlp.fc1.output  scale:  37.57615661621094\n",
      "vpm.encoder.layers.19.mlp.fc2.input  scale:  28.70844841003418\n",
      "vpm.encoder.layers.19.mlp.fc2.output  scale:  16.018016815185547\n",
      "vpm.encoder.layers.20.self_attn.q_proj.input  scale:  26.394100189208984\n",
      "vpm.encoder.layers.20.self_attn.q_proj.output  scale:  12.920717239379883\n",
      "vpm.encoder.layers.20.self_attn.k_proj.input  scale:  26.394100189208984\n",
      "vpm.encoder.layers.20.self_attn.k_proj.output  scale:  9.16006088256836\n",
      "vpm.encoder.layers.20.self_attn.v_proj.input  scale:  26.394100189208984\n",
      "vpm.encoder.layers.20.self_attn.v_proj.output  scale:  6.499055862426758\n",
      "vpm.encoder.layers.20.self_attn.out_proj.input  scale:  4.596158027648926\n",
      "vpm.encoder.layers.20.self_attn.out_proj.output  scale:  3.7170631885528564\n",
      "vpm.encoder.layers.20.mlp.fc1.input  scale:  73.04068756103516\n",
      "vpm.encoder.layers.20.mlp.fc1.output  scale:  44.976226806640625\n",
      "vpm.encoder.layers.20.mlp.fc2.input  scale:  34.15727615356445\n",
      "vpm.encoder.layers.20.mlp.fc2.output  scale:  20.082887649536133\n",
      "vpm.encoder.layers.21.self_attn.q_proj.input  scale:  28.1030216217041\n",
      "vpm.encoder.layers.21.self_attn.q_proj.output  scale:  12.759543418884277\n",
      "vpm.encoder.layers.21.self_attn.k_proj.input  scale:  28.1030216217041\n",
      "vpm.encoder.layers.21.self_attn.k_proj.output  scale:  8.931530952453613\n",
      "vpm.encoder.layers.21.self_attn.v_proj.input  scale:  28.1030216217041\n",
      "vpm.encoder.layers.21.self_attn.v_proj.output  scale:  8.354801177978516\n",
      "vpm.encoder.layers.21.self_attn.out_proj.input  scale:  5.620591640472412\n",
      "vpm.encoder.layers.21.self_attn.out_proj.output  scale:  4.6508283615112305\n",
      "vpm.encoder.layers.21.mlp.fc1.input  scale:  179.18594360351562\n",
      "vpm.encoder.layers.21.mlp.fc1.output  scale:  67.08499145507812\n",
      "vpm.encoder.layers.21.mlp.fc2.input  scale:  40.455596923828125\n",
      "vpm.encoder.layers.21.mlp.fc2.output  scale:  9.924783706665039\n",
      "vpm.encoder.layers.22.self_attn.q_proj.input  scale:  28.6701717376709\n",
      "vpm.encoder.layers.22.self_attn.q_proj.output  scale:  13.097484588623047\n",
      "vpm.encoder.layers.22.self_attn.k_proj.input  scale:  28.6701717376709\n",
      "vpm.encoder.layers.22.self_attn.k_proj.output  scale:  8.522778511047363\n",
      "vpm.encoder.layers.22.self_attn.v_proj.input  scale:  28.6701717376709\n",
      "vpm.encoder.layers.22.self_attn.v_proj.output  scale:  7.980649948120117\n",
      "vpm.encoder.layers.22.self_attn.out_proj.input  scale:  5.35523796081543\n",
      "vpm.encoder.layers.22.self_attn.out_proj.output  scale:  4.720991134643555\n",
      "vpm.encoder.layers.22.mlp.fc1.input  scale:  153.22476196289062\n",
      "vpm.encoder.layers.22.mlp.fc1.output  scale:  66.18560028076172\n",
      "vpm.encoder.layers.22.mlp.fc2.input  scale:  47.49610900878906\n",
      "vpm.encoder.layers.22.mlp.fc2.output  scale:  11.122030258178711\n",
      "vpm.encoder.layers.23.self_attn.q_proj.input  scale:  31.3848819732666\n",
      "vpm.encoder.layers.23.self_attn.q_proj.output  scale:  13.39101791381836\n",
      "vpm.encoder.layers.23.self_attn.k_proj.input  scale:  31.3848819732666\n",
      "vpm.encoder.layers.23.self_attn.k_proj.output  scale:  10.495219230651855\n",
      "vpm.encoder.layers.23.self_attn.v_proj.input  scale:  31.3848819732666\n",
      "vpm.encoder.layers.23.self_attn.v_proj.output  scale:  8.600749969482422\n",
      "vpm.encoder.layers.23.self_attn.out_proj.input  scale:  6.916721820831299\n",
      "vpm.encoder.layers.23.self_attn.out_proj.output  scale:  4.926693439483643\n",
      "vpm.encoder.layers.23.mlp.fc1.input  scale:  270.27764892578125\n",
      "vpm.encoder.layers.23.mlp.fc1.output  scale:  84.74235534667969\n",
      "vpm.encoder.layers.23.mlp.fc2.input  scale:  67.4324951171875\n",
      "vpm.encoder.layers.23.mlp.fc2.output  scale:  17.681503295898438\n",
      "vpm.encoder.layers.24.self_attn.q_proj.input  scale:  32.573097229003906\n",
      "vpm.encoder.layers.24.self_attn.q_proj.output  scale:  11.648115158081055\n",
      "vpm.encoder.layers.24.self_attn.k_proj.input  scale:  32.573097229003906\n",
      "vpm.encoder.layers.24.self_attn.k_proj.output  scale:  12.463631629943848\n",
      "vpm.encoder.layers.24.self_attn.v_proj.input  scale:  32.573097229003906\n",
      "vpm.encoder.layers.24.self_attn.v_proj.output  scale:  10.292866706848145\n",
      "vpm.encoder.layers.24.self_attn.out_proj.input  scale:  8.261892318725586\n",
      "vpm.encoder.layers.24.self_attn.out_proj.output  scale:  15.071619987487793\n",
      "vpm.encoder.layers.24.mlp.fc1.input  scale:  294.88177490234375\n",
      "vpm.encoder.layers.24.mlp.fc1.output  scale:  107.44819641113281\n",
      "vpm.encoder.layers.24.mlp.fc2.input  scale:  97.95767211914062\n",
      "vpm.encoder.layers.24.mlp.fc2.output  scale:  20.846513748168945\n",
      "vpm.encoder.layers.25.self_attn.q_proj.input  scale:  38.25333023071289\n",
      "vpm.encoder.layers.25.self_attn.q_proj.output  scale:  11.730119705200195\n",
      "vpm.encoder.layers.25.self_attn.k_proj.input  scale:  38.25333023071289\n",
      "vpm.encoder.layers.25.self_attn.k_proj.output  scale:  9.753336906433105\n",
      "vpm.encoder.layers.25.self_attn.v_proj.input  scale:  38.25333023071289\n",
      "vpm.encoder.layers.25.self_attn.v_proj.output  scale:  13.370960235595703\n",
      "vpm.encoder.layers.25.self_attn.out_proj.input  scale:  8.357008934020996\n",
      "vpm.encoder.layers.25.self_attn.out_proj.output  scale:  21.825389862060547\n",
      "vpm.encoder.layers.25.mlp.fc1.input  scale:  261.2272644042969\n",
      "vpm.encoder.layers.25.mlp.fc1.output  scale:  149.8111114501953\n",
      "vpm.encoder.layers.25.mlp.fc2.input  scale:  100.16165924072266\n",
      "vpm.encoder.layers.25.mlp.fc2.output  scale:  39.26945877075195\n",
      "vpm.encoder.layers.26.self_attn.q_proj.input  scale:  41.79985809326172\n",
      "vpm.encoder.layers.26.self_attn.q_proj.output  scale:  8.59249496459961\n",
      "vpm.encoder.layers.26.self_attn.k_proj.input  scale:  41.79985809326172\n",
      "vpm.encoder.layers.26.self_attn.k_proj.output  scale:  9.256844520568848\n",
      "vpm.encoder.layers.26.self_attn.v_proj.input  scale:  41.79985809326172\n",
      "vpm.encoder.layers.26.self_attn.v_proj.output  scale:  14.56513500213623\n",
      "vpm.encoder.layers.26.self_attn.out_proj.input  scale:  12.476614952087402\n",
      "vpm.encoder.layers.26.self_attn.out_proj.output  scale:  20.553241729736328\n",
      "vpm.encoder.layers.26.mlp.fc1.input  scale:  201.06988525390625\n",
      "vpm.encoder.layers.26.mlp.fc1.output  scale:  259.2696533203125\n",
      "vpm.encoder.layers.26.mlp.fc2.input  scale:  259.2696533203125\n",
      "vpm.encoder.layers.26.mlp.fc2.output  scale:  177.17477416992188\n"
     ]
    }
   ],
   "source": [
    "max_values = {}\n",
    "for k,v in activations.items():\n",
    "    # print(k)\n",
    "    for kk,vv in v.items():\n",
    "        scale_name = k + \".\" + kk\n",
    "        if kk == \"input\":\n",
    "            if len(vv) == 1:\n",
    "                scale_value = torch.max(torch.abs(vv[0])) \n",
    "            else:\n",
    "                print(vv[1])\n",
    "                print(\"input 有多个元素\")  \n",
    "        elif kk == \"output\":\n",
    "            scale_value = torch.max(torch.abs(vv)) \n",
    "        print(k + \".\" + kk, \" scale: \",scale_value.item())\n",
    "        max_values[scale_name] = scale_value.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"max_abs_value_vit_quant.json\", \"w\") as json_file:\n",
    "    json.dump(max_values, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 09:01:37 INFO [transformers_modules.MiniCPM-V-1B-sft-v2-1B.configuration_minicpm] vision_config is None, using default vision config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer,AutoProcessor,set_seed\n",
    "\n",
    "set_seed(42)\n",
    "# torch.manual_seed(0)\n",
    "# model_path = \"/home/workspace/model/MiniCPM-Llama3-V-2_5\"\n",
    "# model_path = \"/home/workspace/model/minicpm-vit-1b-w8-lenovo-llama-w8-pergroup128\"\n",
    "# model_path = \"/home/workspace/model/minicpm-vit-1b-w8-lenovo\"\n",
    "# model_path = \"/home/workspace/model/llava-1___5-7b-hf\"\n",
    "# model_path = \"/home/workspace/model/minicpm-gptq-w4-32-perchannel-only_quant_downproj\"\n",
    "model_path = \"/home/workspace/model/MiniCPM-V-1B-sft-v2-1B\"\n",
    "model = AutoModel.from_pretrained(model_path, \n",
    "                                  trust_remote_code=True,\n",
    "                                  device_map=None) \n",
    "# model_weight_path = \"/home/workspace/model/minicpm_v_navit_250_0927.pt\"\n",
    "# model.load_state_dict(torch.load(model_weight_path))\n",
    "model = model.cuda().eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipVisionTransformer(\n",
       "  (embeddings): SiglipVisionEmbeddings(\n",
       "    (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "    (position_embedding): Embedding(4900, 1152)\n",
       "  )\n",
       "  (encoder): SiglipEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-26): 27 x SiglipEncoderLayer(\n",
       "        (self_attn): SiglipAttention(\n",
       "          (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): SiglipMLP(\n",
       "          (activation_fn): PytorchGELUTanh()\n",
       "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpm = model.vpm\n",
    "input_shape = (1, 1024, 1152)\n",
    "example_input = torch.ones(input_shape, dtype=torch.float32)\n",
    "\n",
    "# print(model_resampler)\n",
    "vpm.prepare_layernorm()\n",
    "for bolck in vpm.encoder.layers:\n",
    "    bolck.prepare_layernorm()\n",
    "    bolck.self_attn.prepare_sha()\n",
    "    bolck.mlp.prepare_conv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipVisionTransformer(\n",
       "  (embeddings): SiglipVisionEmbeddings(\n",
       "    (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "    (position_embedding): Embedding(4900, 1152)\n",
       "  )\n",
       "  (encoder): SiglipEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-26): 27 x SiglipEncoderLayer(\n",
       "        (self_attn): SiglipAttention(\n",
       "          (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (q_proj_sha): ModuleList(\n",
       "            (0-15): 16 x Conv2d(1152, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (k_proj_sha): ModuleList(\n",
       "            (0-15): 16 x Conv2d(1152, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (v_proj_sha): ModuleList(\n",
       "            (0-15): 16 x Conv2d(1152, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (o_proj_conv): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): SiglipMLP(\n",
       "          (activation_fn): PytorchGELUTanh()\n",
       "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "          (fc1_conv): Conv2d(1152, 4304, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2_conv): Conv2d(4304, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_norm1_ane): LayerNormANE()\n",
       "        (layer_norm2_ane): LayerNormANE()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "  (post_layernorm_ane): LayerNormANE()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = vpm(example_input.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x-x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "def get_mme(nsamples, seed, seqlen, processor):\n",
    "    dataset = datasets.load_from_disk(\"/home/workspace/dataset/MME\")[\"test\"]\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "    # rng = random.Random(42)\n",
    "    samples, num_tokens = [], 0\n",
    "    prompts_lists = []\n",
    "    input_images_lists = []\n",
    "    for index, _data in enumerate(dataset):\n",
    "        promt = _data[\"question\"]\n",
    "        image = _data[\"image\"]\n",
    "        msgs = [{'role': 'user', 'content': \"(<image>./</image>)\\n\"+ promt}]\n",
    "        prompts_lists.append(processor.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True))\n",
    "        input_images_lists.append([image])\n",
    "        if index >= nsamples:\n",
    "            break\n",
    "    inputs = processor(\n",
    "        prompts_lists,\n",
    "        input_images_lists,\n",
    "        max_slice_nums=9,\n",
    "        use_image_id=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=8192\n",
    "    )[\"input_ids\"]\n",
    "    trainloader = []\n",
    "    import torch.nn.functional as F\n",
    "    for i in range(inputs.size(0)):  # tensor.size(0) == 33\n",
    "        inp = inputs.select(0, i).unsqueeze(0)  # 获取第 i 行并增加一个维度\n",
    "        # pad_size = seqlen - inp.size(1)\n",
    "        # # 在右侧填充，左边填充 0，右边填充 pad_size 个值\n",
    "        # inp = F.pad(inp, (pad_size,0), \"constant\", 0)\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-1] = -100\n",
    "        trainloader.append((inp, tar))\n",
    "    return trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1\"\n",
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(pretrained_model_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MiniCPMVTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "a = get_mme(128, 0, 2048, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m a:\n",
      "\u001b[1;32m      3\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(a[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],s)\n",
      "\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(s)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for i in a:\n",
    "    s = max(a[0][0].shape[1],s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 量化参数校验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lmquant/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from auto_gptq.modeling.minicpm.modeling_minicpmv import MiniCPMV\n",
    "from torch import nn\n",
    "\n",
    "model_path='/home/workspace/model/MiniCPM-3o-1B-sft-v1-vit_w8_pc_llm_pc'\n",
    "model = MiniCPMV.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from auto_gptq.modeling.minicpm.modeling_minicpmv import MiniCPMV\n",
    "from torch import nn\n",
    "\n",
    "model_fp_path='/home/workspace/model/MiniCPM-3o-1B-sft-v1'\n",
    "model_fp = MiniCPMV.from_pretrained(model_fp_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数量化错误:  llm.model.layers.0.self_attn.q_proj\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "ignore_layers = [\"resampler\",\"llm.lm_head\",\"vpm\"]\n",
    "for (name, module), (name_fp, module_fp) in zip(model.named_modules(),model_fp.named_modules()):\n",
    "    if isinstance(module, nn.Linear) and not any(name.startswith(ignore_layer) for ignore_layer in ignore_layers):\n",
    "        # 选择你想要添加 hook 的模块，比如 Linear, Conv2d, 或 LayerNorm\n",
    "        if torch.all((module_fp.weight-module.weight).eq(0)):\n",
    "            print(\"参数量化错误: \",name)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 权重合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {'gptq_group_size': '-1',\n",
    "            'gptq_damp_percent': '0.01',\n",
    "            'auto_gptq_version': '0.8.0.dev0',\n",
    "            'gptq_quant_method': 'gptq',\n",
    "            'gptq_desc_act': 'True',\n",
    "            'llm_gptq_bits': '8',\n",
    "            'vit_gptq_bits':'4',\n",
    "            'gptq_checkpoint_format': 'gptq',\n",
    "            'format': 'pt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined weights saved to /home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256-llm-w4-pc/model.safetensors\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load, save_file\n",
    "\n",
    "# 路径定义\n",
    "path2 = '/home/workspace/model/MiniCPM-3o-1B-sft-v1-llm_pc_w4/model.safetensors'\n",
    "path1 = '/home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256/model.safetensors'\n",
    "output_path = '/home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256-llm-w4-pc/model.safetensors'\n",
    "\n",
    "# 加载 safetensors 文件\n",
    "with open(path1, \"rb\") as f:\n",
    "    file1 = f.read()\n",
    "weights1 = load(file1)\n",
    "with open(path2, \"rb\") as f:\n",
    "    file2 = f.read()\n",
    "weights2 = load(file2)\n",
    "\n",
    "# 筛选权重\n",
    "selected_weights = {}\n",
    "\n",
    "for key, value in weights1.items():\n",
    "    if 'vpm' in key:\n",
    "        selected_weights[key] = value\n",
    "for key, value in weights2.items():\n",
    "    if 'vpm' not in key:\n",
    "        selected_weights[key] = value\n",
    "# selected_weights[\"metadata\"] = metadata\n",
    "save_file(selected_weights, output_path, metadata=metadata)\n",
    "\n",
    "print(f\"Combined weights saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load, save_file\n",
    "\n",
    "output_path = '/home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256-llm-w4-pc/model.safetensors'\n",
    "with open(output_path, \"rb\") as f:\n",
    "    file2 = f.read()\n",
    "weights2 = load(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vpm.encoder.layers.0.self_attn.q_proj.bias': tensor([-0.4961, -0.2480,  0.5977,  ...,  0.2539,  0.3809,  3.5781]),\n",
       " 'llm.model.layers.14.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.21.layer_norm1.weight': tensor([1.2891, 1.1016, 1.3516,  ..., 1.2734, 1.2734, 1.3047]),\n",
       " 'vpm.encoder.layers.4.layer_norm1.bias': tensor([ 0.0596,  0.0542, -0.0053,  ..., -0.0815, -0.0219,  0.0388]),\n",
       " 'vpm.encoder.layers.6.self_attn.v_proj.scales': tensor([[0.0007, 0.0009, 0.0008,  ..., 0.0008, 0.0007, 0.0008]]),\n",
       " 'llm.model.layers.37.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.41.self_attn.q_proj.scales': tensor([[0.0195, 0.0180, 0.0211,  ..., 0.0262, 0.0227, 0.0286]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.0.mlp.fc2.scales': tensor([[0.0012, 0.0009, 0.0015,  ..., 0.0017, 0.0009, 0.0014]]),\n",
       " 'llm.model.layers.4.mlp.down_proj.scales': tensor([[0.0234, 0.0245, 0.0258,  ..., 0.0273, 0.0280, 0.0516]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.48.mlp.up_proj.scales': tensor([[0.0296, 0.0312, 0.0262,  ..., 0.0229, 0.0286, 0.0268]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.38.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.3.mlp.fc1.scales': tensor([[0.0014, 0.0008, 0.0008,  ..., 0.0009, 0.0008, 0.0007]]),\n",
       " 'vpm.encoder.layers.8.layer_norm2.weight': tensor([0.6914, 0.8945, 0.8672,  ..., 0.7148, 0.7930, 0.8633]),\n",
       " 'llm.model.layers.18.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.2.self_attn.q_proj.bias': tensor([-0.0649, -0.0016,  0.0222,  ..., -0.4473, -0.1387, -0.0791]),\n",
       " 'llm.model.layers.23.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.8.self_attn.out_proj.scales': tensor([[0.0007, 0.0006, 0.0007,  ..., 0.0007, 0.0007, 0.0008]]),\n",
       " 'llm.model.layers.24.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.3.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.12.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.33.mlp.down_proj.qweight': tensor([[-1753577287,  2022139735, -1716025689,  ..., -1700234875,\n",
       "           2018408552, -1955108438],\n",
       "         [ 2041936023,  2019280806, -1484236936,  ...,  2005506727,\n",
       "          -1971741049, -1703315305],\n",
       "         [-1449616487,  -628655767, -1785222536,  ..., -1165609065,\n",
       "           2036697192, -1701148554],\n",
       "         ...,\n",
       "         [-2054849915,  1469745112, -2023254172,  ...,  1755813801,\n",
       "          -1718712476, -1987536744],\n",
       "         [ 1753589912,  2103092889, -1714845285,  ...,  1487370120,\n",
       "           1753709190,  1517787272],\n",
       "         [ 2042067013,  1758034295, -1465161590,  ...,  1989708182,\n",
       "          -1971951304,  1733798054]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.5.self_attn.q_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.47.mlp.gate_proj.qweight': tensor([[-1752659831,  1718192569,  2021099915,  ...,  1990874775,\n",
       "          -1199142249, -2023197080],\n",
       "         [-1753585785, -1503229304, -2036889478,  ..., -1211602294,\n",
       "           -935958604, -1752774233],\n",
       "         [-1735816840,  2005362886,  1983158631,  ...,  1986357635,\n",
       "           1250510220,  1975032232],\n",
       "         ...,\n",
       "         [-1970701913, -1735689640, -1184200341,  ..., -1435909556,\n",
       "          -1184598663, -2002483049],\n",
       "         [-2021160328, -2003134091, -2038064985,  ...,  -947418489,\n",
       "           -675699052, -2040030549],\n",
       "         [-2004318089,  1955232377, -1970772074,  ..., -1749579593,\n",
       "          -1951766680, -1285006760]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.8.self_attn.v_proj.qweight': tensor([[ 2137358275, -2136643973, -1868701352,  ..., -2022465876,\n",
       "           1554999439,  1852075140],\n",
       "         [ -763536226,  1631420314,  2007252622,  ...,  1534101370,\n",
       "          -1957189502, -2037483384],\n",
       "         [-1381136282,  2020178817,  1753321892,  ..., -2053262488,\n",
       "           1836551801,  1938391672],\n",
       "         ...,\n",
       "         [ 1687186055,  -695504280,  1939038384,  ..., -1618899563,\n",
       "          -2125739921,  2004910494],\n",
       "         [-2104674358,  1584565598,  1177714545,  ..., -1702602118,\n",
       "          -1749197233, -1886561682],\n",
       "         [-1799833993,  1197647958,  1082692505,  ..., -1520206189,\n",
       "           2137826186, -2090556537]], dtype=torch.int32),\n",
       " 'llm.model.layers.10.self_attn.v_proj.scales': tensor([[0.0258, 0.0211, 0.0202, 0.0193, 0.0216, 0.0198, 0.0188, 0.0219, 0.0229,\n",
       "          0.0238, 0.0206, 0.0198, 0.0213, 0.0249, 0.0177, 0.0211, 0.0193, 0.0193,\n",
       "          0.0207, 0.0211, 0.0217, 0.0257, 0.0191, 0.0215, 0.0191, 0.0206, 0.0216,\n",
       "          0.0229, 0.0212, 0.0236, 0.0202, 0.0211, 0.0193, 0.0204, 0.0194, 0.0249,\n",
       "          0.0217, 0.0254, 0.0223, 0.0216, 0.0215, 0.0219, 0.0221, 0.0208, 0.0211,\n",
       "          0.0190, 0.0204, 0.0212, 0.0201, 0.0212, 0.0194, 0.0213, 0.0228, 0.0228,\n",
       "          0.0197, 0.0213, 0.0293, 0.0195, 0.0210, 0.0204, 0.0223, 0.0207, 0.0202,\n",
       "          0.0213, 0.0219, 0.0223, 0.0195, 0.0211, 0.0181, 0.0232, 0.0221, 0.0207,\n",
       "          0.0225, 0.0233, 0.0195, 0.0221, 0.0228, 0.0178, 0.0219, 0.0223, 0.0220,\n",
       "          0.0236, 0.0220, 0.0193, 0.0227, 0.0246, 0.0188, 0.0246, 0.0259, 0.0229,\n",
       "          0.0206, 0.0197, 0.0221, 0.0203, 0.0188, 0.0249, 0.0206, 0.0210, 0.0253,\n",
       "          0.0217, 0.0240, 0.0227, 0.0224, 0.0208, 0.0250, 0.0204, 0.0245, 0.0237,\n",
       "          0.0225, 0.0204, 0.0207, 0.0238, 0.0229, 0.0227, 0.0186, 0.0223, 0.0215,\n",
       "          0.0202, 0.0207, 0.0241, 0.0202, 0.0181, 0.0199, 0.0234, 0.0190, 0.0208,\n",
       "          0.0229, 0.0206, 0.0280, 0.0247, 0.0213, 0.0217, 0.0206, 0.0206, 0.0217,\n",
       "          0.0204, 0.0217, 0.0215, 0.0215, 0.0217, 0.0228, 0.0227, 0.0223, 0.0266,\n",
       "          0.0193, 0.0264, 0.0213, 0.0221, 0.0221, 0.0185, 0.0199, 0.0228, 0.0234,\n",
       "          0.0207, 0.0193, 0.0215, 0.0268, 0.0229, 0.0240, 0.0193, 0.0216, 0.0201,\n",
       "          0.0181, 0.0208, 0.0180, 0.0191, 0.0210, 0.0219, 0.0206, 0.0181, 0.0234,\n",
       "          0.0186, 0.0189, 0.0208, 0.0204, 0.0188, 0.0186, 0.0212, 0.0204, 0.0211,\n",
       "          0.0207, 0.0208, 0.0362, 0.0208, 0.0234, 0.0241, 0.0182, 0.0219, 0.0238,\n",
       "          0.0207, 0.0206, 0.0211, 0.0206, 0.0221, 0.0224, 0.0229, 0.0238, 0.0184,\n",
       "          0.0238, 0.0212, 0.0211, 0.0191, 0.0195, 0.0203, 0.0234, 0.0224, 0.0201,\n",
       "          0.0178, 0.0169, 0.0199, 0.0178, 0.0204, 0.0233, 0.0176, 0.0184, 0.0191,\n",
       "          0.0189, 0.0195, 0.0215, 0.0199, 0.0181, 0.0216, 0.0172, 0.0211, 0.0190,\n",
       "          0.0220, 0.0219, 0.0188, 0.0190, 0.0199, 0.0207, 0.0195, 0.0198, 0.0204,\n",
       "          0.0191, 0.0211, 0.0173, 0.0188, 0.0195, 0.0204, 0.0208, 0.0191, 0.0186,\n",
       "          0.0184, 0.0189, 0.0203, 0.0184, 0.0210, 0.0185, 0.0220, 0.0211, 0.0204,\n",
       "          0.0206, 0.0182, 0.0246, 0.0188, 0.0250, 0.0207, 0.0220, 0.0221, 0.0202,\n",
       "          0.0203, 0.0217, 0.0197, 0.0257, 0.0216, 0.0204, 0.0198, 0.0195, 0.0188,\n",
       "          0.0242, 0.0229, 0.0216, 0.0250, 0.0190, 0.0206, 0.0219, 0.0191, 0.0180,\n",
       "          0.0198, 0.0207, 0.0215, 0.0219, 0.0197, 0.0198, 0.0204, 0.0204, 0.0206,\n",
       "          0.0219, 0.0254, 0.0224, 0.0227, 0.0199, 0.0229, 0.0193, 0.0212, 0.0189,\n",
       "          0.0247, 0.0201, 0.0210, 0.0223, 0.0249, 0.0197, 0.0203, 0.0186, 0.0212,\n",
       "          0.0208, 0.0217, 0.0266, 0.0197, 0.0215, 0.0217, 0.0197, 0.0194, 0.0242,\n",
       "          0.0216, 0.0210, 0.0194, 0.0212, 0.0208, 0.0221, 0.0220, 0.0203, 0.0193,\n",
       "          0.0227, 0.0186, 0.0223, 0.0199, 0.0188, 0.0213, 0.0208, 0.0212, 0.0215,\n",
       "          0.0212, 0.0223, 0.0224, 0.0208, 0.0206, 0.0246, 0.0255, 0.0224, 0.0208,\n",
       "          0.0233, 0.0223, 0.0181, 0.0221, 0.0237, 0.0221, 0.0211, 0.0242, 0.0242,\n",
       "          0.0232, 0.0198, 0.0204, 0.0197, 0.0208, 0.0245, 0.0229, 0.0212, 0.0233,\n",
       "          0.0202, 0.0217, 0.0220, 0.0227, 0.0254, 0.0217, 0.0262, 0.0219, 0.0198,\n",
       "          0.0215, 0.0224, 0.0206, 0.0211, 0.0254, 0.0210, 0.0216, 0.0247, 0.0206,\n",
       "          0.0215, 0.0201, 0.0237, 0.0216, 0.0208, 0.0232, 0.0254, 0.0208, 0.0223,\n",
       "          0.0250, 0.0238, 0.0228, 0.0203, 0.0216, 0.0184, 0.0195, 0.0215, 0.0269,\n",
       "          0.0197, 0.0207, 0.0233, 0.0204, 0.0220, 0.0258, 0.0198, 0.0262, 0.0190,\n",
       "          0.0212, 0.0199, 0.0228, 0.0191, 0.0188, 0.0253, 0.0249, 0.0203, 0.0225,\n",
       "          0.0215, 0.0216, 0.0253, 0.0220, 0.0264, 0.0241, 0.0197, 0.0208, 0.0206,\n",
       "          0.0197, 0.0276, 0.0220, 0.0230, 0.0197, 0.0208, 0.0197, 0.0220, 0.0182,\n",
       "          0.0195, 0.0262, 0.0210, 0.0215, 0.0202, 0.0197, 0.0249, 0.0206, 0.0233,\n",
       "          0.0194, 0.0263, 0.0258, 0.0221, 0.0238, 0.0227, 0.0229, 0.0245, 0.0262,\n",
       "          0.0236, 0.0219, 0.0223, 0.0208, 0.0210, 0.0259, 0.0249, 0.0246, 0.0203,\n",
       "          0.0197, 0.0312, 0.0217, 0.0229, 0.0229, 0.0259, 0.0236, 0.0247, 0.0259,\n",
       "          0.0241, 0.0237, 0.0224, 0.0257, 0.0253, 0.0232, 0.0264, 0.0234, 0.0273,\n",
       "          0.0262, 0.0266, 0.0249, 0.0236, 0.0240, 0.0220, 0.0227, 0.0247, 0.0236,\n",
       "          0.0215, 0.0229, 0.0253, 0.0258, 0.0260, 0.0238, 0.0215, 0.0247, 0.0247,\n",
       "          0.0257, 0.0233, 0.0213, 0.0264, 0.0227, 0.0241, 0.0220, 0.0215, 0.0215,\n",
       "          0.0199, 0.0223, 0.0246, 0.0221, 0.0223, 0.0300, 0.0215, 0.0224]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.27.mlp.up_proj.scales': tensor([[0.0264, 0.0233, 0.0213,  ..., 0.0284, 0.0220, 0.0225]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.1.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.19.self_attn.v_proj.qweight': tensor([[-1986379914,  1464700568,  1418169752,  ..., -2035251558,\n",
       "          -1102453340, -1991340677],\n",
       "         [ 1537241455, -1985115242, -1306811233,  ..., -1547337322,\n",
       "            624179365,  1046379866],\n",
       "         [-2125562989,  1622118535, -1802348711,  ...,  1570343041,\n",
       "           1838886547, -1453496456],\n",
       "         ...,\n",
       "         [-1685153192, -2024106592, -2089841327,  ..., -1822707045,\n",
       "           -781149396,  1268477884],\n",
       "         [  829140623, -1249725282, -2069926305,  ...,  2005832882,\n",
       "          -1382901847,  -983469427],\n",
       "         [-1702716599,  2117710490,  1348632690,  ..., -2020177458,\n",
       "          -1734721908, -2053013349]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.24.mlp.fc2.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.2.mlp.fc1.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.17.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.9.mlp.fc2.qweight': tensor([[ 1903287943,  2138722445,  1953988481,  ...,  2119716950,\n",
       "           1872024716,  1546425183],\n",
       "         [ 1536859516,  1502894466,  2106630269,  ...,  1566922097,\n",
       "          -2021094232, -1713490568],\n",
       "         [ 2106357360, -2087949954, -2037152842,  ...,  1518697302,\n",
       "           1954500253, -1785360285],\n",
       "         ...,\n",
       "         [ 1836034422,  2069657737,  1999456070,  ...,  1837201533,\n",
       "          -1868798866, -1721723019],\n",
       "         [-1262527915,  1866106766,  1773957257,  ...,  1826066289,\n",
       "           1654236831,  1972738970],\n",
       "         [ -710765148, -1899655883,  1800498753,  ...,  1966305936,\n",
       "          -1603965057,  1446398838]], dtype=torch.int32),\n",
       " 'llm.model.layers.5.post_attention_layernorm.weight': tensor([1.1641, 1.0391, 0.8125,  ..., 0.8047, 1.0078, 0.2051]),\n",
       " 'llm.model.layers.7.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.31.mlp.down_proj.qweight': tensor([[-1751737724, -1754760570, -1768318810,  ...,  2106095501,\n",
       "          -1989506471, -1804237208],\n",
       "         [-1753778813, -1231513754,  2073655688,  ...,  2039064472,\n",
       "           -927430010, -1985304409],\n",
       "         [-1450796665,  1771534266,  1398241976,  ...,  1954055979,\n",
       "          -1702389643, -1786083194],\n",
       "         ...,\n",
       "         [-1734957209, -1486457208, -1904760713,  ..., -2035960167,\n",
       "          -1957263751, -1720219255],\n",
       "         [ 1952025238, -1753769591,  2005568152,  ..., -1416005287,\n",
       "           2073462922, -1972921978],\n",
       "         [-1733981861, -1972995689, -2021107335,  ..., -1518691975,\n",
       "           1753577579, -1752663909]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.12.layer_norm2.bias': tensor([ 0.3125, -0.1895,  0.5977,  ...,  0.2227,  0.1328,  0.2080]),\n",
       " 'llm.model.layers.14.self_attn.k_proj.qweight': tensor([[ -726100088, -2038846088, -2033599931,  ..., -1717921881,\n",
       "          -1149592953,  2023184234],\n",
       "         [ 1970771833,  -629642821, -1934009463,  ...,  1449687450,\n",
       "          -1447577156, -2037868151],\n",
       "         [-2002486948,  1466460329, -1199085766,  ...,  1533574059,\n",
       "           1991542169, -1517766823],\n",
       "         ...,\n",
       "         [-1451718838, -1970767240, -1954322548,  ...,  2044245368,\n",
       "           2019198857,  2023331704],\n",
       "         [-1839830937,  1874495162, -1467705961,  ..., -1132831335,\n",
       "           1955178841, -1837667974],\n",
       "         [-1499105161,  1788508282,  1786362739,  ..., -2040231292,\n",
       "          -2019846491,  1789429159]], dtype=torch.int32),\n",
       " 'llm.model.layers.15.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.26.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.5.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.13.layer_norm1.bias': tensor([ 0.0952,  0.0381,  0.1279,  ...,  0.0115,  0.0192, -0.1177]),\n",
       " 'vpm.encoder.layers.16.self_attn.out_proj.qweight': tensor([[ 1186684496,  1246661724,  2075556709,  ...,  1973447856,\n",
       "          -1621530062, -1887200863],\n",
       "         [-1333705889, -1803692936, -1783990459,  ..., -1838778183,\n",
       "          -1148089763,  1822972529],\n",
       "         [-2006414185,  2022818427, -1618253672,  ...,  1499684027,\n",
       "          -1891797366, -1452963540],\n",
       "         ...,\n",
       "         [ 1568435531,  2120514708,  1872527467,  ..., -2037212244,\n",
       "          -1283680043, -2087810899],\n",
       "         [-2071622820,  1302888795,  1820904569,  ..., -1853717676,\n",
       "           1823762006,  1451264105],\n",
       "         [ 1482602876,  1167378280, -1754889366,  ...,  1804040834,\n",
       "          -2018339904, -1268890792]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.8.self_attn.k_proj.scales': tensor([[0.0008, 0.0006, 0.0007,  ..., 0.0009, 0.0006, 0.0006]]),\n",
       " 'vpm.encoder.layers.19.self_attn.out_proj.qweight': tensor([[ 1599376504, -1583965235,  1415751777,  ...,  1669220704,\n",
       "          -1972329851,  2077004675],\n",
       "         [-1655007109,  1808170624, -1215985814,  ..., -1992067910,\n",
       "          -2103345799, -1567837055],\n",
       "         [-1300240990,  1452648087, -2004843952,  ..., -1733336521,\n",
       "          -2088854639,  1133221783],\n",
       "         ...,\n",
       "         [-1384064323, -1388730197, -1718396034,  ..., -1516869553,\n",
       "          -1720475732,  1450345848],\n",
       "         [-1719575683,  1852942934, -1217357947,  ..., -1837656746,\n",
       "           1670933849, -1802657176],\n",
       "         [-1498505886,  1985318022, -1281977254,  ...,  1803836297,\n",
       "           2087935584, -2056685677]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.11.mlp.fc2.scales': tensor([[0.0009, 0.0010, 0.0011,  ..., 0.0008, 0.0010, 0.0009]]),\n",
       " 'vpm.encoder.layers.10.self_attn.k_proj.qweight': tensor([[ 1920182647, -2040500076,  -968336207,  ...,  1971749002,\n",
       "           1803922050,  2138804817],\n",
       "         [  394105727,  2068020099,  1919838538,  ..., -1986696028,\n",
       "          -1736139868,  1214726258],\n",
       "         [-1517708803, -1869763471,  1317375590,  ..., -1859284828,\n",
       "           1820550507,  1532723619],\n",
       "         ...,\n",
       "         [-1532454810,  1482130006,  2105982021,  ..., -2086565540,\n",
       "          -1922068365,  1798327132],\n",
       "         [  761214799, -1719037605,  1802016409,  ...,  2054121875,\n",
       "           1198553986, -1418882431],\n",
       "         [-1194100351, -1969516981, -1269530223,  ..., -1920890270,\n",
       "           1298816402, -2136380800]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.18.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.26.mlp.fc1.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.3.self_attn.k_proj.qweight': tensor([[-2138539135, -1904245121, -1803664027,  ..., -1888716933,\n",
       "           2087349641,  2087943526],\n",
       "         [ 2055179409, -2073203592, -1353083527,  ..., -1970375565,\n",
       "          -2123464805, -1754889337],\n",
       "         [ 2005306239,  1887538037, -2071688855,  ...,  1502774917,\n",
       "          -2004451713, -1988530336],\n",
       "         ...,\n",
       "         [-2055563903,  1921483134,  2103472280,  ...,  2106225533,\n",
       "           1954380918, -2072017793],\n",
       "         [ 1854238587, -2038464894,  2021363067,  ..., -2054716561,\n",
       "           2089651328, -1736015730],\n",
       "         [-2072146815,  2137951105,  2006810724,  ...,  1985448314,\n",
       "          -1820420476,  1955294568]], dtype=torch.int32),\n",
       " 'llm.model.layers.11.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.34.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.18.self_attn.q_proj.scales': tensor([[0.0245, 0.0186, 0.0247,  ..., 0.0242, 0.0240, 0.0238]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.12.mlp.fc1.scales': tensor([[0.0009, 0.0009, 0.0009,  ..., 0.0009, 0.0009, 0.0015]]),\n",
       " 'llm.model.layers.34.self_attn.q_proj.scales': tensor([[0.0133, 0.0165, 0.0155,  ..., 0.0195, 0.0260, 0.0237]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.36.self_attn.o_proj.scales': tensor([[0.0267, 0.0266, 0.0225,  ..., 0.0310, 0.0300, 0.0159]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.16.layer_norm1.bias': tensor([ 0.1055, -0.0698,  0.0042,  ...,  0.0459,  0.0092, -0.0913]),\n",
       " 'llm.model.layers.41.self_attn.o_proj.scales': tensor([[0.0267, 0.0296, 0.0288,  ..., 0.0281, 0.0257, 0.0139]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.46.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.14.self_attn.q_proj.qweight': tensor([[-1954457223,  1403364727,  -730423666,  ..., -1253024568,\n",
       "          -1869440904, -1855429538],\n",
       "         [-1734246022,  1754750863,  2021281154,  ..., -2104847265,\n",
       "           2003213169, -1671576491],\n",
       "         [-1684972378,  1448390028,  1952881562,  ...,  1513071513,\n",
       "           1937731173,     5999766],\n",
       "         ...,\n",
       "         [ 1820493199,  1957847665,  2033881955,  ..., -2104713863,\n",
       "          -1986951010,  2120717437],\n",
       "         [ 2055176095,  2091684988,  1870768236,  ..., -1733830821,\n",
       "          -1889638347, -1616002917],\n",
       "         [ 1551276661, -2022994799, -1674210682,  ...,  1468835464,\n",
       "           -462254227,   912276827]], dtype=torch.int32),\n",
       " 'llm.model.layers.51.post_attention_layernorm.weight': tensor([1.2422, 1.2188, 1.1797,  ..., 1.1641, 1.1953, 1.1328]),\n",
       " 'vpm.encoder.layers.9.self_attn.k_proj.qweight': tensor([[-1903980403,  1587240552,  1954577766,  ...,  -914462306,\n",
       "           1081716840,  1905635921],\n",
       "         [ 1921348807,  1922600043, -1468814734,  ..., -2053272956,\n",
       "          -1854638704,  1554226515],\n",
       "         [ 2136247694, -1924757926,  1268011425,  ..., -2090047374,\n",
       "           1606307966,  2032243799],\n",
       "         ...,\n",
       "         [-1801039466, -1401380174,   897934491,  ...,  1651335798,\n",
       "          -2137485692,  1787994216],\n",
       "         [ -865893488,  1637973343,  1735043193,  ..., -1904558959,\n",
       "          -1921741730, -1670141020],\n",
       "         [ 1722388599, -1871612284,  1774478966,  ...,  1904183210,\n",
       "           1836598963, -1533776781]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.0.mlp.fc1.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.25.layer_norm2.weight': tensor([9.4375, 6.9062, 7.0938,  ..., 7.5000, 9.2500, 6.9062]),\n",
       " 'vpm.encoder.layers.9.self_attn.k_proj.scales': tensor([[0.0009, 0.0010, 0.0011,  ..., 0.0008, 0.0009, 0.0009]]),\n",
       " 'llm.model.layers.33.input_layernorm.weight': tensor([2.9688, 2.4375, 0.6914,  ..., 0.7812, 2.5938, 0.2559]),\n",
       " 'llm.model.layers.10.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.18.mlp.up_proj.scales': tensor([[0.0249, 0.0228, 0.0266,  ..., 0.0293, 0.0224, 0.0217]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.6.self_attn.out_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.13.mlp.fc2.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.18.mlp.fc2.bias': tensor([ 0.1494,  0.0825, -0.4492,  ..., -0.1924, -0.0133,  0.0576]),\n",
       " 'vpm.encoder.layers.17.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.51.self_attn.q_proj.qweight': tensor([[-1989830278,  1487243639,  1789249435,  ...,  2036889735,\n",
       "           1754941301, -1985447560],\n",
       "         [-1466341224, -2005484680, -1413134182,  ..., -2018809227,\n",
       "          -2054644805, -2001295733],\n",
       "         [-2019072123,  1988655240,  2073660073,  ...,  1732606922,\n",
       "           1987881350, -2021099416],\n",
       "         ...,\n",
       "         [-1988655208, -1452836953,  2006432139,  ...,  2087373224,\n",
       "           1233897833, -2038977896],\n",
       "         [-1453954919, -1736066969, -1466399115,  ...,  2072603256,\n",
       "           2040166825,  2025289900],\n",
       "         [ 2021161353, -1818723965,  1722516857,  ..., -2014607260,\n",
       "           -881563517,  2057742456]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.18.mlp.fc2.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.29.mlp.down_proj.scales': tensor([[0.0241, 0.0255, 0.0293,  ..., 0.0249, 0.0236, 0.0216]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.12.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.26.layer_norm1.weight': tensor([0.8984, 0.8633, 0.8711,  ..., 0.7734, 0.8203, 0.8516]),\n",
       " 'vpm.encoder.layers.1.self_attn.q_proj.scales': tensor([[0.0006, 0.0006, 0.0009,  ..., 0.0009, 0.0006, 0.0021]]),\n",
       " 'llm.model.layers.5.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.37.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.2.mlp.fc1.qzeros': tensor([[2139062143, 2139062143, 2139062143,  ..., 2139062143, 2139062143,\n",
       "          2139062143]], dtype=torch.int32),\n",
       " 'llm.model.layers.21.self_attn.v_proj.scales': tensor([[0.0247, 0.0281, 0.0267, 0.0268, 0.0255, 0.0269, 0.0292, 0.0290, 0.0289,\n",
       "          0.0266, 0.0264, 0.0284, 0.0305, 0.0294, 0.0245, 0.0250, 0.0276, 0.0275,\n",
       "          0.0245, 0.0280, 0.0279, 0.0297, 0.0272, 0.0266, 0.0253, 0.0277, 0.0244,\n",
       "          0.0297, 0.0249, 0.0259, 0.0250, 0.0303, 0.0324, 0.0258, 0.0255, 0.0230,\n",
       "          0.0322, 0.0279, 0.0281, 0.0254, 0.0298, 0.0249, 0.0319, 0.0264, 0.0303,\n",
       "          0.0251, 0.0242, 0.0245, 0.0249, 0.0293, 0.0272, 0.0254, 0.0301, 0.0224,\n",
       "          0.0254, 0.0275, 0.0238, 0.0240, 0.0250, 0.0254, 0.0281, 0.0290, 0.0273,\n",
       "          0.0268, 0.0202, 0.0203, 0.0230, 0.0227, 0.0258, 0.0182, 0.0216, 0.0220,\n",
       "          0.0195, 0.0221, 0.0204, 0.0220, 0.0221, 0.0240, 0.0211, 0.0194, 0.0241,\n",
       "          0.0289, 0.0207, 0.0217, 0.0273, 0.0244, 0.0229, 0.0221, 0.0225, 0.0219,\n",
       "          0.0254, 0.0213, 0.0220, 0.0237, 0.0241, 0.0198, 0.0247, 0.0221, 0.0240,\n",
       "          0.0284, 0.0217, 0.0237, 0.0228, 0.0234, 0.0195, 0.0223, 0.0202, 0.0186,\n",
       "          0.0199, 0.0213, 0.0197, 0.0227, 0.0247, 0.0250, 0.0259, 0.0241, 0.0213,\n",
       "          0.0197, 0.0241, 0.0189, 0.0260, 0.0223, 0.0207, 0.0242, 0.0216, 0.0216,\n",
       "          0.0250, 0.0221, 0.0206, 0.0230, 0.0228, 0.0281, 0.0219, 0.0189, 0.0215,\n",
       "          0.0208, 0.0238, 0.0268, 0.0203, 0.0240, 0.0206, 0.0236, 0.0233, 0.0217,\n",
       "          0.0212, 0.0228, 0.0269, 0.0220, 0.0215, 0.0203, 0.0208, 0.0211, 0.0199,\n",
       "          0.0198, 0.0221, 0.0240, 0.0206, 0.0224, 0.0190, 0.0213, 0.0211, 0.0193,\n",
       "          0.0212, 0.0224, 0.0223, 0.0213, 0.0216, 0.0228, 0.0244, 0.0232, 0.0255,\n",
       "          0.0216, 0.0216, 0.0204, 0.0191, 0.0198, 0.0238, 0.0194, 0.0251, 0.0197,\n",
       "          0.0264, 0.0204, 0.0207, 0.0194, 0.0237, 0.0194, 0.0212, 0.0233, 0.0207,\n",
       "          0.0259, 0.0236, 0.0223, 0.0198, 0.0213, 0.0212, 0.0197, 0.0228, 0.0224,\n",
       "          0.0189, 0.0172, 0.0198, 0.0225, 0.0318, 0.0251, 0.0230, 0.0197, 0.0257,\n",
       "          0.0191, 0.0238, 0.0190, 0.0216, 0.0194, 0.0197, 0.0232, 0.0207, 0.0191,\n",
       "          0.0189, 0.0162, 0.0185, 0.0223, 0.0215, 0.0197, 0.0204, 0.0193, 0.0162,\n",
       "          0.0189, 0.0173, 0.0185, 0.0199, 0.0199, 0.0207, 0.0184, 0.0180, 0.0199,\n",
       "          0.0184, 0.0212, 0.0217, 0.0199, 0.0236, 0.0211, 0.0188, 0.0207, 0.0173,\n",
       "          0.0184, 0.0180, 0.0227, 0.0189, 0.0189, 0.0184, 0.0212, 0.0204, 0.0207,\n",
       "          0.0173, 0.0199, 0.0223, 0.0193, 0.0326, 0.0230, 0.0259, 0.0236, 0.0267,\n",
       "          0.0257, 0.0215, 0.0232, 0.0255, 0.0223, 0.0246, 0.0283, 0.0225, 0.0212,\n",
       "          0.0223, 0.0219, 0.0223, 0.0268, 0.0244, 0.0234, 0.0220, 0.0219, 0.0203,\n",
       "          0.0247, 0.0250, 0.0215, 0.0241, 0.0229, 0.0259, 0.0257, 0.0310, 0.0217,\n",
       "          0.0215, 0.0228, 0.0232, 0.0201, 0.0230, 0.0216, 0.0244, 0.0195, 0.0266,\n",
       "          0.0211, 0.0229, 0.0224, 0.0190, 0.0204, 0.0220, 0.0213, 0.0232, 0.0215,\n",
       "          0.0223, 0.0211, 0.0207, 0.0229, 0.0228, 0.0285, 0.0233, 0.0233, 0.0201,\n",
       "          0.0246, 0.0302, 0.0220, 0.0203, 0.0225, 0.0234, 0.0241, 0.0257, 0.0189,\n",
       "          0.0259, 0.0212, 0.0220, 0.0229, 0.0221, 0.0229, 0.0246, 0.0213, 0.0232,\n",
       "          0.0277, 0.0242, 0.0244, 0.0223, 0.0213, 0.0245, 0.0230, 0.0217, 0.0259,\n",
       "          0.0242, 0.0230, 0.0217, 0.0217, 0.0212, 0.0212, 0.0221, 0.0245, 0.0251,\n",
       "          0.0237, 0.0268, 0.0225, 0.0229, 0.0210, 0.0309, 0.0212, 0.0276, 0.0250,\n",
       "          0.0249, 0.0230, 0.0215, 0.0326, 0.0246, 0.0237, 0.0212, 0.0247, 0.0253,\n",
       "          0.0238, 0.0264, 0.0242, 0.0268, 0.0305, 0.0217, 0.0219, 0.0264, 0.0232,\n",
       "          0.0310, 0.0286, 0.0269, 0.0211, 0.0249, 0.0230, 0.0328, 0.0283, 0.0275,\n",
       "          0.0305, 0.0260, 0.0266, 0.0283, 0.0266, 0.0260, 0.0293, 0.0318, 0.0255,\n",
       "          0.0288, 0.0277, 0.0264, 0.0307, 0.0301, 0.0281, 0.0289, 0.0296, 0.0251,\n",
       "          0.0292, 0.0336, 0.0309, 0.0297, 0.0284, 0.0303, 0.0257, 0.0300, 0.0297,\n",
       "          0.0277, 0.0296, 0.0242, 0.0357, 0.0266, 0.0240, 0.0273, 0.0279, 0.0297,\n",
       "          0.0247, 0.0281, 0.0293, 0.0258, 0.0249, 0.0300, 0.0319, 0.0241, 0.0236,\n",
       "          0.0250, 0.0253, 0.0298, 0.0237, 0.0250, 0.0272, 0.0279, 0.0251, 0.0268,\n",
       "          0.0318, 0.0302, 0.0327, 0.0319, 0.0285, 0.0309, 0.0289, 0.0259, 0.0217,\n",
       "          0.0212, 0.0255, 0.0216, 0.0234, 0.0221, 0.0224, 0.0272, 0.0224, 0.0242,\n",
       "          0.0223, 0.0194, 0.0233, 0.0213, 0.0212, 0.0219, 0.0227, 0.0277, 0.0212,\n",
       "          0.0234, 0.0229, 0.0254, 0.0273, 0.0238, 0.0245, 0.0277, 0.0230, 0.0319,\n",
       "          0.0217, 0.0245, 0.0262, 0.0197, 0.0244, 0.0250, 0.0208, 0.0271, 0.0258,\n",
       "          0.0204, 0.0238, 0.0208, 0.0240, 0.0215, 0.0201, 0.0273, 0.0223, 0.0240,\n",
       "          0.0230, 0.0238, 0.0213, 0.0220, 0.0201, 0.0258, 0.0228, 0.0249, 0.0207,\n",
       "          0.0236, 0.0257, 0.0207, 0.0247, 0.0251, 0.0213, 0.0281, 0.0213]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.post_layernorm.bias': tensor([0.3301, 0.2812, 0.1748,  ..., 0.0289, 0.2422, 0.0569]),\n",
       " 'vpm.encoder.layers.18.self_attn.k_proj.scales': tensor([[0.0007, 0.0010, 0.0011,  ..., 0.0008, 0.0008, 0.0011]]),\n",
       " 'llm.model.layers.33.mlp.up_proj.qweight': tensor([[ 1789561257, -1449568103, -2057930631,  ...,  -929384293,\n",
       "          -1969603974, -2038986361],\n",
       "         [ 1752930886, -1467582344, -1549232260,  ...,  1268283069,\n",
       "           1503361721, -2020177768],\n",
       "         [-2022099090,  2054503479,  2055563910,  ..., -1167689577,\n",
       "           1486317446, -2021947016],\n",
       "         ...,\n",
       "         [   93685655, -1463374745, -2036693162,  ..., -2022082952,\n",
       "            715762550,  2022086826],\n",
       "         [-1181177206,  1517716567, -1987655574,  ..., -1736940394,\n",
       "          -1484236618, -1937274744],\n",
       "         [-1263052713, -1435978135,  1755948667,  ..., -1447524202,\n",
       "          -1452570247,   948607352]], dtype=torch.int32),\n",
       " 'llm.model.layers.6.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.44.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.10.self_attn.q_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.28.mlp.down_proj.scales': tensor([[0.0255, 0.0228, 0.0249,  ..., 0.0236, 0.0244, 0.0280]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.23.self_attn.v_proj.bias': tensor([ 0.0104, -0.0287,  0.1367,  ..., -0.0376, -0.1318, -0.0564]),\n",
       " 'llm.model.layers.28.input_layernorm.weight': tensor([1.9766, 2.0312, 0.8906,  ..., 1.0391, 2.0469, 0.1289]),\n",
       " 'vpm.encoder.layers.13.mlp.fc1.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.26.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.38.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.8.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.21.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.17.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.6.self_attn.k_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.9.self_attn.v_proj.scales': tensor([[0.0224, 0.0212, 0.0249, 0.0247, 0.0213, 0.0232, 0.0216, 0.0216, 0.0260,\n",
       "          0.0208, 0.0202, 0.0201, 0.0272, 0.0233, 0.0186, 0.0223, 0.0195, 0.0194,\n",
       "          0.0223, 0.0249, 0.0236, 0.0228, 0.0234, 0.0260, 0.0216, 0.0230, 0.0224,\n",
       "          0.0245, 0.0238, 0.0253, 0.0238, 0.0216, 0.0193, 0.0203, 0.0251, 0.0220,\n",
       "          0.0244, 0.0237, 0.0225, 0.0310, 0.0253, 0.0215, 0.0250, 0.0241, 0.0194,\n",
       "          0.0223, 0.0217, 0.0244, 0.0289, 0.0225, 0.0199, 0.0199, 0.0217, 0.0204,\n",
       "          0.0230, 0.0188, 0.0204, 0.0230, 0.0227, 0.0199, 0.0223, 0.0230, 0.0211,\n",
       "          0.0212, 0.0224, 0.0219, 0.0216, 0.0247, 0.0234, 0.0234, 0.0202, 0.0221,\n",
       "          0.0206, 0.0246, 0.0221, 0.0230, 0.0232, 0.0228, 0.0216, 0.0219, 0.0220,\n",
       "          0.0199, 0.0224, 0.0189, 0.0219, 0.0188, 0.0247, 0.0224, 0.0237, 0.0225,\n",
       "          0.0216, 0.0244, 0.0213, 0.0263, 0.0198, 0.0246, 0.0219, 0.0212, 0.0217,\n",
       "          0.0224, 0.0246, 0.0238, 0.0220, 0.0212, 0.0227, 0.0236, 0.0197, 0.0228,\n",
       "          0.0210, 0.0197, 0.0217, 0.0213, 0.0212, 0.0221, 0.0227, 0.0194, 0.0215,\n",
       "          0.0220, 0.0227, 0.0233, 0.0212, 0.0208, 0.0234, 0.0234, 0.0221, 0.0215,\n",
       "          0.0212, 0.0244, 0.0228, 0.0275, 0.0224, 0.0215, 0.0244, 0.0284, 0.0241,\n",
       "          0.0290, 0.0292, 0.0301, 0.0279, 0.0219, 0.0284, 0.0275, 0.0233, 0.0229,\n",
       "          0.0221, 0.0233, 0.0286, 0.0332, 0.0285, 0.0279, 0.0223, 0.0241, 0.0238,\n",
       "          0.0173, 0.0257, 0.0240, 0.0267, 0.0229, 0.0279, 0.0255, 0.0227, 0.0219,\n",
       "          0.0242, 0.0237, 0.0292, 0.0237, 0.0229, 0.0233, 0.0258, 0.0280, 0.0233,\n",
       "          0.0273, 0.0251, 0.0246, 0.0253, 0.0309, 0.0241, 0.0255, 0.0259, 0.0280,\n",
       "          0.0266, 0.0263, 0.0259, 0.0234, 0.0271, 0.0271, 0.0253, 0.0264, 0.0240,\n",
       "          0.0250, 0.0234, 0.0275, 0.0208, 0.0216, 0.0245, 0.0202, 0.0249, 0.0227,\n",
       "          0.0258, 0.0210, 0.0236, 0.0227, 0.0258, 0.0229, 0.0203, 0.0225, 0.0204,\n",
       "          0.0204, 0.0234, 0.0208, 0.0213, 0.0262, 0.0197, 0.0232, 0.0202, 0.0215,\n",
       "          0.0219, 0.0225, 0.0238, 0.0190, 0.0259, 0.0217, 0.0227, 0.0194, 0.0201,\n",
       "          0.0269, 0.0224, 0.0207, 0.0220, 0.0240, 0.0233, 0.0213, 0.0233, 0.0238,\n",
       "          0.0249, 0.0240, 0.0250, 0.0237, 0.0188, 0.0181, 0.0225, 0.0232, 0.0249,\n",
       "          0.0185, 0.0215, 0.0216, 0.0215, 0.0233, 0.0260, 0.0228, 0.0206, 0.0212,\n",
       "          0.0221, 0.0234, 0.0198, 0.0198, 0.0212, 0.0217, 0.0249, 0.0232, 0.0251,\n",
       "          0.0225, 0.0225, 0.0260, 0.0289, 0.0224, 0.0293, 0.0228, 0.0236, 0.0208,\n",
       "          0.0234, 0.0258, 0.0219, 0.0223, 0.0224, 0.0263, 0.0221, 0.0227, 0.0220,\n",
       "          0.0228, 0.0255, 0.0227, 0.0232, 0.0267, 0.0228, 0.0208, 0.0217, 0.0232,\n",
       "          0.0225, 0.0210, 0.0210, 0.0229, 0.0249, 0.0194, 0.0224, 0.0269, 0.0244,\n",
       "          0.0221, 0.0276, 0.0229, 0.0309, 0.0273, 0.0240, 0.0216, 0.0259, 0.0220,\n",
       "          0.0238, 0.0233, 0.0206, 0.0191, 0.0246, 0.0236, 0.0284, 0.0219, 0.0262,\n",
       "          0.0233, 0.0207, 0.0232, 0.0269, 0.0245, 0.0259, 0.0250, 0.0204, 0.0234,\n",
       "          0.0305, 0.0258, 0.0242, 0.0238, 0.0246, 0.0251, 0.0260, 0.0303, 0.0264,\n",
       "          0.0216, 0.0275, 0.0230, 0.0251, 0.0245, 0.0221, 0.0216, 0.0263, 0.0258,\n",
       "          0.0224, 0.0273, 0.0215, 0.0279, 0.0290, 0.0249, 0.0263, 0.0234, 0.0236,\n",
       "          0.0228, 0.0228, 0.0221, 0.0238, 0.0217, 0.0217, 0.0240, 0.0267, 0.0255,\n",
       "          0.0314, 0.0263, 0.0228, 0.0419, 0.0232, 0.0213, 0.0232, 0.0253, 0.0228,\n",
       "          0.0253, 0.0260, 0.0263, 0.0255, 0.0221, 0.0236, 0.0228, 0.0294, 0.0232,\n",
       "          0.0319, 0.0244, 0.0249, 0.0247, 0.0289, 0.0247, 0.0225, 0.0210, 0.0207,\n",
       "          0.0254, 0.0258, 0.0227, 0.0234, 0.0220, 0.0284, 0.0249, 0.0216, 0.0215,\n",
       "          0.0216, 0.0246, 0.0223, 0.0289, 0.0238, 0.0238, 0.0227, 0.0301, 0.0221,\n",
       "          0.0246, 0.0237, 0.0241, 0.0259, 0.0275, 0.0230, 0.0216, 0.0258, 0.0286,\n",
       "          0.0303, 0.0237, 0.0236, 0.0264, 0.0215, 0.0221, 0.0227, 0.0281, 0.0215,\n",
       "          0.0229, 0.0244, 0.0237, 0.0242, 0.0238, 0.0220, 0.0294, 0.0228, 0.0237,\n",
       "          0.0228, 0.0246, 0.0213, 0.0237, 0.0254, 0.0306, 0.0233, 0.0238, 0.0260,\n",
       "          0.0258, 0.0208, 0.0213, 0.0225, 0.0272, 0.0277, 0.0251, 0.0267, 0.0190,\n",
       "          0.0210, 0.0273, 0.0212, 0.0220, 0.0201, 0.0236, 0.0199, 0.0191, 0.0215,\n",
       "          0.0202, 0.0195, 0.0203, 0.0228, 0.0211, 0.0220, 0.0207, 0.0217, 0.0193,\n",
       "          0.0204, 0.0202, 0.0190, 0.0208, 0.0197, 0.0197, 0.0204, 0.0199, 0.0307,\n",
       "          0.0191, 0.0204, 0.0219, 0.0198, 0.0241, 0.0232, 0.0223, 0.0285, 0.0215,\n",
       "          0.0198, 0.0223, 0.0204, 0.0229, 0.0247, 0.0208, 0.0191, 0.0188, 0.0182,\n",
       "          0.0233, 0.0199, 0.0253, 0.0197, 0.0206, 0.0204, 0.0197, 0.0198, 0.0232,\n",
       "          0.0230, 0.0203, 0.0233, 0.0202, 0.0323, 0.0240, 0.0211, 0.0238]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.10.mlp.down_proj.qweight': tensor([[-1283033273, -1937131656, -1480956326,  ..., -1988576873,\n",
       "          -1481164699, -2036037494],\n",
       "         [ 1550296214, -1181181768, -2005571224,  ..., -2023249768,\n",
       "          -1703299115,  -929593192],\n",
       "         [-2089379672,  -898090377, -1751812218,  ..., -1769559673,\n",
       "          -1937205167,  2005510295],\n",
       "         ...,\n",
       "         [-1751603336, -1136171353,  1771534168,  ..., -2000266825,\n",
       "          -1990631080, -2039052157],\n",
       "         [ 2004441272, -1722120841, -1212704106,  ..., -1467509334,\n",
       "          -1768523318,  1452702841],\n",
       "         [-1768325733, -1700226725, -1686668666,  ..., -1682282631,\n",
       "          -2036815946, -2023066248]], dtype=torch.int32),\n",
       " 'llm.model.layers.31.self_attn.q_proj.scales': tensor([[0.0211, 0.0211, 0.0257,  ..., 0.0254, 0.0237, 0.0234]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.15.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.13.self_attn.v_proj.qweight': tensor([[ 1478924424,  2001238633,  2007598731,  ..., -1467593575,\n",
       "          -1756788615, -1988658295],\n",
       "         [-1218745446, -1987606663, -1484098182,  ...,  1466656099,\n",
       "           -966097000, -2034731592],\n",
       "         [ 1770563771, -1716942424, -1434026889,  ...,  1246284010,\n",
       "          -1720281529, -2037814675],\n",
       "         ...,\n",
       "         [-1687787399, -1681286265,  1721346199,  ..., -1162172058,\n",
       "          -1215927467,  -711562120],\n",
       "         [-2003994824, -2004337784, -2004264342,  ..., -1735951959,\n",
       "          -1467582073,  1216903306],\n",
       "         [-2054464068, -1990629271, -2005428087,  ..., -1452643721,\n",
       "          -1767212392,  1968731338]], dtype=torch.int32),\n",
       " 'llm.model.layers.40.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.19.mlp.fc2.qweight': tensor([[-1602523276,  1669341012,  1655000388,  ..., -1800493743,\n",
       "          -1520666498, -2123724949],\n",
       "         [-1988056720,  1199669848,  1770356351,  ...,  1149286253,\n",
       "          -1484690293, -1619105140],\n",
       "         [ 1904117609,  1966050629, -1905370205,  ..., -1266783937,\n",
       "           2020897381, -1923195975],\n",
       "         ...,\n",
       "         [-1854107036, -1868719248,  1551123105,  ..., -1872058733,\n",
       "           -931507018, -2107529885],\n",
       "         [ 1686796634, -1969247089,  1233545314,  ..., -1654816858,\n",
       "          -1683646049,  1972669543],\n",
       "         [ 1700362565,  2020956811, -1588238679,  ...,  1700439442,\n",
       "          -1316773764, -2024100213]], dtype=torch.int32),\n",
       " 'llm.model.layers.13.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.23.self_attn.out_proj.qweight': tensor([[-1906544518,  2088336776,  1869969812,  ...,  2009299253,\n",
       "          -1452112035, -1621912724],\n",
       "         [-1906715734, -2127008486,  -899904371,  ..., -1584114355,\n",
       "           -813663132, -1620650344],\n",
       "         [-1533189043,  1969251902, -1839637611,  ...,  1971820812,\n",
       "          -2048884338, -1320059760],\n",
       "         ...,\n",
       "         [-1752673453,  2106025869, -1183435672,  ..., -1668898703,\n",
       "          -1166053748, -1049063844],\n",
       "         [ 1667945589,  -526337136, -1987803769,  ...,   833122422,\n",
       "          -1186618475, -2106812052],\n",
       "         [ 1303073337, -1608237759, -1448638825,  ...,  1262072907,\n",
       "          -2088130911, -1601072223]], dtype=torch.int32),\n",
       " 'llm.model.layers.46.mlp.up_proj.scales': tensor([[0.0329, 0.0259, 0.0271,  ..., 0.0245, 0.0279, 0.0227]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.4.input_layernorm.weight': tensor([0.9961, 0.7188, 0.5273,  ..., 0.5547, 0.7500, 0.2832]),\n",
       " 'llm.model.layers.43.post_attention_layernorm.weight': tensor([1.0469, 1.0938, 0.9570,  ..., 0.9531, 1.2031, 0.6055]),\n",
       " 'llm.model.layers.26.mlp.down_proj.qweight': tensor([[ -928545400, -1716111254, -1199027318,  ...,  1988728200,\n",
       "          -1464308824, -2021037673],\n",
       "         [-1149728891,  1789233545,  1165731703,  ..., -2002090122,\n",
       "          -1212704839, -1736930952],\n",
       "         [ 2020246905, -2057733032, -2075489606,  ..., -1419155589,\n",
       "           2006484873, -2020964215],\n",
       "         ...,\n",
       "         [-1989499030,  1519033433,  -965061031,  ...,  1968727928,\n",
       "           1808373128, -1735878266],\n",
       "         [ 1468442647, -1434944872, -1972667748,  ..., -1986487933,\n",
       "          -1466271112, -1718970487],\n",
       "         [-2002151803, -1971807646,  1754892454,  ...,  2020071065,\n",
       "           2040182663, -1988654969]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.13.mlp.fc2.scales': tensor([[0.0010, 0.0010, 0.0011,  ..., 0.0012, 0.0010, 0.0011]]),\n",
       " 'llm.model.layers.29.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.23.post_attention_layernorm.weight': tensor([2.0000, 1.8906, 0.8516,  ..., 0.9922, 1.8438, 0.1226]),\n",
       " 'llm.model.layers.48.post_attention_layernorm.weight': tensor([1.0312, 1.0469, 1.0312,  ..., 0.9961, 1.0625, 0.7539]),\n",
       " 'llm.model.layers.18.self_attn.v_proj.qweight': tensor([[-1466526923, -1200133994, -1958331783,  ..., -2019133257,\n",
       "           -429421416,  2004316981],\n",
       "         [ 2056689801,  1502254985, -2018993740,  ..., -1735882326,\n",
       "          -2005168709,   934966390],\n",
       "         [ -343447115,  2005367206,  2035923033,  ...,  1454004877,\n",
       "          -1752603546, -1736000679],\n",
       "         ...,\n",
       "         [-1751479402,  1518971207,  1718241416,  ...,  1751619943,\n",
       "          -1499891594, -1701336679],\n",
       "         [-2004130184,  2022275193,  2005445558,  ..., -1987872618,\n",
       "           2022274957, -1987467096],\n",
       "         [ 1302890823,  1971759530, -2016900714,  ..., -1802922358,\n",
       "          -1749579321, -1465621113]], dtype=torch.int32),\n",
       " 'llm.model.layers.31.self_attn.o_proj.qweight': tensor([[-2053653879, -1952946312, -1786148248,  ..., -2005432184,\n",
       "          -1986611067, -1753843272],\n",
       "         [-1720018313, -2054698824,  1790490744,  ..., -1479829670,\n",
       "          -2003400330, -1450665832],\n",
       "         [ 1720166759, -1737852312, -1196905896,  ..., -1447250839,\n",
       "          -2001037434,  1517848791],\n",
       "         ...,\n",
       "         [-1955289495, -1463188600, -1986378087,  ...,  1774750121,\n",
       "           1990686840, -1972865381],\n",
       "         [-2003072602,  1534577033, -1721280391,  ..., -1699967913,\n",
       "           2020120967, -2023249769],\n",
       "         [-1736009606,  2036758154, -1951831653,  ..., -1451775645,\n",
       "          -1700305495, -2004187016]], dtype=torch.int32),\n",
       " 'llm.model.layers.21.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.4.mlp.gate_proj.scales': tensor([[0.0186, 0.0228, 0.0208,  ..., 0.0242, 0.0195, 0.0230]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.4.layer_norm2.weight': tensor([0.9336, 0.8047, 0.6094,  ..., 0.9609, 0.7617, 0.9414]),\n",
       " 'vpm.encoder.layers.11.self_attn.v_proj.scales': tensor([[0.0008, 0.0008, 0.0008,  ..., 0.0006, 0.0008, 0.0008]]),\n",
       " 'vpm.encoder.layers.15.mlp.fc1.bias': tensor([-0.2637, -1.1406, -0.2852,  ..., -1.0000, -0.2363, -0.5312]),\n",
       " 'llm.model.layers.24.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.37.self_attn.k_proj.qweight': tensor([[-2025350519,  2026399414,  1736996747,  ...,  1467516809,\n",
       "          -2002273382,  1774744777],\n",
       "         [-1800890538,  2088134279,  2055768233,  ..., -1938323031,\n",
       "          -1465223033,  1438095979],\n",
       "         [-1449756007, -2021025368, -1739036297,  ...,  2040092520,\n",
       "          -1770297221,  1975023175],\n",
       "         ...,\n",
       "         [-1184720727, -2018870392, -2004052360,  ..., -1215731032,\n",
       "           1520027509, -1451723624],\n",
       "         [-2006414694, -2003257207, -2055768201,  ..., -2040035175,\n",
       "          -1500149640,  2024286375],\n",
       "         [-1435002439, -2020050521,  1938196875,  ...,  2023192421,\n",
       "          -2023184711, -2007464600]], dtype=torch.int32),\n",
       " 'llm.model.layers.30.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.41.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.41.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.11.self_attn.q_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.28.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.34.self_attn.v_proj.scales': tensor([[0.0237, 0.0268, 0.0294, 0.0233, 0.0297, 0.0213, 0.0254, 0.0234, 0.0212,\n",
       "          0.0267, 0.0245, 0.0208, 0.0266, 0.0250, 0.0211, 0.0232, 0.0229, 0.0249,\n",
       "          0.0238, 0.0283, 0.0215, 0.0262, 0.0221, 0.0227, 0.0210, 0.0244, 0.0241,\n",
       "          0.0232, 0.0264, 0.0250, 0.0233, 0.0255, 0.0229, 0.0216, 0.0326, 0.0277,\n",
       "          0.0262, 0.0273, 0.0216, 0.0166, 0.0294, 0.0271, 0.0262, 0.0286, 0.0303,\n",
       "          0.0217, 0.0237, 0.0234, 0.0208, 0.0229, 0.0213, 0.0236, 0.0247, 0.0255,\n",
       "          0.0242, 0.0251, 0.0237, 0.0228, 0.0182, 0.0230, 0.0251, 0.0257, 0.0289,\n",
       "          0.0194, 0.0208, 0.0202, 0.0267, 0.0233, 0.0212, 0.0264, 0.0161, 0.0215,\n",
       "          0.0208, 0.0211, 0.0216, 0.0237, 0.0234, 0.0250, 0.0207, 0.0228, 0.0208,\n",
       "          0.0267, 0.0211, 0.0215, 0.0223, 0.0253, 0.0203, 0.0267, 0.0237, 0.0244,\n",
       "          0.0216, 0.0234, 0.0201, 0.0219, 0.0227, 0.0253, 0.0201, 0.0228, 0.0254,\n",
       "          0.0236, 0.0215, 0.0246, 0.0232, 0.0223, 0.0240, 0.0234, 0.0212, 0.0271,\n",
       "          0.0234, 0.0237, 0.0203, 0.0204, 0.0211, 0.0232, 0.0237, 0.0217, 0.0213,\n",
       "          0.0228, 0.0206, 0.0233, 0.0296, 0.0195, 0.0237, 0.0236, 0.0230, 0.0216,\n",
       "          0.0273, 0.0241, 0.0249, 0.0246, 0.0228, 0.0315, 0.0217, 0.0227, 0.0241,\n",
       "          0.0242, 0.0215, 0.0249, 0.0224, 0.0233, 0.0236, 0.0229, 0.0254, 0.0273,\n",
       "          0.0260, 0.0257, 0.0241, 0.0249, 0.0215, 0.0228, 0.0263, 0.0225, 0.0267,\n",
       "          0.0229, 0.0225, 0.0230, 0.0217, 0.0204, 0.0207, 0.0242, 0.0244, 0.0251,\n",
       "          0.0223, 0.0254, 0.0284, 0.0251, 0.0266, 0.0263, 0.0233, 0.0236, 0.0262,\n",
       "          0.0242, 0.0275, 0.0228, 0.0236, 0.0229, 0.0230, 0.0246, 0.0230, 0.0221,\n",
       "          0.0215, 0.0264, 0.0233, 0.0225, 0.0254, 0.0228, 0.0225, 0.0236, 0.0245,\n",
       "          0.0215, 0.0257, 0.0216, 0.0238, 0.0245, 0.0275, 0.0238, 0.0260, 0.0247,\n",
       "          0.0246, 0.0288, 0.0251, 0.0323, 0.0250, 0.0259, 0.0233, 0.0236, 0.0284,\n",
       "          0.0264, 0.0253, 0.0298, 0.0257, 0.0257, 0.0285, 0.0228, 0.0269, 0.0230,\n",
       "          0.0311, 0.0303, 0.0257, 0.0242, 0.0286, 0.0267, 0.0262, 0.0285, 0.0269,\n",
       "          0.0258, 0.0281, 0.0269, 0.0245, 0.0247, 0.0253, 0.0233, 0.0302, 0.0244,\n",
       "          0.0273, 0.0255, 0.0349, 0.0245, 0.0251, 0.0285, 0.0254, 0.0245, 0.0240,\n",
       "          0.0268, 0.0266, 0.0259, 0.0259, 0.0255, 0.0255, 0.0269, 0.0285, 0.0232,\n",
       "          0.0260, 0.0288, 0.0276, 0.0276, 0.0234, 0.0206, 0.0199, 0.0217, 0.0236,\n",
       "          0.0283, 0.0249, 0.0227, 0.0221, 0.0195, 0.0208, 0.0199, 0.0221, 0.0276,\n",
       "          0.0204, 0.0207, 0.0250, 0.0216, 0.0244, 0.0254, 0.0207, 0.0225, 0.0240,\n",
       "          0.0227, 0.0254, 0.0269, 0.0229, 0.0202, 0.0247, 0.0228, 0.0238, 0.0234,\n",
       "          0.0233, 0.0230, 0.0217, 0.0238, 0.0233, 0.0223, 0.0206, 0.0195, 0.0294,\n",
       "          0.0267, 0.0211, 0.0217, 0.0202, 0.0191, 0.0237, 0.0244, 0.0213, 0.0221,\n",
       "          0.0229, 0.0225, 0.0215, 0.0208, 0.0217, 0.0230, 0.0186, 0.0202, 0.0211,\n",
       "          0.0212, 0.0219, 0.0199, 0.0234, 0.0197, 0.0224, 0.0277, 0.0253, 0.0224,\n",
       "          0.0262, 0.0225, 0.0238, 0.0213, 0.0194, 0.0294, 0.0229, 0.0238, 0.0298,\n",
       "          0.0207, 0.0245, 0.0333, 0.0267, 0.0288, 0.0230, 0.0250, 0.0224, 0.0233,\n",
       "          0.0269, 0.0237, 0.0219, 0.0250, 0.0238, 0.0208, 0.0245, 0.0255, 0.0250,\n",
       "          0.0219, 0.0264, 0.0272, 0.0276, 0.0232, 0.0259, 0.0223, 0.0249, 0.0251,\n",
       "          0.0257, 0.0217, 0.0212, 0.0298, 0.0260, 0.0255, 0.0267, 0.0221, 0.0241,\n",
       "          0.0244, 0.0238, 0.0244, 0.0250, 0.0302, 0.0211, 0.0204, 0.0247, 0.0314,\n",
       "          0.0329, 0.0292, 0.0220, 0.0236, 0.0303, 0.0242, 0.0241, 0.0264, 0.0294,\n",
       "          0.0271, 0.0242, 0.0242, 0.0275, 0.0228, 0.0221, 0.0267, 0.0238, 0.0263,\n",
       "          0.0260, 0.0237, 0.0247, 0.0238, 0.0221, 0.0240, 0.0292, 0.0234, 0.0303,\n",
       "          0.0255, 0.0268, 0.0238, 0.0259, 0.0280, 0.0244, 0.0229, 0.0249, 0.0251,\n",
       "          0.0250, 0.0254, 0.0241, 0.0251, 0.0232, 0.0263, 0.0251, 0.0250, 0.0216,\n",
       "          0.0246, 0.0246, 0.0234, 0.0275, 0.0326, 0.0234, 0.0242, 0.0253, 0.0260,\n",
       "          0.0237, 0.0297, 0.0253, 0.0249, 0.0255, 0.0245, 0.0272, 0.0284, 0.0281,\n",
       "          0.0232, 0.0247, 0.0349, 0.0253, 0.0273, 0.0254, 0.0229, 0.0210, 0.0224,\n",
       "          0.0207, 0.0213, 0.0221, 0.0189, 0.0211, 0.0177, 0.0230, 0.0227, 0.0178,\n",
       "          0.0201, 0.0233, 0.0211, 0.0168, 0.0199, 0.0203, 0.0221, 0.0203, 0.0191,\n",
       "          0.0225, 0.0207, 0.0162, 0.0198, 0.0201, 0.0189, 0.0201, 0.0213, 0.0207,\n",
       "          0.0206, 0.0225, 0.0202, 0.0199, 0.0272, 0.0169, 0.0213, 0.0232, 0.0215,\n",
       "          0.0255, 0.0204, 0.0176, 0.0202, 0.0211, 0.0177, 0.0182, 0.0190, 0.0228,\n",
       "          0.0202, 0.0204, 0.0279, 0.0180, 0.0236, 0.0194, 0.0202, 0.0212, 0.0279,\n",
       "          0.0193, 0.0188, 0.0188, 0.0190, 0.0220, 0.0215, 0.0297, 0.0229]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.37.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.17.self_attn.v_proj.qweight': tensor([[-1803576934, -1921164628,  1537777033,  ..., -1938133121,\n",
       "           1334083963,  1822244252],\n",
       "         [ 1619243571,  1784100239,  1938781792,  ..., -1753713538,\n",
       "          -1753239443,   712875320],\n",
       "         [-2107205545, -2054999702, -1837597589,  ...,  1834582433,\n",
       "          -1956274339,  1887332536],\n",
       "         ...,\n",
       "         [-2121880400,  1566741644,  2058238579,  ...,  1179304548,\n",
       "          -1586529376, -1548061049],\n",
       "         [-1982306453,  1687386292,  1869843120,  ..., -1971030916,\n",
       "            797133730,  1786941085],\n",
       "         [ 1550086595, -1936099681, -1082033556,  ..., -1737666461,\n",
       "           1505138253, -1566545036]], dtype=torch.int32),\n",
       " 'llm.model.layers.17.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.4.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.20.layer_norm1.bias': tensor([ 0.2773,  0.2832,  0.2949,  ..., -0.0723, -0.1445,  0.0688]),\n",
       " 'vpm.encoder.layers.26.self_attn.v_proj.bias': tensor([-0.2256,  0.3223,  0.2598,  ...,  0.2734,  0.1982, -0.1406]),\n",
       " 'llm.model.layers.21.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.30.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.1.self_attn.k_proj.qweight': tensor([[ 2022148470, -1247323541, -1721203046,  ...,  1486391945,\n",
       "          -1469605961,  2002091449],\n",
       "         [ 1804110522,  2041092246,  1704434873,  ..., -2038658956,\n",
       "           2058008441, -1484609400],\n",
       "         [-1201059448, -1768389765, -1702204538,  ..., -1200137849,\n",
       "          -1499835015, -1431795511],\n",
       "         ...,\n",
       "         [ 1517774727,  2006489722, -2007660730,  ...,  1805241494,\n",
       "           1971026360,  1804061127],\n",
       "         [ 2005371016, -1200060281,   396785031,  ...,  2023065543,\n",
       "           1753970521, -1433962087],\n",
       "         [-1734846585, -1483113577, -1936361876,  ..., -1617468299,\n",
       "          -1662542423,  2021042664]], dtype=torch.int32),\n",
       " 'llm.model.layers.10.self_attn.q_proj.qweight': tensor([[ 1502038906, -1498830730,  1450731132,  ..., -1717135223,\n",
       "           2022144631, -2037810293],\n",
       "         [-2041157978, -1756849448, -1790544469,  ...,  1502180740,\n",
       "           1516734855,  2053473431],\n",
       "         [-1498838617, -1686468245, -1164416892,  ...,  2043119958,\n",
       "           2025298295, -1503167834],\n",
       "         ...,\n",
       "         [ 1970845303, -1473022293, -2006333304,  ..., -1969919657,\n",
       "          -1736873864,  2006223243],\n",
       "         [-2021021302, -1517650885, -1954981256,  ..., -1752667394,\n",
       "           1989904294, -1450612087],\n",
       "         [-1449626967,  2056762535,  2025207433,  ..., -1736873813,\n",
       "           2056620455, -1990825065]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.12.self_attn.k_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.17.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.9.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.8.self_attn.v_proj.qweight': tensor([[ 1535601322,  1484420664,  1970832522,  ...,  2036894326,\n",
       "          -1211722872,  1482398104],\n",
       "         [ 1967618181, -1485269557, -1967490982,  ..., -1770416040,\n",
       "           1774684853, -2022340245],\n",
       "         [ -716666262,  1453945737, -2039055443,  ..., -1500915239,\n",
       "          -1484286039,  2068281480],\n",
       "         ...,\n",
       "         [ -645376323, -2007332409, -1704555914,  ..., -1449621624,\n",
       "          -1769494390, -1703385735],\n",
       "         [ 2038732681, -1736157129,  -963929494,  ...,  1216559702,\n",
       "           1737070663, -1450722377],\n",
       "         [ 1765370519,  1986476456, -1670076535,  ...,  2072476007,\n",
       "          -1736065911, -1749644917]], dtype=torch.int32),\n",
       " 'llm.model.layers.1.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.45.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.24.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.15.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.11.self_attn.v_proj.scales': tensor([[0.0204, 0.0190, 0.0199, 0.0217, 0.0221, 0.0228, 0.0188, 0.0203, 0.0228,\n",
       "          0.0194, 0.0242, 0.0195, 0.0191, 0.0225, 0.0215, 0.0219, 0.0206, 0.0189,\n",
       "          0.0249, 0.0203, 0.0213, 0.0206, 0.0188, 0.0219, 0.0215, 0.0233, 0.0219,\n",
       "          0.0221, 0.0211, 0.0191, 0.0220, 0.0193, 0.0232, 0.0198, 0.0212, 0.0195,\n",
       "          0.0210, 0.0206, 0.0246, 0.0207, 0.0225, 0.0191, 0.0263, 0.0246, 0.0221,\n",
       "          0.0198, 0.0221, 0.0233, 0.0206, 0.0202, 0.0212, 0.0228, 0.0221, 0.0223,\n",
       "          0.0229, 0.0193, 0.0227, 0.0195, 0.0220, 0.0224, 0.0246, 0.0210, 0.0232,\n",
       "          0.0225, 0.0228, 0.0199, 0.0225, 0.0241, 0.0220, 0.0225, 0.0181, 0.0208,\n",
       "          0.0221, 0.0225, 0.0216, 0.0211, 0.0201, 0.0255, 0.0221, 0.0210, 0.0219,\n",
       "          0.0228, 0.0263, 0.0236, 0.0191, 0.0230, 0.0190, 0.0213, 0.0213, 0.0204,\n",
       "          0.0212, 0.0225, 0.0197, 0.0207, 0.0225, 0.0204, 0.0215, 0.0189, 0.0195,\n",
       "          0.0203, 0.0227, 0.0268, 0.0271, 0.0194, 0.0211, 0.0215, 0.0253, 0.0230,\n",
       "          0.0240, 0.0217, 0.0182, 0.0202, 0.0176, 0.0230, 0.0206, 0.0228, 0.0178,\n",
       "          0.0197, 0.0203, 0.0211, 0.0199, 0.0203, 0.0207, 0.0193, 0.0188, 0.0184,\n",
       "          0.0199, 0.0227, 0.0234, 0.0229, 0.0266, 0.0225, 0.0294, 0.0288, 0.0232,\n",
       "          0.0212, 0.0204, 0.0257, 0.0296, 0.0247, 0.0232, 0.0284, 0.0211, 0.0240,\n",
       "          0.0203, 0.0238, 0.0262, 0.0251, 0.0236, 0.0241, 0.0258, 0.0280, 0.0244,\n",
       "          0.0242, 0.0225, 0.0227, 0.0257, 0.0233, 0.0269, 0.0208, 0.0211, 0.0237,\n",
       "          0.0300, 0.0305, 0.0280, 0.0271, 0.0275, 0.0217, 0.0207, 0.0277, 0.0221,\n",
       "          0.0249, 0.0260, 0.0216, 0.0281, 0.0273, 0.0249, 0.0297, 0.0268, 0.0220,\n",
       "          0.0263, 0.0241, 0.0199, 0.0258, 0.0224, 0.0237, 0.0250, 0.0225, 0.0253,\n",
       "          0.0219, 0.0229, 0.0213, 0.0266, 0.0216, 0.0223, 0.0225, 0.0271, 0.0277,\n",
       "          0.0190, 0.0247, 0.0245, 0.0194, 0.0190, 0.0244, 0.0215, 0.0201, 0.0245,\n",
       "          0.0250, 0.0199, 0.0219, 0.0188, 0.0198, 0.0198, 0.0233, 0.0237, 0.0258,\n",
       "          0.0241, 0.0210, 0.0197, 0.0189, 0.0189, 0.0230, 0.0229, 0.0275, 0.0237,\n",
       "          0.0233, 0.0219, 0.0220, 0.0190, 0.0241, 0.0249, 0.0229, 0.0203, 0.0189,\n",
       "          0.0210, 0.0216, 0.0215, 0.0220, 0.0229, 0.0212, 0.0229, 0.0220, 0.0197,\n",
       "          0.0215, 0.0202, 0.0223, 0.0241, 0.0271, 0.0249, 0.0215, 0.0221, 0.0203,\n",
       "          0.0215, 0.0216, 0.0224, 0.0193, 0.0223, 0.0258, 0.0254, 0.0217, 0.0247,\n",
       "          0.0259, 0.0263, 0.0221, 0.0208, 0.0233, 0.0212, 0.0221, 0.0220, 0.0212,\n",
       "          0.0229, 0.0251, 0.0215, 0.0220, 0.0211, 0.0250, 0.0225, 0.0224, 0.0253,\n",
       "          0.0215, 0.0215, 0.0241, 0.0211, 0.0217, 0.0216, 0.0225, 0.0250, 0.0201,\n",
       "          0.0236, 0.0216, 0.0210, 0.0210, 0.0238, 0.0207, 0.0249, 0.0195, 0.0238,\n",
       "          0.0238, 0.0240, 0.0238, 0.0220, 0.0236, 0.0240, 0.0250, 0.0257, 0.0251,\n",
       "          0.0259, 0.0230, 0.0234, 0.0233, 0.0240, 0.0211, 0.0260, 0.0220, 0.0238,\n",
       "          0.0246, 0.0244, 0.0232, 0.0224, 0.0238, 0.0255, 0.0229, 0.0272, 0.0284,\n",
       "          0.0280, 0.0217, 0.0221, 0.0250, 0.0238, 0.0259, 0.0207, 0.0234, 0.0245,\n",
       "          0.0242, 0.0276, 0.0327, 0.0249, 0.0250, 0.0228, 0.0266, 0.0249, 0.0268,\n",
       "          0.0213, 0.0217, 0.0262, 0.0280, 0.0217, 0.0211, 0.0241, 0.0262, 0.0255,\n",
       "          0.0206, 0.0251, 0.0272, 0.0206, 0.0251, 0.0260, 0.0232, 0.0267, 0.0223,\n",
       "          0.0228, 0.0259, 0.0249, 0.0294, 0.0305, 0.0251, 0.0224, 0.0215, 0.0258,\n",
       "          0.0229, 0.0277, 0.0251, 0.0215, 0.0267, 0.0253, 0.0271, 0.0215, 0.0309,\n",
       "          0.0250, 0.0249, 0.0322, 0.0232, 0.0220, 0.0253, 0.0210, 0.0211, 0.0188,\n",
       "          0.0203, 0.0193, 0.0212, 0.0181, 0.0190, 0.0227, 0.0199, 0.0181, 0.0210,\n",
       "          0.0240, 0.0208, 0.0212, 0.0189, 0.0191, 0.0227, 0.0189, 0.0204, 0.0182,\n",
       "          0.0189, 0.0194, 0.0210, 0.0213, 0.0188, 0.0199, 0.0232, 0.0311, 0.0201,\n",
       "          0.0220, 0.0199, 0.0177, 0.0181, 0.0172, 0.0197, 0.0212, 0.0202, 0.0286,\n",
       "          0.0206, 0.0216, 0.0199, 0.0206, 0.0168, 0.0213, 0.0204, 0.0193, 0.0194,\n",
       "          0.0194, 0.0202, 0.0233, 0.0206, 0.0189, 0.0199, 0.0264, 0.0207, 0.0188,\n",
       "          0.0199, 0.0184, 0.0399, 0.0197, 0.0198, 0.0204, 0.0215, 0.0273, 0.0276,\n",
       "          0.0269, 0.0263, 0.0246, 0.0264, 0.0242, 0.0272, 0.0247, 0.0267, 0.0260,\n",
       "          0.0297, 0.0233, 0.0255, 0.0290, 0.0234, 0.0306, 0.0251, 0.0266, 0.0262,\n",
       "          0.0258, 0.0240, 0.0249, 0.0305, 0.0245, 0.0246, 0.0289, 0.0245, 0.0203,\n",
       "          0.0293, 0.0264, 0.0219, 0.0241, 0.0271, 0.0263, 0.0320, 0.0260, 0.0277,\n",
       "          0.0251, 0.0275, 0.0240, 0.0266, 0.0277, 0.0271, 0.0286, 0.0249, 0.0229,\n",
       "          0.0272, 0.0324, 0.0271, 0.0257, 0.0257, 0.0230, 0.0244, 0.0241, 0.0269,\n",
       "          0.0310, 0.0300, 0.0257, 0.0254, 0.0258, 0.0257, 0.0236, 0.0280]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.49.mlp.up_proj.scales': tensor([[0.0245, 0.0245, 0.0292,  ..., 0.0228, 0.0305, 0.0249]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.28.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.17.mlp.down_proj.scales': tensor([[0.0254, 0.0247, 0.0266,  ..., 0.0268, 0.0237, 0.0471]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.32.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.30.post_attention_layernorm.weight': tensor([1.7422, 1.7734, 0.8203,  ..., 0.9414, 1.9375, 0.1187]),\n",
       " 'llm.model.layers.1.self_attn.q_proj.scales': tensor([[0.0147, 0.0139, 0.0144,  ..., 0.0198, 0.0185, 0.0189]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.51.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.29.mlp.up_proj.scales': tensor([[0.0258, 0.0242, 0.0263,  ..., 0.0257, 0.0263, 0.0301]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.30.self_attn.v_proj.qweight': tensor([[ 1986755703, -2003339639,  2036950652,  ...,   915966069,\n",
       "          -1217939613,  1733855115],\n",
       "         [-2065136216, -1734829450, -1717024693,  ..., -1227326553,\n",
       "          -1751807544,  1987414678],\n",
       "         [-1166521973, -2018982007,  1487452551,  ..., -1734903721,\n",
       "           2053675621,  1737008794],\n",
       "         ...,\n",
       "         [ 2006419642, -1131976057,  1984530601,  ..., -2006475862,\n",
       "          -1766270344, -2007525241],\n",
       "         [ 1518966358,  2022147993,  1485350537,  ..., -2003065161,\n",
       "           1485343627,  1770316665],\n",
       "         [-1484086186, -1451907159,  1234803320,  ..., -2021148294,\n",
       "          -1988576647, -1650103686]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.16.mlp.fc2.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.21.post_attention_layernorm.weight': tensor([1.7734, 1.7344, 0.8320,  ..., 0.9531, 1.6641, 0.1240]),\n",
       " 'vpm.encoder.layers.17.self_attn.v_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.25.mlp.fc1.qweight': tensor([[-1823693694, -2002934721, -1889688208,  ..., -1900433838,\n",
       "          -1567400567,  1549237810],\n",
       "         [ 2135462553, -1787599527,  1732288151,  ..., -1467308178,\n",
       "          -1805343860,  1590323906],\n",
       "         [-1752344431, -2055889568, -2054979671,  ..., -1747926697,\n",
       "           1959757424, -2070772845],\n",
       "         ...,\n",
       "         [-2101971084, -2055429244, -1499946351,  ..., -1347200125,\n",
       "          -1519825016, -1264677793],\n",
       "         [-1514650016,   833129822,  1653754969,  ...,  1402249118,\n",
       "           2053274221,  1852923258],\n",
       "         [ 1533180064, -1870424945,  1363903200,  ...,  1904449734,\n",
       "          -1265533314,  1537889373]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.23.mlp.fc1.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.7.layer_norm1.bias': tensor([-0.0420, -0.0277,  0.0630,  ..., -0.0615, -0.0036,  0.0330]),\n",
       " 'llm.model.layers.43.self_attn.o_proj.scales': tensor([[0.0293, 0.0297, 0.0277,  ..., 0.0285, 0.0266, 0.0326]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.18.mlp.down_proj.scales': tensor([[0.0267, 0.0230, 0.0240,  ..., 0.0220, 0.0232, 0.0341]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.30.mlp.down_proj.qweight': tensor([[-1767224918,  1770752359,  1991804842,  ..., -1486185836,\n",
       "          -2006353306, -1463252600],\n",
       "         [ 1820764314, -1709734039,  1971759001,  ...,  2024237450,\n",
       "          -1719175304, -1751611000],\n",
       "         [-1754773469, -1969649015,  2022025097,  ..., -1703302772,\n",
       "          -1937332072, -1734834057],\n",
       "         ...,\n",
       "         [ 2037950599, -1517713783,  1787321224,  ..., -1501074533,\n",
       "          -1466398073, -1986557560],\n",
       "         [-1985439605, -1466393992, -1737005688,  ..., -1214740155,\n",
       "          -1734903368,  1989703560],\n",
       "         [ 1772525944, -2006287721, -2037953910,  ..., -1704552072,\n",
       "          -1450866791, -2037868407]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.24.self_attn.out_proj.bias': tensor([ 0.2715,  0.0149, -0.1592,  ...,  0.0181,  0.0996,  0.0153]),\n",
       " 'vpm.encoder.layers.6.mlp.fc1.qweight': tensor([[-2122077314,  1031169382,  1232624467,  ...,  2123920725,\n",
       "          -1684830794, -1436316473],\n",
       "         [ -967406768, -1901042823,  2138152546,  ...,  -979013267,\n",
       "          -1666679911, -1600893846],\n",
       "         [-1907801461,  2020906142, -1970771838,  ...,  1636805732,\n",
       "          -2004845696,  1949541775],\n",
       "         ...,\n",
       "         [ 1588880250, -1751428256, -1687400857,  ..., -2069584481,\n",
       "          -2120518558, -1923857027],\n",
       "         [ 1252754558,   966624401,  2019518350,  ...,  1771209410,\n",
       "          -2040489082, -1568947574],\n",
       "         [-1573678217, -2054001806,  1034309243,  ...,  1600756635,\n",
       "           1556070079,  1234217879]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.0.self_attn.v_proj.bias': tensor([ 0.0291, -0.0143,  0.0075,  ..., -0.0114,  0.0019,  0.0030]),\n",
       " 'llm.model.layers.24.self_attn.q_proj.qweight': tensor([[ 1987609976, -1985508473, -1733916827,  ..., -1394181447,\n",
       "          -1467583175,  2038921080],\n",
       "         [-1803253639, -1733527947,  1204455607,  ...,  2001365399,\n",
       "          -1972795001, -1984456362],\n",
       "         [-1250273978, -2022160214,  2005502362,  ..., -1738038151,\n",
       "           1732748681, -1937222533],\n",
       "         ...,\n",
       "         [-2035840918,  1785226884, -2022131034,  ...,  2007524536,\n",
       "          -1486665863, -1719998106],\n",
       "         [ 1502124391, -1196009562,  1955022792,  ..., -2038000344,\n",
       "          -1720351066, -1971743835],\n",
       "         [ 1445304695, -1680382281,  1756911752,  ...,  2004461206,\n",
       "           1822009706,  2007464075]], dtype=torch.int32),\n",
       " 'llm.model.layers.7.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.15.self_attn.v_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.5.self_attn.k_proj.scales': tensor([[0.0173, 0.0198, 0.0186, 0.0189, 0.0212, 0.0197, 0.0225, 0.0217, 0.0152,\n",
       "          0.0177, 0.0172, 0.0176, 0.0206, 0.0185, 0.0220, 0.0172, 0.0185, 0.0230,\n",
       "          0.0207, 0.0247, 0.0225, 0.0208, 0.0263, 0.0211, 0.0189, 0.0249, 0.0242,\n",
       "          0.0234, 0.0257, 0.0198, 0.0286, 0.0224, 0.0210, 0.0169, 0.0204, 0.0263,\n",
       "          0.0182, 0.0177, 0.0203, 0.0160, 0.0181, 0.0197, 0.0207, 0.0229, 0.0201,\n",
       "          0.0251, 0.0216, 0.0176, 0.0242, 0.0177, 0.0185, 0.0191, 0.0219, 0.0255,\n",
       "          0.0249, 0.0219, 0.0380, 0.0206, 0.0290, 0.0240, 0.0241, 0.0176, 0.0285,\n",
       "          0.0275, 0.0185, 0.0149, 0.0176, 0.0283, 0.0208, 0.0217, 0.0232, 0.0272,\n",
       "          0.0217, 0.0190, 0.0254, 0.0285, 0.0161, 0.0201, 0.0346, 0.0193, 0.0191,\n",
       "          0.0254, 0.0238, 0.0229, 0.0241, 0.0155, 0.0230, 0.0246, 0.0302, 0.0828,\n",
       "          0.0245, 0.0432, 0.0316, 0.0216, 0.0297, 0.0492, 0.0182, 0.0172, 0.0174,\n",
       "          0.0128, 0.0139, 0.0272, 0.0178, 0.0314, 0.0210, 0.0232, 0.0198, 0.0215,\n",
       "          0.0212, 0.0204, 0.0341, 0.0238, 0.0212, 0.0310, 0.0241, 0.0245, 0.0297,\n",
       "          0.0432, 0.0292, 0.0349, 0.0240, 0.0269, 0.0254, 0.0249, 0.0284, 0.0298,\n",
       "          0.0375, 0.0284, 0.0176, 0.0169, 0.0133, 0.0160, 0.0158, 0.0158, 0.0186,\n",
       "          0.0184, 0.0130, 0.0177, 0.0206, 0.0202, 0.0233, 0.0172, 0.0182, 0.0296,\n",
       "          0.0201, 0.0197, 0.0177, 0.0245, 0.0422, 0.0365, 0.0272, 0.0288, 0.0682,\n",
       "          0.0464, 0.0260, 0.0338, 0.0391, 0.0293, 0.0310, 0.0327, 0.0161, 0.0182,\n",
       "          0.0160, 0.0195, 0.0180, 0.0164, 0.0154, 0.0184, 0.0225, 0.0169, 0.0193,\n",
       "          0.0161, 0.0156, 0.0201, 0.0165, 0.0141, 0.0238, 0.0217, 0.0203, 0.0234,\n",
       "          0.0719, 0.0479, 0.0352, 0.0630, 0.0464, 0.0771, 0.0288, 0.0327, 0.0323,\n",
       "          0.0354, 0.0609, 0.0357, 0.0150, 0.0141, 0.0167, 0.0141, 0.0185, 0.0185,\n",
       "          0.0238, 0.0230, 0.0181, 0.0195, 0.0241, 0.0185, 0.0236, 0.0186, 0.0272,\n",
       "          0.0232, 0.0232, 0.0263, 0.0254, 0.0346, 0.0234, 0.0240, 0.0352, 0.0251,\n",
       "          0.0724, 0.0370, 0.0286, 0.0427, 0.0378, 0.0272, 0.0294, 0.0372, 0.0163,\n",
       "          0.0143, 0.0137, 0.0167, 0.0158, 0.0255, 0.0195, 0.0260, 0.0207, 0.0314,\n",
       "          0.0236, 0.0202, 0.0215, 0.0238, 0.0191, 0.0181, 0.0300, 0.0253, 0.0202,\n",
       "          0.0327, 0.0249, 0.0271, 0.0263, 0.0241, 0.0469, 0.0599, 0.0296, 0.0329,\n",
       "          0.0388, 0.0380, 0.0354, 0.0781, 0.0147, 0.0206, 0.0241, 0.0155, 0.0294,\n",
       "          0.0262, 0.0202, 0.0234, 0.0227, 0.0241, 0.0228, 0.0316, 0.0245, 0.0280,\n",
       "          0.0285, 0.0263, 0.0244, 0.0242, 0.0240, 0.0359, 0.0224, 0.0227, 0.0464,\n",
       "          0.0234, 0.0257, 0.1042, 0.0257, 0.0202, 0.0296, 0.0450, 0.0238, 0.0352,\n",
       "          0.0372, 0.0173, 0.0227, 0.0173, 0.0269, 0.0188, 0.0184, 0.0285, 0.0293,\n",
       "          0.0203, 0.0283, 0.0203, 0.0236, 0.0272, 0.0450, 0.0237, 0.0207, 0.0223,\n",
       "          0.0262, 0.0503, 0.0255, 0.0208, 0.0255, 0.0266, 0.0306, 0.0341, 0.0310,\n",
       "          0.0327, 0.0328, 0.0328, 0.0262, 0.0324, 0.0203, 0.0201, 0.0202, 0.0207,\n",
       "          0.0230, 0.0174, 0.0272, 0.0197, 0.0268, 0.0259, 0.0225, 0.0233, 0.0272,\n",
       "          0.0266, 0.0250, 0.0251, 0.0284, 0.0234, 0.0260, 0.0294, 0.0238, 0.0247,\n",
       "          0.0280, 0.0284, 0.0503, 0.0305, 0.0266, 0.0273, 0.0333, 0.0247, 0.0258,\n",
       "          0.0294, 0.0185, 0.0272, 0.0158, 0.0190, 0.0236, 0.0198, 0.0242, 0.0217,\n",
       "          0.0227, 0.0215, 0.0241, 0.0249, 0.0225, 0.0233, 0.0233, 0.0227, 0.0298,\n",
       "          0.0281, 0.0281, 0.0244, 0.0253, 0.0244, 0.0237, 0.0228, 0.0526, 0.0244,\n",
       "          0.0250, 0.0238, 0.0249, 0.0237, 0.0207, 0.0273, 0.0186, 0.0263, 0.0208,\n",
       "          0.0229, 0.0272, 0.0210, 0.0219, 0.0216, 0.0233, 0.0324, 0.0240, 0.0262,\n",
       "          0.0211, 0.0262, 0.0310, 0.0255, 0.0286, 0.0238, 0.0280, 0.0272, 0.0230,\n",
       "          0.0284, 0.0292, 0.0254, 0.0258, 0.0314, 0.0482, 0.0284, 0.0286, 0.0332,\n",
       "          0.0573, 0.0338, 0.0168, 0.0250, 0.0223, 0.0199, 0.0258, 0.0198, 0.0232,\n",
       "          0.0220, 0.0178, 0.0216, 0.0296, 0.0245, 0.0202, 0.0234, 0.0250, 0.0246,\n",
       "          0.0237, 0.0306, 0.0262, 0.0440, 0.0267, 0.0246, 0.0298, 0.0241, 0.0264,\n",
       "          0.0854, 0.0250, 0.0246, 0.0411, 0.0300, 0.0292, 0.0635, 0.0190, 0.0271,\n",
       "          0.0190, 0.0298, 0.0177, 0.0244, 0.0242, 0.0225, 0.0232, 0.0250, 0.0267,\n",
       "          0.0207, 0.0322, 0.0219, 0.0268, 0.0233, 0.0254, 0.0254, 0.0215, 0.0267,\n",
       "          0.0267, 0.0236, 0.0275, 0.0269, 0.0521, 0.0552, 0.0250, 0.0281, 0.0258,\n",
       "          0.0267, 0.0237, 0.0283, 0.0220, 0.0185, 0.0173, 0.0257, 0.0212, 0.0271,\n",
       "          0.0254, 0.0269, 0.0219, 0.0233, 0.0323, 0.0250, 0.0276, 0.0257, 0.0258,\n",
       "          0.0344, 0.0336, 0.0257, 0.0240, 0.0275, 0.0280, 0.0453, 0.0292, 0.0259,\n",
       "          0.0560, 0.0306, 0.0251, 0.0277, 0.0269, 0.0288, 0.0272, 0.0251]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.11.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.12.mlp.gate_proj.scales': tensor([[0.0262, 0.0227, 0.0238,  ..., 0.0217, 0.0206, 0.0247]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.22.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.18.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.21.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.0.self_attn.v_proj.qweight': tensor([[-2054646104,   997774968, -1736997016,  ..., -1433885892,\n",
       "          -1704424573,  1465353933],\n",
       "         [-1987397272, -1770342007,  2009700776,  ...,   428517513,\n",
       "          -1485281127, -1684567914],\n",
       "         [ 2057934990,  1753639542,  1773779097,  ...,  -711341415,\n",
       "           2007456134, -1920436664],\n",
       "         ...,\n",
       "         [-1683446153, -1669879656,  2090309765,  ...,  2004199570,\n",
       "          -2020108693,  1705740150],\n",
       "         [ 2055637462, -1702122584,  2005092696,  ..., -1736017798,\n",
       "          -1752730249, -1232556936],\n",
       "         [ 2024294777, -1449370777,  2001188728,  ..., -1748396137,\n",
       "           1769518505, -1246259785]], dtype=torch.int32),\n",
       " 'llm.model.layers.3.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.31.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.34.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.39.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.14.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.28.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.18.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.47.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.50.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.16.mlp.fc2.bias': tensor([-0.0708, -0.0008,  0.1030,  ..., -0.1885,  0.1328, -0.1167]),\n",
       " 'llm.model.layers.50.self_attn.q_proj.qweight': tensor([[-2005538407,  1753839767,  1971759741,  ...,  2022217848,\n",
       "          -1734571384,  1754834298],\n",
       "         [-1194747189, -2069398904, -1719163493,  ..., -1770473352,\n",
       "          -1753659003, -1468413817],\n",
       "         [-1264093079, -1738046377, -1201177946,  ..., -1469548665,\n",
       "          -1719101062,  1517779608],\n",
       "         ...,\n",
       "         [ 1418380984,  2005240185,  2007475368,  ..., -1735821177,\n",
       "           2022275447, -2056739479],\n",
       "         [-1705736296, -1196000919,  2051716440,  ...,  1767343991,\n",
       "          -1736922746, -1742182518],\n",
       "         [ 2038785911, -1515484564, -2003470232,  ..., -2004252810,\n",
       "          -1231521640,  1737001336]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.14.layer_norm2.weight': tensor([0.8477, 1.1953, 1.2031,  ..., 0.9883, 1.1406, 1.1562]),\n",
       " 'vpm.encoder.layers.1.layer_norm2.bias': tensor([0.0004, 0.0625, 0.1396,  ..., 0.1050, 0.0500, 0.0781]),\n",
       " 'vpm.encoder.layers.20.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.8.self_attn.v_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.29.self_attn.q_proj.qweight': tensor([[-1432779143, -1954056090,  2006476684,  ..., -2041009817,\n",
       "          -1737971334,  2007537831],\n",
       "         [ 1786345128, -1482320006, -1567003784,  ...,  -914847639,\n",
       "          -2018894984, -1986742407],\n",
       "         [-1485019242,  2037943113, -1685550197,  ...,  -910322573,\n",
       "           1790552233, -1971807849],\n",
       "         ...,\n",
       "         [-1284138885,  1770510490, -1450547561,  ..., -1986693272,\n",
       "          -1705544071,  1986435704],\n",
       "         [-1735877802, -1231517813,  1736136618,  ...,  1486533816,\n",
       "           2038929289, -1451787627],\n",
       "         [-2006533482, -1484167014, -2024442424,  ...,  2070308969,\n",
       "           2002418331, -1482913383]], dtype=torch.int32),\n",
       " 'llm.model.layers.8.self_attn.o_proj.scales': tensor([[0.0286, 0.0269, 0.0216,  ..., 0.0224, 0.0227, 0.0160]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.25.self_attn.out_proj.qweight': tensor([[ 1619305056,  1438355785, -2022148223,  ..., -1438414435,\n",
       "           1771590286,  1950520398],\n",
       "         [-2106884234,  1885944154, -1431867511,  ...,  1903011667,\n",
       "           1621319787,  1670344537],\n",
       "         [ 1369336973, -1552558973, -1668978051,  ...,  1871873159,\n",
       "          -1569023392,  -878563976],\n",
       "         ...,\n",
       "         [-1967679601, -1650093183, -1740794547,  ...,  1616998725,\n",
       "          -1821603743, -1835435906],\n",
       "         [-1953723553, -1689364850, -1366057851,  ...,  1702067665,\n",
       "           2073857401,  1179669876],\n",
       "         [ 1285271649,  1892331127,  1717138814,  ...,  2039568785,\n",
       "          -1758438316, -1965519780]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.11.mlp.fc2.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.1.self_attn.out_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.16.self_attn.q_proj.qweight': tensor([[  358387582,  1989839250,  2125490338,  ..., -1618202742,\n",
       "           1351113594,  1771463005],\n",
       "         [-2103215200, -2039393147,  1717273472,  ...,   928705142,\n",
       "           1532455333,  1437564311],\n",
       "         [-1080793029, -1836612515, -1587112067,  ...,  1991407475,\n",
       "           1969580922,   282421658],\n",
       "         ...,\n",
       "         [ 1416336814, -1619564375, -2089648311,  ...,  1903663308,\n",
       "          -1937680479,  1378460291],\n",
       "         [ 1886741380,  1870367893,  1987027364,  ..., -1786339710,\n",
       "           1655413110,  1570939002],\n",
       "         [-1718375776,  1568300388, -1921093789,  ...,  2070721654,\n",
       "          -1988194142,   850236529]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.10.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.8.self_attn.q_proj.qweight': tensor([[-2024105355,  2005502092, -1234863944,  ..., -2023376534,\n",
       "            996775836, -1150784325],\n",
       "         [-2007529549, -1800762727,  2004379337,  ..., -1463117924,\n",
       "          -1985374566, -1451714171],\n",
       "         [ 1754748534, -1967560040,  1505195655,  ...,  1770301638,\n",
       "          -2020975977, -1919190840],\n",
       "         ...,\n",
       "         [ 1502247002,  2006824857, -2073454407,  ...,  1985591668,\n",
       "           1802139002, -1718184761],\n",
       "         [-1767287144, -2022074182, -1482082428,  ..., -1787270517,\n",
       "           -845772710, -2072352858],\n",
       "         [ 1521117878, -2004384392, -1451779147,  ..., -2018866586,\n",
       "            -90941307, -1151038566]], dtype=torch.int32),\n",
       " 'llm.model.layers.1.self_attn.q_proj.qweight': tensor([[ 1684630411,  -747059288,  1721206650,  ..., -1753729673,\n",
       "          -1734903111, -1933010791],\n",
       "         [-1431803721,  2006423219,  1722312618,  ...,  2021033816,\n",
       "          -1751816583,  2004388007],\n",
       "         [ 2022218618, -1416083026, -1737963688,  ..., -2052610185,\n",
       "          -2023127142, -1432905881],\n",
       "         ...,\n",
       "         [-1716213880, -1200183993, -1787210632,  ..., -1467570329,\n",
       "           2038999175,  1469610088],\n",
       "         [ 2022074806,  2038855556, -1183144073,  ..., -1467373672,\n",
       "          -1484293751, -1214604970],\n",
       "         [-2071361414, -1421244795, -1467382121,  ..., -1199024313,\n",
       "           1487156870, -1193969270]], dtype=torch.int32),\n",
       " 'llm.model.layers.50.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.37.post_attention_layernorm.weight': tensor([1.3750, 1.3906, 0.9258,  ..., 1.0156, 1.5469, 0.1953]),\n",
       " 'vpm.encoder.layers.9.self_attn.q_proj.qweight': tensor([[ 1920947062,  1369137799, -2125157498,  ..., -1213561947,\n",
       "           1317432497, -2084229504],\n",
       "         [ -229140584,  1499771263, -1498517385,  ..., -2104056894,\n",
       "           1838639457, -1587123554],\n",
       "         [-1905883730,  1620998032,  1200508298,  ...,  1759023538,\n",
       "          -1751148697,  2108463253],\n",
       "         ...,\n",
       "         [ 1653311870,  2057227442,  1764515933,  ...,  1586464885,\n",
       "          -1501459104, -2091813044],\n",
       "         [ 1803371482, -1531611519, -2004711775,  ..., -2021027677,\n",
       "          -1650030726,  2004381587],\n",
       "         [ 1857005428,  2103690649, -2104930651,  ..., -1500488347,\n",
       "           1567791059,  1584612229]], dtype=torch.int32),\n",
       " 'llm.model.layers.18.mlp.up_proj.qweight': tensor([[-1989634165,  -427394725, -1472759385,  ..., -1470670936,\n",
       "           2003338103,  1216964921],\n",
       "         [-1099265415, -1684436102,  2008521848,  ..., -1165498740,\n",
       "           1213761161, -1435333781],\n",
       "         [ 1704565384, -1450617206,  2041149559,  ..., -2058778470,\n",
       "           1446410326,  1266132616],\n",
       "         ...,\n",
       "         [-1952933225,  1819977879,  1669888377,  ...,  2070436279,\n",
       "          -1213709688,  -397960821],\n",
       "         [ 1753646998,  1183488123, -1720223306,  ...,  2005370229,\n",
       "          -1718183257, -2004514684],\n",
       "         [  945133175, -1719163591,  1803200372,  ..., -2022209160,\n",
       "          -2006537847, -2009815238]], dtype=torch.int32),\n",
       " 'llm.model.layers.40.mlp.down_proj.scales': tensor([[0.0309, 0.0311, 0.0269,  ..., 0.0246, 0.0245, 0.0424]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.17.input_layernorm.weight': tensor([1.5391, 1.3438, 0.9102,  ..., 0.9766, 1.2734, 0.2520]),\n",
       " 'vpm.encoder.layers.15.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.45.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.9.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.21.self_attn.q_proj.bias': tensor([-0.0486, -0.0249, -0.0569,  ...,  0.0503, -0.0603,  0.0757]),\n",
       " 'vpm.encoder.layers.5.self_attn.out_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.18.layer_norm1.weight': tensor([1.0625, 1.0547, 1.0547,  ..., 1.0469, 1.0312, 1.0859]),\n",
       " 'llm.model.layers.29.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.22.self_attn.v_proj.qweight': tensor([[ 2071567491, -2008453248,  1823423337,  ..., -1918007126,\n",
       "           1836543325,  1633570177],\n",
       "         [-1855685549,  2070386590,  1990095994,  ...,  1178627748,\n",
       "           1028557688,  1569493575],\n",
       "         [-1649242533,  1836599943,  1733796237,  ..., -2120311733,\n",
       "          -1704432003,  1382709351],\n",
       "         ...,\n",
       "         [-1820693638,  1970700168, -2086781080,  ...,  1081642629,\n",
       "           2107088792,  1589292949],\n",
       "         [  176916595, -1147711390,  1668469103,  ...,  1935109494,\n",
       "           1888265831, -1856853860],\n",
       "         [-1550280032, -1548843186, -2122683536,  ..., -2006088599,\n",
       "          -1571780694,  1770090874]], dtype=torch.int32),\n",
       " 'llm.model.layers.36.input_layernorm.weight': tensor([2.2969, 2.0938, 0.8945,  ..., 1.0469, 2.3906, 0.1934]),\n",
       " 'llm.model.layers.45.input_layernorm.weight': tensor([0.8320, 0.7383, 0.7383,  ..., 0.6758, 0.8789, 1.6094]),\n",
       " 'llm.model.layers.29.mlp.down_proj.qweight': tensor([[-2023135353, -2037020533, -2005248135,  ..., -1884915879,\n",
       "           1537770917,  1986369899],\n",
       "         [ 2003531622, -1470593110, -1738115175,  ...,  1718987403,\n",
       "           1199995256, -1468368987],\n",
       "         [  644057943,  1755599736, -1737906329,  ..., -1215788892,\n",
       "          -1166566840,  2039908554],\n",
       "         ...,\n",
       "         [-2020968531,  1767336089,  1722120265,  ...,  1738193047,\n",
       "          -1970828901,  1504355720],\n",
       "         [-1467517080,  1449633709,  1470728873,  ...,  2025486761,\n",
       "           -407083097, -2036826777],\n",
       "         [-1184470375, -1532389509,  2023073708,  ..., -1924622211,\n",
       "           2019072632, -1769437557]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.18.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.6.mlp.fc2.qweight': tensor([[ 2091732047,  1117350781,  1551468136,  ..., -2036828536,\n",
       "           1702082444, -2072022715],\n",
       "         [ 1918470529,  1671335299, -1487517010,  ..., -1685096574,\n",
       "          -1973852579,  2054135943],\n",
       "         [ 1923640206,  1402706559, -2107544235,  ..., -1385069460,\n",
       "          -1416566898,  1869512072],\n",
       "         ...,\n",
       "         [ 1333099919,  1836364409, -2056815984,  ..., -1967819392,\n",
       "           1703185529,  1602702946],\n",
       "         [ 1970641760, -1235380631, -1883599970,  ...,  1769367174,\n",
       "          -1485230445, -2053280642],\n",
       "         [-1686681136, -2074311059,  2006362220,  ...,  2021360275,\n",
       "           2036886469,  1605144748]], dtype=torch.int32),\n",
       " 'llm.model.layers.25.mlp.gate_proj.qweight': tensor([[-2006476167,  1735752071,  2025367177,  ...,  1787394169,\n",
       "          -1484108923, -1452828505],\n",
       "         [-1734817640, -1434745193, -1720072055,  ...,  2086246756,\n",
       "          -1247233879,  1968863064],\n",
       "         [-2020042873, -1465293417,  2039048330,  ..., -2003276888,\n",
       "           1955042954,  2019977543],\n",
       "         ...,\n",
       "         [-1717852024, -1770575481, -1162377320,  ..., -1484281993,\n",
       "          -1651869646,  1488820662],\n",
       "         [ 1787271272, -1230599991, -1733740904,  ..., -1918338983,\n",
       "          -1717020251,  1771468390],\n",
       "         [ 2039187576,  1503037783, -1214740105,  ..., -1233421943,\n",
       "           1769498168, -1183471210]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.11.self_attn.out_proj.bias': tensor([-0.1572, -0.2832,  0.1670,  ...,  0.0688, -0.1270,  0.2852]),\n",
       " 'llm.model.layers.35.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.4.self_attn.q_proj.scales': tensor([[0.0432, 0.0241, 0.0297,  ..., 0.0221, 0.0319, 0.0285]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.24.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.33.mlp.gate_proj.qweight': tensor([[-1956948344, -1467447643, -1969977176,  ...,  1217951881,\n",
       "          -1719957848, -2005362807],\n",
       "         [ 2006669433,  1772665273, -2039118214,  ..., -2035574654,\n",
       "           1532590215, -2037938057],\n",
       "         [-1182169251, -1182169492, -1178092614,  ...,  2057971832,\n",
       "          -1752660072, -1987475048],\n",
       "         ...,\n",
       "         [-2005244327,  2026343050, -1987266869,  ...,  2002349258,\n",
       "          -1750497674, -2002147209],\n",
       "         [-2003462042, -1483049850,  1971627895,  ...,  1806276247,\n",
       "          -2003130007, -2021230456],\n",
       "         [  933730154,  2024249990,  -663378311,  ..., -2003076711,\n",
       "          -2003208297, -1468499816]], dtype=torch.int32),\n",
       " 'llm.model.layers.47.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.14.self_attn.out_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.42.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.14.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.26.self_attn.out_proj.scales': tensor([[0.0009, 0.0009, 0.0009,  ..., 0.0009, 0.0008, 0.0008]]),\n",
       " 'llm.model.layers.0.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.9.mlp.down_proj.qweight': tensor([[ 2057860456,  1806136936, -1482987386,  ...,  2054575482,\n",
       "          -1484146583, -1737984359],\n",
       "         [ 2038934229, -1941342070,  1824090009,  ..., -1401410150,\n",
       "           2040027767, -2037876904],\n",
       "         [-1199867526,  2003547766, -1969903197,  ..., -1702475898,\n",
       "           1452898181, -1498847145],\n",
       "         ...,\n",
       "         [ -928352857, -1432926055, -1921279926,  ..., -1497859916,\n",
       "          -1665746839,  2038856088],\n",
       "         [-1723447465,  2021096072, -2054568135,  ...,  1433823112,\n",
       "          -1468365192, -1417103470],\n",
       "         [-1483114805, -1482065530, -2039970935,  ...,  1503037815,\n",
       "           2004511306, -1987487878]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.11.mlp.fc2.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.1.layer_norm2.weight': tensor([1.0625, 0.4551, 0.3770,  ..., 0.8711, 0.0630, 0.3496]),\n",
       " 'llm.model.layers.20.self_attn.q_proj.scales': tensor([[0.0203, 0.0224, 0.0219,  ..., 0.0244, 0.0199, 0.0238]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.6.mlp.gate_proj.qweight': tensor([[ 2090363527,  -965064855,  2041075299,  ..., -2022152040,\n",
       "          -1199073898, -1733654924],\n",
       "         [ 1419283672,  1267214731, -1671784345,  ..., -2021226103,\n",
       "          -1990952823, -1198090905],\n",
       "         [-1968399750,  1751491962,  2019916106,  ...,  2006490025,\n",
       "          -2069329816, -1720280921],\n",
       "         ...,\n",
       "         [ 1185376900,  1986313626,  1803200952,  ..., -1768392808,\n",
       "          -2070250619, -1818658695],\n",
       "         [ 1975094168,  2024115117,  1774012566,  ..., -1733720169,\n",
       "           1955907143, -1781838647],\n",
       "         [-1451711303, -1717975978,  2056829593,  ..., -1988659319,\n",
       "          -1985435478, -1706522473]], dtype=torch.int32),\n",
       " 'llm.model.layers.14.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.49.self_attn.o_proj.scales': tensor([[0.0283, 0.0298, 0.0272,  ..., 0.0268, 0.0289, 0.0578]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.23.layer_norm1.weight': tensor([1.3203, 1.2422, 1.3281,  ..., 1.2266, 1.2656, 1.3594]),\n",
       " 'vpm.encoder.layers.17.self_attn.q_proj.scales': tensor([[0.0008, 0.0008, 0.0007,  ..., 0.0009, 0.0008, 0.0008]]),\n",
       " 'llm.model.layers.15.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.33.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.31.post_attention_layernorm.weight': tensor([1.6562, 1.7188, 0.7539,  ..., 0.8828, 1.9375, 0.1064]),\n",
       " 'llm.model.layers.50.mlp.up_proj.qweight': tensor([[-2003314052, -1984263784,  1755883914,  ...,  1483315545,\n",
       "           1769768616,  1468714410],\n",
       "         [ -656898153,  2019194714,  1504274054,  ..., -1684433320,\n",
       "           1705613719,  1708693190],\n",
       "         [ 1671981413, -1756838248, -1734903910,  ..., -1732676456,\n",
       "           1768511910, -1938182532],\n",
       "         ...,\n",
       "         [-1047881284,  1934174136, -1721407109,  ..., -2040034951,\n",
       "          -2004319350, -1453492089],\n",
       "         [-1452519976, -1763075655, -1752528520,  ..., -2004260709,\n",
       "           2021230216, -1702259386],\n",
       "         [-1231513988,  2040043704,  1770429017,  ..., -1752725142,\n",
       "          -1231574438, -1717856358]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.20.self_attn.v_proj.qweight': tensor([[-1669173604, -1935704252, -1584871806,  ...,  1986437807,\n",
       "           1318873439,  1571843754],\n",
       "         [ 1534488664, -2119138375,  1400410292,  ..., -2034727038,\n",
       "           2069209205, -1719942023],\n",
       "         [-1818074252,  2037417800,  2100281194,  ...,  1920451937,\n",
       "           2021819992,  1839251541],\n",
       "         ...,\n",
       "         [ 1754763179,  1666682231,  1533443204,  ..., -1200778114,\n",
       "          -2001307823, -1786538907],\n",
       "         [ -696023928,  1320978012,  1752591001,  ..., -1821608871,\n",
       "           1921667513,  -828156814],\n",
       "         [-1754686603, -1870893956,  1297468539,  ...,  2120299412,\n",
       "           1044806534,  1718253652]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.11.self_attn.k_proj.qweight': tensor([[-1852016254,  2123603644,  1742383247,  ..., -2006346109,\n",
       "          -2052044145,  1953502081],\n",
       "         [-1684430407,  1921872483, -1382258055,  ...,  1973912968,\n",
       "           -377782724,  1732478025],\n",
       "         [ 1419873880, -1973314928,  1852544869,  ...,  1669820780,\n",
       "           1751757056, -1866804380],\n",
       "         ...,\n",
       "         [-2023703108, -1886219588,  1518365294,  ...,  1786931330,\n",
       "           1304138119,   663713211],\n",
       "         [-1980388470,  1866033030, -1903184009,  ..., -2055121801,\n",
       "           1163173053,  1035917194],\n",
       "         [ 1303869786, -1466278503, -1200390789,  ...,  1938523760,\n",
       "           -596558710,  1939238582]], dtype=torch.int32),\n",
       " 'llm.model.layers.35.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.14.self_attn.o_proj.qweight': tensor([[-1983358793, -1482069643, -1785218937,  ..., -1482245161,\n",
       "          -1732655238,  -658798695],\n",
       "         [ 2018939546,  2020386437,  1773774998,  ..., -1453008558,\n",
       "          -1484085418,   914781799],\n",
       "         [-1197057661, -1451710294, -2041040228,  ..., -1740998841,\n",
       "          -1768588617,  -980707381],\n",
       "         ...,\n",
       "         [-1968875076, -1248347784,  2023135095,  ..., -1703429241,\n",
       "          -1754687098, -2003269274],\n",
       "         [ 1739237772,  2023192998,  1470858922,  ..., -1450670458,\n",
       "          -2022266506,  1768446071],\n",
       "         [-1418112075,  1739036809,  2057873768,  ...,  1989707686,\n",
       "           1772640072, -1752790104]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.2.mlp.fc1.qweight': tensor([[-1953138338, -1986430316, -1872382352,  ...,  1705358421,\n",
       "           2089390426,  1871220590],\n",
       "         [ -933655981,  2072150641,  2056617329,  ...,  1081450162,\n",
       "           1899985794,  1532461887],\n",
       "         [ 1904052107,  2021491612, -1935572068,  ..., -1785954724,\n",
       "           2037733185,  2139518321],\n",
       "         ...,\n",
       "         [ 2105388335,  2104391295,  1436790378,  ..., -2104125048,\n",
       "           2044104011,  1907467906],\n",
       "         [ 1686345069,  1804895103, -2040699523,  ..., -2140308656,\n",
       "           2104329599,  1519819420],\n",
       "         [-1718188187,  1871798932,  1972146571,  ...,  1772542710,\n",
       "          -1432121704, -1016120179]], dtype=torch.int32),\n",
       " 'llm.model.layers.30.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.23.mlp.fc1.qweight': tensor([[ 1984650922,  1466792239, -2020697490,  ...,  2124640126,\n",
       "          -2055252142,  1939315333],\n",
       "         [ 1498782098,  1447589797,  1755017615,  ...,  1515681199,\n",
       "          -1984595643,  1720747632],\n",
       "         [-1637508989, -1701219599,  1804430765,  ..., -2088794266,\n",
       "           1735686223,  1214540408],\n",
       "         ...,\n",
       "         [ 2055701629, -2138408808,  1880392635,  ...,  1600553091,\n",
       "           1331193718, -2058647681],\n",
       "         [ 1937089669, -1657314480, -1567125380,  ...,  2006744691,\n",
       "           2127058020, -1817750417],\n",
       "         [ 2088796531,  1667589228,  1614321046,  ...,  1989060966,\n",
       "           1519088522, -1834787969]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.22.self_attn.v_proj.scales': tensor([[0.0008, 0.0010, 0.0009,  ..., 0.0010, 0.0011, 0.0009]]),\n",
       " 'vpm.encoder.layers.3.self_attn.k_proj.bias': tensor([-0.6211,  0.8125, -0.7539,  ..., -0.5195, -1.0000, -1.5234]),\n",
       " 'vpm.encoder.layers.7.self_attn.k_proj.bias': tensor([-0.9375,  0.2695,  0.9766,  ...,  0.0349, -0.4375, -0.0508]),\n",
       " 'llm.model.layers.16.input_layernorm.weight': tensor([1.5312, 1.2500, 0.9453,  ..., 1.0391, 1.3203, 0.2217]),\n",
       " 'llm.model.layers.16.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.3.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.34.self_attn.o_proj.qweight': tensor([[-1721268101,  1799993208, -1985378985,  ...,  1717994390,\n",
       "          -1467312410,  2072602428],\n",
       "         [-1511500133, -2018994008,  -710576791,  ...,  1231464569,\n",
       "           2039899529, -2054752071],\n",
       "         [ -713451098, -1699900249,  1217948808,  ...,  1181338231,\n",
       "          -1952995448,  2042342245],\n",
       "         ...,\n",
       "         [-2057664949,  2037811032, -2025351292,  ...,  2024302996,\n",
       "          -1768310407, -1485212264],\n",
       "         [ 1435987077, -2001107272,  1985448567,  ..., -1720084070,\n",
       "           1589028935,  1987422151],\n",
       "         [-2036827495, -1435874665, -1419205191,  ..., -1920435847,\n",
       "           2007468216, -2053531769]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.14.self_attn.k_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.24.mlp.fc2.qweight': tensor([[-1938323574, -2001961853,  1586724484,  ..., -2104856885,\n",
       "          -1368753805,  1518566456],\n",
       "         [-1999801481,   784309642,  1265478496,  ...,  -794642815,\n",
       "           1619761827, -1988067477],\n",
       "         [ 1907321975, -1912061049, -2121696687,  ...,  2023981428,\n",
       "           2002360197,  1266404228],\n",
       "         ...,\n",
       "         [ 2006024321, -1652981627,  1601208437,  ..., -1806401433,\n",
       "          -1919582094, -1654488966],\n",
       "         [-1535595665,  -695240622, -2068023400,  ...,  1990376307,\n",
       "           2093380159,  1489285242],\n",
       "         [ 1402175931,  1233091473, -1701607550,  ...,  2123136409,\n",
       "           1903404953, -1438290898]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.0.mlp.fc2.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.0.self_attn.q_proj.scales': tensor([[0.0143, 0.0230, 0.0181,  ..., 0.0234, 0.0230, 0.0260]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.45.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.14.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.11.mlp.down_proj.scales': tensor([[0.0269, 0.0228, 0.0237,  ..., 0.0230, 0.0251, 0.0314]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.48.self_attn.q_proj.qweight': tensor([[ 2004400521, -1717991063, -2005362039,  ...,  2019068504,\n",
       "           1182374493, -2006349175],\n",
       "         [-2024172184,  1688775337,  1486120809,  ..., -1767228806,\n",
       "          -1803843702, -1215862871],\n",
       "         [ 1480116167,  2041088198,  2019142807,  ..., -1737909879,\n",
       "          -1752794778, -1720146025],\n",
       "         ...,\n",
       "         [ 1716893239,  2004461431, -1194875512,  ...,  1721321593,\n",
       "           1481090696, -1734830744],\n",
       "         [ 2039076953, -1465284760, -1450662264,  ...,  1971619510,\n",
       "          -1705547659,  2063108230],\n",
       "         [ 2043373943, -2070443114, -1770432806,  ...,  1785169788,\n",
       "          -2005497479, -2072397671]], dtype=torch.int32),\n",
       " 'llm.model.layers.32.mlp.up_proj.scales': tensor([[0.0250, 0.0257, 0.0302,  ..., 0.0432, 0.0241, 0.0232]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.26.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.35.self_attn.v_proj.scales': tensor([[0.0250, 0.0257, 0.0259, 0.0232, 0.0260, 0.0262, 0.0245, 0.0237, 0.0253,\n",
       "          0.0233, 0.0240, 0.0216, 0.0262, 0.0316, 0.0258, 0.0319, 0.0227, 0.0272,\n",
       "          0.0244, 0.0254, 0.0217, 0.0288, 0.0260, 0.0228, 0.0241, 0.0357, 0.0213,\n",
       "          0.0240, 0.0242, 0.0263, 0.0286, 0.0246, 0.0260, 0.0258, 0.0297, 0.0246,\n",
       "          0.0247, 0.0220, 0.0242, 0.0266, 0.0237, 0.0281, 0.0303, 0.0280, 0.0217,\n",
       "          0.0307, 0.0225, 0.0238, 0.0234, 0.0249, 0.0309, 0.0247, 0.0319, 0.0182,\n",
       "          0.0279, 0.0257, 0.0262, 0.0250, 0.0220, 0.0294, 0.0241, 0.0263, 0.0230,\n",
       "          0.0206, 0.0210, 0.0203, 0.0194, 0.0224, 0.0194, 0.0277, 0.0201, 0.0220,\n",
       "          0.0250, 0.0223, 0.0240, 0.0220, 0.0229, 0.0215, 0.0223, 0.0203, 0.0210,\n",
       "          0.0193, 0.0184, 0.0197, 0.0245, 0.0195, 0.0204, 0.0240, 0.0238, 0.0204,\n",
       "          0.0210, 0.0213, 0.0203, 0.0242, 0.0216, 0.0219, 0.0198, 0.0267, 0.0213,\n",
       "          0.0241, 0.0210, 0.0230, 0.0203, 0.0241, 0.0241, 0.0220, 0.0216, 0.0234,\n",
       "          0.0221, 0.0199, 0.0233, 0.0204, 0.0207, 0.0228, 0.0227, 0.0199, 0.0194,\n",
       "          0.0207, 0.0229, 0.0250, 0.0191, 0.0189, 0.0210, 0.0215, 0.0249, 0.0229,\n",
       "          0.0224, 0.0269, 0.0234, 0.0241, 0.0288, 0.0266, 0.0263, 0.0275, 0.0238,\n",
       "          0.0272, 0.0232, 0.0250, 0.0236, 0.0246, 0.0268, 0.0306, 0.0258, 0.0232,\n",
       "          0.0267, 0.0238, 0.0244, 0.0250, 0.0246, 0.0289, 0.0294, 0.0245, 0.0234,\n",
       "          0.0234, 0.0286, 0.0238, 0.0230, 0.0293, 0.0249, 0.0242, 0.0219, 0.0238,\n",
       "          0.0300, 0.0285, 0.0227, 0.0237, 0.0276, 0.0260, 0.0286, 0.0215, 0.0251,\n",
       "          0.0216, 0.0259, 0.0258, 0.0262, 0.0204, 0.0234, 0.0262, 0.0297, 0.0257,\n",
       "          0.0268, 0.0284, 0.0219, 0.0223, 0.0290, 0.0210, 0.0257, 0.0253, 0.0254,\n",
       "          0.0257, 0.0286, 0.0230, 0.0232, 0.0326, 0.0221, 0.0251, 0.0185, 0.0249,\n",
       "          0.0411, 0.0212, 0.0288, 0.0233, 0.0227, 0.0286, 0.0202, 0.0246, 0.0249,\n",
       "          0.0207, 0.0202, 0.0290, 0.0212, 0.0234, 0.0201, 0.0336, 0.0336, 0.0240,\n",
       "          0.0269, 0.0221, 0.0237, 0.0271, 0.0207, 0.0236, 0.0309, 0.0422, 0.0207,\n",
       "          0.0273, 0.0255, 0.0225, 0.0359, 0.0267, 0.0193, 0.0450, 0.0255, 0.0194,\n",
       "          0.0314, 0.0213, 0.0203, 0.0190, 0.0324, 0.0286, 0.0259, 0.0213, 0.0259,\n",
       "          0.0316, 0.0217, 0.0220, 0.0259, 0.0456, 0.0316, 0.0212, 0.0228, 0.0280,\n",
       "          0.0219, 0.0229, 0.0204, 0.0280, 0.0254, 0.0268, 0.0246, 0.0244, 0.0284,\n",
       "          0.0286, 0.0276, 0.0275, 0.0289, 0.0284, 0.0245, 0.0255, 0.0244, 0.0241,\n",
       "          0.0230, 0.0301, 0.0245, 0.0276, 0.0324, 0.0277, 0.0286, 0.0285, 0.0276,\n",
       "          0.0280, 0.0225, 0.0303, 0.0290, 0.0257, 0.0267, 0.0247, 0.0257, 0.0255,\n",
       "          0.0309, 0.0246, 0.0352, 0.0283, 0.0255, 0.0258, 0.0258, 0.0302, 0.0296,\n",
       "          0.0268, 0.0269, 0.0271, 0.0333, 0.0288, 0.0264, 0.0254, 0.0228, 0.0399,\n",
       "          0.0277, 0.0271, 0.0267, 0.0307, 0.0285, 0.0257, 0.0247, 0.0277, 0.0268,\n",
       "          0.0204, 0.0229, 0.0303, 0.0245, 0.0301, 0.0230, 0.0233, 0.0249, 0.0324,\n",
       "          0.0285, 0.0276, 0.0223, 0.0266, 0.0253, 0.0277, 0.0266, 0.0272, 0.0336,\n",
       "          0.0257, 0.0228, 0.0297, 0.0249, 0.0219, 0.0285, 0.0251, 0.0250, 0.0275,\n",
       "          0.0257, 0.0273, 0.0286, 0.0260, 0.0424, 0.0267, 0.0324, 0.0327, 0.0284,\n",
       "          0.0230, 0.0286, 0.0271, 0.0341, 0.0246, 0.0283, 0.0262, 0.0268, 0.0311,\n",
       "          0.0237, 0.0227, 0.0244, 0.0241, 0.0277, 0.0267, 0.0267, 0.0255, 0.0264,\n",
       "          0.0281, 0.0309, 0.0281, 0.0266, 0.0236, 0.0242, 0.0266, 0.0260, 0.0242,\n",
       "          0.0277, 0.0249, 0.0245, 0.0246, 0.0251, 0.0247, 0.0263, 0.0220, 0.0266,\n",
       "          0.0186, 0.0223, 0.0215, 0.0224, 0.0224, 0.0267, 0.0294, 0.0232, 0.0240,\n",
       "          0.0215, 0.0237, 0.0219, 0.0215, 0.0249, 0.0281, 0.0197, 0.0246, 0.0224,\n",
       "          0.0219, 0.0233, 0.0227, 0.0198, 0.0197, 0.0289, 0.0210, 0.0228, 0.0221,\n",
       "          0.0224, 0.0264, 0.0267, 0.0232, 0.0253, 0.0247, 0.0223, 0.0255, 0.0212,\n",
       "          0.0234, 0.0201, 0.0215, 0.0238, 0.0220, 0.0216, 0.0294, 0.0250, 0.0220,\n",
       "          0.0236, 0.0240, 0.0247, 0.0221, 0.0198, 0.0234, 0.0223, 0.0236, 0.0247,\n",
       "          0.0307, 0.0247, 0.0277, 0.0220, 0.0227, 0.0217, 0.0211, 0.0171, 0.0165,\n",
       "          0.0199, 0.0199, 0.0190, 0.0176, 0.0197, 0.0197, 0.0208, 0.0190, 0.0237,\n",
       "          0.0223, 0.0219, 0.0177, 0.0215, 0.0197, 0.0197, 0.0181, 0.0213, 0.0194,\n",
       "          0.0197, 0.0181, 0.0176, 0.0189, 0.0177, 0.0213, 0.0184, 0.0186, 0.0216,\n",
       "          0.0228, 0.0182, 0.0190, 0.0207, 0.0197, 0.0194, 0.0180, 0.0223, 0.0180,\n",
       "          0.0217, 0.0217, 0.0191, 0.0236, 0.0190, 0.0167, 0.0163, 0.0182, 0.0176,\n",
       "          0.0174, 0.0206, 0.0203, 0.0212, 0.0193, 0.0217, 0.0186, 0.0188, 0.0186,\n",
       "          0.0206, 0.0181, 0.0210, 0.0190, 0.0211, 0.0215, 0.0186, 0.0190]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.37.mlp.gate_proj.scales': tensor([[0.0238, 0.0269, 0.0210,  ..., 0.0215, 0.0247, 0.0232]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.21.self_attn.out_proj.scales': tensor([[0.0007, 0.0008, 0.0008,  ..., 0.0008, 0.0007, 0.0008]]),\n",
       " 'vpm.encoder.layers.9.layer_norm1.weight': tensor([0.7031, 0.7656, 0.8672,  ..., 0.5898, 0.6367, 0.7148]),\n",
       " 'llm.model.layers.8.mlp.gate_proj.qweight': tensor([[-1987409304, -1984521897, -1752725621,  ...,  1770575511,\n",
       "          -2040236182, -2033927001],\n",
       "         [ 2074909335,  1993116053, -1448753495,  ...,  2036844940,\n",
       "           1786148726, -1770481559],\n",
       "         [-1986688884,  1517799000,  -948533402,  ..., -2040099930,\n",
       "          -2019841145, -1251371114],\n",
       "         ...,\n",
       "         [ 2008381865,  1150912649,  2022017403,  ..., -1196910695,\n",
       "          -1770337898, -1958241956],\n",
       "         [-1935120455, -1767475319,  2042119801,  ..., -1180071850,\n",
       "          -1716880501,  2024368761],\n",
       "         [-1263167637, -1985307816, -1737069944,  ...,  1702353257,\n",
       "           1755081595,  1986567065]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.25.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.42.self_attn.o_proj.scales': tensor([[0.0255, 0.0237, 0.0279,  ..., 0.0279, 0.0234, 0.0165]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.6.self_attn.o_proj.scales': tensor([[0.0275, 0.0251, 0.0201,  ..., 0.0212, 0.0229, 0.0249]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.16.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.7.self_attn.k_proj.scales': tensor([[0.0123, 0.0121, 0.0185, 0.0129, 0.0232, 0.0140, 0.0171, 0.0137, 0.0223,\n",
       "          0.0130, 0.0227, 0.0197, 0.0257, 0.0251, 0.0198, 0.0191, 0.0246, 0.0180,\n",
       "          0.0286, 0.0229, 0.0236, 0.0244, 0.0198, 0.0242, 0.0750, 0.0289, 0.0241,\n",
       "          0.0254, 0.0309, 0.0406, 0.0290, 0.0314, 0.0141, 0.0172, 0.0190, 0.0133,\n",
       "          0.0191, 0.0172, 0.0244, 0.0154, 0.0173, 0.0176, 0.0217, 0.0238, 0.0204,\n",
       "          0.0207, 0.0301, 0.0177, 0.0259, 0.0236, 0.0229, 0.0338, 0.0202, 0.0176,\n",
       "          0.0230, 0.0264, 0.0497, 0.0202, 0.0241, 0.0237, 0.0268, 0.0458, 0.0223,\n",
       "          0.0233, 0.0229, 0.0195, 0.0198, 0.0236, 0.0233, 0.0236, 0.0269, 0.0207,\n",
       "          0.0241, 0.0171, 0.0228, 0.0269, 0.0215, 0.0223, 0.0241, 0.0234, 0.0257,\n",
       "          0.0258, 0.0346, 0.0246, 0.0323, 0.0262, 0.0297, 0.0263, 0.0294, 0.0521,\n",
       "          0.0250, 0.0344, 0.0223, 0.0249, 0.0283, 0.0372, 0.0208, 0.0255, 0.0272,\n",
       "          0.0280, 0.0234, 0.0240, 0.0221, 0.0229, 0.0238, 0.0240, 0.0264, 0.0264,\n",
       "          0.0216, 0.0323, 0.0198, 0.0296, 0.0305, 0.0259, 0.0257, 0.0238, 0.0315,\n",
       "          0.0346, 0.0262, 0.0300, 0.0271, 0.0391, 0.0344, 0.0237, 0.0253, 0.0283,\n",
       "          0.0228, 0.0288, 0.0173, 0.0152, 0.0148, 0.0165, 0.0171, 0.0180, 0.0168,\n",
       "          0.0197, 0.0210, 0.0227, 0.0172, 0.0133, 0.0210, 0.0159, 0.0186, 0.0220,\n",
       "          0.0152, 0.0254, 0.0259, 0.0215, 0.0288, 0.0414, 0.0314, 0.0236, 0.0298,\n",
       "          0.0487, 0.0294, 0.0238, 0.0427, 0.0253, 0.0311, 0.0276, 0.0178, 0.0165,\n",
       "          0.0161, 0.0156, 0.0169, 0.0168, 0.0166, 0.0165, 0.0194, 0.0173, 0.0211,\n",
       "          0.0182, 0.0182, 0.0237, 0.0198, 0.0193, 0.0216, 0.0212, 0.0138, 0.0249,\n",
       "          0.0203, 0.0245, 0.0233, 0.0241, 0.0285, 0.0474, 0.0269, 0.0324, 0.0289,\n",
       "          0.0237, 0.0259, 0.0273, 0.0133, 0.0126, 0.0117, 0.0146, 0.0110, 0.0117,\n",
       "          0.0155, 0.0148, 0.0156, 0.0223, 0.0133, 0.0227, 0.0224, 0.0273, 0.0332,\n",
       "          0.0349, 0.0264, 0.0279, 0.0310, 0.0286, 0.0263, 0.0244, 0.0257, 0.0250,\n",
       "          0.0242, 0.0354, 0.0302, 0.0275, 0.0237, 0.0233, 0.0250, 0.0322, 0.0173,\n",
       "          0.0181, 0.0126, 0.0124, 0.0114, 0.0135, 0.0124, 0.0122, 0.0155, 0.0148,\n",
       "          0.0189, 0.0213, 0.0180, 0.0245, 0.0370, 0.0344, 0.0294, 0.0269, 0.0236,\n",
       "          0.0238, 0.0258, 0.0232, 0.0229, 0.0232, 0.0281, 0.0264, 0.0263, 0.0273,\n",
       "          0.0240, 0.0232, 0.0314, 0.0352, 0.0194, 0.0204, 0.0215, 0.0159, 0.0181,\n",
       "          0.0233, 0.0207, 0.0194, 0.0146, 0.0195, 0.0277, 0.0238, 0.0210, 0.0273,\n",
       "          0.0213, 0.0277, 0.0230, 0.0253, 0.0341, 0.0244, 0.0296, 0.0240, 0.0298,\n",
       "          0.0258, 0.0547, 0.0255, 0.0424, 0.0324, 0.0378, 0.0269, 0.0359, 0.0296,\n",
       "          0.0210, 0.0185, 0.0229, 0.0135, 0.0182, 0.0176, 0.0168, 0.0177, 0.0181,\n",
       "          0.0189, 0.0201, 0.0198, 0.0228, 0.0191, 0.0201, 0.0227, 0.0197, 0.0259,\n",
       "          0.0236, 0.0272, 0.0244, 0.0286, 0.0305, 0.0268, 0.0557, 0.0315, 0.0279,\n",
       "          0.0314, 0.0332, 0.0383, 0.0315, 0.0297, 0.0288, 0.0224, 0.0250, 0.0327,\n",
       "          0.0184, 0.0254, 0.0193, 0.0206, 0.0193, 0.0208, 0.0204, 0.0266, 0.0230,\n",
       "          0.0208, 0.0236, 0.0242, 0.0224, 0.0188, 0.0208, 0.0216, 0.0245, 0.0247,\n",
       "          0.0228, 0.0158, 0.0275, 0.0234, 0.0298, 0.0241, 0.0229, 0.0251, 0.0232,\n",
       "          0.0273, 0.0189, 0.0203, 0.0182, 0.0267, 0.0194, 0.0249, 0.0230, 0.0246,\n",
       "          0.0240, 0.0242, 0.0238, 0.0247, 0.0250, 0.0168, 0.0216, 0.0172, 0.0240,\n",
       "          0.0253, 0.0234, 0.0245, 0.0207, 0.0280, 0.0251, 0.0266, 0.0241, 0.0202,\n",
       "          0.0246, 0.0236, 0.0275, 0.0238, 0.0298, 0.0237, 0.0130, 0.0115, 0.0124,\n",
       "          0.0139, 0.0112, 0.0132, 0.0148, 0.0163, 0.0163, 0.0147, 0.0139, 0.0150,\n",
       "          0.0161, 0.0177, 0.0103, 0.0127, 0.0147, 0.0150, 0.0202, 0.0327, 0.0359,\n",
       "          0.0338, 0.0570, 0.0388, 0.0430, 0.0419, 0.0367, 0.0427, 0.0362, 0.0388,\n",
       "          0.0568, 0.0391, 0.0113, 0.0148, 0.0112, 0.0112, 0.0130, 0.0126, 0.0105,\n",
       "          0.0121, 0.0117, 0.0153, 0.0143, 0.0128, 0.0114, 0.0109, 0.0138, 0.0118,\n",
       "          0.0132, 0.0144, 0.0254, 0.0198, 0.0469, 0.0409, 0.0568, 0.0464, 0.0438,\n",
       "          0.0464, 0.0466, 0.0365, 0.0362, 0.0367, 0.0349, 0.0396, 0.0145, 0.0193,\n",
       "          0.0204, 0.0225, 0.0184, 0.0195, 0.0223, 0.0193, 0.0202, 0.0161, 0.0272,\n",
       "          0.0201, 0.0307, 0.0203, 0.0217, 0.0229, 0.0242, 0.0228, 0.0249, 0.0322,\n",
       "          0.0258, 0.0263, 0.0224, 0.0240, 0.0487, 0.0195, 0.0260, 0.0296, 0.0264,\n",
       "          0.0365, 0.0253, 0.0260, 0.0156, 0.0146, 0.0198, 0.0184, 0.0212, 0.0191,\n",
       "          0.0198, 0.0195, 0.0201, 0.0167, 0.0216, 0.0195, 0.0207, 0.0234, 0.0219,\n",
       "          0.0221, 0.0237, 0.0262, 0.0188, 0.0306, 0.0247, 0.0273, 0.0411, 0.0253,\n",
       "          0.0406, 0.0244, 0.0240, 0.0284, 0.0268, 0.0703, 0.0284, 0.0229]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.47.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.21.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.27.mlp.gate_proj.scales': tensor([[0.0262, 0.0323, 0.0251,  ..., 0.0221, 0.0220, 0.0215]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.9.mlp.up_proj.scales': tensor([[0.0208, 0.0215, 0.0236,  ..., 0.0249, 0.0255, 0.0246]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.23.mlp.fc1.bias': tensor([-0.3203, -0.1191, -0.0203,  ..., -0.1748,  0.0928, -0.3477]),\n",
       " 'llm.model.layers.36.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.29.self_attn.o_proj.scales': tensor([[0.0245, 0.0210, 0.0210,  ..., 0.0215, 0.0259, 0.0182]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.19.self_attn.o_proj.qweight': tensor([[ -930502230, -1751479402, -1230539129,  ...,  1599630236,\n",
       "          -1499941465,  1791387014],\n",
       "         [-1769490089, -1751697030,  1465538921,  ...,  1690606186,\n",
       "           1253611640,  1988405129],\n",
       "         [-1470654311, -2026211449, -1769243735,  ..., -1783654746,\n",
       "          -2039895899, -1429825335],\n",
       "         ...,\n",
       "         [ -900171593,  1988725146,  1754688597,  ..., -1181185701,\n",
       "          -1987610199,  -932672637],\n",
       "         [-1768188039, -1751615365,  -945256330,  ..., -1988530516,\n",
       "          -1703316376,  2008627093],\n",
       "         [ -662140505, -1700042904,  1991797125,  ...,  1804110215,\n",
       "          -1466321080, -1735943291]], dtype=torch.int32),\n",
       " 'llm.model.layers.38.mlp.down_proj.qweight': tensor([[ 1990826599,  1251391624, -1968469910,  ...,  2037942422,\n",
       "          -1972860564, -1968469592],\n",
       "         [ 2034592101,  1753893547, -1953916809,  ..., -2003068520,\n",
       "           1234729334, -2004326740],\n",
       "         [-1787324265, -1731500646,  2004465590,  ...,  1974958777,\n",
       "           1753712807,  1972795272],\n",
       "         ...,\n",
       "         [-1984121017, -2089642406,  1736926824,  ..., -1737845111,\n",
       "          -1686664824,  2056611175],\n",
       "         [ 1732937844, -1954899271,  1517926503,  ...,  -980969301,\n",
       "           2020244117, -1719690871],\n",
       "         [ 1703516350,  1754626424, -1467376731,  ...,  2023340425,\n",
       "          -1766230087,  1203403146]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.21.mlp.fc2.bias': tensor([ 0.4648,  0.6562,  0.1152,  ..., -0.2031,  0.5508,  0.4609]),\n",
       " 'llm.model.layers.13.mlp.gate_proj.qweight': tensor([[ 2073714024,  1449822631,  1514700138,  ...,  1457294230,\n",
       "          -1736858246, -1464755845],\n",
       "         [ 1684580998, -1737832554, -1987610200,  ..., -1954903944,\n",
       "          -2019063212,  1721337496],\n",
       "         [-1233488059, -2017297787,  1485360279,  ...,  1953139368,\n",
       "           2004318841, -1463449931],\n",
       "         ...,\n",
       "         [ -643209814, -2003266132,  2007595402,  ...,  2042198137,\n",
       "           2024589656,  -911885976],\n",
       "         [ -894986172, -2052687753,  1789353861,  ..., -1437038423,\n",
       "          -1482459014, -1750500214],\n",
       "         [ 2040972905, -2017936728,  2037151860,  ...,  1753775495,\n",
       "          -1200085147, -1160345943]], dtype=torch.int32),\n",
       " 'llm.model.layers.18.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.46.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.13.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.13.self_attn.q_proj.qweight': tensor([[ 2056486880, -1473868720,  1649850792,  ..., -2090502503,\n",
       "          -1771352690,  2087811208],\n",
       "         [-2053296311, -1635351490,  1789281120,  ...,  2036703071,\n",
       "          -2006408575,  1632325990],\n",
       "         [-1802416742,  1941064033,  2138603379,  ...,  1519484481,\n",
       "           1671917473, -1165461609],\n",
       "         ...,\n",
       "         [  611160369,  1734188176,  -698323877,  ...,  1466328459,\n",
       "          -1633887164,  2138199690],\n",
       "         [-1479957416, -1737783186,  1922730654,  ...,  1756995459,\n",
       "          -1179688328,  1901896038],\n",
       "         [-1404528601,  1972657093,  2052697219,  ...,  1182886580,\n",
       "           1767991129, -1333078987]], dtype=torch.int32),\n",
       " 'llm.model.layers.10.mlp.down_proj.scales': tensor([[0.0224, 0.0247, 0.0285,  ..., 0.0250, 0.0203, 0.0300]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.20.self_attn.out_proj.bias': tensor([-0.2910, -0.0894, -0.0649,  ...,  0.1670, -0.0164, -0.0518]),\n",
       " 'llm.model.layers.23.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.27.self_attn.k_proj.scales': tensor([[0.0229, 0.0145, 0.0161, 0.0163, 0.0273, 0.0156, 0.0288, 0.0177, 0.0156,\n",
       "          0.0204, 0.0167, 0.0208, 0.0177, 0.0210, 0.0311, 0.0178, 0.0210, 0.0228,\n",
       "          0.0227, 0.0234, 0.0255, 0.0237, 0.0193, 0.0288, 0.0254, 0.0145, 0.0336,\n",
       "          0.0267, 0.0290, 0.0380, 0.0318, 0.0359, 0.0154, 0.0163, 0.0269, 0.0168,\n",
       "          0.0167, 0.0157, 0.0232, 0.0316, 0.0167, 0.0204, 0.0150, 0.0194, 0.0178,\n",
       "          0.0227, 0.0225, 0.0161, 0.0221, 0.0241, 0.0268, 0.0277, 0.0229, 0.0212,\n",
       "          0.0263, 0.0294, 0.0302, 0.0280, 0.0288, 0.0341, 0.0262, 0.0249, 0.0315,\n",
       "          0.0266, 0.0181, 0.0210, 0.0171, 0.0324, 0.0150, 0.0260, 0.0314, 0.0169,\n",
       "          0.0244, 0.0234, 0.0293, 0.0159, 0.0307, 0.0219, 0.0309, 0.0315, 0.0341,\n",
       "          0.0323, 0.0370, 0.0300, 0.0399, 0.0329, 0.0315, 0.0302, 0.0314, 0.0169,\n",
       "          0.0292, 0.0359, 0.0406, 0.0163, 0.0311, 0.0332, 0.0367, 0.0329, 0.0168,\n",
       "          0.0438, 0.0204, 0.0456, 0.0312, 0.0194, 0.0286, 0.0168, 0.0293, 0.0237,\n",
       "          0.0331, 0.0276, 0.0281, 0.0297, 0.0309, 0.0301, 0.0316, 0.0266, 0.0419,\n",
       "          0.0292, 0.0338, 0.0311, 0.0372, 0.0406, 0.0279, 0.0281, 0.0297, 0.0349,\n",
       "          0.0269, 0.0329, 0.0139, 0.0124, 0.0129, 0.0138, 0.0220, 0.0146, 0.0193,\n",
       "          0.0178, 0.0189, 0.0157, 0.0255, 0.0184, 0.0184, 0.0204, 0.0254, 0.0237,\n",
       "          0.0233, 0.0242, 0.0211, 0.0298, 0.0288, 0.0146, 0.0300, 0.0260, 0.0289,\n",
       "          0.0276, 0.0536, 0.0440, 0.0277, 0.0290, 0.0311, 0.0284, 0.0186, 0.0147,\n",
       "          0.0132, 0.0129, 0.0210, 0.0150, 0.0244, 0.0185, 0.0188, 0.0184, 0.0221,\n",
       "          0.0228, 0.0227, 0.0240, 0.0127, 0.0311, 0.0245, 0.0203, 0.0257, 0.0249,\n",
       "          0.0253, 0.0320, 0.0253, 0.0331, 0.0322, 0.0300, 0.0319, 0.0319, 0.0292,\n",
       "          0.0296, 0.0288, 0.0322, 0.0166, 0.0124, 0.0156, 0.0157, 0.0139, 0.0174,\n",
       "          0.0191, 0.0191, 0.0212, 0.0188, 0.0250, 0.0199, 0.0141, 0.0236, 0.0233,\n",
       "          0.0272, 0.0279, 0.0268, 0.0258, 0.0184, 0.0273, 0.0275, 0.0285, 0.0272,\n",
       "          0.0293, 0.0456, 0.0286, 0.0285, 0.0341, 0.0281, 0.0490, 0.0301, 0.0135,\n",
       "          0.0127, 0.0127, 0.0220, 0.0162, 0.0188, 0.0131, 0.0158, 0.0160, 0.0207,\n",
       "          0.0211, 0.0181, 0.0207, 0.0208, 0.0216, 0.0158, 0.0247, 0.0264, 0.0254,\n",
       "          0.0311, 0.0284, 0.0297, 0.0305, 0.0277, 0.0329, 0.0230, 0.0328, 0.0306,\n",
       "          0.0383, 0.0417, 0.0341, 0.0281, 0.0112, 0.0102, 0.0122, 0.0137, 0.0163,\n",
       "          0.0146, 0.0134, 0.0127, 0.0181, 0.0182, 0.0193, 0.0215, 0.0246, 0.0225,\n",
       "          0.0306, 0.0220, 0.0352, 0.0266, 0.0324, 0.0406, 0.0284, 0.0352, 0.0362,\n",
       "          0.0383, 0.0365, 0.0378, 0.0245, 0.0367, 0.0383, 0.0341, 0.0303, 0.0396,\n",
       "          0.0133, 0.0128, 0.0166, 0.0131, 0.0127, 0.0140, 0.0152, 0.0152, 0.0185,\n",
       "          0.0193, 0.0208, 0.0191, 0.0296, 0.0240, 0.0312, 0.0344, 0.0333, 0.0301,\n",
       "          0.0322, 0.0336, 0.0302, 0.0327, 0.0383, 0.0336, 0.0378, 0.0362, 0.0602,\n",
       "          0.0359, 0.0344, 0.0341, 0.0422, 0.0324, 0.0138, 0.0123, 0.0135, 0.0153,\n",
       "          0.0147, 0.0176, 0.0167, 0.0194, 0.0207, 0.0208, 0.0197, 0.0258, 0.0197,\n",
       "          0.0156, 0.0326, 0.0269, 0.0333, 0.0257, 0.0169, 0.0263, 0.0288, 0.0306,\n",
       "          0.0298, 0.0277, 0.0275, 0.0160, 0.0365, 0.0289, 0.0338, 0.0401, 0.0280,\n",
       "          0.0303, 0.0115, 0.0126, 0.0146, 0.0141, 0.0204, 0.0182, 0.0257, 0.0165,\n",
       "          0.0197, 0.0181, 0.0233, 0.0184, 0.0245, 0.0229, 0.0262, 0.0315, 0.0273,\n",
       "          0.0303, 0.0289, 0.0277, 0.0324, 0.0301, 0.0260, 0.0269, 0.0331, 0.0557,\n",
       "          0.0310, 0.0383, 0.0281, 0.0296, 0.0440, 0.0383, 0.0404, 0.0318, 0.0349,\n",
       "          0.0204, 0.0305, 0.0230, 0.0307, 0.0199, 0.0352, 0.0318, 0.0191, 0.0333,\n",
       "          0.0258, 0.0349, 0.0450, 0.0385, 0.0336, 0.0255, 0.0292, 0.0359, 0.0272,\n",
       "          0.0393, 0.0268, 0.0362, 0.0322, 0.0296, 0.0297, 0.0165, 0.0302, 0.0315,\n",
       "          0.0365, 0.0438, 0.0219, 0.0194, 0.0199, 0.0212, 0.0362, 0.0213, 0.0354,\n",
       "          0.0172, 0.0263, 0.0285, 0.0162, 0.0383, 0.0156, 0.0388, 0.0443, 0.0338,\n",
       "          0.0383, 0.0247, 0.0365, 0.0293, 0.0365, 0.0344, 0.0302, 0.0292, 0.0289,\n",
       "          0.0314, 0.0346, 0.0329, 0.0323, 0.0399, 0.0253, 0.0367, 0.0185, 0.0180,\n",
       "          0.0211, 0.0158, 0.0257, 0.0117, 0.0266, 0.0232, 0.0277, 0.0301, 0.0300,\n",
       "          0.0250, 0.0219, 0.0285, 0.0292, 0.0230, 0.0244, 0.0284, 0.0144, 0.0266,\n",
       "          0.0277, 0.0280, 0.0338, 0.0327, 0.0329, 0.0315, 0.0294, 0.0409, 0.0276,\n",
       "          0.0306, 0.0332, 0.0288, 0.0169, 0.0145, 0.0244, 0.0171, 0.0211, 0.0184,\n",
       "          0.0273, 0.0147, 0.0269, 0.0241, 0.0198, 0.0240, 0.0237, 0.0236, 0.0191,\n",
       "          0.0249, 0.0249, 0.0341, 0.0318, 0.0273, 0.0327, 0.0324, 0.0254, 0.0312,\n",
       "          0.0341, 0.0336, 0.0281, 0.0357, 0.0388, 0.0329, 0.0349, 0.0301]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.31.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.20.self_attn.v_proj.bias': tensor([-0.2275,  0.0061,  0.0532,  ..., -0.0854, -0.0006,  0.0391]),\n",
       " 'llm.model.layers.9.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.4.self_attn.out_proj.scales': tensor([[0.0007, 0.0007, 0.0007,  ..., 0.0008, 0.0006, 0.0006]]),\n",
       " 'llm.model.layers.25.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.49.mlp.down_proj.qweight': tensor([[-1754569131, -1482258329,  1956271785,  ...,  1754806135,\n",
       "           1784175514, -1466407001],\n",
       "         [-1987679624, -1952941705,  1988732781,  ..., -1181123689,\n",
       "          -2003085171, -1735747720],\n",
       "         [-1300637561,  1471776932, -1748481623,  ..., -1233282922,\n",
       "            916097128, -1486383241],\n",
       "         ...,\n",
       "         [-1465542840, -1667794552,  1785104249,  ..., -1924753510,\n",
       "          -1973775769, -1751414169],\n",
       "         [-1764202089, -1972856667,  1737980838,  ..., -1688627492,\n",
       "           1519822267, -2005305209],\n",
       "         [-1756079770, -1989690984,  1487637110,  ...,  1768453730,\n",
       "          -1752659319, -1467516794]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.10.mlp.fc2.bias': tensor([-0.0479,  0.0869,  0.0461,  ...,  0.1943, -0.0275, -0.1807]),\n",
       " 'vpm.encoder.layers.20.self_attn.out_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.24.self_attn.v_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.22.self_attn.out_proj.scales': tensor([[0.0009, 0.0008, 0.0008,  ..., 0.0007, 0.0007, 0.0008]]),\n",
       " 'llm.model.layers.14.self_attn.q_proj.scales': tensor([[0.0306, 0.0275, 0.0230,  ..., 0.0244, 0.0206, 0.0249]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.4.self_attn.v_proj.scales': tensor([[0.0298, 0.0303, 0.0300, 0.0336, 0.0312, 0.0365, 0.0346, 0.0246, 0.0375,\n",
       "          0.0293, 0.0259, 0.0362, 0.0217, 0.0251, 0.0607, 0.0271, 0.0250, 0.0430,\n",
       "          0.0581, 0.0290, 0.0341, 0.0307, 0.0229, 0.0290, 0.0531, 0.0293, 0.0237,\n",
       "          0.0258, 0.0266, 0.0292, 0.0289, 0.0271, 0.0440, 0.0232, 0.0234, 0.0260,\n",
       "          0.0221, 0.0232, 0.0269, 0.0262, 0.0227, 0.0289, 0.0323, 0.0273, 0.0254,\n",
       "          0.0417, 0.0216, 0.0531, 0.0212, 0.0240, 0.0289, 0.0251, 0.0250, 0.0378,\n",
       "          0.0269, 0.0414, 0.0237, 0.0346, 0.0266, 0.0253, 0.0228, 0.0244, 0.0266,\n",
       "          0.0411, 0.0208, 0.0264, 0.0217, 0.0217, 0.0233, 0.0242, 0.0227, 0.0255,\n",
       "          0.0237, 0.0203, 0.0195, 0.0247, 0.0250, 0.0263, 0.0224, 0.0319, 0.0204,\n",
       "          0.0230, 0.0245, 0.0237, 0.0211, 0.0266, 0.0212, 0.0227, 0.0245, 0.0266,\n",
       "          0.0223, 0.0250, 0.0223, 0.0216, 0.0318, 0.0294, 0.0280, 0.0289, 0.0229,\n",
       "          0.0259, 0.0198, 0.0210, 0.0251, 0.0211, 0.0234, 0.0241, 0.0245, 0.0253,\n",
       "          0.0180, 0.0232, 0.0251, 0.0262, 0.0246, 0.0213, 0.0242, 0.0206, 0.0272,\n",
       "          0.0224, 0.0212, 0.0259, 0.0223, 0.0234, 0.0245, 0.0230, 0.0242, 0.0223,\n",
       "          0.0236, 0.0220, 0.0275, 0.0245, 0.0240, 0.0283, 0.0260, 0.0268, 0.0251,\n",
       "          0.0277, 0.0232, 0.0263, 0.0227, 0.0253, 0.0225, 0.0234, 0.0250, 0.0281,\n",
       "          0.0208, 0.0238, 0.0251, 0.0246, 0.0255, 0.0242, 0.0217, 0.0272, 0.0216,\n",
       "          0.0247, 0.0399, 0.0245, 0.0269, 0.0237, 0.0246, 0.0307, 0.0294, 0.0303,\n",
       "          0.0263, 0.0221, 0.0314, 0.0267, 0.0240, 0.0245, 0.0236, 0.0233, 0.0275,\n",
       "          0.0300, 0.0316, 0.0286, 0.0249, 0.0246, 0.0266, 0.0241, 0.0303, 0.0253,\n",
       "          0.0254, 0.0234, 0.0290, 0.0266, 0.0237, 0.0250, 0.0241, 0.0238, 0.0314,\n",
       "          0.0234, 0.0259, 0.0260, 0.0236, 0.0169, 0.0199, 0.0202, 0.0194, 0.0236,\n",
       "          0.0215, 0.0228, 0.0160, 0.0204, 0.0181, 0.0189, 0.0172, 0.0215, 0.0199,\n",
       "          0.0184, 0.0186, 0.0221, 0.0199, 0.0186, 0.0263, 0.0202, 0.0185, 0.0221,\n",
       "          0.0176, 0.0182, 0.0168, 0.0193, 0.0173, 0.0191, 0.0220, 0.0202, 0.0221,\n",
       "          0.0208, 0.0257, 0.0166, 0.0186, 0.0176, 0.0191, 0.0171, 0.0223, 0.0208,\n",
       "          0.0190, 0.0212, 0.0281, 0.0172, 0.0181, 0.0184, 0.0201, 0.0197, 0.0190,\n",
       "          0.0195, 0.0216, 0.0194, 0.0212, 0.0202, 0.0162, 0.0213, 0.0207, 0.0208,\n",
       "          0.0182, 0.0198, 0.0223, 0.0173, 0.0246, 0.0232, 0.0232, 0.0234, 0.0223,\n",
       "          0.0236, 0.0237, 0.0362, 0.0251, 0.0319, 0.0224, 0.0260, 0.0241, 0.0268,\n",
       "          0.0228, 0.0197, 0.0219, 0.0217, 0.0224, 0.0285, 0.0230, 0.0259, 0.0221,\n",
       "          0.0267, 0.0298, 0.0213, 0.0242, 0.0251, 0.0279, 0.0286, 0.0269, 0.0241,\n",
       "          0.0217, 0.0250, 0.0269, 0.0217, 0.0229, 0.0233, 0.0254, 0.0266, 0.0244,\n",
       "          0.0246, 0.0206, 0.0199, 0.0254, 0.0251, 0.0311, 0.0232, 0.0233, 0.0238,\n",
       "          0.0213, 0.0251, 0.0315, 0.0326, 0.0244, 0.0238, 0.0247, 0.0283, 0.0258,\n",
       "          0.0271, 0.0251, 0.0303, 0.0223, 0.0247, 0.0276, 0.0279, 0.0272, 0.0241,\n",
       "          0.0349, 0.0258, 0.0283, 0.0234, 0.0275, 0.0240, 0.0263, 0.0240, 0.0251,\n",
       "          0.0305, 0.0280, 0.0234, 0.0367, 0.0275, 0.0281, 0.0262, 0.0264, 0.0318,\n",
       "          0.0341, 0.0259, 0.0255, 0.0264, 0.0277, 0.0263, 0.0267, 0.0267, 0.0283,\n",
       "          0.0286, 0.0324, 0.0310, 0.0296, 0.0232, 0.0223, 0.0251, 0.0290, 0.0271,\n",
       "          0.0271, 0.0259, 0.0305, 0.0237, 0.0296, 0.0253, 0.0259, 0.0229, 0.0260,\n",
       "          0.0230, 0.0244, 0.0246, 0.0212, 0.0260, 0.0280, 0.0296, 0.0236, 0.0244,\n",
       "          0.0242, 0.0233, 0.0310, 0.0247, 0.0259, 0.0275, 0.0238, 0.0223, 0.0357,\n",
       "          0.0225, 0.0212, 0.0213, 0.0233, 0.0220, 0.0280, 0.0244, 0.0267, 0.0229,\n",
       "          0.0229, 0.0223, 0.0208, 0.0250, 0.0232, 0.0223, 0.0220, 0.0220, 0.0250,\n",
       "          0.0279, 0.0254, 0.0244, 0.0230, 0.0208, 0.0267, 0.0215, 0.0268, 0.0240,\n",
       "          0.0217, 0.0227, 0.0247, 0.0245, 0.0215, 0.0204, 0.0255, 0.0307, 0.0228,\n",
       "          0.0217, 0.0263, 0.0194, 0.0223, 0.0221, 0.0206, 0.0233, 0.0255, 0.0223,\n",
       "          0.0223, 0.0267, 0.0236, 0.0232, 0.0195, 0.0233, 0.0204, 0.0238, 0.0207,\n",
       "          0.0228, 0.0249, 0.0201, 0.0228, 0.0232, 0.0229, 0.0286, 0.0189, 0.0204,\n",
       "          0.0213, 0.0185, 0.0188, 0.0259, 0.0219, 0.0193, 0.0201, 0.0212, 0.0253,\n",
       "          0.0204, 0.0189, 0.0195, 0.0221, 0.0184, 0.0228, 0.0212, 0.0202, 0.0197,\n",
       "          0.0263, 0.0227, 0.0219, 0.0224, 0.0229, 0.0194, 0.0159, 0.0184, 0.0201,\n",
       "          0.0204, 0.0180, 0.0213, 0.0206, 0.0199, 0.0194, 0.0204, 0.0189, 0.0215,\n",
       "          0.0220, 0.0266, 0.0225, 0.0230, 0.0194, 0.0206, 0.0230, 0.0249, 0.0193,\n",
       "          0.0176, 0.0210, 0.0224, 0.0202, 0.0216, 0.0206, 0.0223, 0.0201, 0.0213,\n",
       "          0.0207, 0.0237, 0.0182, 0.0177, 0.0208, 0.0191, 0.0234, 0.0203]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.16.self_attn.k_proj.qweight': tensor([[ 1502321071,  1906667921, -1163685784,  ..., -1788651663,\n",
       "           1805412713,  2122343248],\n",
       "         [-2001568180,  1987734411, -2106821482,  ...,   962040456,\n",
       "          -2073001083,  1905826428],\n",
       "         [ 2002160524,  1936033409, -1687514970,  ...,  1938447455,\n",
       "          -2020897933, -1919458416],\n",
       "         ...,\n",
       "         [ 1265668524, -1233949317,  1638315367,  ...,  1298891427,\n",
       "           2058062982,  1164874359],\n",
       "         [ 1549950353,  1654363541,  1635549364,  ...,  1685226142,\n",
       "           2089123953, -1904237443],\n",
       "         [ 1770161830,  1821210743,  1485798525,  ...,  2089000828,\n",
       "          -1820554629, -1513719469]], dtype=torch.int32),\n",
       " 'llm.model.layers.36.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.11.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.28.mlp.up_proj.qweight': tensor([[ 1498909515, -2022995866,  1787341446,  ...,  1767336041,\n",
       "          -2006287975, -1304921945],\n",
       "         [ 1756657270,  2024512186, -1717990743,  ...,  -881236309,\n",
       "          -2002147415, -1216845724],\n",
       "         [  913083273, -1733658233, -1735759737,  ..., -1798801255,\n",
       "          -1719105160, -1448581032],\n",
       "         ...,\n",
       "         [-2035570569,  1533720183, -2037745529,  ..., -1773585783,\n",
       "           2006484854, -1949735017],\n",
       "         [ -947240085, -1467585817, -1483109750,  ...,  2005457079,\n",
       "          -1736022137, -1987595387],\n",
       "         [ 2055710599, -1486366088,  2001373579,  ...,  1970760086,\n",
       "           1753844633, -1448641916]], dtype=torch.int32),\n",
       " 'llm.model.layers.16.mlp.gate_proj.scales': tensor([[0.0264, 0.0234, 0.0323,  ..., 0.0232, 0.0228, 0.0230]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.3.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.10.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.26.self_attn.k_proj.qweight': tensor([[-1736931192, -2004252535, -2021230440,  ..., -1735620679,\n",
       "            679979960,  1517779078],\n",
       "         [-2002294632, -2038003319, -2023188344,  ...,  1750697945,\n",
       "          -1739946280,  2022279561],\n",
       "         [ 2007598776,  2022214009, -1451718246,  ...,  2054785704,\n",
       "          -1752663877, -1732609353],\n",
       "         ...,\n",
       "         [-1751607656, -2004387689,  2004322455,  ..., -1765185418,\n",
       "           2057803387,  -914839672],\n",
       "         [-1752594311, -2020046713, -1970693736,  ..., -1502955159,\n",
       "           1450867126, -1819818311],\n",
       "         [-1718974073, -1752590200,  1502123655,  ..., -1970752100,\n",
       "          -1218017320,  1753790073]], dtype=torch.int32),\n",
       " 'llm.model.layers.2.self_attn.o_proj.scales': tensor([[0.0349, 0.0191, 0.0186,  ..., 0.0249, 0.0240, 0.0172]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.33.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.22.mlp.gate_proj.scales': tensor([[0.0250, 0.0180, 0.0206,  ..., 0.0257, 0.0204, 0.0195]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.18.self_attn.q_proj.scales': tensor([[0.0007, 0.0008, 0.0014,  ..., 0.0008, 0.0008, 0.0010]]),\n",
       " 'llm.model.layers.32.mlp.down_proj.qweight': tensor([[-1722194058, -1781818983,  1991805064,  ...,  1485470840,\n",
       "           1969711257,  2039183001],\n",
       "         [ 1750746710,  1787253928, -1228363641,  ..., -1942460266,\n",
       "           1536927577, -1990826152],\n",
       "         [-1737852009,  2039138919,  1736029832,  ..., -2019054970,\n",
       "          -1717332074,  1753839466],\n",
       "         ...,\n",
       "         [ -879131190, -1784187254, -2034935400,  ..., -1686533993,\n",
       "           1468565143,  1718143383],\n",
       "         [ 2003529066,  2088294536, -1199077032,  ...,  2056484920,\n",
       "           1486464901,  1431878009],\n",
       "         [ 1219336089, -1717860502,  2055763592,  ...,  2005313879,\n",
       "           1165465765,  1986697385]], dtype=torch.int32),\n",
       " 'llm.model.layers.29.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.1.self_attn.v_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.21.self_attn.k_proj.qweight': tensor([[-1955896696,  1853124782, -2090042486,  ..., -1837606453,\n",
       "          -1719037100,  2038274955],\n",
       "         [-1786668936,  2003013266,  -964983922,  ...,   968073947,\n",
       "          -1721344828,  1752208790],\n",
       "         [ 1821480069,  2039522426,  1250345595,  ..., -2015012522,\n",
       "           1485398169,  1180015209],\n",
       "         ...,\n",
       "         [ 1368558165,  1132307067,  2054635621,  ..., -1282704031,\n",
       "           1854046909, -1870902707],\n",
       "         [ 1616084798, -1819908261, -1687585915,  ...,  1551807640,\n",
       "           1918670473,  1518767489],\n",
       "         [  679712290, -1485275982,  2016504953,  ..., -2138016115,\n",
       "           1551457128, -1716154212]], dtype=torch.int32),\n",
       " 'llm.model.layers.2.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.32.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.13.self_attn.q_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.26.mlp.fc1.bias': tensor([ 0.1758,  0.9414, -0.3418,  ...,  0.0386, -0.7539, -0.3418]),\n",
       " 'vpm.encoder.layers.24.self_attn.out_proj.qweight': tensor([[ 2123139186, -2036294500,  1734832751,  ...,  2038589591,\n",
       "          -1970557572,   645557919],\n",
       "         [ 2022153321,  1098947193,  1834779030,  ..., -1681610356,\n",
       "           1870295166,  1315198888],\n",
       "         [-1933869762,  2057016691,  2033078449,  ..., -1421370487,\n",
       "          -1920441478, -2110962068],\n",
       "         ...,\n",
       "         [-1445818484,  2056948116, -2037151359,  ...,  2073721255,\n",
       "          -1796947297,  1551797394],\n",
       "         [ 1450603352, -2018942575,  2037546301,  ...,  1148079482,\n",
       "          -2022409081,  1301779614],\n",
       "         [ 1786084718,  1970572923,  1866875997,  ...,  1704502125,\n",
       "           1317894738,  1385060735]], dtype=torch.int32),\n",
       " 'llm.model.layers.46.post_attention_layernorm.weight': tensor([1.0156, 1.0000, 0.9961,  ..., 0.9805, 1.0234, 0.6953]),\n",
       " 'llm.model.layers.37.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.16.mlp.up_proj.qweight': tensor([[ 1769515146, -1717983882, -2004318072,  ...,  1751820377,\n",
       "          -1196857479, -1984595766],\n",
       "         [ 1217106571, -2021103208, -1987540856,  ..., -1736909720,\n",
       "           2041158025, -1685494395],\n",
       "         [ 2069345898, -1450596425, -2004318072,  ...,  1986618507,\n",
       "           2056743287,   933852791],\n",
       "         ...,\n",
       "         [-1212565112,  1752533112, -2004318073,  ...,  -948328601,\n",
       "          -2022217593, -1182178310],\n",
       "         [ 2038979224, -1751480231, -2005436280,  ..., -1702192741,\n",
       "          -1987618154,  1468635770],\n",
       "         [-1741120902,  -661153654, -1735882616,  ...,  1738975400,\n",
       "          -1751545992, -1738360248]], dtype=torch.int32),\n",
       " 'llm.model.layers.0.mlp.up_proj.scales': tensor([[0.0213, 0.0224, 0.0242,  ..., 0.0213, 0.0259, 0.0244]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.10.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.19.self_attn.q_proj.scales': tensor([[0.0255, 0.0202, 0.0201,  ..., 0.0312, 0.0220, 0.0318]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.39.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.5.self_attn.q_proj.bias': tensor([ 0.0547,  0.0232, -0.0028,  ...,  0.0114,  0.0361,  0.0315]),\n",
       " 'llm.model.layers.16.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.44.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.49.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.11.self_attn.v_proj.qweight': tensor([[-1720088969, -1482909100,  1774422215,  ..., -2038924679,\n",
       "          -1735042133,  2004323224],\n",
       "         [-1683462250,   978950745, -2022213494,  ..., -2038655307,\n",
       "            928691339, -1782274155],\n",
       "         [ 1201113180, -1571252085,  1507556727,  ...,   948594828,\n",
       "          -2006595417, -2037814902],\n",
       "         ...,\n",
       "         [-2006535052,  1990678429, -1465477020,  ..., -1665832598,\n",
       "           1978218406, -1731822970],\n",
       "         [-2021235800, -1970636888,  1736997692,  ...,  1519036266,\n",
       "           1737128086,  -376855147],\n",
       "         [-2006284409, -1733916251, -2090158424,  ..., -1966695541,\n",
       "          -2008659835, -1670945163]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.15.mlp.fc1.qweight': tensor([[-1449881926, -1837985240, -1768590438,  ...,  1904319372,\n",
       "          -1952940124, -1893482390],\n",
       "         [ 2055448686,  1955054992,  1433637206,  ..., -1853651090,\n",
       "           1587571573, -2072029001],\n",
       "         [-2068942457, -1965984355, -1801742227,  ..., -1836223669,\n",
       "          -1651196781, -2074770524],\n",
       "         ...,\n",
       "         [  750163367, -1250402117, -1737390493,  ...,  1803780496,\n",
       "           1973389650,  1772136594],\n",
       "         [ 1920374972,  2142788215,  1984926619,  ...,  2019586431,\n",
       "           1183931522, -1098669910],\n",
       "         [ 1789546895,  1902943852,  1668981866,  ..., -1736798825,\n",
       "           1789497741, -1922798799]], dtype=torch.int32),\n",
       " 'llm.model.layers.43.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.26.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.6.self_attn.out_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'resampler.ln_kv.bias': tensor([ 0.0097, -0.0122, -0.0223,  ..., -0.0481, -0.0239,  0.0212]),\n",
       " 'vpm.encoder.layers.22.self_attn.v_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.46.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.49.post_attention_layernorm.weight': tensor([1.0312, 1.0391, 1.0469,  ..., 1.0547, 1.0781, 0.9727]),\n",
       " 'llm.model.layers.43.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.8.mlp.fc1.scales': tensor([[0.0009, 0.0008, 0.0010,  ..., 0.0009, 0.0010, 0.0008]]),\n",
       " 'llm.model.layers.45.mlp.up_proj.qweight': tensor([[ 2042203244, -2036840504,  2024311398,  ..., -1465345944,\n",
       "          -1684567896,  -911780456],\n",
       "         [ 1800120455, -1820559144, -1450789000,  ..., -2022131815,\n",
       "          -1448441236, -1705482124],\n",
       "         [-1265870218,  2005301627, -2021086843,  ..., -1970685239,\n",
       "          -1200261000, -2033752963],\n",
       "         ...,\n",
       "         [-1785165368, -1212429403,  1802996360,  ..., -1718990887,\n",
       "           1753769623, -2020108969],\n",
       "         [ -410999129,  2006418856,  -929462405,  ...,  2004457352,\n",
       "           1250528106,  2071493018],\n",
       "         [ -142104439, -2017678968,  2074576169,  ..., -2070382213,\n",
       "           1804032375,  1499965561]], dtype=torch.int32),\n",
       " 'llm.model.layers.19.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.46.self_attn.o_proj.qweight': tensor([[-1972816216,  -933791843, -1466387319,  ...,  1717078407,\n",
       "          -1165329544,  2035915144],\n",
       "         [  911837383, -1234859685, -2022203973,  ..., -1194829738,\n",
       "          -1967678603, -1985439127],\n",
       "         [-1989834044, -1497798776, -2023331737,  ..., -1785156969,\n",
       "          -1754745452, -1789433754],\n",
       "         ...,\n",
       "         [ 2076891322,  1232774966, -1984386474,  ..., -2034595687,\n",
       "           2003396820,  1937156217],\n",
       "         [ -995519829,  -965231706, -2004383588,  ..., -2000132247,\n",
       "          -1754695801,  1754916984],\n",
       "         [-1501280649,  2073651606, -1499878778,  ...,  2042129272,\n",
       "          -1523799416, -1752798262]], dtype=torch.int32),\n",
       " 'llm.model.layers.5.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.50.input_layernorm.weight': tensor([1.0156, 0.9688, 0.9219,  ..., 0.9570, 0.9375, 1.7891]),\n",
       " 'llm.model.layers.11.self_attn.q_proj.qweight': tensor([[-2056873350, -1652905846, -2039051083,  ..., -1736796280,\n",
       "          -2067236742,  1500870550],\n",
       "         [-2035779172,  1229359178, -2020109144,  ...,  -409426005,\n",
       "          -1517762683,  2053613492],\n",
       "         [-2003339352, -2005497210,  1504150886,  ...,  1516664989,\n",
       "           1790479510, -1227061643],\n",
       "         ...,\n",
       "         [ 1988393352,  1733797769, -1369147769,  ..., -1151747994,\n",
       "           1418156440,  1454819512],\n",
       "         [-2004327080, -1736083831,  1786153656,  ...,  1263966587,\n",
       "           1484299369, -1185508200],\n",
       "         [-2021214121, -1938396828, -1768434310,  ...,  -930519128,\n",
       "          -1269389253, -1249208471]], dtype=torch.int32),\n",
       " 'llm.model.layers.2.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.29.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.48.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.29.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.35.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.0.self_attn.out_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.50.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.11.self_attn.out_proj.qweight': tensor([[-1619290504,  2024304017,  2075150194,  ...,  1213490552,\n",
       "          -1977366235, -2087100072],\n",
       "         [-1183495495, -1766166655, -2091607888,  ..., -1566538391,\n",
       "            731939690, -2038597301],\n",
       "         [ 1081897846,  1987811227, -2002749896,  ..., -1937261137,\n",
       "           1684844920,  1448516416],\n",
       "         ...,\n",
       "         [-1984917092,  1384545659,  1784847695,  ...,  2121762421,\n",
       "          -1856616294,  2057475190],\n",
       "         [-1852482665,  1398911322, -1849524859,  ...,  2005431687,\n",
       "           1908120188,  1671321981],\n",
       "         [-1969787046,  1686731349,  1385198505,  ...,  1602202767,\n",
       "          -1988346226,  2072542330]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.0.layer_norm2.bias': tensor([ 0.0132,  0.0141, -0.0152,  ..., -0.0127, -0.0019, -0.0104]),\n",
       " 'vpm.encoder.layers.19.mlp.fc2.bias': tensor([ 0.0090,  0.3203,  0.1089,  ..., -0.2852, -0.0188, -0.1738]),\n",
       " 'llm.model.layers.7.self_attn.o_proj.scales': tensor([[0.0225, 0.0234, 0.0181,  ..., 0.0246, 0.0203, 0.0137]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.36.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.46.self_attn.v_proj.scales': tensor([[0.0424, 0.0372, 0.0399, 0.0370, 0.0362, 0.0365, 0.0365, 0.0404, 0.0367,\n",
       "          0.0404, 0.0344, 0.0336, 0.0385, 0.0354, 0.0318, 0.0346, 0.0396, 0.0508,\n",
       "          0.0372, 0.0372, 0.0302, 0.0435, 0.0401, 0.0471, 0.0393, 0.0372, 0.0367,\n",
       "          0.0336, 0.0323, 0.0370, 0.0393, 0.0453, 0.0438, 0.0492, 0.0349, 0.0354,\n",
       "          0.0367, 0.0422, 0.0365, 0.0438, 0.0333, 0.0378, 0.0380, 0.0349, 0.0329,\n",
       "          0.0349, 0.0435, 0.0430, 0.0427, 0.0357, 0.0354, 0.0372, 0.0495, 0.0411,\n",
       "          0.0359, 0.0372, 0.0314, 0.0380, 0.0365, 0.0372, 0.0438, 0.0419, 0.0544,\n",
       "          0.0471, 0.0424, 0.0359, 0.0346, 0.0359, 0.0399, 0.0383, 0.0466, 0.0319,\n",
       "          0.0378, 0.0380, 0.0417, 0.0450, 0.0362, 0.0319, 0.0281, 0.0385, 0.0322,\n",
       "          0.0393, 0.0518, 0.0341, 0.0458, 0.0424, 0.0320, 0.0375, 0.0326, 0.0422,\n",
       "          0.0440, 0.0331, 0.0391, 0.0354, 0.0329, 0.0453, 0.0354, 0.0357, 0.0362,\n",
       "          0.0526, 0.0396, 0.0354, 0.0401, 0.0372, 0.0322, 0.0344, 0.0311, 0.0367,\n",
       "          0.0370, 0.0344, 0.0362, 0.0324, 0.0367, 0.0349, 0.0362, 0.0411, 0.0448,\n",
       "          0.0388, 0.0314, 0.0487, 0.0411, 0.0391, 0.0396, 0.0380, 0.0331, 0.0393,\n",
       "          0.0316, 0.0362, 0.0534, 0.0516, 0.0503, 0.0453, 0.0409, 0.0521, 0.0531,\n",
       "          0.0615, 0.0643, 0.0654, 0.0625, 0.0620, 0.0568, 0.0664, 0.0529, 0.0576,\n",
       "          0.0625, 0.0662, 0.0531, 0.0622, 0.0672, 0.0550, 0.0709, 0.0672, 0.0589,\n",
       "          0.0576, 0.0557, 0.0529, 0.0651, 0.0667, 0.0589, 0.0479, 0.0578, 0.0529,\n",
       "          0.0562, 0.0687, 0.0573, 0.0550, 0.0555, 0.0495, 0.0555, 0.0609, 0.0641,\n",
       "          0.0534, 0.0662, 0.0511, 0.0609, 0.0550, 0.0709, 0.0508, 0.0682, 0.0578,\n",
       "          0.0526, 0.0677, 0.0555, 0.0612, 0.0513, 0.0516, 0.0612, 0.0531, 0.0615,\n",
       "          0.0617, 0.0544, 0.0568, 0.0458, 0.0359, 0.0424, 0.0432, 0.0385, 0.0341,\n",
       "          0.0332, 0.0500, 0.0440, 0.0409, 0.0323, 0.0419, 0.0388, 0.0357, 0.0393,\n",
       "          0.0362, 0.0372, 0.0346, 0.0516, 0.0448, 0.0388, 0.0370, 0.0333, 0.0417,\n",
       "          0.0399, 0.0450, 0.0362, 0.0409, 0.0365, 0.0318, 0.0388, 0.0406, 0.0401,\n",
       "          0.0438, 0.0344, 0.0326, 0.0450, 0.0370, 0.0354, 0.0388, 0.0323, 0.0388,\n",
       "          0.0396, 0.0396, 0.0357, 0.0367, 0.0401, 0.0388, 0.0294, 0.0396, 0.0352,\n",
       "          0.0409, 0.0464, 0.0406, 0.0435, 0.0316, 0.0404, 0.0393, 0.0464, 0.0344,\n",
       "          0.0336, 0.0380, 0.0404, 0.0370, 0.0573, 0.0505, 0.0557, 0.0516, 0.0354,\n",
       "          0.0456, 0.0456, 0.0581, 0.0479, 0.0482, 0.0627, 0.0443, 0.0450, 0.0531,\n",
       "          0.0495, 0.0414, 0.0440, 0.0411, 0.0409, 0.0516, 0.0490, 0.0495, 0.0536,\n",
       "          0.0411, 0.0417, 0.0338, 0.0539, 0.0445, 0.0529, 0.0550, 0.0477, 0.0560,\n",
       "          0.0448, 0.0424, 0.0471, 0.0516, 0.0609, 0.0531, 0.0329, 0.0409, 0.0380,\n",
       "          0.0445, 0.0729, 0.0424, 0.0391, 0.0365, 0.0466, 0.0466, 0.0500, 0.0531,\n",
       "          0.0448, 0.0523, 0.0380, 0.0495, 0.0565, 0.0448, 0.0443, 0.0503, 0.0417,\n",
       "          0.0547, 0.0516, 0.0560, 0.0464, 0.0615, 0.0419, 0.0406, 0.0380, 0.0404,\n",
       "          0.0399, 0.0396, 0.0357, 0.0396, 0.0294, 0.0419, 0.0341, 0.0435, 0.0370,\n",
       "          0.0430, 0.0401, 0.0311, 0.0430, 0.0301, 0.0385, 0.0399, 0.0344, 0.0492,\n",
       "          0.0424, 0.0414, 0.0316, 0.0396, 0.0372, 0.0495, 0.0404, 0.0341, 0.0458,\n",
       "          0.0383, 0.0354, 0.0393, 0.0383, 0.0443, 0.0393, 0.0327, 0.0440, 0.0341,\n",
       "          0.0333, 0.0359, 0.0417, 0.0401, 0.0419, 0.0338, 0.0396, 0.0359, 0.0430,\n",
       "          0.0357, 0.0393, 0.0365, 0.0399, 0.0477, 0.0362, 0.0401, 0.0365, 0.0445,\n",
       "          0.0352, 0.0417, 0.0391, 0.0354, 0.0391, 0.0362, 0.0333, 0.0333, 0.0352,\n",
       "          0.0385, 0.0365, 0.0314, 0.0327, 0.0318, 0.0301, 0.0328, 0.0354, 0.0290,\n",
       "          0.0315, 0.0305, 0.0298, 0.0341, 0.0328, 0.0385, 0.0319, 0.0297, 0.0303,\n",
       "          0.0288, 0.0396, 0.0352, 0.0319, 0.0322, 0.0365, 0.0359, 0.0310, 0.0341,\n",
       "          0.0352, 0.0375, 0.0341, 0.0324, 0.0322, 0.0346, 0.0315, 0.0307, 0.0312,\n",
       "          0.0324, 0.0338, 0.0450, 0.0333, 0.0288, 0.0336, 0.0309, 0.0297, 0.0285,\n",
       "          0.0316, 0.0314, 0.0300, 0.0359, 0.0281, 0.0378, 0.0269, 0.0349, 0.0264,\n",
       "          0.0332, 0.0336, 0.0332, 0.0349, 0.0406, 0.0320, 0.0285, 0.0417, 0.0346,\n",
       "          0.0497, 0.0477, 0.0456, 0.0380, 0.0383, 0.0401, 0.0362, 0.0380, 0.0422,\n",
       "          0.0427, 0.0531, 0.0448, 0.0391, 0.0354, 0.0385, 0.0357, 0.0612, 0.0409,\n",
       "          0.0409, 0.0380, 0.0430, 0.0346, 0.0385, 0.0380, 0.0354, 0.0393, 0.0448,\n",
       "          0.0443, 0.0458, 0.0365, 0.0357, 0.0331, 0.0338, 0.0372, 0.0409, 0.0393,\n",
       "          0.0414, 0.0435, 0.0362, 0.0482, 0.0414, 0.0396, 0.0367, 0.0414, 0.0391,\n",
       "          0.0401, 0.0367, 0.0409, 0.0359, 0.0417, 0.0435, 0.0411, 0.0383, 0.0430,\n",
       "          0.0399, 0.0367, 0.0435, 0.0479, 0.0329, 0.0359, 0.0367, 0.0383]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.23.self_attn.out_proj.bias': tensor([0.3066, 0.3496, 0.4219,  ..., 0.1699, 0.1089, 0.0576]),\n",
       " 'llm.model.layers.9.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.2.mlp.fc1.bias': tensor([-2.3125, -2.1406, -0.9961,  ..., -0.4883, -3.2500, -3.0938]),\n",
       " 'vpm.encoder.layers.21.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.7.self_attn.q_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.0.self_attn.o_proj.qweight': tensor([[-1936344934,  2021950869,  1703709578,  ..., -1719113353,\n",
       "          -1753783131, -2038990456],\n",
       "         [ 1722251322, -1196972133,  1936218693,  ...,  1805157480,\n",
       "          -2003203705, -1987470967],\n",
       "         [-2024360266, -1755818118,   984132248,  ...,  1723365482,\n",
       "          -1181260186, -1417053815],\n",
       "         ...,\n",
       "         [-1214867126, -1483117993, -1771473497,  ...,  1772849543,\n",
       "           2041219209,  2040031656],\n",
       "         [-2018916167,  1768524438,  1787189380,  ..., -1719027573,\n",
       "          -1734707289, -1484170617],\n",
       "         [ 2036758665, -1719113080, -2005281431,  ...,  -979982196,\n",
       "           1723361144,  2040105111]], dtype=torch.int32),\n",
       " 'llm.model.layers.17.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.2.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.37.mlp.up_proj.qweight': tensor([[-1735820699, -1751672457, -1701287818,  ..., -1533638809,\n",
       "          -1184452198,  2056733767],\n",
       "         [ 1733852056, -1973848169, -1987422120,  ..., -2021999782,\n",
       "           2007529385,  2055764905],\n",
       "         [-1466533259,  -898144087,  2038999434,  ..., -2023004009,\n",
       "           1535662473, -1216764794],\n",
       "         ...,\n",
       "         [ 1432856249,  1752799592,  1737070726,  ..., -1971952073,\n",
       "           1685879194, -1752664422],\n",
       "         [-2003282313, -2020960118, -1752737384,  ...,  1770625467,\n",
       "           1536735335, -1735743352],\n",
       "         [ 1489144441, -1471424155, -1936361082,  ..., -1750635382,\n",
       "          -1201108376, -1432918904]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.13.self_attn.v_proj.bias': tensor([-0.0192, -0.0591,  0.0903,  ...,  0.0081,  0.0031,  0.0349]),\n",
       " 'llm.model.layers.18.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.14.mlp.fc2.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.16.mlp.fc1.qweight': tensor([[-2023391849, -2075672688, -1674333073,  ...,  1837342548,\n",
       "          -1752792407, -1937532481],\n",
       "         [ 1902220400,  1651543675,   967217040,  ...,  1415571821,\n",
       "           1839704451,  2054896338],\n",
       "         [ 1602587278, -1452585037,  2071962003,  ...,  2037421975,\n",
       "          -1700686452,  1907594898],\n",
       "         ...,\n",
       "         [-1786873999,  1802074216, -2020636037,  ..., -2024323723,\n",
       "           2142728097,  2139060661],\n",
       "         [-1920953437,  1350472304, -1789297273,  ..., -1703981686,\n",
       "          -1921215888, -1249683853],\n",
       "         [ 1998690403, -1886560389,   578719138,  ...,  -967016046,\n",
       "           1838003582, -1689480622]], dtype=torch.int32),\n",
       " 'llm.model.layers.47.self_attn.q_proj.scales': tensor([[0.0169, 0.0132, 0.0148,  ..., 0.0240, 0.0207, 0.0191]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.4.mlp.gate_proj.qweight': tensor([[-2000008540, -1987671639, -1788564136,  ..., -1520207784,\n",
       "          -2073651629, -1771542677],\n",
       "         [-2023192135,  1685485674, -1738955607,  ...,  1483318890,\n",
       "           1718254763, -1232504729],\n",
       "         [-2006670251, -1752987495,  1784444824,  ..., -1719048026,\n",
       "          -2036709786,  1769314681],\n",
       "         ...,\n",
       "         [-1185515420, -1967818553,  -610903413,  ...,  1785304985,\n",
       "          -2023135399, -1417115272],\n",
       "         [ 1201439868, -1750632569, -1502041447,  ..., -1987594822,\n",
       "          -1718072711, -1735903078],\n",
       "         [-2071504985, -2039858536, -1782798182,  ..., -2040945019,\n",
       "          -1195792217, -1751610984]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.23.mlp.fc2.qweight': tensor([[ 2038866570, -1251315359, -2040361829,  ...,  1418226350,\n",
       "          -2021618647,  1952852919],\n",
       "         [-1771475272, -2103677050,  1885953431,  ..., -1938847069,\n",
       "          -1936414848, -1904564856],\n",
       "         [-2105506170,  1702912656,  1635812448,  ..., -2038399093,\n",
       "          -1434427553, -1686100130],\n",
       "         ...,\n",
       "         [ 1134271601,  1952876706,   797401201,  ...,  1870626713,\n",
       "            712542064, -2071378054],\n",
       "         [ 1499698047, -1719240511,  1804297627,  ..., -2119514716,\n",
       "          -1232698053, -1600178281],\n",
       "         [-1602387831, -1883648592, -1101636479,  ..., -2039392865,\n",
       "           2104260715, -2037072713]], dtype=torch.int32),\n",
       " 'llm.model.layers.5.mlp.gate_proj.scales': tensor([[0.0234, 0.0241, 0.0198,  ..., 0.0181, 0.0228, 0.0289]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.19.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.48.mlp.gate_proj.qweight': tensor([[ 1970967674, -2018928996,  1214732196,  ...,  1501906574,\n",
       "          -1453814890, -2141550712],\n",
       "         [-1729248090,  1753525125, -1649035622,  ..., -2018928745,\n",
       "          -2021964151, -1468567895],\n",
       "         [ 1151972694, -1799902617, -1198892682,  ...,  1988463238,\n",
       "           1988598154,  1993980326],\n",
       "         ...,\n",
       "         [-2053399688,  2022086568,  -880175704,  ...,  1768393849,\n",
       "           1470519400,  -895124839],\n",
       "         [-1486395243, -1735100010, -1468425075,  ...,  2106087866,\n",
       "          -1768265896, -2004334967],\n",
       "         [ 2036884871,  2007529913, -1770342711,  ..., -1183225209,\n",
       "          -1213630329, -1476880505]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.13.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.32.self_attn.q_proj.qweight': tensor([[-2001180037, -1515676550, -1434932794,  ...,  1519949978,\n",
       "           1199089227,  1769507452],\n",
       "         [-1981179481, -1461294456,  1783142039,  ..., -1485334887,\n",
       "          -2005236116,  1273788325],\n",
       "         [-1500162231, -1467704981, -2037675371,  ...,  -949454471,\n",
       "          -2024175721, -2005439879],\n",
       "         ...,\n",
       "         [-2036484490, -2022013114,  2038929593,  ...,  2021292662,\n",
       "           2071508169, -1970680167],\n",
       "         [-1785120859, -1769318504, -1987659367,  ...,  2022480263,\n",
       "           1754748809,  2023259029],\n",
       "         [-2007647047,  2053282405, -1966442602,  ..., -1818589096,\n",
       "           2021095575,  1751619181]], dtype=torch.int32),\n",
       " 'llm.model.layers.39.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.1.self_attn.q_proj.qweight': tensor([[ 1972550258,  1817272700, -2121687957,  ...,  -947948923,\n",
       "           -445934114,  2021550454],\n",
       "         [-1319925596,  1936098433, -1571192953,  ..., -2124445834,\n",
       "          -1833659545,  2072675708],\n",
       "         [ 1968813444, -1248759420, -1368098425,  ..., -1318423174,\n",
       "          -1283173793,  1971551869],\n",
       "         ...,\n",
       "         [-1972406902, -2122941574, -1186622591,  ...,  2137821301,\n",
       "           -750025646,  1869905537],\n",
       "         [ 1301186673,  1972410260,  1754232430,  ...,  1919842207,\n",
       "           1969908660, -2088206733],\n",
       "         [-1503048273,  1552198292, -1875540353,  ...,  1403022212,\n",
       "          -1136489895,  2022537862]], dtype=torch.int32),\n",
       " 'llm.model.layers.5.self_attn.q_proj.qweight': tensor([[ 1673108326, -1467651994,  1523346070,  ...,  1703179386,\n",
       "          -1921140326, -1232647527],\n",
       "         [-1686738838, -1972730535, -1786271867,  ..., -1685623701,\n",
       "          -1701403491, -1769629528],\n",
       "         [-1989573237,  1819829429,  1715112632,  ..., -1429567015,\n",
       "          -2054649702,  1216841595],\n",
       "         ...,\n",
       "         [ 1772459381, -1714833306,  2002343560,  ...,  -931821224,\n",
       "           1523030616, -1484101225],\n",
       "         [ 2021037978, -1720211320,  1754498966,  ...,  1737393277,\n",
       "            982685623, -1498774442],\n",
       "         [-1718203494, -1702340215,  2023210118,  ..., -2006607223,\n",
       "          -2055701868, -1648203384]], dtype=torch.int32),\n",
       " 'llm.model.layers.10.input_layernorm.weight': tensor([1.1797, 1.1250, 0.8398,  ..., 0.9297, 1.0547, 0.2754]),\n",
       " 'vpm.encoder.layers.25.self_attn.out_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.41.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.47.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.2.mlp.fc2.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.46.self_attn.q_proj.qweight': tensor([[ 1484298070,  2041092202, -1470592853,  ...,  1771681674,\n",
       "           2019981416, -2006292118],\n",
       "         [-1719231642,  1183095450,  1722513032,  ..., -2003003511,\n",
       "           1235727755,  1820812679],\n",
       "         [ -727213158, -1950771561, -1675053159,  ..., -1936095575,\n",
       "          -2005374791, -1098340249],\n",
       "         ...,\n",
       "         [-1663542952, -1704355223,  1721341337,  ...,  1184397719,\n",
       "          -2022139752, -1464100968],\n",
       "         [-1500072028,  2024499849,  1786554513,  ..., -1731536518,\n",
       "          -1191729546, -1721125782],\n",
       "         [-1936099197,  2040043638,  1986492121,  ..., -1734838117,\n",
       "          -1984267878, -1750566249]], dtype=torch.int32),\n",
       " 'llm.model.layers.47.self_attn.k_proj.scales': tensor([[0.0115, 0.0113, 0.0133, 0.0117, 0.0107, 0.0106, 0.0106, 0.0112, 0.0143,\n",
       "          0.0145, 0.0130, 0.0151, 0.0140, 0.0122, 0.0146, 0.0133, 0.0162, 0.0189,\n",
       "          0.0190, 0.0177, 0.0123, 0.0188, 0.0208, 0.0288, 0.0242, 0.0309, 0.0314,\n",
       "          0.0319, 0.0333, 0.0247, 0.0217, 0.0277, 0.0102, 0.0112, 0.0122, 0.0109,\n",
       "          0.0102, 0.0097, 0.0127, 0.0096, 0.0103, 0.0098, 0.0148, 0.0094, 0.0134,\n",
       "          0.0161, 0.0150, 0.0148, 0.0145, 0.0159, 0.0145, 0.0189, 0.0269, 0.0199,\n",
       "          0.0211, 0.0283, 0.0316, 0.0300, 0.0283, 0.0357, 0.0378, 0.0289, 0.0249,\n",
       "          0.0273, 0.0165, 0.0227, 0.0210, 0.0221, 0.0216, 0.0215, 0.0173, 0.0253,\n",
       "          0.0213, 0.0262, 0.0165, 0.0223, 0.0225, 0.0216, 0.0255, 0.0233, 0.0258,\n",
       "          0.0281, 0.0258, 0.0210, 0.0234, 0.0253, 0.0242, 0.0251, 0.0233, 0.0272,\n",
       "          0.0260, 0.0133, 0.0283, 0.0258, 0.0269, 0.0262, 0.0195, 0.0207, 0.0181,\n",
       "          0.0203, 0.0185, 0.0201, 0.0155, 0.0227, 0.0145, 0.0283, 0.0234, 0.0293,\n",
       "          0.0166, 0.0217, 0.0257, 0.0162, 0.0236, 0.0329, 0.0245, 0.0262, 0.0290,\n",
       "          0.0267, 0.0240, 0.0257, 0.0251, 0.0267, 0.0237, 0.0399, 0.0280, 0.0259,\n",
       "          0.0294, 0.0276, 0.0155, 0.0169, 0.0202, 0.0173, 0.0145, 0.0217, 0.0163,\n",
       "          0.0225, 0.0118, 0.0206, 0.0186, 0.0188, 0.0208, 0.0215, 0.0190, 0.0221,\n",
       "          0.0290, 0.0286, 0.0233, 0.0228, 0.0240, 0.0260, 0.0271, 0.0296, 0.0251,\n",
       "          0.0424, 0.0224, 0.0253, 0.0297, 0.0210, 0.0230, 0.0234, 0.0162, 0.0186,\n",
       "          0.0152, 0.0169, 0.0184, 0.0169, 0.0182, 0.0189, 0.0181, 0.0233, 0.0143,\n",
       "          0.0194, 0.0165, 0.0212, 0.0202, 0.0225, 0.0244, 0.0216, 0.0224, 0.0263,\n",
       "          0.0227, 0.0262, 0.0276, 0.0254, 0.0276, 0.0203, 0.0307, 0.0267, 0.0279,\n",
       "          0.0406, 0.0409, 0.0225, 0.0159, 0.0127, 0.0130, 0.0145, 0.0118, 0.0112,\n",
       "          0.0173, 0.0128, 0.0158, 0.0105, 0.0145, 0.0118, 0.0230, 0.0206, 0.0139,\n",
       "          0.0230, 0.0135, 0.0221, 0.0207, 0.0237, 0.0233, 0.0233, 0.0206, 0.0207,\n",
       "          0.0224, 0.0201, 0.0289, 0.0177, 0.0215, 0.0260, 0.0213, 0.0230, 0.0139,\n",
       "          0.0130, 0.0111, 0.0115, 0.0114, 0.0120, 0.0152, 0.0121, 0.0173, 0.0153,\n",
       "          0.0132, 0.0147, 0.0151, 0.0202, 0.0168, 0.0203, 0.0229, 0.0244, 0.0217,\n",
       "          0.0244, 0.0225, 0.0210, 0.0227, 0.0199, 0.0217, 0.0264, 0.0477, 0.0221,\n",
       "          0.0223, 0.0250, 0.0208, 0.0298, 0.0117, 0.0095, 0.0107, 0.0083, 0.0081,\n",
       "          0.0079, 0.0082, 0.0092, 0.0109, 0.0098, 0.0100, 0.0114, 0.0105, 0.0146,\n",
       "          0.0124, 0.0169, 0.0188, 0.0161, 0.0167, 0.0186, 0.0242, 0.0230, 0.0202,\n",
       "          0.0290, 0.0319, 0.0254, 0.0367, 0.0365, 0.0372, 0.0269, 0.0288, 0.0246,\n",
       "          0.0078, 0.0069, 0.0071, 0.0108, 0.0095, 0.0078, 0.0087, 0.0081, 0.0099,\n",
       "          0.0091, 0.0081, 0.0132, 0.0163, 0.0161, 0.0190, 0.0115, 0.0145, 0.0219,\n",
       "          0.0212, 0.0188, 0.0249, 0.0191, 0.0300, 0.0396, 0.0354, 0.0409, 0.0333,\n",
       "          0.0414, 0.0307, 0.0333, 0.0219, 0.0219, 0.0115, 0.0073, 0.0106, 0.0077,\n",
       "          0.0083, 0.0093, 0.0098, 0.0077, 0.0098, 0.0102, 0.0113, 0.0088, 0.0116,\n",
       "          0.0128, 0.0172, 0.0129, 0.0154, 0.0236, 0.0188, 0.0216, 0.0288, 0.0190,\n",
       "          0.0314, 0.0307, 0.0349, 0.0300, 0.0357, 0.0341, 0.0322, 0.0432, 0.0329,\n",
       "          0.0275, 0.0081, 0.0100, 0.0096, 0.0122, 0.0105, 0.0085, 0.0083, 0.0085,\n",
       "          0.0091, 0.0099, 0.0104, 0.0123, 0.0113, 0.0165, 0.0119, 0.0194, 0.0204,\n",
       "          0.0148, 0.0158, 0.0190, 0.0244, 0.0213, 0.0242, 0.0409, 0.0294, 0.0396,\n",
       "          0.0352, 0.0323, 0.0359, 0.0293, 0.0286, 0.0414, 0.0172, 0.0186, 0.0188,\n",
       "          0.0163, 0.0259, 0.0215, 0.0162, 0.0203, 0.0180, 0.0240, 0.0133, 0.0254,\n",
       "          0.0139, 0.0229, 0.0246, 0.0139, 0.0203, 0.0208, 0.0206, 0.0215, 0.0259,\n",
       "          0.0288, 0.0246, 0.0259, 0.0280, 0.0242, 0.0258, 0.0234, 0.0240, 0.0318,\n",
       "          0.0306, 0.0201, 0.0141, 0.0149, 0.0212, 0.0123, 0.0213, 0.0184, 0.0157,\n",
       "          0.0208, 0.0161, 0.0193, 0.0169, 0.0229, 0.0197, 0.0198, 0.0280, 0.0213,\n",
       "          0.0266, 0.0284, 0.0316, 0.0130, 0.0286, 0.0310, 0.0254, 0.0263, 0.0283,\n",
       "          0.0228, 0.0244, 0.0250, 0.0224, 0.0257, 0.0259, 0.0212, 0.0071, 0.0092,\n",
       "          0.0115, 0.0079, 0.0072, 0.0119, 0.0087, 0.0104, 0.0099, 0.0097, 0.0114,\n",
       "          0.0098, 0.0156, 0.0123, 0.0146, 0.0181, 0.0197, 0.0184, 0.0194, 0.0216,\n",
       "          0.0204, 0.0240, 0.0211, 0.0372, 0.0344, 0.0237, 0.0367, 0.0264, 0.0259,\n",
       "          0.0383, 0.0393, 0.0225, 0.0113, 0.0089, 0.0068, 0.0096, 0.0101, 0.0093,\n",
       "          0.0094, 0.0100, 0.0100, 0.0125, 0.0119, 0.0115, 0.0143, 0.0149, 0.0148,\n",
       "          0.0180, 0.0171, 0.0212, 0.0193, 0.0176, 0.0234, 0.0234, 0.0220, 0.0367,\n",
       "          0.0244, 0.0352, 0.0406, 0.0276, 0.0281, 0.0367, 0.0238, 0.0221]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.51.input_layernorm.weight': tensor([0.9492, 0.9844, 0.9336,  ..., 1.0156, 1.0078, 1.5703]),\n",
       " 'llm.model.layers.6.mlp.down_proj.qweight': tensor([[ 1803991209,  1751821196,  2073712535,  ...,  2055907224,\n",
       "          -1750751609, -1987606649],\n",
       "         [-1717077849, -1735620309, -1769498185,  ..., -1772902587,\n",
       "          -2003200171, -2022016646],\n",
       "         [-1498912152, -1987553639, -1232431002,  ...,  2053748325,\n",
       "           1235331720, -1735886471],\n",
       "         ...,\n",
       "         [ 1787213992, -2067023978, -1433753736,  ..., -1754626424,\n",
       "           1989569400,  2040035464],\n",
       "         [ 1989711736, -1971680857, -2022078806,  ...,  1770633897,\n",
       "            711543496,  2054523286],\n",
       "         [-1164273528,  1504205047,  1786284697,  ..., -2004199256,\n",
       "           1502128024, -1987536775]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.19.self_attn.k_proj.qweight': tensor([[ 1754688627,   994401931, -1435460776,  ...,  1414799541,\n",
       "          -1213415337,  2085590705],\n",
       "         [-1748072321,  1822582100,  1268884795,  ...,  1082500660,\n",
       "           1718131568, -1873639572],\n",
       "         [-1465413802,  1870687136, -1585599666,  ..., -2057853305,\n",
       "           1931123270, -1599825579],\n",
       "         ...,\n",
       "         [-1992320952, -1766484674,  1870298937,  ..., -1501906385,\n",
       "           1957059721,  1957987981],\n",
       "         [-1784785829, -1873847494, -1888440894,  ..., -1683123582,\n",
       "           1637847409,  1632788637],\n",
       "         [ 1247064130, -2121978217,  1115314581,  ...,  1720277923,\n",
       "          -2127725456,  1755561116]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.18.self_attn.v_proj.bias': tensor([ 0.0188, -0.0374, -0.0237,  ..., -0.0352, -0.0461, -0.0072]),\n",
       " 'llm.model.layers.50.mlp.down_proj.qweight': tensor([[ 1770289019,  1770363478, -1650034554,  ...,  -946112390,\n",
       "           2036946825, -1214608246],\n",
       "         [-1518110854,  -677730454,  2025428663,  ...,   945465783,\n",
       "          -2005362280, -1448560283],\n",
       "         [-1466348648,  2057733275, -1216968599,  ..., -1974049114,\n",
       "           2105047162,  -646538585],\n",
       "         ...,\n",
       "         [ 1750767224, -1433832775, -1986824025,  ...,  1753512153,\n",
       "          -1969842584,  1739950184],\n",
       "         [-1989703067, -2038933174,  2005440153,  ...,  -979928966,\n",
       "          -1465361754, -2088208575],\n",
       "         [-1517848710,  1204332667,  1785231016,  ...,  2108327531,\n",
       "           2005436295, -2007319688]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.4.self_attn.k_proj.qweight': tensor([[-2007013751, -1736407974,  1128626539,  ..., -1485347959,\n",
       "           1771072132, -1584423568],\n",
       "         [ 1989049682, -1732220257,  1416988053,  ...,  1748804839,\n",
       "          -1768917354, -2140369231],\n",
       "         [ 1232056960,  1434285461, -1937013104,  ...,  1738699870,\n",
       "           2090564264, -1940417904],\n",
       "         ...,\n",
       "         [-1750694556, -1717077143,  1968401257,  ..., -2106807169,\n",
       "          -1870884998, -1768649107],\n",
       "         [ 1768865145,  1197837136, -1986754939,  ..., -1421920359,\n",
       "           2072467575,  1182167665],\n",
       "         [ 1699518319, -2136237751,  1884852889,  ...,  1988587590,\n",
       "           2088396661,  1973521342]], dtype=torch.int32),\n",
       " 'llm.model.layers.26.self_attn.v_proj.scales': tensor([[0.0232, 0.0234, 0.0225, 0.0223, 0.0202, 0.0230, 0.0194, 0.0212, 0.0257,\n",
       "          0.0203, 0.0213, 0.0246, 0.0220, 0.0294, 0.0219, 0.0234, 0.0253, 0.0188,\n",
       "          0.0202, 0.0237, 0.0245, 0.0213, 0.0201, 0.0240, 0.0234, 0.0283, 0.0221,\n",
       "          0.0208, 0.0211, 0.0213, 0.0263, 0.0298, 0.0254, 0.0251, 0.0198, 0.0245,\n",
       "          0.0259, 0.0198, 0.0233, 0.0215, 0.0249, 0.0237, 0.0211, 0.0255, 0.0221,\n",
       "          0.0257, 0.0242, 0.0254, 0.0240, 0.0240, 0.0199, 0.0188, 0.0215, 0.0221,\n",
       "          0.0275, 0.0232, 0.0267, 0.0251, 0.0237, 0.0216, 0.0260, 0.0241, 0.0219,\n",
       "          0.0229, 0.0232, 0.0189, 0.0216, 0.0241, 0.0242, 0.0234, 0.0328, 0.0280,\n",
       "          0.0255, 0.0234, 0.0210, 0.0228, 0.0233, 0.0238, 0.0204, 0.0251, 0.0198,\n",
       "          0.0208, 0.0249, 0.0204, 0.0211, 0.0236, 0.0245, 0.0238, 0.0246, 0.0195,\n",
       "          0.0193, 0.0245, 0.0224, 0.0279, 0.0241, 0.0215, 0.0219, 0.0219, 0.0220,\n",
       "          0.0197, 0.0227, 0.0211, 0.0208, 0.0206, 0.0212, 0.0230, 0.0217, 0.0232,\n",
       "          0.0211, 0.0266, 0.0232, 0.0189, 0.0198, 0.0188, 0.0240, 0.0194, 0.0210,\n",
       "          0.0232, 0.0212, 0.0198, 0.0233, 0.0234, 0.0220, 0.0219, 0.0208, 0.0220,\n",
       "          0.0240, 0.0236, 0.0204, 0.0275, 0.0211, 0.0212, 0.0220, 0.0237, 0.0258,\n",
       "          0.0199, 0.0203, 0.0215, 0.0199, 0.0213, 0.0212, 0.0220, 0.0302, 0.0225,\n",
       "          0.0191, 0.0251, 0.0241, 0.0232, 0.0225, 0.0211, 0.0249, 0.0189, 0.0199,\n",
       "          0.0224, 0.0237, 0.0217, 0.0219, 0.0221, 0.0211, 0.0229, 0.0338, 0.0233,\n",
       "          0.0232, 0.0229, 0.0247, 0.0240, 0.0242, 0.0229, 0.0212, 0.0223, 0.0234,\n",
       "          0.0227, 0.0208, 0.0247, 0.0297, 0.0290, 0.0232, 0.0215, 0.0198, 0.0206,\n",
       "          0.0201, 0.0211, 0.0236, 0.0255, 0.0206, 0.0203, 0.0202, 0.0201, 0.0229,\n",
       "          0.0310, 0.0246, 0.0225, 0.0237, 0.0215, 0.0238, 0.0212, 0.0258, 0.0201,\n",
       "          0.0263, 0.0223, 0.0227, 0.0232, 0.0224, 0.0210, 0.0224, 0.0237, 0.0232,\n",
       "          0.0260, 0.0251, 0.0238, 0.0224, 0.0230, 0.0210, 0.0245, 0.0227, 0.0233,\n",
       "          0.0257, 0.0229, 0.0266, 0.0229, 0.0223, 0.0206, 0.0283, 0.0245, 0.0292,\n",
       "          0.0213, 0.0216, 0.0259, 0.0203, 0.0253, 0.0227, 0.0246, 0.0333, 0.0255,\n",
       "          0.0219, 0.0223, 0.0229, 0.0237, 0.0220, 0.0223, 0.0197, 0.0229, 0.0197,\n",
       "          0.0249, 0.0221, 0.0211, 0.0246, 0.0211, 0.0249, 0.0215, 0.0236, 0.0233,\n",
       "          0.0210, 0.0211, 0.0219, 0.0259, 0.0271, 0.0223, 0.0249, 0.0227, 0.0236,\n",
       "          0.0229, 0.0264, 0.0228, 0.0332, 0.0262, 0.0383, 0.0238, 0.0242, 0.0230,\n",
       "          0.0221, 0.0230, 0.0260, 0.0237, 0.0230, 0.0241, 0.0230, 0.0272, 0.0251,\n",
       "          0.0258, 0.0223, 0.0245, 0.0210, 0.0227, 0.0269, 0.0271, 0.0204, 0.0237,\n",
       "          0.0279, 0.0210, 0.0225, 0.0233, 0.0272, 0.0221, 0.0236, 0.0210, 0.0208,\n",
       "          0.0237, 0.0247, 0.0160, 0.0259, 0.0233, 0.0254, 0.0246, 0.0238, 0.0264,\n",
       "          0.0228, 0.0240, 0.0284, 0.0233, 0.0242, 0.0241, 0.0255, 0.0301, 0.0276,\n",
       "          0.0238, 0.0251, 0.0263, 0.0267, 0.0232, 0.0190, 0.0223, 0.0185, 0.0247,\n",
       "          0.0198, 0.0229, 0.0204, 0.0234, 0.0204, 0.0229, 0.0191, 0.0242, 0.0233,\n",
       "          0.0245, 0.0246, 0.0259, 0.0232, 0.0199, 0.0260, 0.0263, 0.0216, 0.0201,\n",
       "          0.0250, 0.0213, 0.0197, 0.0268, 0.0220, 0.0202, 0.0260, 0.0245, 0.0198,\n",
       "          0.0242, 0.0203, 0.0199, 0.0254, 0.0236, 0.0195, 0.0219, 0.0199, 0.0207,\n",
       "          0.0219, 0.0210, 0.0206, 0.0204, 0.0194, 0.0250, 0.0203, 0.0194, 0.0216,\n",
       "          0.0184, 0.0284, 0.0211, 0.0247, 0.0211, 0.0181, 0.0233, 0.0202, 0.0210,\n",
       "          0.0203, 0.0233, 0.0194, 0.0198, 0.0225, 0.0237, 0.0268, 0.0251, 0.0264,\n",
       "          0.0257, 0.0290, 0.0237, 0.0232, 0.0259, 0.0298, 0.0246, 0.0250, 0.0301,\n",
       "          0.0251, 0.0236, 0.0332, 0.0292, 0.0245, 0.0241, 0.0242, 0.0349, 0.0258,\n",
       "          0.0315, 0.0245, 0.0250, 0.0257, 0.0300, 0.0259, 0.0263, 0.0300, 0.0233,\n",
       "          0.0273, 0.0298, 0.0259, 0.0305, 0.0346, 0.0354, 0.0264, 0.0266, 0.0241,\n",
       "          0.0276, 0.0253, 0.0262, 0.0267, 0.0246, 0.0240, 0.0258, 0.0249, 0.0267,\n",
       "          0.0267, 0.0288, 0.0262, 0.0263, 0.0280, 0.0267, 0.0283, 0.0284, 0.0305,\n",
       "          0.0253, 0.0241, 0.0269, 0.0292, 0.0213, 0.0246, 0.0246, 0.0263, 0.0228,\n",
       "          0.0259, 0.0296, 0.0242, 0.0254, 0.0286, 0.0315, 0.0305, 0.0225, 0.0245,\n",
       "          0.0280, 0.0242, 0.0269, 0.0206, 0.0285, 0.0233, 0.0244, 0.0253, 0.0332,\n",
       "          0.0269, 0.0315, 0.0254, 0.0247, 0.0300, 0.0273, 0.0247, 0.0286, 0.0281,\n",
       "          0.0234, 0.0268, 0.0232, 0.0253, 0.0237, 0.0245, 0.0267, 0.0310, 0.0320,\n",
       "          0.0238, 0.0266, 0.0309, 0.0245, 0.0264, 0.0210, 0.0262, 0.0266, 0.0267,\n",
       "          0.0224, 0.0280, 0.0251, 0.0266, 0.0233, 0.0242, 0.0255, 0.0277, 0.0255,\n",
       "          0.0290, 0.0314, 0.0303, 0.0269, 0.0227, 0.0238, 0.0245, 0.0259]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.8.self_attn.v_proj.bias': tensor([ 0.0092, -0.0374,  0.0148,  ..., -0.0771, -0.0165,  0.0229]),\n",
       " 'vpm.encoder.layers.22.mlp.fc2.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.48.self_attn.v_proj.scales': tensor([[0.0327, 0.0310, 0.0332, 0.0378, 0.0388, 0.0301, 0.0362, 0.0448, 0.0323,\n",
       "          0.0354, 0.0370, 0.0393, 0.0322, 0.0435, 0.0406, 0.0331, 0.0354, 0.0365,\n",
       "          0.0320, 0.0362, 0.0329, 0.0362, 0.0399, 0.0289, 0.0341, 0.0357, 0.0352,\n",
       "          0.0306, 0.0322, 0.0346, 0.0338, 0.0259, 0.0354, 0.0327, 0.0320, 0.0383,\n",
       "          0.0331, 0.0336, 0.0388, 0.0324, 0.0370, 0.0346, 0.0354, 0.0409, 0.0391,\n",
       "          0.0341, 0.0357, 0.0318, 0.0375, 0.0362, 0.0328, 0.0329, 0.0341, 0.0344,\n",
       "          0.0297, 0.0367, 0.0326, 0.0391, 0.0336, 0.0349, 0.0331, 0.0346, 0.0320,\n",
       "          0.0346, 0.0372, 0.0391, 0.0419, 0.0315, 0.0435, 0.0385, 0.0372, 0.0357,\n",
       "          0.0344, 0.0383, 0.0409, 0.0409, 0.0344, 0.0365, 0.0362, 0.0404, 0.0362,\n",
       "          0.0341, 0.0352, 0.0367, 0.0383, 0.0464, 0.0438, 0.0391, 0.0424, 0.0417,\n",
       "          0.0341, 0.0406, 0.0383, 0.0479, 0.0414, 0.0385, 0.0401, 0.0365, 0.0359,\n",
       "          0.0349, 0.0370, 0.0378, 0.0430, 0.0357, 0.0383, 0.0367, 0.0341, 0.0445,\n",
       "          0.0346, 0.0396, 0.0393, 0.0438, 0.0380, 0.0367, 0.0399, 0.0318, 0.0393,\n",
       "          0.0370, 0.0370, 0.0378, 0.0391, 0.0450, 0.0406, 0.0375, 0.0445, 0.0422,\n",
       "          0.0385, 0.0383, 0.0300, 0.0288, 0.0283, 0.0352, 0.0309, 0.0267, 0.0286,\n",
       "          0.0298, 0.0311, 0.0289, 0.0309, 0.0305, 0.0305, 0.0319, 0.0303, 0.0276,\n",
       "          0.0315, 0.0260, 0.0286, 0.0314, 0.0269, 0.0331, 0.0260, 0.0302, 0.0303,\n",
       "          0.0312, 0.0263, 0.0301, 0.0301, 0.0289, 0.0306, 0.0250, 0.0311, 0.0297,\n",
       "          0.0283, 0.0312, 0.0286, 0.0297, 0.0281, 0.0316, 0.0316, 0.0307, 0.0288,\n",
       "          0.0331, 0.0297, 0.0283, 0.0316, 0.0305, 0.0306, 0.0324, 0.0255, 0.0262,\n",
       "          0.0336, 0.0300, 0.0318, 0.0237, 0.0311, 0.0344, 0.0244, 0.0316, 0.0281,\n",
       "          0.0271, 0.0260, 0.0298, 0.0346, 0.0344, 0.0290, 0.0326, 0.0272, 0.0380,\n",
       "          0.0409, 0.0341, 0.0288, 0.0349, 0.0301, 0.0327, 0.0411, 0.0352, 0.0324,\n",
       "          0.0309, 0.0352, 0.0320, 0.0333, 0.0302, 0.0336, 0.0284, 0.0309, 0.0383,\n",
       "          0.0289, 0.0336, 0.0271, 0.0362, 0.0336, 0.0323, 0.0393, 0.0293, 0.0306,\n",
       "          0.0320, 0.0275, 0.0388, 0.0367, 0.0367, 0.0333, 0.0320, 0.0354, 0.0341,\n",
       "          0.0267, 0.0284, 0.0315, 0.0346, 0.0320, 0.0357, 0.0375, 0.0372, 0.0414,\n",
       "          0.0306, 0.0391, 0.0341, 0.0332, 0.0411, 0.0385, 0.0310, 0.0329, 0.0303,\n",
       "          0.0292, 0.0331, 0.0307, 0.0307, 0.0542, 0.0445, 0.0568, 0.0448, 0.0576,\n",
       "          0.0422, 0.0581, 0.0602, 0.0464, 0.0432, 0.0503, 0.0458, 0.0456, 0.0531,\n",
       "          0.0508, 0.0586, 0.0500, 0.0461, 0.0568, 0.0591, 0.0586, 0.0497, 0.0516,\n",
       "          0.0438, 0.0586, 0.0529, 0.0531, 0.0471, 0.0573, 0.0503, 0.0511, 0.0458,\n",
       "          0.0550, 0.0589, 0.0505, 0.0477, 0.0594, 0.0482, 0.0602, 0.0487, 0.0596,\n",
       "          0.0565, 0.0469, 0.0523, 0.0503, 0.0550, 0.0513, 0.0464, 0.0521, 0.0448,\n",
       "          0.0547, 0.0438, 0.0539, 0.0565, 0.0531, 0.0497, 0.0523, 0.0482, 0.0427,\n",
       "          0.0557, 0.0404, 0.0552, 0.0568, 0.0557, 0.0503, 0.0518, 0.0568, 0.0560,\n",
       "          0.0469, 0.0550, 0.0576, 0.0581, 0.0547, 0.0503, 0.0516, 0.0511, 0.0578,\n",
       "          0.0555, 0.0443, 0.0557, 0.0490, 0.0508, 0.0602, 0.0445, 0.0594, 0.0513,\n",
       "          0.0526, 0.0482, 0.0471, 0.0492, 0.0492, 0.0461, 0.0552, 0.0490, 0.0630,\n",
       "          0.0456, 0.0461, 0.0513, 0.0627, 0.0523, 0.0659, 0.0461, 0.0544, 0.0536,\n",
       "          0.0550, 0.0521, 0.0542, 0.0513, 0.0516, 0.0469, 0.0687, 0.0521, 0.0450,\n",
       "          0.0461, 0.0477, 0.0511, 0.0612, 0.0466, 0.0422, 0.0466, 0.0492, 0.0542,\n",
       "          0.0469, 0.0555, 0.0534, 0.0461, 0.0511, 0.0422, 0.0492, 0.0581, 0.0380,\n",
       "          0.0401, 0.0458, 0.0411, 0.0469, 0.0474, 0.0523, 0.0508, 0.0560, 0.0508,\n",
       "          0.0607, 0.0677, 0.0432, 0.0430, 0.0552, 0.0511, 0.0508, 0.0492, 0.0547,\n",
       "          0.0409, 0.0521, 0.0484, 0.0550, 0.0414, 0.0544, 0.0440, 0.0547, 0.0417,\n",
       "          0.0560, 0.0497, 0.0570, 0.0482, 0.0492, 0.0536, 0.0578, 0.0635, 0.0550,\n",
       "          0.0482, 0.0557, 0.0477, 0.0576, 0.0450, 0.0492, 0.0562, 0.0521, 0.0427,\n",
       "          0.0552, 0.0604, 0.0547, 0.0529, 0.0578, 0.0479, 0.0550, 0.0479, 0.0471,\n",
       "          0.0417, 0.0521, 0.0589, 0.0427, 0.0539, 0.0471, 0.0456, 0.0346, 0.0332,\n",
       "          0.0333, 0.0288, 0.0341, 0.0329, 0.0316, 0.0352, 0.0378, 0.0346, 0.0322,\n",
       "          0.0375, 0.0333, 0.0349, 0.0417, 0.0305, 0.0326, 0.0315, 0.0352, 0.0302,\n",
       "          0.0344, 0.0333, 0.0312, 0.0312, 0.0277, 0.0344, 0.0388, 0.0300, 0.0318,\n",
       "          0.0326, 0.0380, 0.0332, 0.0346, 0.0346, 0.0319, 0.0310, 0.0385, 0.0316,\n",
       "          0.0322, 0.0315, 0.0331, 0.0367, 0.0427, 0.0311, 0.0277, 0.0303, 0.0354,\n",
       "          0.0292, 0.0385, 0.0338, 0.0341, 0.0322, 0.0320, 0.0279, 0.0383, 0.0404,\n",
       "          0.0344, 0.0290, 0.0301, 0.0422, 0.0309, 0.0301, 0.0385, 0.0296]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.17.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.36.self_attn.v_proj.qweight': tensor([[-1970702472,  1970640278,  1736873608,  ..., -1467513177,\n",
       "           1971825335,  1719232681],\n",
       "         [-2002155113, -2005497975, -1468627110,  ..., -2003199814,\n",
       "           -983070376, -2024240695],\n",
       "         [-1717929847, -1734895463,  1771542904,  ..., -1449364056,\n",
       "          -1732793798, -1990760310],\n",
       "         ...,\n",
       "         [-2038007929,  1738115712, -2003277480,  ...,  2057935479,\n",
       "          -2038659207, -2021025671],\n",
       "         [-1986495883, -2067302265, -1734702934,  ..., -1485277320,\n",
       "           1753721496, -2003277960],\n",
       "         [ 2005505672,  -963020411,  2005575543,  ...,  2024053111,\n",
       "           2003203192, -1488737641]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.5.self_attn.out_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.32.input_layernorm.weight': tensor([2.6719, 2.4062, 0.7148,  ..., 0.8516, 2.5781, 0.1934]),\n",
       " 'llm.model.layers.11.mlp.down_proj.qweight': tensor([[-1985508744,  1987794843, -1969510759,  ...,  1703319977,\n",
       "          -2021042788,  1987680409],\n",
       "         [ 1752594360, -2022016329, -1500088952,  ..., -1217954920,\n",
       "          -1448510807, -1735608202],\n",
       "         [-1487501978,  -964078216, -2037926279,  ..., -1185323414,\n",
       "           1754896503,  1486326680],\n",
       "         ...,\n",
       "         [-1486190733, -1464088664,  1790478761,  ..., -2037729911,\n",
       "           1789499562, -1753700199],\n",
       "         [ 2005440634, -2018064231,  1484495495,  ...,  1989711511,\n",
       "           1987606918,  2023323799],\n",
       "         [-1987681111, -1718061176,  1201235900,  ..., -1482274468,\n",
       "          -1987593860,  2038860169]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.23.mlp.fc2.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.6.mlp.fc1.scales': tensor([[0.0008, 0.0009, 0.0008,  ..., 0.0010, 0.0010, 0.0009]]),\n",
       " 'llm.model.layers.23.self_attn.v_proj.qweight': tensor([[-1733785525,  1484307093, -1736935273,  ..., -1450657415,\n",
       "           1467516842,  2043107686],\n",
       "         [-1753835125,  2004318088, -1721002105,  ..., -1770419285,\n",
       "          -1686795659,  2055047526],\n",
       "         [-1264019605, -1952941973,  2020111720,  ..., -1468573572,\n",
       "            980915880, -1184396650],\n",
       "         ...,\n",
       "         [-1986410393, -2034603623, -1971742328,  ..., -1986491271,\n",
       "          -1486255496,  1754773127],\n",
       "         [ 1200916139,  1787468423,  2038732443,  ..., -1182431112,\n",
       "          -1735902553,  1988601301],\n",
       "         [-2022057571,  2024241286, -2005485208,  ...,  2006358406,\n",
       "           1453869224,  1823980117]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.1.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.43.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.20.layer_norm1.weight': tensor([1.2812, 1.2344, 1.2812,  ..., 1.2344, 1.1484, 1.2891]),\n",
       " 'vpm.encoder.layers.5.mlp.fc2.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.10.self_attn.v_proj.qweight': tensor([[-2033747835, -2020960138,  1489671545,  ..., -1718180217,\n",
       "          -1973975386, -1733977784],\n",
       "         [-1197894025, -1972861271,  1770551917,  ..., -1215663463,\n",
       "          -1219008664, -1518957175],\n",
       "         [  695780730, -1754048840, -1997112709,  ..., -1988729158,\n",
       "          -2092341541, -1985627991],\n",
       "         ...,\n",
       "         [ 1990884232,  2010618489, -2037811895,  ..., -1972864137,\n",
       "           1770227847, -1197951115],\n",
       "         [-2004051575,  1250387127, -1987740294,  ..., -1450740040,\n",
       "          -1467578264, -2004333433],\n",
       "         [-1968670043, -1685480806, -2000385894,  ..., -2005370710,\n",
       "          -1738033273,  2070510792]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.19.mlp.fc2.scales': tensor([[0.0009, 0.0011, 0.0011,  ..., 0.0009, 0.0010, 0.0010]]),\n",
       " 'llm.model.layers.19.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.5.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.24.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.6.mlp.fc2.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.10.mlp.fc1.bias': tensor([-1.7891, -1.7656, -1.2812,  ..., -0.7109, -1.2812, -0.5664]),\n",
       " 'llm.model.layers.38.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.42.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.0.self_attn.v_proj.qweight': tensor([[-1504872812, -1787725934,  1548468089,  ..., -2123461250,\n",
       "          -2102937230, -1725464434],\n",
       "         [-2038267752, -1685680518,  2067768949,  ..., -1866441354,\n",
       "          -1468367251,  1820155775],\n",
       "         [-2138144901,  1686602119, -1988789939,  ..., -1481605594,\n",
       "          -1515793259,  2137023326],\n",
       "         ...,\n",
       "         [ 1368290682, -2023070349, -1805341254,  ...,  2070587016,\n",
       "           1533434732, -1939969943],\n",
       "         [-1953214603, -1520141167,  1486916731,  ..., -1231652517,\n",
       "           2054196346,  1790155393],\n",
       "         [ 1971095169,  2021294893, -2124839855,  ...,  1332985965,\n",
       "          -1803404115, -1701684064]], dtype=torch.int32),\n",
       " 'llm.model.layers.28.mlp.up_proj.scales': tensor([[0.0220, 0.0220, 0.0290,  ..., 0.0259, 0.0365, 0.0227]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.1.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.28.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.1.post_attention_layernorm.weight': tensor([0.6445, 0.5352, 0.4727,  ..., 0.4785, 0.5977, 0.1514]),\n",
       " 'llm.model.layers.48.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.0.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.7.self_attn.o_proj.qweight': tensor([[  930953370, -1750305417, -2002039929,  ..., -1718069095,\n",
       "          -1435935849,  1956067432],\n",
       "         [ 1777968969,  2039047800, -1953785710,  ..., -1971751067,\n",
       "          -1216719209,  1504331671],\n",
       "         [ 1231607896,  -944010617,  1752803738,  ..., -1770821528,\n",
       "          -1770145898, -1183290967],\n",
       "         ...,\n",
       "         [ 2004199800,  1181178230, -2073322613,  ..., -1234801562,\n",
       "           1492744266,  2056878200],\n",
       "         [ 1991940486,  2024384904,  2023611513,  ..., -1719220344,\n",
       "           2004452488, -2007463753],\n",
       "         [ 1742448489,  1451649147,  1399167078,  ..., -2038981737,\n",
       "           1523156344, -1197868984]], dtype=torch.int32),\n",
       " 'llm.model.layers.27.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.11.mlp.fc1.bias': tensor([-0.9414, -1.6094, -0.9688,  ..., -1.8125, -1.0078, -1.5547]),\n",
       " 'llm.model.layers.35.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.8.mlp.down_proj.qweight': tensor([[ 1770502278, -1451861337, -1967679880,  ...,  2004461990,\n",
       "          -1989580664, -2019981656],\n",
       "         [-1448712298, -2022086533, -2004395705,  ..., -1450535015,\n",
       "          -1399420533, -2022274952],\n",
       "         [-1970825030,  1987413879, -2008245899,  ...,  1200978040,\n",
       "          -1539994935, -2020042872],\n",
       "         ...,\n",
       "         [-1482246263,  -661158746, -2001381718,  ...,  1456175992,\n",
       "           2039060410, -1752586855],\n",
       "         [-2051368807, -2002094200,  1718264970,  ..., -1953986407,\n",
       "           1438280330, -1523026262],\n",
       "         [-1701344856, -2005362314, -2085059210,  ..., -1749629240,\n",
       "           1481009046, -2022267019]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.12.self_attn.v_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.10.self_attn.q_proj.bias': tensor([ 0.0488, -0.2100,  1.4297,  ..., -0.0693, -0.0356, -0.0381]),\n",
       " 'vpm.encoder.layers.21.self_attn.out_proj.bias': tensor([ 0.0090,  0.1069, -0.0815,  ...,  0.1885, -0.0095, -0.0112]),\n",
       " 'llm.model.layers.13.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.51.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.23.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.25.self_attn.q_proj.qweight': tensor([[-1766570655, -2086045041,  2055751280,  ...,  1984732808,\n",
       "           1869505431,  1752799305],\n",
       "         [ 2091154264, -2139374726,  1934189441,  ..., -1754511229,\n",
       "           -843878319,  1922087825],\n",
       "         [ 2023396965, -1520855667, -1032282955,  ..., -2127338311,\n",
       "          -2036565347, -1444500872],\n",
       "         ...,\n",
       "         [-1870816128,  1295882111, -1907258785,  ...,  2128639611,\n",
       "           1732547766,  1921108643],\n",
       "         [-2124111776, -1634881694, -1752658783,  ...,  1954335598,\n",
       "           1503232447, -1654153343],\n",
       "         [ 2106552429, -1818206846,  1380571047,  ...,  1834651485,\n",
       "          -1179300460,  1921932408]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.13.layer_norm1.weight': tensor([0.8359, 0.8633, 1.0312,  ..., 0.6641, 0.8047, 0.8633]),\n",
       " 'vpm.encoder.layers.9.layer_norm2.weight': tensor([0.7266, 0.9883, 0.9766,  ..., 0.7773, 0.8477, 1.0391]),\n",
       " 'llm.model.layers.16.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.42.self_attn.v_proj.qweight': tensor([[ 2023324519, -1738049415,  2072537529,  ..., -1754822040,\n",
       "           1467451496, -2007394933],\n",
       "         [ 1987676227,  2036828325,  2072349030,  ..., -2003269272,\n",
       "           1217038453, -1749841494],\n",
       "         [-1770616698, -1433036886, -1183213209,  ...,  2023193496,\n",
       "          -1747412574,  1482271095],\n",
       "         ...,\n",
       "         [-2021033593, -1751557495,  2004445624,  ...,  2038995303,\n",
       "           1431926617, -1465415735],\n",
       "         [-2021160549, -1735874138,  1753794458,  ..., -2004256632,\n",
       "          -2004309845,  1754892472],\n",
       "         [-2004318569,  2037938328, -1201047161,  ...,  2016962232,\n",
       "           2040170872,  2025420664]], dtype=torch.int32),\n",
       " 'llm.model.layers.3.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.9.self_attn.out_proj.bias': tensor([-0.1128, -0.0481, -0.1245,  ...,  0.0938, -0.0288, -0.0850]),\n",
       " 'llm.model.layers.8.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.0.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.31.mlp.gate_proj.qweight': tensor([[  982026650,  1987610780, -1986615159,  ...,  2053666601,\n",
       "           1752666809, -1463248759],\n",
       "         [-1149733221,  1516930921,  2021095622,  ...,  2027472280,\n",
       "           2088269975,  1766360966],\n",
       "         [-1752664469, -2070173207, -1981118357,  ..., -1800825736,\n",
       "          -1719232072,  2023131287],\n",
       "         ...,\n",
       "         [ 1554675816, -2018937211, -1500075610,  ...,  2071435450,\n",
       "           1500224376, -1282835561],\n",
       "         [-1199913660, -1717004681, -1719949434,  ...,  1769628744,\n",
       "           1804233110,  2073725303],\n",
       "         [-1802065592, -1532388984, -1736873353,  ..., -1735673529,\n",
       "           2038143370, -1748399689]], dtype=torch.int32),\n",
       " 'llm.model.layers.21.self_attn.k_proj.scales': tensor([[0.0152, 0.0207, 0.0163, 0.0208, 0.0253, 0.0143, 0.0204, 0.0254, 0.0181,\n",
       "          0.0288, 0.0189, 0.0258, 0.0238, 0.0199, 0.0229, 0.0227, 0.0210, 0.0227,\n",
       "          0.0254, 0.0269, 0.0240, 0.0229, 0.0189, 0.0221, 0.0279, 0.0469, 0.0275,\n",
       "          0.0213, 0.0285, 0.0260, 0.0388, 0.0290, 0.0201, 0.0206, 0.0168, 0.0215,\n",
       "          0.0234, 0.0216, 0.0195, 0.0228, 0.0207, 0.0216, 0.0207, 0.0216, 0.0254,\n",
       "          0.0195, 0.0233, 0.0204, 0.0253, 0.0232, 0.0217, 0.0185, 0.0275, 0.0284,\n",
       "          0.0260, 0.0271, 0.0281, 0.0203, 0.0336, 0.0285, 0.0279, 0.0289, 0.0341,\n",
       "          0.0240, 0.0163, 0.0159, 0.0144, 0.0158, 0.0137, 0.0173, 0.0201, 0.0212,\n",
       "          0.0273, 0.0215, 0.0275, 0.0250, 0.0257, 0.0166, 0.0267, 0.0201, 0.0263,\n",
       "          0.0324, 0.0352, 0.0311, 0.0285, 0.0263, 0.0331, 0.0268, 0.0255, 0.0297,\n",
       "          0.0450, 0.0288, 0.0289, 0.0290, 0.0327, 0.0288, 0.0165, 0.0169, 0.0172,\n",
       "          0.0180, 0.0181, 0.0176, 0.0217, 0.0207, 0.0233, 0.0293, 0.0264, 0.0219,\n",
       "          0.0230, 0.0284, 0.0236, 0.0227, 0.0290, 0.0266, 0.0234, 0.0310, 0.0352,\n",
       "          0.0277, 0.0289, 0.0294, 0.0324, 0.0314, 0.0185, 0.0272, 0.0288, 0.0272,\n",
       "          0.0284, 0.0285, 0.0153, 0.0165, 0.0327, 0.0144, 0.0240, 0.0199, 0.0211,\n",
       "          0.0189, 0.0230, 0.0201, 0.0257, 0.0257, 0.0271, 0.0220, 0.0281, 0.0268,\n",
       "          0.0232, 0.0298, 0.0267, 0.0266, 0.0250, 0.0338, 0.0281, 0.0314, 0.0312,\n",
       "          0.0276, 0.0450, 0.0286, 0.0242, 0.0260, 0.0253, 0.0267, 0.0204, 0.0172,\n",
       "          0.0194, 0.0260, 0.0259, 0.0314, 0.0233, 0.0314, 0.0236, 0.0250, 0.0264,\n",
       "          0.0217, 0.0277, 0.0250, 0.0283, 0.0309, 0.0262, 0.0332, 0.0238, 0.0404,\n",
       "          0.0289, 0.0296, 0.0156, 0.0290, 0.0245, 0.0273, 0.0150, 0.0281, 0.0311,\n",
       "          0.0223, 0.0262, 0.0365, 0.0146, 0.0120, 0.0135, 0.0140, 0.0125, 0.0138,\n",
       "          0.0128, 0.0257, 0.0189, 0.0255, 0.0250, 0.0296, 0.0365, 0.0259, 0.0267,\n",
       "          0.0318, 0.0249, 0.0247, 0.0259, 0.0301, 0.0338, 0.0272, 0.0311, 0.0275,\n",
       "          0.0279, 0.0305, 0.0228, 0.0268, 0.0293, 0.0318, 0.0259, 0.0227, 0.0147,\n",
       "          0.0087, 0.0109, 0.0106, 0.0137, 0.0165, 0.0158, 0.0349, 0.0279, 0.0232,\n",
       "          0.0260, 0.0255, 0.0237, 0.0305, 0.0331, 0.0289, 0.0249, 0.0246, 0.0227,\n",
       "          0.0221, 0.0258, 0.0165, 0.0251, 0.0288, 0.0247, 0.0272, 0.0568, 0.0234,\n",
       "          0.0271, 0.0257, 0.0318, 0.0244, 0.0154, 0.0110, 0.0107, 0.0115, 0.0118,\n",
       "          0.0120, 0.0161, 0.0118, 0.0135, 0.0156, 0.0275, 0.0221, 0.0264, 0.0391,\n",
       "          0.0375, 0.0612, 0.0456, 0.0427, 0.0482, 0.0474, 0.0484, 0.0495, 0.0469,\n",
       "          0.0464, 0.0511, 0.0521, 0.0570, 0.0375, 0.0471, 0.0594, 0.0349, 0.0482,\n",
       "          0.0145, 0.0113, 0.0128, 0.0125, 0.0102, 0.0117, 0.0104, 0.0140, 0.0211,\n",
       "          0.0150, 0.0150, 0.0204, 0.0236, 0.0306, 0.0327, 0.0560, 0.0380, 0.0503,\n",
       "          0.0521, 0.0552, 0.0602, 0.0589, 0.0471, 0.0427, 0.0495, 0.0393, 0.0534,\n",
       "          0.0352, 0.0542, 0.0513, 0.0385, 0.0448, 0.0154, 0.0213, 0.0172, 0.0165,\n",
       "          0.0177, 0.0177, 0.0178, 0.0182, 0.0186, 0.0210, 0.0161, 0.0198, 0.0249,\n",
       "          0.0202, 0.0208, 0.0184, 0.0189, 0.0245, 0.0244, 0.0197, 0.0206, 0.0409,\n",
       "          0.0318, 0.0677, 0.0906, 0.0333, 0.0253, 0.0359, 0.0336, 0.0272, 0.0257,\n",
       "          0.0443, 0.0174, 0.0132, 0.0152, 0.0162, 0.0198, 0.0232, 0.0177, 0.0246,\n",
       "          0.0171, 0.0173, 0.0206, 0.0217, 0.0176, 0.0208, 0.0181, 0.0194, 0.0228,\n",
       "          0.0238, 0.0203, 0.0237, 0.0215, 0.0411, 0.0208, 0.0241, 0.0139, 0.0229,\n",
       "          0.0385, 0.0219, 0.0539, 0.0388, 0.0211, 0.0341, 0.0156, 0.0145, 0.0144,\n",
       "          0.0143, 0.0126, 0.0173, 0.0197, 0.0133, 0.0232, 0.0184, 0.0147, 0.0297,\n",
       "          0.0198, 0.0244, 0.0215, 0.0173, 0.0288, 0.0238, 0.0262, 0.0303, 0.0319,\n",
       "          0.0284, 0.0246, 0.0312, 0.0435, 0.0324, 0.0388, 0.0396, 0.0311, 0.0249,\n",
       "          0.0284, 0.0283, 0.0154, 0.0162, 0.0120, 0.0120, 0.0125, 0.0124, 0.0156,\n",
       "          0.0152, 0.0211, 0.0178, 0.0168, 0.0233, 0.0138, 0.0206, 0.0247, 0.0199,\n",
       "          0.0217, 0.0275, 0.0333, 0.0378, 0.0275, 0.0266, 0.0268, 0.0285, 0.0378,\n",
       "          0.0362, 0.0555, 0.0354, 0.0300, 0.0259, 0.0346, 0.0253, 0.0198, 0.0380,\n",
       "          0.0224, 0.0215, 0.0210, 0.0234, 0.0238, 0.0283, 0.0223, 0.0301, 0.0297,\n",
       "          0.0315, 0.0211, 0.0319, 0.0309, 0.0276, 0.0298, 0.0271, 0.0305, 0.0276,\n",
       "          0.0283, 0.0365, 0.0322, 0.0320, 0.0311, 0.0298, 0.0338, 0.0286, 0.0305,\n",
       "          0.0220, 0.0288, 0.0268, 0.0448, 0.0198, 0.0227, 0.0212, 0.0201, 0.0306,\n",
       "          0.0234, 0.0286, 0.0244, 0.0300, 0.0268, 0.0362, 0.0279, 0.0314, 0.0312,\n",
       "          0.0263, 0.0290, 0.0440, 0.0272, 0.0294, 0.0341, 0.0331, 0.0310, 0.0385,\n",
       "          0.0326, 0.0305, 0.0173, 0.0309, 0.0290, 0.0311, 0.0267, 0.0301]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.1.self_attn.k_proj.scales': tensor([[0.0182, 0.0144, 0.0104, 0.0117, 0.0128, 0.0225, 0.0093, 0.0262, 0.0315,\n",
       "          0.0232, 0.0271, 0.0293, 0.0198, 0.0245, 0.0354, 0.0262, 0.0289, 0.0264,\n",
       "          0.0301, 0.0289, 0.0275, 0.0272, 0.0283, 0.0312, 0.0276, 0.0328, 0.0709,\n",
       "          0.0258, 0.0246, 0.0289, 0.0370, 0.0309, 0.0155, 0.0108, 0.0128, 0.0126,\n",
       "          0.0132, 0.0268, 0.0140, 0.0307, 0.0264, 0.0131, 0.0276, 0.0254, 0.0254,\n",
       "          0.0254, 0.0264, 0.0352, 0.0258, 0.0283, 0.0272, 0.0213, 0.0234, 0.0233,\n",
       "          0.0272, 0.0314, 0.0246, 0.0277, 0.0156, 0.0292, 0.0262, 0.0279, 0.0262,\n",
       "          0.0285, 0.0260, 0.0193, 0.0276, 0.0145, 0.0253, 0.0297, 0.0306, 0.0307,\n",
       "          0.0250, 0.0269, 0.0322, 0.0297, 0.0316, 0.0333, 0.0292, 0.0318, 0.0290,\n",
       "          0.0341, 0.0352, 0.0303, 0.0365, 0.0237, 0.0312, 0.0312, 0.0344, 0.0332,\n",
       "          0.0198, 0.0365, 0.0275, 0.0284, 0.0354, 0.0211, 0.0189, 0.0197, 0.0197,\n",
       "          0.0204, 0.0294, 0.0237, 0.0247, 0.0286, 0.0227, 0.0311, 0.0224, 0.0322,\n",
       "          0.0328, 0.0314, 0.0191, 0.0367, 0.0221, 0.0300, 0.0290, 0.0318, 0.0316,\n",
       "          0.0354, 0.0301, 0.0303, 0.0242, 0.0318, 0.0755, 0.0333, 0.0268, 0.0315,\n",
       "          0.0289, 0.0344, 0.0189, 0.0186, 0.0144, 0.0216, 0.0169, 0.0203, 0.0169,\n",
       "          0.0227, 0.0223, 0.0213, 0.0233, 0.0240, 0.0251, 0.0319, 0.0238, 0.0241,\n",
       "          0.0184, 0.0262, 0.0280, 0.0259, 0.0277, 0.0174, 0.0277, 0.0246, 0.0220,\n",
       "          0.0151, 0.0257, 0.0230, 0.0251, 0.0232, 0.0288, 0.0227, 0.0198, 0.0197,\n",
       "          0.0165, 0.0193, 0.0134, 0.0263, 0.0153, 0.0191, 0.0212, 0.0233, 0.0228,\n",
       "          0.0189, 0.0290, 0.0249, 0.0184, 0.0254, 0.0242, 0.0254, 0.0260, 0.0246,\n",
       "          0.0241, 0.0354, 0.0271, 0.0233, 0.0255, 0.0781, 0.0236, 0.0267, 0.0293,\n",
       "          0.0219, 0.0212, 0.0262, 0.0223, 0.0154, 0.0271, 0.0181, 0.0293, 0.0310,\n",
       "          0.0266, 0.0292, 0.0249, 0.0285, 0.0333, 0.0113, 0.0152, 0.0294, 0.0156,\n",
       "          0.0276, 0.0242, 0.0319, 0.0162, 0.0318, 0.0362, 0.0320, 0.0362, 0.0380,\n",
       "          0.0331, 0.0352, 0.0461, 0.0417, 0.0365, 0.0332, 0.0328, 0.0312, 0.0320,\n",
       "          0.0186, 0.0203, 0.0283, 0.0297, 0.0277, 0.0159, 0.0244, 0.0113, 0.0276,\n",
       "          0.0246, 0.0213, 0.0146, 0.0306, 0.0133, 0.0292, 0.0272, 0.0320, 0.0158,\n",
       "          0.0375, 0.0332, 0.0323, 0.0312, 0.0316, 0.0333, 0.0318, 0.0284, 0.0327,\n",
       "          0.0336, 0.0312, 0.0311, 0.0338, 0.0148, 0.0216, 0.0127, 0.0206, 0.0177,\n",
       "          0.0199, 0.0150, 0.0212, 0.0189, 0.0230, 0.0263, 0.0244, 0.0216, 0.0246,\n",
       "          0.0195, 0.0193, 0.0213, 0.0221, 0.0184, 0.0223, 0.0227, 0.0234, 0.0199,\n",
       "          0.0206, 0.0262, 0.0075, 0.0227, 0.0189, 0.0776, 0.0215, 0.0224, 0.0221,\n",
       "          0.0367, 0.0148, 0.0194, 0.0165, 0.0158, 0.0177, 0.0204, 0.0206, 0.0197,\n",
       "          0.0210, 0.0228, 0.0185, 0.0212, 0.0215, 0.0204, 0.0185, 0.0269, 0.0273,\n",
       "          0.0242, 0.0181, 0.0201, 0.0189, 0.0225, 0.0195, 0.0204, 0.0682, 0.0201,\n",
       "          0.0210, 0.0171, 0.0194, 0.0203, 0.0204, 0.0135, 0.0143, 0.0151, 0.0131,\n",
       "          0.0137, 0.0122, 0.0096, 0.0125, 0.0154, 0.0148, 0.0163, 0.0254, 0.0362,\n",
       "          0.0233, 0.0440, 0.0336, 0.0419, 0.0396, 0.0399, 0.0445, 0.0372, 0.0344,\n",
       "          0.0365, 0.0327, 0.0320, 0.0251, 0.0349, 0.0393, 0.0327, 0.0365, 0.0380,\n",
       "          0.0380, 0.0146, 0.0154, 0.0146, 0.0129, 0.0132, 0.0160, 0.0133, 0.0118,\n",
       "          0.0129, 0.0177, 0.0254, 0.0185, 0.0318, 0.0346, 0.0370, 0.0391, 0.0375,\n",
       "          0.0372, 0.0383, 0.0346, 0.0370, 0.0309, 0.0341, 0.0336, 0.0396, 0.0380,\n",
       "          0.0315, 0.0328, 0.0396, 0.0332, 0.0456, 0.0293, 0.0237, 0.0280, 0.0158,\n",
       "          0.0193, 0.0419, 0.0268, 0.0249, 0.0223, 0.0242, 0.0163, 0.0199, 0.0259,\n",
       "          0.0242, 0.0236, 0.0245, 0.0250, 0.0220, 0.0253, 0.0250, 0.0223, 0.0271,\n",
       "          0.0193, 0.0223, 0.0236, 0.0242, 0.0255, 0.0201, 0.0255, 0.0870, 0.0220,\n",
       "          0.0242, 0.0189, 0.0213, 0.0178, 0.0249, 0.0268, 0.0288, 0.0213, 0.0234,\n",
       "          0.0178, 0.0238, 0.0238, 0.0238, 0.0162, 0.0237, 0.0268, 0.0232, 0.0266,\n",
       "          0.0225, 0.0281, 0.0236, 0.0242, 0.0233, 0.0314, 0.0228, 0.0238, 0.0246,\n",
       "          0.0293, 0.0254, 0.0223, 0.0172, 0.0266, 0.0292, 0.0258, 0.0181, 0.0182,\n",
       "          0.0151, 0.0314, 0.0285, 0.0301, 0.0247, 0.0300, 0.0245, 0.0306, 0.0289,\n",
       "          0.0311, 0.0264, 0.0305, 0.0298, 0.0273, 0.0180, 0.0286, 0.0215, 0.0314,\n",
       "          0.0289, 0.0272, 0.0247, 0.0182, 0.0293, 0.0277, 0.0263, 0.0140, 0.0283,\n",
       "          0.0259, 0.0267, 0.0303, 0.0146, 0.0189, 0.0173, 0.0324, 0.0326, 0.0276,\n",
       "          0.0264, 0.0275, 0.0147, 0.0267, 0.0273, 0.0289, 0.0174, 0.0281, 0.0277,\n",
       "          0.0225, 0.0286, 0.0293, 0.0190, 0.0244, 0.0273, 0.0319, 0.0264, 0.0193,\n",
       "          0.0284, 0.0267, 0.0275, 0.0581, 0.0309, 0.0286, 0.0324, 0.0277]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.17.layer_norm1.bias': tensor([ 0.1309, -0.0245, -0.0884,  ...,  0.1040, -0.0564, -0.1123]),\n",
       " 'llm.model.layers.0.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.34.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.1.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.6.input_layernorm.weight': tensor([0.9219, 0.8242, 0.6016,  ..., 0.6523, 0.7578, 0.3457]),\n",
       " 'llm.model.layers.15.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.19.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.49.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.10.mlp.fc2.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.16.self_attn.k_proj.scales': tensor([[0.0010, 0.0010, 0.0008,  ..., 0.0009, 0.0010, 0.0008]]),\n",
       " 'vpm.encoder.layers.24.mlp.fc1.scales': tensor([[0.0010, 0.0011, 0.0011,  ..., 0.0009, 0.0011, 0.0012]]),\n",
       " 'llm.model.layers.16.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.15.self_attn.k_proj.scales': tensor([[0.0233, 0.0145, 0.0182, 0.0158, 0.0173, 0.0232, 0.0213, 0.0207, 0.0234,\n",
       "          0.0193, 0.0269, 0.0267, 0.0257, 0.0236, 0.0285, 0.0349, 0.0320, 0.0269,\n",
       "          0.0266, 0.0148, 0.0250, 0.0267, 0.0288, 0.0338, 0.0792, 0.0288, 0.0336,\n",
       "          0.0257, 0.0283, 0.0300, 0.0338, 0.0290, 0.0165, 0.0220, 0.0211, 0.0247,\n",
       "          0.0203, 0.0349, 0.0221, 0.0276, 0.0251, 0.0244, 0.0264, 0.0232, 0.0280,\n",
       "          0.0215, 0.0280, 0.0177, 0.0275, 0.0241, 0.0298, 0.0263, 0.0320, 0.0272,\n",
       "          0.0268, 0.0259, 0.0615, 0.0341, 0.0264, 0.0294, 0.0314, 0.0316, 0.0333,\n",
       "          0.0276, 0.0285, 0.0267, 0.0188, 0.0253, 0.0341, 0.0246, 0.0320, 0.0204,\n",
       "          0.0269, 0.0185, 0.0244, 0.0224, 0.0229, 0.0172, 0.0255, 0.0208, 0.0271,\n",
       "          0.0267, 0.0244, 0.0305, 0.0309, 0.0253, 0.0338, 0.0303, 0.0244, 0.0297,\n",
       "          0.0602, 0.0275, 0.0288, 0.0328, 0.0316, 0.0285, 0.0264, 0.0316, 0.0241,\n",
       "          0.0269, 0.0266, 0.0242, 0.0241, 0.0194, 0.0285, 0.0221, 0.0241, 0.0280,\n",
       "          0.0240, 0.0263, 0.0300, 0.0277, 0.0230, 0.0319, 0.0250, 0.0254, 0.0279,\n",
       "          0.0238, 0.0290, 0.0281, 0.0253, 0.0271, 0.0453, 0.0315, 0.0280, 0.0263,\n",
       "          0.0359, 0.0285, 0.0367, 0.0458, 0.0185, 0.0281, 0.0178, 0.0322, 0.0173,\n",
       "          0.0329, 0.0213, 0.0241, 0.0213, 0.0294, 0.0309, 0.0241, 0.0228, 0.0241,\n",
       "          0.0219, 0.0296, 0.0266, 0.0268, 0.0193, 0.0246, 0.0284, 0.0285, 0.0251,\n",
       "          0.0247, 0.0492, 0.0303, 0.0293, 0.0277, 0.0272, 0.0281, 0.0163, 0.0319,\n",
       "          0.0302, 0.0359, 0.0232, 0.0307, 0.0224, 0.0298, 0.0273, 0.0253, 0.0225,\n",
       "          0.0228, 0.0285, 0.0207, 0.0229, 0.0273, 0.0203, 0.0236, 0.0237, 0.0303,\n",
       "          0.0269, 0.0244, 0.0245, 0.0245, 0.0251, 0.0240, 0.0627, 0.0273, 0.0290,\n",
       "          0.0267, 0.0288, 0.0318, 0.0174, 0.0145, 0.0186, 0.0167, 0.0208, 0.0221,\n",
       "          0.0233, 0.0202, 0.0230, 0.0199, 0.0318, 0.0221, 0.0216, 0.0280, 0.0281,\n",
       "          0.0247, 0.0300, 0.0258, 0.0255, 0.0212, 0.0424, 0.0332, 0.0283, 0.0296,\n",
       "          0.0288, 0.0693, 0.0362, 0.0411, 0.0479, 0.0333, 0.0378, 0.0383, 0.0216,\n",
       "          0.0173, 0.0311, 0.0151, 0.0219, 0.0193, 0.0280, 0.0201, 0.0223, 0.0203,\n",
       "          0.0273, 0.0283, 0.0176, 0.0269, 0.0234, 0.0234, 0.0263, 0.0236, 0.0294,\n",
       "          0.0245, 0.0370, 0.0284, 0.0272, 0.0380, 0.0289, 0.0599, 0.0362, 0.0469,\n",
       "          0.0414, 0.0422, 0.0367, 0.0508, 0.0193, 0.0195, 0.0223, 0.0234, 0.0206,\n",
       "          0.0281, 0.0240, 0.0250, 0.0227, 0.0242, 0.0207, 0.0294, 0.0258, 0.0254,\n",
       "          0.0300, 0.0254, 0.0191, 0.0253, 0.0283, 0.0336, 0.0456, 0.0276, 0.0254,\n",
       "          0.0409, 0.0318, 0.0241, 0.0667, 0.0346, 0.0341, 0.0283, 0.0284, 0.0284,\n",
       "          0.0258, 0.0227, 0.0229, 0.0190, 0.0224, 0.0211, 0.0167, 0.0245, 0.0255,\n",
       "          0.0238, 0.0221, 0.0227, 0.0255, 0.0259, 0.0246, 0.0258, 0.0223, 0.0245,\n",
       "          0.0241, 0.0288, 0.0424, 0.0258, 0.0380, 0.0275, 0.0310, 0.0258, 0.0526,\n",
       "          0.0242, 0.0474, 0.0267, 0.0280, 0.0279, 0.0236, 0.0198, 0.0292, 0.0297,\n",
       "          0.0253, 0.0284, 0.0263, 0.0185, 0.0262, 0.0289, 0.0202, 0.0195, 0.0331,\n",
       "          0.0228, 0.0233, 0.0223, 0.0207, 0.0225, 0.0290, 0.0240, 0.0250, 0.0249,\n",
       "          0.0263, 0.0223, 0.0244, 0.0492, 0.0289, 0.0220, 0.0279, 0.0199, 0.0198,\n",
       "          0.0319, 0.0172, 0.0283, 0.0204, 0.0202, 0.0266, 0.0184, 0.0193, 0.0301,\n",
       "          0.0266, 0.0201, 0.0228, 0.0279, 0.0208, 0.0221, 0.0324, 0.0224, 0.0204,\n",
       "          0.0204, 0.0217, 0.0194, 0.0216, 0.0233, 0.0216, 0.0198, 0.0216, 0.0698,\n",
       "          0.0206, 0.0195, 0.0204, 0.0203, 0.0293, 0.0229, 0.0161, 0.0186, 0.0186,\n",
       "          0.0193, 0.0185, 0.0197, 0.0172, 0.0190, 0.0190, 0.0219, 0.0224, 0.0197,\n",
       "          0.0217, 0.0227, 0.0267, 0.0284, 0.0280, 0.0195, 0.0294, 0.0264, 0.0258,\n",
       "          0.0250, 0.0289, 0.0275, 0.0267, 0.0583, 0.0288, 0.0283, 0.0263, 0.0461,\n",
       "          0.0296, 0.0338, 0.0208, 0.0158, 0.0199, 0.0181, 0.0188, 0.0210, 0.0174,\n",
       "          0.0215, 0.0247, 0.0212, 0.0207, 0.0254, 0.0217, 0.0244, 0.0190, 0.0230,\n",
       "          0.0246, 0.0244, 0.0303, 0.0346, 0.0417, 0.0307, 0.0359, 0.0241, 0.0349,\n",
       "          0.0724, 0.0314, 0.0285, 0.0349, 0.0319, 0.0372, 0.0419, 0.0129, 0.0136,\n",
       "          0.0132, 0.0149, 0.0152, 0.0132, 0.0180, 0.0137, 0.0134, 0.0153, 0.0174,\n",
       "          0.0135, 0.0189, 0.0176, 0.0227, 0.0178, 0.0268, 0.0223, 0.0251, 0.0220,\n",
       "          0.0241, 0.0760, 0.0359, 0.0259, 0.0875, 0.0370, 0.0430, 0.0333, 0.0745,\n",
       "          0.0296, 0.0318, 0.0324, 0.0143, 0.0128, 0.0141, 0.0133, 0.0169, 0.0180,\n",
       "          0.0142, 0.0137, 0.0188, 0.0180, 0.0158, 0.0230, 0.0156, 0.0202, 0.0210,\n",
       "          0.0191, 0.0189, 0.0234, 0.0232, 0.0230, 0.0211, 0.0406, 0.0272, 0.0275,\n",
       "          0.0576, 0.0445, 0.0583, 0.0307, 0.0562, 0.0338, 0.0612, 0.0307]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.25.self_attn.o_proj.scales': tensor([[0.0233, 0.0224, 0.0275,  ..., 0.0201, 0.0267, 0.0152]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.46.mlp.gate_proj.qweight': tensor([[-1467384872, -1467332964, -1765311097,  ...,  2003261292,\n",
       "          -1723431325, -2003257464],\n",
       "         [-1986573352,  2015913865, -2037942426,  ..., -1532581737,\n",
       "           2006419064, -1519031896],\n",
       "         [-2040952235,  2053457272, -2037797498,  ...,  1252575096,\n",
       "          -2016831658,  1468569737],\n",
       "         ...,\n",
       "         [ -127357607,  1719503754,  2003270298,  ..., -2055688266,\n",
       "          -1167541928,  1771538823],\n",
       "         [ 1488415131, -1733846396,  2055763796,  ...,  2023385496,\n",
       "           2073466999, -2003204234],\n",
       "         [-1821922902,  1768458360,  1483248786,  ..., -1738037658,\n",
       "           2040101496, -1968596359]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.12.mlp.fc2.qweight': tensor([[ 2123397516, -1750886791, -2123600767,  ..., -1718722649,\n",
       "          -2087226531, -1885451105],\n",
       "         [-1971627121, -1870242456,  1668657307,  ..., -1417846158,\n",
       "          -1265084057, -1482586478],\n",
       "         [ 1870235797, -1586259269, -1654103973,  ...,  2074117002,\n",
       "          -2058190466, -2022209911],\n",
       "         ...,\n",
       "         [ 1853728613,  2055037314,  2139446618,  ...,  1466928519,\n",
       "           1920237423,  1940223383],\n",
       "         [-1701612182,  2121439637, -2071372680,  ..., -1319593354,\n",
       "           2018676869, -1752809342],\n",
       "         [ 1633647229, -2069391483, -1551994509,  ..., -1955427203,\n",
       "           1803112025, -1768062508]], dtype=torch.int32),\n",
       " 'llm.model.layers.3.mlp.down_proj.qweight': tensor([[-1177781863,  2021299830,  1974958248,  ..., -1738053461,\n",
       "          -1686464396, -2053477993],\n",
       "         [-1720223286, -2022139223, -2004313448,  ...,   982022584,\n",
       "          -1182295879, -2020972392],\n",
       "         [-1739081605, -1166378869, -1736861803,  ...,  2022283945,\n",
       "           -959858535,  1737058182],\n",
       "         ...,\n",
       "         [ 1759017383, -1940224168,   748062615,  ...,  1988659578,\n",
       "           2088535943, -1734776664],\n",
       "         [ 1688922519, -1718061431, -1970698162,  ...,  2038727832,\n",
       "          -1706791831, -1452701849],\n",
       "         [-1215912058,  1736009591, -1735748235,  ..., -1219004169,\n",
       "           1519949720, -1739024012]], dtype=torch.int32),\n",
       " 'llm.model.layers.31.mlp.up_proj.scales': tensor([[0.0250, 0.0285, 0.0228,  ..., 0.0173, 0.0225, 0.0253]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.35.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.28.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.20.self_attn.q_proj.scales': tensor([[0.0015, 0.0009, 0.0008,  ..., 0.0007, 0.0011, 0.0008]]),\n",
       " 'vpm.encoder.layers.25.mlp.fc1.qzeros': tensor([[2139062143, 2139062143, 2139062143,  ..., 2139062143, 2139062143,\n",
       "          2139062143]], dtype=torch.int32),\n",
       " 'llm.model.layers.14.self_attn.k_proj.scales': tensor([[0.0190, 0.0161, 0.0186, 0.0167, 0.0312, 0.0221, 0.0207, 0.0229, 0.0202,\n",
       "          0.0292, 0.0203, 0.0362, 0.0246, 0.0280, 0.0212, 0.0305, 0.0275, 0.0245,\n",
       "          0.0251, 0.0249, 0.0267, 0.0292, 0.0276, 0.0259, 0.0687, 0.0249, 0.0258,\n",
       "          0.0249, 0.0258, 0.0341, 0.0435, 0.0303, 0.0194, 0.0211, 0.0246, 0.0220,\n",
       "          0.0227, 0.0228, 0.0203, 0.0234, 0.0212, 0.0213, 0.0233, 0.0238, 0.0249,\n",
       "          0.0288, 0.0269, 0.0246, 0.0246, 0.0213, 0.0263, 0.0284, 0.0273, 0.0276,\n",
       "          0.0244, 0.0276, 0.0490, 0.0329, 0.0307, 0.0298, 0.0383, 0.0378, 0.0257,\n",
       "          0.0293, 0.0714, 0.0186, 0.0207, 0.0211, 0.0176, 0.0254, 0.0208, 0.0233,\n",
       "          0.0186, 0.0173, 0.0201, 0.0227, 0.0140, 0.0249, 0.0190, 0.0199, 0.0154,\n",
       "          0.0208, 0.0258, 0.0322, 0.0246, 0.0260, 0.0745, 0.0359, 0.0263, 0.0776,\n",
       "          0.0280, 0.0327, 0.0383, 0.0310, 0.0294, 0.0293, 0.0249, 0.0181, 0.0168,\n",
       "          0.0212, 0.0227, 0.0234, 0.0186, 0.0164, 0.0201, 0.0207, 0.0227, 0.0207,\n",
       "          0.0267, 0.0219, 0.0288, 0.0273, 0.0232, 0.0238, 0.0247, 0.0242, 0.0212,\n",
       "          0.0237, 0.0450, 0.0263, 0.0250, 0.0536, 0.0268, 0.0242, 0.0419, 0.0318,\n",
       "          0.0508, 0.0607, 0.0180, 0.0190, 0.0143, 0.0159, 0.0152, 0.0137, 0.0155,\n",
       "          0.0198, 0.0185, 0.0198, 0.0162, 0.0245, 0.0210, 0.0145, 0.0164, 0.0246,\n",
       "          0.0210, 0.0223, 0.0296, 0.0245, 0.0354, 0.0273, 0.0302, 0.0329, 0.0380,\n",
       "          0.0523, 0.0456, 0.0443, 0.0542, 0.0338, 0.0430, 0.0450, 0.0181, 0.0147,\n",
       "          0.0128, 0.0135, 0.0150, 0.0173, 0.0141, 0.0193, 0.0158, 0.0153, 0.0228,\n",
       "          0.0176, 0.0212, 0.0201, 0.0279, 0.0237, 0.0167, 0.0234, 0.0273, 0.0479,\n",
       "          0.0254, 0.0302, 0.0296, 0.0393, 0.0329, 0.0839, 0.0315, 0.0314, 0.0302,\n",
       "          0.0698, 0.0453, 0.0508, 0.0225, 0.0182, 0.0154, 0.0184, 0.0182, 0.0203,\n",
       "          0.0257, 0.0188, 0.0225, 0.0193, 0.0253, 0.0234, 0.0258, 0.0251, 0.0208,\n",
       "          0.0223, 0.0240, 0.0300, 0.0327, 0.0241, 0.0267, 0.0229, 0.0244, 0.0254,\n",
       "          0.0246, 0.0682, 0.0424, 0.0285, 0.0280, 0.0253, 0.0263, 0.0264, 0.0230,\n",
       "          0.0211, 0.0207, 0.0207, 0.0199, 0.0203, 0.0233, 0.0242, 0.0199, 0.0249,\n",
       "          0.0246, 0.0298, 0.0277, 0.0227, 0.0245, 0.0237, 0.0230, 0.0233, 0.0234,\n",
       "          0.0236, 0.0277, 0.0251, 0.0258, 0.0253, 0.0283, 0.0599, 0.0296, 0.0269,\n",
       "          0.0240, 0.0263, 0.0250, 0.0244, 0.0210, 0.0184, 0.0233, 0.0207, 0.0362,\n",
       "          0.0191, 0.0257, 0.0228, 0.0234, 0.0253, 0.0257, 0.0259, 0.0230, 0.0286,\n",
       "          0.0230, 0.0260, 0.0293, 0.0237, 0.0258, 0.0236, 0.0266, 0.0249, 0.0300,\n",
       "          0.0284, 0.0285, 0.0250, 0.0255, 0.0607, 0.0254, 0.0250, 0.0233, 0.0370,\n",
       "          0.0208, 0.0201, 0.0224, 0.0208, 0.0241, 0.0220, 0.0303, 0.0224, 0.0195,\n",
       "          0.0247, 0.0203, 0.0262, 0.0279, 0.0233, 0.0240, 0.0251, 0.0269, 0.0189,\n",
       "          0.0276, 0.0275, 0.0244, 0.0251, 0.0232, 0.0301, 0.0306, 0.0240, 0.0268,\n",
       "          0.0411, 0.0253, 0.0336, 0.0290, 0.0247, 0.0198, 0.0176, 0.0221, 0.0207,\n",
       "          0.0168, 0.0219, 0.0193, 0.0213, 0.0172, 0.0233, 0.0173, 0.0221, 0.0184,\n",
       "          0.0249, 0.0245, 0.0263, 0.0244, 0.0257, 0.0257, 0.0232, 0.0312, 0.0240,\n",
       "          0.0266, 0.0303, 0.0294, 0.0698, 0.0329, 0.0259, 0.0555, 0.0288, 0.0288,\n",
       "          0.0306, 0.0186, 0.0195, 0.0174, 0.0181, 0.0181, 0.0201, 0.0144, 0.0210,\n",
       "          0.0177, 0.0221, 0.0190, 0.0206, 0.0310, 0.0240, 0.0188, 0.0250, 0.0246,\n",
       "          0.0254, 0.0269, 0.0306, 0.0286, 0.0576, 0.0271, 0.0424, 0.0298, 0.0594,\n",
       "          0.0349, 0.0391, 0.0320, 0.0298, 0.0320, 0.0319, 0.0197, 0.0153, 0.0189,\n",
       "          0.0184, 0.0219, 0.0188, 0.0207, 0.0174, 0.0221, 0.0195, 0.0221, 0.0262,\n",
       "          0.0215, 0.0294, 0.0283, 0.0172, 0.0244, 0.0264, 0.0332, 0.0285, 0.0328,\n",
       "          0.0250, 0.0251, 0.0336, 0.0458, 0.0307, 0.0280, 0.0326, 0.0267, 0.0326,\n",
       "          0.0314, 0.0349, 0.0195, 0.0173, 0.0172, 0.0145, 0.0247, 0.0199, 0.0232,\n",
       "          0.0228, 0.0276, 0.0202, 0.0272, 0.0223, 0.0215, 0.0288, 0.0234, 0.0302,\n",
       "          0.0269, 0.0272, 0.0271, 0.0190, 0.0264, 0.0266, 0.0320, 0.0280, 0.0792,\n",
       "          0.0280, 0.0393, 0.0292, 0.0362, 0.0641, 0.0565, 0.0268, 0.0210, 0.0211,\n",
       "          0.0298, 0.0217, 0.0253, 0.0216, 0.0216, 0.0264, 0.0249, 0.0211, 0.0307,\n",
       "          0.0263, 0.0229, 0.0223, 0.0293, 0.0189, 0.0224, 0.0237, 0.0258, 0.0254,\n",
       "          0.0267, 0.0217, 0.0240, 0.0258, 0.0202, 0.0656, 0.0225, 0.0215, 0.0206,\n",
       "          0.0197, 0.0241, 0.0244, 0.0228, 0.0280, 0.0269, 0.0171, 0.0258, 0.0262,\n",
       "          0.0283, 0.0207, 0.0201, 0.0242, 0.0242, 0.0219, 0.0275, 0.0204, 0.0241,\n",
       "          0.0251, 0.0280, 0.0241, 0.0177, 0.0271, 0.0229, 0.0227, 0.0253, 0.0245,\n",
       "          0.0250, 0.0648, 0.0210, 0.0207, 0.0230, 0.0216, 0.0224, 0.0245]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.9.self_attn.o_proj.scales': tensor([[0.0219, 0.0228, 0.0213,  ..., 0.0194, 0.0236, 0.0212]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.0.self_attn.k_proj.qweight': tensor([[ 1939510710,  1789643577, -1200073291,  ..., -1732732570,\n",
       "           1753651832, -1734714759],\n",
       "         [-1786013527,  1186568780, -1196980088,  ...,  2004518792,\n",
       "          -2019071367, -1218996552],\n",
       "         [ 1990822038,  -408377481,  1182299831,  ..., -1987401593,\n",
       "          -1433896836, -1734760099],\n",
       "         ...,\n",
       "         [  883608199, -1452965542, -1531205514,  ..., -2071426936,\n",
       "           2055775881, -1771531144],\n",
       "         [-1249466952, -1919070919,  1971824249,  ...,  2070440296,\n",
       "           2021436313,  1703242614],\n",
       "         [-2005493362, -1955809369, -2018936422,  ..., -1987606377,\n",
       "          -1955112281, -1201158534]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.20.mlp.fc2.bias': tensor([ 0.2344,  0.3340,  0.0267,  ..., -0.1011,  0.0830,  0.1582]),\n",
       " 'llm.model.layers.41.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.38.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.33.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.47.mlp.up_proj.scales': tensor([[0.0792, 0.0224, 0.0234,  ..., 0.0227, 0.0352, 0.0211]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.50.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.6.mlp.fc1.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.27.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.34.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.1.self_attn.out_proj.qweight': tensor([[ 1837343879,  1083867282, -1921086858,  ..., -1936747644,\n",
       "           1564181904, -1820041553],\n",
       "         [ 1382057867, -1992257939,  1668574099,  ..., -1988849812,\n",
       "          -1097625968, -1451256188],\n",
       "         [ 2070120598, -1401263718, -1935380858,  ...,  2006677395,\n",
       "          -1903128448,  1940164961],\n",
       "         ...,\n",
       "         [ 1702724528, -1398371239, -1548452919,  ...,  2138263677,\n",
       "           1317578626, -1601423202],\n",
       "         [-2054453110,  1772573560, -1501332901,  ..., -1300597064,\n",
       "           1721273970, -1640012413],\n",
       "         [ 1937861247,  1770932373,  1402760274,  ...,  2139391641,\n",
       "          -2125753718,  1958439047]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.2.self_attn.k_proj.qweight': tensor([[ 1703638108, -1400601225,   999325559,  ...,  1651286132,\n",
       "           1937910920, -2023911795],\n",
       "         [ 2119075945, -1889566364, -2055901068,  ...,  1938191239,\n",
       "          -1770878105, -1735741555],\n",
       "         [ 1736461945,  1635750033, -2139401035,  ..., -1754439604,\n",
       "          -1535158915,  1938196628],\n",
       "         ...,\n",
       "         [ 2072480137, -1654432622, -1534674837,  ..., -2118352780,\n",
       "          -1921483146,  2039783536],\n",
       "         [ 1787060819, -1720792464,  2089986150,  ...,  1620414337,\n",
       "          -1788239244, -1870306437],\n",
       "         [-1972071316,  1399624566, -1989118601,  ...,  1251254666,\n",
       "           1850321770, -1887134830]], dtype=torch.int32),\n",
       " 'llm.model.layers.16.self_attn.o_proj.qweight': tensor([[ 2055972755,  2040813676,  1449776504,  ..., -1717077847,\n",
       "          -1768518457,  2055759930],\n",
       "         [-2003068841,   945191338, -1748473436,  ..., -1464555673,\n",
       "          -1785039703,  1989711000],\n",
       "         [ 1735895159,  2071697546, -1480079496,  ...,  2070832815,\n",
       "          -2006328920,  1502182266],\n",
       "         ...,\n",
       "         [-2004203845, -1235839075,  2041031047,  ..., -1752798586,\n",
       "           2024113574,  2004392359],\n",
       "         [-2039776870,  1750628762,  1483318602,  ...,  2021046185,\n",
       "           1989773176, -2021226137],\n",
       "         [-1966708586, -1964594791,  1503556982,  ..., -2036823397,\n",
       "           2019977077,  2038868105]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.7.layer_norm2.weight': tensor([0.6797, 0.8906, 0.8750,  ..., 0.7344, 0.8086, 0.8789]),\n",
       " 'llm.model.layers.23.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.43.mlp.up_proj.scales': tensor([[0.0245, 0.0234, 0.0244,  ..., 0.0269, 0.0189, 0.0453]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.1.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.17.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.8.mlp.fc1.bias': tensor([-1.4844, -1.3906, -0.9727,  ..., -1.2422, -2.0469, -0.6016]),\n",
       " 'vpm.encoder.layers.3.mlp.fc2.scales': tensor([[0.0009, 0.0011, 0.0009,  ..., 0.0013, 0.0012, 0.0010]]),\n",
       " 'vpm.encoder.layers.26.self_attn.q_proj.bias': tensor([ 0.0327, -0.0566, -0.0791,  ..., -0.0786,  0.2207,  0.5898]),\n",
       " 'llm.model.layers.12.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.31.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.15.mlp.fc2.qweight': tensor([[ 2107079056, -1700553590, -1650621782,  ..., -2002759306,\n",
       "           2070113954,  1954788174],\n",
       "         [-1803252636,   932078463, -1804885099,  ...,  2137758087,\n",
       "          -1469734548, -1855629223],\n",
       "         [ 1718837583, -1874440320,  1804305707,  ..., -1649439851,\n",
       "          -2085594201,  1868139219],\n",
       "         ...,\n",
       "         [ 1802477698,  1769115515, -2004386496,  ...,  1940758105,\n",
       "           2069918836, -1784983635],\n",
       "         [ 1685419964, -1985318231, -1600744501,  ...,  -983800467,\n",
       "          -1399348627, -2083428807],\n",
       "         [ 2092277105, -1820893293,  1168137587,  ...,  1185058694,\n",
       "          -2086438527, -2025862834]], dtype=torch.int32),\n",
       " 'llm.model.layers.39.self_attn.o_proj.qweight': tensor([[ 2057996678,  2024372392, -1720157850,  ..., -2021091415,\n",
       "          -1902671991,  1703511941],\n",
       "         [-1820687754, -2022074505,  2038994827,  ..., -2020108169,\n",
       "           2039117990, -2004313175],\n",
       "         [-2022270602, -1751680889, -2019981191,  ...,  2022209655,\n",
       "          -1698069320, -1469618550],\n",
       "         ...,\n",
       "         [-1720072344,  2005379720, -1735943784,  ..., -1770555000,\n",
       "          -1183078523, -1803057002],\n",
       "         [-1166370136, -2020963943, -2005436247,  ...,  2022156661,\n",
       "           2072810872, -1770534520],\n",
       "         [  949517209, -1732867976, -1986553209,  ..., -1770346103,\n",
       "           -916028022,  1769384056]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.17.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.18.self_attn.o_proj.qweight': tensor([[-2055767927,  -408130898, -1751620215,  ..., -1500019577,\n",
       "          -1198098268, -1718974327],\n",
       "         [-1468364406,  2035774330, -1212572952,  ..., -1701259161,\n",
       "          -1769363866, -2006476664],\n",
       "         [-2021107799, -1733863782, -1214822280,  ...,  1958319972,\n",
       "          -1986483801, -2019919993],\n",
       "         ...,\n",
       "         [-1471654024,  -880240523, -2023246153,  ...,  1183209625,\n",
       "           2037823364, -1986561944],\n",
       "         [ 1976203434, -1417050682,  -693609815,  ...,  1719310520,\n",
       "          -1733833894, -1734838903],\n",
       "         [-1217808499,  1520932298, -1486579385,  ...,  1537579164,\n",
       "           2050652056,  2003339128]], dtype=torch.int32),\n",
       " 'llm.model.layers.30.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.18.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.26.mlp.gate_proj.qweight': tensor([[ -966166617,  2002360440, -2054711432,  ..., -1502189720,\n",
       "          -1383749990, -1529624136],\n",
       "         [-1683256918, -1718122171,  1738448566,  ..., -1215665527,\n",
       "          -1716820041, -1516659815],\n",
       "         [-1970633064, -1754691417,  1283828153,  ...,  2106092083,\n",
       "           1521912006,  1755797830],\n",
       "         ...,\n",
       "         [ -909600922, -1970832984,  2009490534,  ..., -1988443048,\n",
       "          -1481271141, -1719187097],\n",
       "         [-1448518292,  2042005896, -1446360724,  ...,  2043300213,\n",
       "          -1515492999, -1214908293],\n",
       "         [ 1988725629, -1504348328,  1703307678,  ...,  1990752406,\n",
       "           1467513753,  1251104423]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.9.self_attn.v_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.20.self_attn.v_proj.scales': tensor([[0.0281, 0.0238, 0.0259, 0.0279, 0.0267, 0.0227, 0.0267, 0.0273, 0.0263,\n",
       "          0.0254, 0.0259, 0.0250, 0.0288, 0.0245, 0.0269, 0.0326, 0.0268, 0.0169,\n",
       "          0.0260, 0.0242, 0.0249, 0.0251, 0.0228, 0.0263, 0.0244, 0.0221, 0.0257,\n",
       "          0.0249, 0.0240, 0.0279, 0.0292, 0.0275, 0.0307, 0.0223, 0.0247, 0.0259,\n",
       "          0.0300, 0.0253, 0.0266, 0.0262, 0.0268, 0.0232, 0.0211, 0.0283, 0.0233,\n",
       "          0.0242, 0.0279, 0.0271, 0.0292, 0.0246, 0.0247, 0.0253, 0.0275, 0.0211,\n",
       "          0.0246, 0.0277, 0.0246, 0.0232, 0.0250, 0.0314, 0.0206, 0.0242, 0.0280,\n",
       "          0.0320, 0.0189, 0.0195, 0.0240, 0.0185, 0.0194, 0.0211, 0.0221, 0.0336,\n",
       "          0.0208, 0.0202, 0.0194, 0.0199, 0.0212, 0.0194, 0.0208, 0.0182, 0.0266,\n",
       "          0.0206, 0.0201, 0.0215, 0.0198, 0.0186, 0.0217, 0.0188, 0.0206, 0.0219,\n",
       "          0.0194, 0.0194, 0.0221, 0.0204, 0.0216, 0.0213, 0.0186, 0.0198, 0.0254,\n",
       "          0.0224, 0.0184, 0.0177, 0.0210, 0.0197, 0.0199, 0.0219, 0.0246, 0.0197,\n",
       "          0.0195, 0.0216, 0.0211, 0.0184, 0.0223, 0.0244, 0.0250, 0.0207, 0.0207,\n",
       "          0.0216, 0.0217, 0.0230, 0.0300, 0.0186, 0.0190, 0.0204, 0.0238, 0.0185,\n",
       "          0.0204, 0.0206, 0.0352, 0.0253, 0.0324, 0.0275, 0.0273, 0.0309, 0.0245,\n",
       "          0.0257, 0.0357, 0.0262, 0.0269, 0.0296, 0.0276, 0.0294, 0.0257, 0.0236,\n",
       "          0.0253, 0.0288, 0.0285, 0.0234, 0.0297, 0.0255, 0.0258, 0.0307, 0.0286,\n",
       "          0.0286, 0.0285, 0.0292, 0.0296, 0.0290, 0.0280, 0.0288, 0.0238, 0.0277,\n",
       "          0.0318, 0.0281, 0.0285, 0.0284, 0.0281, 0.0399, 0.0242, 0.0264, 0.0354,\n",
       "          0.0296, 0.0289, 0.0288, 0.0294, 0.0253, 0.0297, 0.0296, 0.0292, 0.0236,\n",
       "          0.0284, 0.0259, 0.0305, 0.0259, 0.0230, 0.0346, 0.0370, 0.0331, 0.0303,\n",
       "          0.0251, 0.0306, 0.0326, 0.0238, 0.0169, 0.0184, 0.0303, 0.0234, 0.0294,\n",
       "          0.0232, 0.0145, 0.0190, 0.0159, 0.0228, 0.0201, 0.0182, 0.0155, 0.0163,\n",
       "          0.0213, 0.0177, 0.0245, 0.0216, 0.0258, 0.0182, 0.0213, 0.0143, 0.0202,\n",
       "          0.0177, 0.0168, 0.0176, 0.0211, 0.0242, 0.0152, 0.0176, 0.0168, 0.0146,\n",
       "          0.0211, 0.0283, 0.0166, 0.0159, 0.0204, 0.0172, 0.0203, 0.0219, 0.0198,\n",
       "          0.0216, 0.0301, 0.0177, 0.0173, 0.0188, 0.0154, 0.0172, 0.0163, 0.0165,\n",
       "          0.0280, 0.0204, 0.0190, 0.0166, 0.0210, 0.0234, 0.0195, 0.0166, 0.0177,\n",
       "          0.0186, 0.0173, 0.0162, 0.0167, 0.0240, 0.0219, 0.0247, 0.0228, 0.0238,\n",
       "          0.0203, 0.0228, 0.0229, 0.0207, 0.0229, 0.0212, 0.0272, 0.0237, 0.0217,\n",
       "          0.0238, 0.0207, 0.0238, 0.0279, 0.0229, 0.0241, 0.0215, 0.0257, 0.0229,\n",
       "          0.0293, 0.0276, 0.0264, 0.0240, 0.0272, 0.0223, 0.0232, 0.0227, 0.0258,\n",
       "          0.0208, 0.0221, 0.0234, 0.0288, 0.0251, 0.0236, 0.0241, 0.0236, 0.0281,\n",
       "          0.0225, 0.0237, 0.0211, 0.0260, 0.0237, 0.0271, 0.0230, 0.0217, 0.0250,\n",
       "          0.0249, 0.0211, 0.0215, 0.0264, 0.0277, 0.0245, 0.0215, 0.0272, 0.0271,\n",
       "          0.0318, 0.0269, 0.0212, 0.0247, 0.0237, 0.0283, 0.0296, 0.0306, 0.0298,\n",
       "          0.0257, 0.0250, 0.0311, 0.0255, 0.0246, 0.0254, 0.0332, 0.0279, 0.0280,\n",
       "          0.0245, 0.0253, 0.0272, 0.0255, 0.0281, 0.0255, 0.0258, 0.0260, 0.0268,\n",
       "          0.0254, 0.0249, 0.0284, 0.0247, 0.0262, 0.0234, 0.0285, 0.0311, 0.0258,\n",
       "          0.0273, 0.0234, 0.0296, 0.0310, 0.0254, 0.0294, 0.0257, 0.0273, 0.0269,\n",
       "          0.0253, 0.0246, 0.0244, 0.0251, 0.0267, 0.0268, 0.0245, 0.0267, 0.0283,\n",
       "          0.0262, 0.0271, 0.0241, 0.0251, 0.0233, 0.0318, 0.0301, 0.0305, 0.0298,\n",
       "          0.0260, 0.0251, 0.0288, 0.0277, 0.0259, 0.0254, 0.0326, 0.0306, 0.0269,\n",
       "          0.0172, 0.0283, 0.0303, 0.0367, 0.0312, 0.0262, 0.0281, 0.0273, 0.0297,\n",
       "          0.0263, 0.0264, 0.0266, 0.0309, 0.0322, 0.0264, 0.0254, 0.0276, 0.0312,\n",
       "          0.0294, 0.0288, 0.0281, 0.0290, 0.0314, 0.0234, 0.0359, 0.0228, 0.0281,\n",
       "          0.0251, 0.0268, 0.0246, 0.0267, 0.0260, 0.0315, 0.0249, 0.0316, 0.0296,\n",
       "          0.0289, 0.0309, 0.0246, 0.0283, 0.0294, 0.0298, 0.0327, 0.0309, 0.0283,\n",
       "          0.0286, 0.0258, 0.0301, 0.0292, 0.0303, 0.0262, 0.0260, 0.0367, 0.0250,\n",
       "          0.0245, 0.0262, 0.0269, 0.0255, 0.0307, 0.0269, 0.0277, 0.0264, 0.0271,\n",
       "          0.0245, 0.0268, 0.0276, 0.0276, 0.0297, 0.0276, 0.0230, 0.0249, 0.0234,\n",
       "          0.0262, 0.0249, 0.0260, 0.0238, 0.0251, 0.0268, 0.0290, 0.0302, 0.0268,\n",
       "          0.0238, 0.0284, 0.0259, 0.0264, 0.0257, 0.0255, 0.0255, 0.0246, 0.0280,\n",
       "          0.0277, 0.0253, 0.0254, 0.0250, 0.0279, 0.0284, 0.0250, 0.0242, 0.0279,\n",
       "          0.0253, 0.0242, 0.0242, 0.0259, 0.0255, 0.0224, 0.0277, 0.0253, 0.0223,\n",
       "          0.0277, 0.0284, 0.0315, 0.0280, 0.0310, 0.0260, 0.0262, 0.0298, 0.0316,\n",
       "          0.0253, 0.0257, 0.0263, 0.0338, 0.0232, 0.0300, 0.0260, 0.0279]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.33.mlp.down_proj.scales': tensor([[0.0269, 0.0251, 0.0297,  ..., 0.0303, 0.0245, 0.0229]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.17.mlp.fc2.bias': tensor([ 0.2490,  0.4219, -0.1357,  ..., -0.2305, -0.0898,  0.1387]),\n",
       " 'llm.model.layers.35.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.2.self_attn.v_proj.scales': tensor([[0.0230, 0.0233, 0.0294, 0.0180, 0.0195, 0.0213, 0.0210, 0.0236, 0.0241,\n",
       "          0.0197, 0.0182, 0.0204, 0.0190, 0.0176, 0.0199, 0.0220, 0.0260, 0.0203,\n",
       "          0.0241, 0.0236, 0.0242, 0.0233, 0.0199, 0.0213, 0.0242, 0.0211, 0.0338,\n",
       "          0.0246, 0.0220, 0.0208, 0.0216, 0.0159, 0.0213, 0.0221, 0.0228, 0.0241,\n",
       "          0.0202, 0.0260, 0.0185, 0.0319, 0.0249, 0.0215, 0.0316, 0.0292, 0.0259,\n",
       "          0.0216, 0.0247, 0.0198, 0.0264, 0.0328, 0.0201, 0.0225, 0.0203, 0.0296,\n",
       "          0.0197, 0.0268, 0.0180, 0.0208, 0.0180, 0.0199, 0.0238, 0.0188, 0.0212,\n",
       "          0.0210, 0.0207, 0.0220, 0.0199, 0.0220, 0.0204, 0.0208, 0.0186, 0.0211,\n",
       "          0.0223, 0.0210, 0.0202, 0.0206, 0.0210, 0.0197, 0.0263, 0.0228, 0.0230,\n",
       "          0.0199, 0.0266, 0.0195, 0.0204, 0.0191, 0.0211, 0.0190, 0.0230, 0.0259,\n",
       "          0.0219, 0.0193, 0.0221, 0.0210, 0.0198, 0.0219, 0.0213, 0.0173, 0.0207,\n",
       "          0.0189, 0.0232, 0.0220, 0.0221, 0.0210, 0.0198, 0.0202, 0.0206, 0.0225,\n",
       "          0.0199, 0.0203, 0.0198, 0.0203, 0.0186, 0.0189, 0.0210, 0.0211, 0.0221,\n",
       "          0.0188, 0.0223, 0.0207, 0.0219, 0.0225, 0.0210, 0.0194, 0.0224, 0.0216,\n",
       "          0.0212, 0.0212, 0.0224, 0.0210, 0.0189, 0.0232, 0.0346, 0.0210, 0.0254,\n",
       "          0.0204, 0.0254, 0.0268, 0.0211, 0.0242, 0.0202, 0.0188, 0.0206, 0.0354,\n",
       "          0.0203, 0.0213, 0.0220, 0.0257, 0.0228, 0.0228, 0.0225, 0.0217, 0.0245,\n",
       "          0.0234, 0.0193, 0.0202, 0.0206, 0.0237, 0.0253, 0.0236, 0.0211, 0.0224,\n",
       "          0.0227, 0.0250, 0.0288, 0.0232, 0.0257, 0.0215, 0.0208, 0.0193, 0.0251,\n",
       "          0.0213, 0.0228, 0.0198, 0.0202, 0.0242, 0.0534, 0.0260, 0.0260, 0.0281,\n",
       "          0.0199, 0.0359, 0.0284, 0.0208, 0.0288, 0.0286, 0.0221, 0.0202, 0.0233,\n",
       "          0.0245, 0.0275, 0.0227, 0.0329, 0.0246, 0.0262, 0.0266, 0.0240, 0.0238,\n",
       "          0.0227, 0.0242, 0.0267, 0.0240, 0.0266, 0.0312, 0.0262, 0.0280, 0.0251,\n",
       "          0.0269, 0.0324, 0.0255, 0.0301, 0.0225, 0.0258, 0.0251, 0.0285, 0.0271,\n",
       "          0.0305, 0.0298, 0.0251, 0.0234, 0.0391, 0.0300, 0.0242, 0.0260, 0.0279,\n",
       "          0.0258, 0.0249, 0.0272, 0.0227, 0.0424, 0.0293, 0.0230, 0.0276, 0.0264,\n",
       "          0.0279, 0.0223, 0.0272, 0.0251, 0.0229, 0.0264, 0.0269, 0.0324, 0.0346,\n",
       "          0.0296, 0.0320, 0.0246, 0.0207, 0.0336, 0.0254, 0.0305, 0.0264, 0.0216,\n",
       "          0.0221, 0.0301, 0.0333, 0.0264, 0.0217, 0.0232, 0.0242, 0.0189, 0.0227,\n",
       "          0.0225, 0.0227, 0.0225, 0.0191, 0.0240, 0.0238, 0.0219, 0.0241, 0.0253,\n",
       "          0.0204, 0.0207, 0.0215, 0.0246, 0.0272, 0.0242, 0.0263, 0.0198, 0.0212,\n",
       "          0.0221, 0.0223, 0.0236, 0.0208, 0.0268, 0.0213, 0.0207, 0.0194, 0.0233,\n",
       "          0.0184, 0.0210, 0.0204, 0.0250, 0.0193, 0.0201, 0.0234, 0.0294, 0.0272,\n",
       "          0.0213, 0.0230, 0.0215, 0.0216, 0.0242, 0.0212, 0.0188, 0.0285, 0.0238,\n",
       "          0.0232, 0.0212, 0.0197, 0.0224, 0.0237, 0.0221, 0.0210, 0.0210, 0.0203,\n",
       "          0.0245, 0.0210, 0.0267, 0.0234, 0.0245, 0.0283, 0.0269, 0.0263, 0.0310,\n",
       "          0.0251, 0.0298, 0.0331, 0.0344, 0.0277, 0.0302, 0.0263, 0.0234, 0.0236,\n",
       "          0.0240, 0.0211, 0.0271, 0.0246, 0.0298, 0.0315, 0.0297, 0.0245, 0.0228,\n",
       "          0.0253, 0.0331, 0.0286, 0.0251, 0.0203, 0.0315, 0.0272, 0.0260, 0.0259,\n",
       "          0.0272, 0.0279, 0.0271, 0.0253, 0.0220, 0.0310, 0.0254, 0.0314, 0.0318,\n",
       "          0.0241, 0.0237, 0.0301, 0.0279, 0.0344, 0.0279, 0.0264, 0.0277, 0.0260,\n",
       "          0.0293, 0.0275, 0.0257, 0.0290, 0.0296, 0.0298, 0.0316, 0.0349, 0.0303,\n",
       "          0.0271, 0.0298, 0.0271, 0.0354, 0.0279, 0.0327, 0.0242, 0.0213, 0.0212,\n",
       "          0.0229, 0.0249, 0.0229, 0.0228, 0.0247, 0.0255, 0.0203, 0.0236, 0.0237,\n",
       "          0.0211, 0.0251, 0.0230, 0.0232, 0.0217, 0.0230, 0.0204, 0.0219, 0.0241,\n",
       "          0.0236, 0.0198, 0.0237, 0.0194, 0.0260, 0.0210, 0.0290, 0.0221, 0.0219,\n",
       "          0.0211, 0.0237, 0.0215, 0.0229, 0.0294, 0.0207, 0.0212, 0.0199, 0.0281,\n",
       "          0.0255, 0.0207, 0.0233, 0.0202, 0.0234, 0.0247, 0.0215, 0.0201, 0.0236,\n",
       "          0.0229, 0.0228, 0.0227, 0.0240, 0.0212, 0.0230, 0.0213, 0.0240, 0.0215,\n",
       "          0.0220, 0.0250, 0.0213, 0.0249, 0.0238, 0.0242, 0.0213, 0.0249, 0.0257,\n",
       "          0.0253, 0.0230, 0.0263, 0.0232, 0.0318, 0.0266, 0.0254, 0.0233, 0.0258,\n",
       "          0.0258, 0.0285, 0.0245, 0.0246, 0.0234, 0.0268, 0.0242, 0.0264, 0.0275,\n",
       "          0.0341, 0.0240, 0.0257, 0.0262, 0.0264, 0.0273, 0.0307, 0.0288, 0.0276,\n",
       "          0.0249, 0.0314, 0.0250, 0.0229, 0.0281, 0.0257, 0.0251, 0.0244, 0.0280,\n",
       "          0.0310, 0.0284, 0.0227, 0.0223, 0.0273, 0.0285, 0.0234, 0.0253, 0.0268,\n",
       "          0.0388, 0.0237, 0.0324, 0.0285, 0.0232, 0.0251, 0.0258, 0.0212, 0.0268,\n",
       "          0.0236, 0.0269, 0.0253, 0.0234, 0.0244, 0.0292, 0.0253, 0.0259]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.46.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.6.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.14.layer_norm2.bias': tensor([ 0.1167, -0.2422,  0.3320,  ...,  0.2168,  0.0105, -0.0889]),\n",
       " 'vpm.encoder.layers.24.self_attn.out_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.24.layer_norm2.weight': tensor([ 9.6250,  9.3125,  9.7500,  ...,  7.8125, 13.6875,  9.4375]),\n",
       " 'vpm.encoder.layers.19.self_attn.q_proj.bias': tensor([ 0.0386,  0.0618, -0.1196,  ...,  0.0623, -0.0217, -0.1230]),\n",
       " 'vpm.encoder.layers.26.self_attn.out_proj.bias': tensor([-1.4453, -0.2539, -0.3809,  ...,  0.1079, -1.3203,  0.3594]),\n",
       " 'llm.model.layers.20.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.10.mlp.fc1.qweight': tensor([[-2019782225, -1027377238, -1431915679,  ...,  1836340106,\n",
       "           1968992624, -1874019416],\n",
       "         [-1333054355, -1452621935, -2090385757,  ...,  1651678784,\n",
       "           2138931614,  1787926852],\n",
       "         [-2139408338,  1974757784,  2054724268,  ...,  2102427016,\n",
       "           2103214967,  2037404828],\n",
       "         ...,\n",
       "         [ 1533573535, -2036373883,  1733445990,  ...,  1904119641,\n",
       "           1551658604, -1919908744],\n",
       "         [ 1956134995, -1277838768, -1934992486,  ..., -2057659777,\n",
       "           1686212902, -1857317279],\n",
       "         [-1951640681, -1886550940, -1637383509,  ...,  1468945482,\n",
       "           2092475997,   880116589]], dtype=torch.int32),\n",
       " 'llm.model.layers.16.self_attn.v_proj.scales': tensor([[0.0224, 0.0219, 0.0228, 0.0288, 0.0266, 0.0258, 0.0240, 0.0227, 0.0217,\n",
       "          0.0217, 0.0250, 0.0212, 0.0232, 0.0229, 0.0215, 0.0225, 0.0259, 0.0217,\n",
       "          0.0233, 0.0223, 0.0253, 0.0227, 0.0216, 0.0264, 0.0250, 0.0234, 0.0204,\n",
       "          0.0258, 0.0246, 0.0220, 0.0253, 0.0259, 0.0254, 0.0225, 0.0221, 0.0213,\n",
       "          0.0220, 0.0273, 0.0225, 0.0210, 0.0229, 0.0249, 0.0245, 0.0277, 0.0232,\n",
       "          0.0272, 0.0273, 0.0251, 0.0212, 0.0230, 0.0223, 0.0203, 0.0277, 0.0250,\n",
       "          0.0236, 0.0215, 0.0253, 0.0224, 0.0332, 0.0225, 0.0290, 0.0204, 0.0250,\n",
       "          0.0236, 0.0188, 0.0193, 0.0188, 0.0206, 0.0203, 0.0208, 0.0178, 0.0193,\n",
       "          0.0210, 0.0197, 0.0191, 0.0173, 0.0194, 0.0177, 0.0188, 0.0182, 0.0210,\n",
       "          0.0180, 0.0220, 0.0191, 0.0229, 0.0188, 0.0201, 0.0189, 0.0199, 0.0229,\n",
       "          0.0210, 0.0206, 0.0199, 0.0182, 0.0181, 0.0293, 0.0186, 0.0188, 0.0174,\n",
       "          0.0180, 0.0417, 0.0224, 0.0201, 0.0194, 0.0181, 0.0208, 0.0185, 0.0181,\n",
       "          0.0263, 0.0185, 0.0191, 0.0203, 0.0181, 0.0184, 0.0186, 0.0212, 0.0188,\n",
       "          0.0194, 0.0199, 0.0204, 0.0255, 0.0236, 0.0237, 0.0207, 0.0190, 0.0211,\n",
       "          0.0322, 0.0180, 0.0197, 0.0220, 0.0229, 0.0263, 0.0207, 0.0237, 0.0250,\n",
       "          0.0241, 0.0247, 0.0225, 0.0288, 0.0300, 0.0224, 0.0227, 0.0263, 0.0223,\n",
       "          0.0219, 0.0228, 0.0215, 0.0230, 0.0208, 0.0220, 0.0257, 0.0245, 0.0258,\n",
       "          0.0233, 0.0223, 0.0271, 0.0224, 0.0237, 0.0254, 0.0223, 0.0228, 0.0238,\n",
       "          0.0238, 0.0293, 0.0240, 0.0269, 0.0228, 0.0223, 0.0212, 0.0303, 0.0269,\n",
       "          0.0221, 0.0281, 0.0224, 0.0338, 0.0237, 0.0258, 0.0260, 0.0263, 0.0254,\n",
       "          0.0230, 0.0254, 0.0240, 0.0246, 0.0290, 0.0229, 0.0253, 0.0242, 0.0280,\n",
       "          0.0224, 0.0268, 0.0260, 0.0207, 0.0229, 0.0242, 0.0221, 0.0195, 0.0185,\n",
       "          0.0207, 0.0176, 0.0215, 0.0190, 0.0249, 0.0188, 0.0223, 0.0197, 0.0206,\n",
       "          0.0212, 0.0201, 0.0201, 0.0211, 0.0223, 0.0211, 0.0210, 0.0203, 0.0180,\n",
       "          0.0216, 0.0202, 0.0204, 0.0208, 0.0223, 0.0212, 0.0242, 0.0184, 0.0202,\n",
       "          0.0190, 0.0188, 0.0241, 0.0201, 0.0227, 0.0210, 0.0225, 0.0197, 0.0240,\n",
       "          0.0224, 0.0238, 0.0197, 0.0204, 0.0202, 0.0225, 0.0206, 0.0184, 0.0188,\n",
       "          0.0191, 0.0212, 0.0202, 0.0197, 0.0185, 0.0262, 0.0210, 0.0198, 0.0240,\n",
       "          0.0206, 0.0199, 0.0201, 0.0228, 0.0213, 0.0247, 0.0189, 0.0225, 0.0233,\n",
       "          0.0237, 0.0219, 0.0203, 0.0233, 0.0221, 0.0221, 0.0220, 0.0229, 0.0215,\n",
       "          0.0211, 0.0267, 0.0211, 0.0225, 0.0230, 0.0213, 0.0234, 0.0254, 0.0232,\n",
       "          0.0227, 0.0216, 0.0221, 0.0255, 0.0249, 0.0232, 0.0219, 0.0204, 0.0227,\n",
       "          0.0208, 0.0208, 0.0219, 0.0219, 0.0217, 0.0294, 0.0220, 0.0201, 0.0247,\n",
       "          0.0220, 0.0259, 0.0241, 0.0246, 0.0251, 0.0250, 0.0216, 0.0230, 0.0266,\n",
       "          0.0281, 0.0220, 0.0251, 0.0204, 0.0199, 0.0206, 0.0225, 0.0199, 0.0244,\n",
       "          0.0240, 0.0223, 0.0260, 0.0201, 0.0208, 0.0193, 0.0268, 0.0302, 0.0276,\n",
       "          0.0225, 0.0327, 0.0213, 0.0224, 0.0236, 0.0207, 0.0223, 0.0244, 0.0212,\n",
       "          0.0221, 0.0177, 0.0219, 0.0227, 0.0217, 0.0229, 0.0207, 0.0202, 0.0254,\n",
       "          0.0225, 0.0198, 0.0244, 0.0225, 0.0233, 0.0213, 0.0230, 0.0283, 0.0213,\n",
       "          0.0197, 0.0271, 0.0210, 0.0234, 0.0217, 0.0206, 0.0191, 0.0212, 0.0236,\n",
       "          0.0224, 0.0237, 0.0245, 0.0233, 0.0238, 0.0216, 0.0223, 0.0238, 0.0271,\n",
       "          0.0202, 0.0258, 0.0233, 0.0201, 0.0236, 0.0253, 0.0212, 0.0240, 0.0217,\n",
       "          0.0190, 0.0198, 0.0228, 0.0241, 0.0232, 0.0219, 0.0204, 0.0302, 0.0201,\n",
       "          0.0215, 0.0241, 0.0257, 0.0257, 0.0224, 0.0238, 0.0219, 0.0216, 0.0232,\n",
       "          0.0245, 0.0223, 0.0188, 0.0221, 0.0228, 0.0251, 0.0269, 0.0198, 0.0232,\n",
       "          0.0251, 0.0207, 0.0224, 0.0257, 0.0259, 0.0230, 0.0204, 0.0253, 0.0221,\n",
       "          0.0237, 0.0224, 0.0225, 0.0250, 0.0242, 0.0224, 0.0281, 0.0220, 0.0225,\n",
       "          0.0236, 0.0253, 0.0255, 0.0232, 0.0288, 0.0207, 0.0219, 0.0225, 0.0228,\n",
       "          0.0323, 0.0251, 0.0207, 0.0254, 0.0227, 0.0251, 0.0259, 0.0224, 0.0230,\n",
       "          0.0206, 0.0228, 0.0246, 0.0240, 0.0224, 0.0253, 0.0263, 0.0220, 0.0206,\n",
       "          0.0254, 0.0212, 0.0217, 0.0180, 0.0199, 0.0203, 0.0217, 0.0207, 0.0217,\n",
       "          0.0212, 0.0210, 0.0219, 0.0220, 0.0219, 0.0241, 0.0219, 0.0220, 0.0219,\n",
       "          0.0233, 0.0244, 0.0188, 0.0216, 0.0203, 0.0219, 0.0206, 0.0203, 0.0271,\n",
       "          0.0212, 0.0202, 0.0212, 0.0203, 0.0199, 0.0251, 0.0210, 0.0213, 0.0204,\n",
       "          0.0198, 0.0272, 0.0236, 0.0207, 0.0223, 0.0206, 0.0194, 0.0236, 0.0241,\n",
       "          0.0219, 0.0184, 0.0242, 0.0230, 0.0201, 0.0215, 0.0215, 0.0236, 0.0204,\n",
       "          0.0224, 0.0227, 0.0207, 0.0219, 0.0186, 0.0207, 0.0207, 0.0193]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.27.self_attn.o_proj.scales': tensor([[0.0260, 0.0223, 0.0216,  ..., 0.0201, 0.0211, 0.0225]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.24.mlp.fc1.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.19.input_layernorm.weight': tensor([1.5547, 1.2656, 0.7070,  ..., 0.7695, 1.1328, 0.2363]),\n",
       " 'llm.model.layers.5.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.15.self_attn.v_proj.scales': tensor([[0.0007, 0.0008, 0.0008,  ..., 0.0007, 0.0008, 0.0008]]),\n",
       " 'llm.model.layers.37.self_attn.o_proj.qweight': tensor([[-1518761833,  1448441273, -1717991559,  ...,  2037864806,\n",
       "           1688709528, -2022085785],\n",
       "         [ 1753716390,   964204936, -1737922678,  ...,  1768519318,\n",
       "          -1415853910, -1702319975],\n",
       "         [ 1736996954, -1399294055,  2004387975,  ...,  2005437113,\n",
       "          -1434990728, -1734764425],\n",
       "         ...,\n",
       "         [ 2021295751, -1198873223, -1987409542,  ...,  2024248714,\n",
       "           1519867766, -1686660490],\n",
       "         [-1433954906,  2051499752, -2003208022,  ..., -1736935576,\n",
       "          -2002228409,  1784121226],\n",
       "         [-1987417432,  1752647033,  2004514697,  ...,  2039060394,\n",
       "           -579094069,  1969657511]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.14.self_attn.out_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.35.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.41.mlp.down_proj.qweight': tensor([[ 2004265113,   933849721, -1467324296,  ..., -1167403115,\n",
       "          -2036753802, -1719101048],\n",
       "         [ 1766370442,  -894854506,  1989503127,  ...,  1268352135,\n",
       "           2040048020, -1735882616],\n",
       "         [ -944002168,  1488223353, -1213818518,  ..., -1753765464,\n",
       "          -1767524697, -1735882633],\n",
       "         ...,\n",
       "         [ -914786680,  1464301926, -1482049080,  ...,  2023135160,\n",
       "          -1887802983, -2004383352],\n",
       "         [ 1953129864,  1467455866, -2001172122,  ..., -1986431128,\n",
       "           2019187590, -2003269512],\n",
       "         [ 1991601064, -2023389285, -1734837895,  ...,  1986492314,\n",
       "          -1214937225, -2004318344]], dtype=torch.int32),\n",
       " 'llm.model.layers.9.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.19.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.24.mlp.down_proj.qweight': tensor([[ 2007201607, -1703164778,  2040956822,  ...,  2037954695,\n",
       "          -1468495512,  2021161129],\n",
       "         [ 1456982616,  1807325545,  1785437047,  ..., -1766421912,\n",
       "          -1972938361, -2052552296],\n",
       "         [ -623282027,  1805031303, -1533576791,  ..., -2004448890,\n",
       "          -1773569642, -1467381352],\n",
       "         ...,\n",
       "         [-1685551258, -1984604040, -2036753798,  ..., -1970899018,\n",
       "           1467447400, -1785230999],\n",
       "         [ 2038991018,  2023262821,  1202165193,  ..., -1722378152,\n",
       "           1502177928,  2021165223],\n",
       "         [-1709008487, -1986353532,  1268357733,  ...,  1769573032,\n",
       "           2022094504,  2055641497]], dtype=torch.int32),\n",
       " 'llm.model.layers.19.mlp.down_proj.qweight': tensor([[-2051569240,  2005515177,  1717003430,  ...,  1787468167,\n",
       "           2040101047, -2004318071],\n",
       "         [ 2022275241,  1753713498,  2037873045,  ...,  2041080233,\n",
       "           -964192103, -2005305223],\n",
       "         [-1467381352, -2039126151, -2024643465,  ...,  1966512008,\n",
       "           1990891401,  2022209672],\n",
       "         ...,\n",
       "         [-1752667786,  2005444506,  2055640714,  ..., -1975036024,\n",
       "          -1752585832, -1734764664],\n",
       "         [ 1771608215,  2022349256, -1448437384,  ..., -1985493368,\n",
       "           1185515959, -1987541112],\n",
       "         [-1719175049, -2006284137,  2005502057,  ..., -1949599068,\n",
       "           2003347032, -2021091433]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.0.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.24.self_attn.v_proj.bias': tensor([ 0.2637, -0.0369, -0.0669,  ..., -0.2949,  0.1709,  0.1807]),\n",
       " 'llm.model.layers.13.mlp.gate_proj.scales': tensor([[0.0216, 0.0225, 0.0227,  ..., 0.0260, 0.0215, 0.0207]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.36.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.3.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.1.mlp.fc2.bias': tensor([-0.0081,  0.1021, -0.0154,  ...,  0.0737,  0.6484,  0.2334]),\n",
       " 'llm.model.layers.16.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.27.self_attn.v_proj.qweight': tensor([[-1197967530,  2042140071,  1772775848,  ..., -1686464359,\n",
       "          -1716082568, -2037876360],\n",
       "         [ 2043185274, -1818786678,  1552451978,  ...,  2055768230,\n",
       "          -1977055143,  1720158583],\n",
       "         [-1413965676,  1767413335,  2023196474,  ..., -1735091832,\n",
       "           1487444584,  2041218440],\n",
       "         ...,\n",
       "         [-1200122007, -2053609322,  1771464346,  ..., -1416132695,\n",
       "           1752659525, -1483175545],\n",
       "         [ 1485538455,  -930706599, -2003474060,  ..., -2021225831,\n",
       "          -1786222713, -2021086824],\n",
       "         [-1984460633, -1953667432, -1720166582,  ..., -1717864072,\n",
       "          -2018878807, -1769637241]], dtype=torch.int32),\n",
       " 'llm.model.layers.51.mlp.up_proj.scales': tensor([[0.0220, 0.0271, 0.0211,  ..., 0.0217, 0.0255, 0.0271]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.9.mlp.fc2.bias': tensor([ 0.1260, -0.0608,  0.1914,  ..., -0.1016,  0.0152, -0.2852]),\n",
       " 'vpm.encoder.layers.9.mlp.fc2.scales': tensor([[0.0008, 0.0011, 0.0009,  ..., 0.0009, 0.0012, 0.0009]]),\n",
       " 'llm.model.layers.7.mlp.down_proj.scales': tensor([[0.0229, 0.0236, 0.0320,  ..., 0.0301, 0.0259, 0.0383]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.20.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.9.post_attention_layernorm.weight': tensor([1.3125, 1.1953, 0.9492,  ..., 1.0078, 1.1719, 0.2158]),\n",
       " 'vpm.encoder.layers.11.mlp.fc2.bias': tensor([ 0.0330,  0.0496,  0.0010,  ..., -0.0559,  0.2070, -0.0679]),\n",
       " 'vpm.encoder.layers.4.mlp.fc1.scales': tensor([[0.0010, 0.0009, 0.0008,  ..., 0.0009, 0.0012, 0.0044]]),\n",
       " 'llm.model.layers.0.mlp.down_proj.qweight': tensor([[ 1453151353, -2020968088, -1752601494,  ..., -2003405689,\n",
       "           2105129638, -1753720659],\n",
       "         [ 1773967478,  1766345338,  2057918351,  ..., -1683329848,\n",
       "          -1985653146,  1453966987],\n",
       "         [ 1787454825,  1975024042,  2038855273,  ..., -1453688198,\n",
       "           1789565802,  2055834038],\n",
       "         ...,\n",
       "         [ -947292330, -1968608598, -1719113274,  ..., -1736987049,\n",
       "          -1233667434, -1975023229],\n",
       "         [-2053400712, -1218868826,  -913545079,  ...,  1719383462,\n",
       "          -1973134972,  1786428823],\n",
       "         [ 1454868330, -1766307946, -1468553323,  ...,  2033830554,\n",
       "          -1214670424, -2004395671]], dtype=torch.int32),\n",
       " 'llm.model.layers.36.mlp.up_proj.qweight': tensor([[-2040088713, -1453828197, -1769305206,  ..., -1512457866,\n",
       "          -2007467369,  1982109372],\n",
       "         [-1702463353, -1717925785, -1720085400,  ...,  2023459176,\n",
       "           1800833943, -1669757313],\n",
       "         [-1947555384, -1181325959, -2086312553,  ..., -1686805113,\n",
       "           1974962856, -1212438647],\n",
       "         ...,\n",
       "         [ 1806264424,  2023138425, -1970619990,  ..., -1717995129,\n",
       "          -1704540553, -1783132808],\n",
       "         [ 1199008136, -1752454313,  1484429691,  ...,  2038990520,\n",
       "           2055842969, -1988659287],\n",
       "         [-1719043430,  -915768679, -1781876345,  ..., -1668830837,\n",
       "          -1667537478, -1752914342]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.9.layer_norm1.bias': tensor([ 0.0038, -0.0249,  0.0403,  ...,  0.0040,  0.0170, -0.0176]),\n",
       " 'llm.model.layers.19.self_attn.k_proj.qweight': tensor([[ 1670940839,  2042259581, -1130071960,  ...,  1252607909,\n",
       "          -1969649512, -2004252536],\n",
       "         [-1683720039,  1515821417, -1968798601,  ..., -2021029718,\n",
       "          -1971615831, -2004317319],\n",
       "         [ 1199422074, -1446811927, -1666091147,  ..., -2005636955,\n",
       "          -1737000838, -1196771218],\n",
       "         ...,\n",
       "         [-2054690147,  1768450392, -1746311801,  ...,  1533450679,\n",
       "          -2052486518,  -950433159],\n",
       "         [ 2055837126,  1802012762,  2070574954,  ...,  2022016649,\n",
       "           1943701896,  1753843817],\n",
       "         [-1990686840, -1717987707, -1788179611,  ..., -2022271558,\n",
       "           1518954872, -1181177768]], dtype=torch.int32),\n",
       " 'llm.model.layers.4.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.44.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.38.self_attn.k_proj.scales': tensor([[0.0145, 0.0107, 0.0117, 0.0120, 0.0125, 0.0125, 0.0133, 0.0140, 0.0143,\n",
       "          0.0177, 0.0171, 0.0202, 0.0208, 0.0173, 0.0229, 0.0202, 0.0238, 0.0229,\n",
       "          0.0232, 0.0232, 0.0271, 0.0260, 0.0233, 0.0289, 0.0273, 0.0279, 0.0354,\n",
       "          0.0288, 0.0322, 0.0375, 0.0414, 0.0495, 0.0114, 0.0135, 0.0129, 0.0133,\n",
       "          0.0151, 0.0117, 0.0139, 0.0148, 0.0143, 0.0180, 0.0144, 0.0177, 0.0180,\n",
       "          0.0177, 0.0202, 0.0233, 0.0241, 0.0227, 0.0258, 0.0241, 0.0378, 0.0298,\n",
       "          0.0259, 0.0305, 0.0341, 0.0271, 0.0424, 0.0362, 0.0328, 0.0466, 0.0458,\n",
       "          0.0320, 0.0112, 0.0139, 0.0151, 0.0126, 0.0126, 0.0143, 0.0150, 0.0128,\n",
       "          0.0210, 0.0173, 0.0188, 0.0229, 0.0171, 0.0211, 0.0195, 0.0182, 0.0241,\n",
       "          0.0296, 0.0215, 0.0277, 0.0255, 0.0182, 0.0267, 0.0236, 0.0237, 0.0236,\n",
       "          0.0288, 0.0254, 0.0309, 0.0262, 0.0242, 0.0279, 0.0152, 0.0111, 0.0163,\n",
       "          0.0153, 0.0146, 0.0152, 0.0153, 0.0176, 0.0142, 0.0157, 0.0145, 0.0232,\n",
       "          0.0189, 0.0213, 0.0255, 0.0155, 0.0237, 0.0267, 0.0168, 0.0288, 0.0255,\n",
       "          0.0271, 0.0275, 0.0289, 0.0281, 0.0292, 0.0258, 0.0250, 0.0346, 0.0276,\n",
       "          0.0258, 0.0359, 0.0095, 0.0102, 0.0097, 0.0091, 0.0117, 0.0104, 0.0154,\n",
       "          0.0141, 0.0129, 0.0116, 0.0124, 0.0111, 0.0167, 0.0204, 0.0139, 0.0229,\n",
       "          0.0238, 0.0247, 0.0281, 0.0303, 0.0284, 0.0328, 0.0301, 0.0500, 0.0456,\n",
       "          0.0327, 0.0422, 0.0367, 0.0372, 0.0417, 0.0336, 0.0288, 0.0104, 0.0100,\n",
       "          0.0096, 0.0092, 0.0085, 0.0120, 0.0123, 0.0124, 0.0122, 0.0113, 0.0124,\n",
       "          0.0137, 0.0115, 0.0177, 0.0139, 0.0195, 0.0193, 0.0225, 0.0230, 0.0332,\n",
       "          0.0267, 0.0290, 0.0349, 0.0385, 0.0409, 0.0332, 0.0401, 0.0375, 0.0362,\n",
       "          0.0375, 0.0311, 0.0367, 0.0228, 0.0202, 0.0203, 0.0204, 0.0165, 0.0207,\n",
       "          0.0136, 0.0285, 0.0165, 0.0302, 0.0191, 0.0232, 0.0223, 0.0181, 0.0221,\n",
       "          0.0211, 0.0208, 0.0198, 0.0188, 0.0266, 0.0199, 0.0229, 0.0212, 0.0204,\n",
       "          0.0208, 0.0247, 0.0272, 0.0245, 0.0129, 0.0220, 0.0202, 0.0227, 0.0378,\n",
       "          0.0477, 0.0224, 0.0165, 0.0189, 0.0250, 0.0189, 0.0324, 0.0216, 0.0229,\n",
       "          0.0241, 0.0219, 0.0174, 0.0202, 0.0198, 0.0257, 0.0225, 0.0236, 0.0237,\n",
       "          0.0260, 0.0211, 0.0232, 0.0198, 0.0240, 0.0201, 0.0290, 0.0225, 0.0210,\n",
       "          0.0346, 0.0207, 0.0232, 0.0212, 0.0147, 0.0144, 0.0172, 0.0156, 0.0163,\n",
       "          0.0182, 0.0134, 0.0164, 0.0193, 0.0176, 0.0161, 0.0132, 0.0191, 0.0149,\n",
       "          0.0177, 0.0172, 0.0181, 0.0191, 0.0135, 0.0212, 0.0229, 0.0224, 0.0217,\n",
       "          0.0249, 0.0212, 0.0227, 0.0237, 0.0316, 0.0198, 0.0260, 0.0227, 0.0191,\n",
       "          0.0190, 0.0168, 0.0234, 0.0119, 0.0216, 0.0203, 0.0129, 0.0242, 0.0113,\n",
       "          0.0146, 0.0168, 0.0147, 0.0207, 0.0150, 0.0190, 0.0132, 0.0182, 0.0206,\n",
       "          0.0173, 0.0181, 0.0213, 0.0207, 0.0198, 0.0208, 0.0181, 0.0195, 0.0224,\n",
       "          0.0281, 0.0227, 0.0223, 0.0204, 0.0293, 0.0163, 0.0130, 0.0141, 0.0174,\n",
       "          0.0181, 0.0132, 0.0172, 0.0140, 0.0204, 0.0162, 0.0180, 0.0182, 0.0193,\n",
       "          0.0228, 0.0182, 0.0219, 0.0186, 0.0228, 0.0259, 0.0206, 0.0246, 0.0253,\n",
       "          0.0262, 0.0263, 0.0272, 0.0251, 0.0296, 0.0399, 0.0275, 0.0269, 0.0280,\n",
       "          0.0329, 0.0213, 0.0443, 0.0155, 0.0197, 0.0130, 0.0178, 0.0156, 0.0138,\n",
       "          0.0191, 0.0165, 0.0173, 0.0166, 0.0186, 0.0198, 0.0169, 0.0215, 0.0216,\n",
       "          0.0229, 0.0258, 0.0185, 0.0257, 0.0225, 0.0263, 0.0288, 0.0292, 0.0294,\n",
       "          0.0307, 0.0341, 0.0246, 0.0349, 0.0269, 0.0305, 0.0244, 0.0238, 0.0159,\n",
       "          0.0154, 0.0238, 0.0193, 0.0167, 0.0195, 0.0181, 0.0225, 0.0195, 0.0236,\n",
       "          0.0206, 0.0197, 0.0232, 0.0184, 0.0211, 0.0268, 0.0169, 0.0224, 0.0250,\n",
       "          0.0220, 0.0220, 0.0232, 0.0241, 0.0093, 0.0221, 0.0217, 0.0264, 0.0193,\n",
       "          0.0203, 0.0236, 0.0163, 0.0113, 0.0185, 0.0219, 0.0159, 0.0198, 0.0186,\n",
       "          0.0182, 0.0165, 0.0221, 0.0238, 0.0188, 0.0189, 0.0236, 0.0228, 0.0204,\n",
       "          0.0215, 0.0215, 0.0240, 0.0213, 0.0207, 0.0197, 0.0275, 0.0280, 0.0220,\n",
       "          0.0284, 0.0230, 0.0232, 0.0217, 0.0225, 0.0204, 0.0247, 0.0115, 0.0158,\n",
       "          0.0132, 0.0186, 0.0153, 0.0219, 0.0143, 0.0198, 0.0180, 0.0174, 0.0210,\n",
       "          0.0227, 0.0229, 0.0221, 0.0158, 0.0217, 0.0324, 0.0251, 0.0286, 0.0229,\n",
       "          0.0259, 0.0312, 0.0310, 0.0331, 0.0247, 0.0617, 0.0312, 0.0269, 0.0326,\n",
       "          0.0329, 0.0393, 0.0375, 0.0137, 0.0152, 0.0139, 0.0159, 0.0127, 0.0159,\n",
       "          0.0138, 0.0165, 0.0225, 0.0145, 0.0177, 0.0201, 0.0172, 0.0148, 0.0213,\n",
       "          0.0227, 0.0156, 0.0234, 0.0311, 0.0178, 0.0275, 0.0245, 0.0275, 0.0269,\n",
       "          0.0411, 0.0212, 0.0354, 0.0300, 0.0385, 0.0599, 0.0301, 0.0385]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.30.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.7.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.17.mlp.fc2.scales': tensor([[0.0010, 0.0011, 0.0010,  ..., 0.0010, 0.0009, 0.0010]]),\n",
       " 'vpm.encoder.layers.10.mlp.fc1.qzeros': tensor([[2139062143, 2139062143, 2139062143,  ..., 2139062143, 2139062143,\n",
       "          2139062143]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.0.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.3.self_attn.v_proj.qweight': tensor([[-1721338966,  -949388358,  2105052040,  ..., -1216832919,\n",
       "           2056759705, -1701209224],\n",
       "         [ 1698528307,  2036693671,  1469602135,  ...,  1803012471,\n",
       "          -1434023495, -2023250055],\n",
       "         [-2022479942, -2019980953, -1984325989,  ..., -1485411895,\n",
       "           2057726108, -1987603066],\n",
       "         ...,\n",
       "         [ 1502058395,  2004527259, -1988584312,  ...,  1986554197,\n",
       "           1465345908, -1989637752],\n",
       "         [-1702401513,  2039065462, -1752799145,  ...,  1988802694,\n",
       "          -2004383079,  1485290422],\n",
       "         [-2043123945,  1803319991, -1939429001,  ...,  1504151416,\n",
       "           1736947334,  2038008232]], dtype=torch.int32),\n",
       " 'llm.model.layers.48.self_attn.k_proj.scales': tensor([[0.0146, 0.0171, 0.0130, 0.0185, 0.0132, 0.0215, 0.0117, 0.0178, 0.0165,\n",
       "          0.0155, 0.0176, 0.0193, 0.0118, 0.0181, 0.0204, 0.0193, 0.0191, 0.0324,\n",
       "          0.0215, 0.0232, 0.0229, 0.0221, 0.0167, 0.0241, 0.0216, 0.0234, 0.0271,\n",
       "          0.0227, 0.0298, 0.0311, 0.0237, 0.0354, 0.0129, 0.0132, 0.0127, 0.0148,\n",
       "          0.0147, 0.0167, 0.0165, 0.0140, 0.0191, 0.0162, 0.0185, 0.0202, 0.0191,\n",
       "          0.0186, 0.0158, 0.0171, 0.0198, 0.0167, 0.0240, 0.0203, 0.0225, 0.0236,\n",
       "          0.0240, 0.0284, 0.0259, 0.0458, 0.0286, 0.0273, 0.0289, 0.0333, 0.0458,\n",
       "          0.0273, 0.0133, 0.0150, 0.0141, 0.0146, 0.0190, 0.0166, 0.0210, 0.0184,\n",
       "          0.0194, 0.0181, 0.0219, 0.0166, 0.0236, 0.0224, 0.0224, 0.0236, 0.0283,\n",
       "          0.0233, 0.0225, 0.0201, 0.0224, 0.0221, 0.0220, 0.0190, 0.0365, 0.0216,\n",
       "          0.0227, 0.0184, 0.0204, 0.0234, 0.0254, 0.0328, 0.0141, 0.0148, 0.0141,\n",
       "          0.0160, 0.0165, 0.0154, 0.0185, 0.0165, 0.0172, 0.0195, 0.0193, 0.0171,\n",
       "          0.0195, 0.0197, 0.0211, 0.0289, 0.0242, 0.0212, 0.0202, 0.0250, 0.0233,\n",
       "          0.0233, 0.0258, 0.0223, 0.0266, 0.0217, 0.0206, 0.0202, 0.0232, 0.0194,\n",
       "          0.0244, 0.0399, 0.0180, 0.0169, 0.0156, 0.0174, 0.0172, 0.0166, 0.0182,\n",
       "          0.0159, 0.0194, 0.0176, 0.0182, 0.0212, 0.0208, 0.0172, 0.0213, 0.0154,\n",
       "          0.0212, 0.0201, 0.0191, 0.0195, 0.0188, 0.0211, 0.0207, 0.0211, 0.0203,\n",
       "          0.0247, 0.0172, 0.0320, 0.0267, 0.0163, 0.0212, 0.0220, 0.0163, 0.0199,\n",
       "          0.0185, 0.0193, 0.0174, 0.0160, 0.0223, 0.0176, 0.0185, 0.0152, 0.0190,\n",
       "          0.0189, 0.0148, 0.0193, 0.0204, 0.0197, 0.0197, 0.0201, 0.0227, 0.0302,\n",
       "          0.0204, 0.0211, 0.0159, 0.0266, 0.0203, 0.0251, 0.0370, 0.0440, 0.0230,\n",
       "          0.0202, 0.0271, 0.0211, 0.0127, 0.0162, 0.0181, 0.0134, 0.0182, 0.0137,\n",
       "          0.0173, 0.0163, 0.0210, 0.0159, 0.0178, 0.0169, 0.0219, 0.0199, 0.0163,\n",
       "          0.0181, 0.0172, 0.0191, 0.0194, 0.0174, 0.0194, 0.0185, 0.0241, 0.0225,\n",
       "          0.0189, 0.0346, 0.0224, 0.0185, 0.0254, 0.0217, 0.0195, 0.0219, 0.0138,\n",
       "          0.0149, 0.0207, 0.0133, 0.0174, 0.0190, 0.0178, 0.0152, 0.0191, 0.0158,\n",
       "          0.0190, 0.0212, 0.0199, 0.0180, 0.0186, 0.0198, 0.0221, 0.0271, 0.0201,\n",
       "          0.0186, 0.0159, 0.0193, 0.0223, 0.0223, 0.0178, 0.0150, 0.0176, 0.0223,\n",
       "          0.0155, 0.0344, 0.0227, 0.0199, 0.0063, 0.0075, 0.0069, 0.0077, 0.0071,\n",
       "          0.0084, 0.0067, 0.0089, 0.0118, 0.0085, 0.0121, 0.0116, 0.0098, 0.0165,\n",
       "          0.0107, 0.0153, 0.0168, 0.0172, 0.0186, 0.0333, 0.0203, 0.0188, 0.0286,\n",
       "          0.0362, 0.0284, 0.0346, 0.0516, 0.0497, 0.0471, 0.0372, 0.0207, 0.0448,\n",
       "          0.0085, 0.0117, 0.0089, 0.0069, 0.0080, 0.0076, 0.0098, 0.0098, 0.0072,\n",
       "          0.0142, 0.0074, 0.0151, 0.0111, 0.0150, 0.0150, 0.0131, 0.0184, 0.0180,\n",
       "          0.0206, 0.0204, 0.0341, 0.0258, 0.0331, 0.0294, 0.0292, 0.0326, 0.0414,\n",
       "          0.0327, 0.0318, 0.0354, 0.0241, 0.0378, 0.0078, 0.0083, 0.0060, 0.0080,\n",
       "          0.0094, 0.0076, 0.0073, 0.0074, 0.0096, 0.0081, 0.0126, 0.0117, 0.0111,\n",
       "          0.0161, 0.0097, 0.0135, 0.0198, 0.0191, 0.0210, 0.0197, 0.0212, 0.0186,\n",
       "          0.0207, 0.0332, 0.0505, 0.0427, 0.0341, 0.0411, 0.0316, 0.0315, 0.0298,\n",
       "          0.0247, 0.0081, 0.0072, 0.0093, 0.0082, 0.0080, 0.0082, 0.0094, 0.0096,\n",
       "          0.0088, 0.0103, 0.0108, 0.0083, 0.0114, 0.0155, 0.0181, 0.0146, 0.0161,\n",
       "          0.0140, 0.0163, 0.0188, 0.0258, 0.0328, 0.0238, 0.0268, 0.0367, 0.0213,\n",
       "          0.0346, 0.0393, 0.0419, 0.0492, 0.0251, 0.0289, 0.0109, 0.0129, 0.0092,\n",
       "          0.0113, 0.0107, 0.0118, 0.0115, 0.0126, 0.0107, 0.0124, 0.0124, 0.0151,\n",
       "          0.0143, 0.0165, 0.0215, 0.0141, 0.0154, 0.0155, 0.0159, 0.0186, 0.0207,\n",
       "          0.0157, 0.0215, 0.0221, 0.0172, 0.0329, 0.0285, 0.0319, 0.0365, 0.0312,\n",
       "          0.0300, 0.0293, 0.0094, 0.0107, 0.0096, 0.0102, 0.0121, 0.0124, 0.0103,\n",
       "          0.0121, 0.0120, 0.0138, 0.0135, 0.0150, 0.0130, 0.0172, 0.0133, 0.0143,\n",
       "          0.0124, 0.0193, 0.0147, 0.0158, 0.0186, 0.0241, 0.0147, 0.0245, 0.0167,\n",
       "          0.0159, 0.0288, 0.0197, 0.0229, 0.0383, 0.0289, 0.0288, 0.0152, 0.0190,\n",
       "          0.0169, 0.0221, 0.0156, 0.0199, 0.0174, 0.0203, 0.0253, 0.0148, 0.0217,\n",
       "          0.0233, 0.0245, 0.0184, 0.0240, 0.0224, 0.0197, 0.0230, 0.0240, 0.0234,\n",
       "          0.0254, 0.0224, 0.0380, 0.0225, 0.0212, 0.0219, 0.0367, 0.0268, 0.0241,\n",
       "          0.0227, 0.0294, 0.0288, 0.0137, 0.0225, 0.0111, 0.0180, 0.0148, 0.0171,\n",
       "          0.0165, 0.0171, 0.0195, 0.0167, 0.0217, 0.0182, 0.0221, 0.0195, 0.0211,\n",
       "          0.0203, 0.0168, 0.0272, 0.0213, 0.0237, 0.0211, 0.0260, 0.0216, 0.0211,\n",
       "          0.0208, 0.0250, 0.0113, 0.0323, 0.0236, 0.0236, 0.0227, 0.0279]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.17.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.19.self_attn.v_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.21.mlp.fc1.qzeros': tensor([[2139062143, 2139062143, 2139062143,  ..., 2139062143, 2139062143,\n",
       "          2139062143]], dtype=torch.int32),\n",
       " 'llm.model.layers.41.self_attn.o_proj.qweight': tensor([[ -928414087, -1773442999,  1751627894,  ...,  2071578440,\n",
       "           2005367176, -1970894490],\n",
       "         [-1955035002,  1753704568,  1490716535,  ..., -1145542792,\n",
       "          -2021288313, -1149720662],\n",
       "         [-1734903416,  2059835256,  1804437930,  ...,  1802738247,\n",
       "          -2035710104, -1984526245],\n",
       "         ...,\n",
       "         [ 1201100711,  2058106733,  2005575750,  ...,  1486517880,\n",
       "          -1740142729, -1479050376],\n",
       "         [-1984390507, -1484404615,  2073646999,  ...,  1741125755,\n",
       "           -664307751,  1752672342],\n",
       "         [ -916022404,  -965195653,  1757847190,  ...,  2009643094,\n",
       "           1735944342, -1433954149]], dtype=torch.int32),\n",
       " 'llm.model.layers.14.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.6.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.13.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.24.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.39.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.7.self_attn.out_proj.scales': tensor([[0.0007, 0.0006, 0.0007,  ..., 0.0007, 0.0007, 0.0007]]),\n",
       " 'vpm.encoder.layers.16.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.1.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.2.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.11.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.24.self_attn.k_proj.qweight': tensor([[-1352149863, -1653258113,  1030065525,  ..., -1970887567,\n",
       "           1871679131,  1586709155],\n",
       "         [-1567917169,  -779646082, -1383891558,  ...,  -828869491,\n",
       "          -1805485456, -1802989936],\n",
       "         [-2056799162,  2089057964,  1835047301,  ...,  1642774714,\n",
       "          -1967357247,  2072670833],\n",
       "         ...,\n",
       "         [-1268212872, -2085389481, -1972599728,  ...,  1420007592,\n",
       "           1725980497,  1786862501],\n",
       "         [ 1635740314,  1973658531,  1873649000,  ...,   844920434,\n",
       "           2089912619, -1901036911],\n",
       "         [-1198755711, -1970492808,  2053613457,  ...,  1787542673,\n",
       "           1918935187, -2071225273]], dtype=torch.int32),\n",
       " 'llm.model.layers.23.mlp.gate_proj.qweight': tensor([[-1971869288,  1503361927,  1989658246,  ..., -1988786313,\n",
       "          -1734681974, -2004255594],\n",
       "         [ 1985586822, -1435928503, -1754744246,  ...,  2038933368,\n",
       "          -2016897685,  1287092583],\n",
       "         [ 2038987174, -1216910708, -1786001526,  ...,  2002102420,\n",
       "           1519953560,  1554865290],\n",
       "         ...,\n",
       "         [ 1485015687,  1988520344,  -882346107,  ..., -1486456999,\n",
       "           -933655673, -1385661254],\n",
       "         [-1770407287, -1985181306, -1969829244,  ...,  1806210905,\n",
       "           1705866600,  1807263642],\n",
       "         [ -391539561, -1970629001, -1431668316,  ..., -1988786568,\n",
       "           2039060886,  1481079255]], dtype=torch.int32),\n",
       " 'llm.model.layers.39.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.3.self_attn.q_proj.bias': tensor([ 0.0222, -0.0167,  0.2383,  ..., -0.1797,  0.6133,  0.2061]),\n",
       " 'llm.model.layers.51.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.51.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.40.self_attn.q_proj.scales': tensor([[0.0159, 0.0173, 0.0158,  ..., 0.0404, 0.0293, 0.0414]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.5.self_attn.v_proj.scales': tensor([[0.0197, 0.0193, 0.0198, 0.0195, 0.0232, 0.0217, 0.0211, 0.0253, 0.0220,\n",
       "          0.0208, 0.0210, 0.0173, 0.0174, 0.0281, 0.0230, 0.0224, 0.0244, 0.0165,\n",
       "          0.0275, 0.0227, 0.0210, 0.0227, 0.0184, 0.0250, 0.0186, 0.0203, 0.0202,\n",
       "          0.0184, 0.0227, 0.0185, 0.0279, 0.0197, 0.0199, 0.0197, 0.0201, 0.0224,\n",
       "          0.0211, 0.0208, 0.0238, 0.0204, 0.0208, 0.0232, 0.0198, 0.0223, 0.0199,\n",
       "          0.0223, 0.0174, 0.0207, 0.0207, 0.0257, 0.0190, 0.0197, 0.0158, 0.0158,\n",
       "          0.0202, 0.0216, 0.0242, 0.0199, 0.0181, 0.0193, 0.0203, 0.0234, 0.0186,\n",
       "          0.0181, 0.0211, 0.0221, 0.0211, 0.0201, 0.0203, 0.0225, 0.0257, 0.0225,\n",
       "          0.0230, 0.0237, 0.0267, 0.0237, 0.0198, 0.0186, 0.0213, 0.0241, 0.0224,\n",
       "          0.0210, 0.0232, 0.0258, 0.0216, 0.0194, 0.0189, 0.0227, 0.0225, 0.0213,\n",
       "          0.0202, 0.0238, 0.0233, 0.0267, 0.0255, 0.0204, 0.0254, 0.0225, 0.0204,\n",
       "          0.0236, 0.0215, 0.0219, 0.0229, 0.0215, 0.0233, 0.0210, 0.0219, 0.0245,\n",
       "          0.0190, 0.0257, 0.0236, 0.0203, 0.0236, 0.0264, 0.0244, 0.0193, 0.0193,\n",
       "          0.0190, 0.0245, 0.0210, 0.0253, 0.0223, 0.0236, 0.0229, 0.0197, 0.0208,\n",
       "          0.0224, 0.0213, 0.0254, 0.0281, 0.0281, 0.0269, 0.0272, 0.0272, 0.0280,\n",
       "          0.0276, 0.0259, 0.0307, 0.0435, 0.0280, 0.0320, 0.0301, 0.0298, 0.0279,\n",
       "          0.0258, 0.0327, 0.0272, 0.0329, 0.0241, 0.0289, 0.0297, 0.0285, 0.0300,\n",
       "          0.0346, 0.0289, 0.0327, 0.0253, 0.0268, 0.0311, 0.0255, 0.0246, 0.0306,\n",
       "          0.0271, 0.0238, 0.0249, 0.0338, 0.0281, 0.0401, 0.0292, 0.0357, 0.0271,\n",
       "          0.0314, 0.0283, 0.0281, 0.0311, 0.0272, 0.0280, 0.0315, 0.0329, 0.0257,\n",
       "          0.0285, 0.0255, 0.0281, 0.0267, 0.0264, 0.0259, 0.0268, 0.0297, 0.0273,\n",
       "          0.0288, 0.0359, 0.0276, 0.0260, 0.0230, 0.0241, 0.0250, 0.0324, 0.0253,\n",
       "          0.0251, 0.0362, 0.0233, 0.0242, 0.0253, 0.0260, 0.0241, 0.0266, 0.0246,\n",
       "          0.0263, 0.0253, 0.0279, 0.0283, 0.0302, 0.0251, 0.0247, 0.0286, 0.0251,\n",
       "          0.0319, 0.0232, 0.0284, 0.0283, 0.0255, 0.0227, 0.0242, 0.0258, 0.0249,\n",
       "          0.0271, 0.0250, 0.0232, 0.0240, 0.0219, 0.0257, 0.0306, 0.0286, 0.0285,\n",
       "          0.0251, 0.0275, 0.0249, 0.0233, 0.0233, 0.0272, 0.0258, 0.0257, 0.0249,\n",
       "          0.0285, 0.0272, 0.0238, 0.0251, 0.0280, 0.0238, 0.0275, 0.0253, 0.0262,\n",
       "          0.0273, 0.0260, 0.0232, 0.0272, 0.0207, 0.0198, 0.0224, 0.0249, 0.0197,\n",
       "          0.0286, 0.0229, 0.0206, 0.0190, 0.0225, 0.0217, 0.0215, 0.0207, 0.0221,\n",
       "          0.0272, 0.0174, 0.0208, 0.0186, 0.0227, 0.0223, 0.0199, 0.0201, 0.0215,\n",
       "          0.0225, 0.0219, 0.0206, 0.0215, 0.0202, 0.0188, 0.0191, 0.0230, 0.0211,\n",
       "          0.0230, 0.0213, 0.0207, 0.0216, 0.0285, 0.0245, 0.0237, 0.0215, 0.0225,\n",
       "          0.0193, 0.0212, 0.0201, 0.0215, 0.0199, 0.0212, 0.0201, 0.0244, 0.0259,\n",
       "          0.0262, 0.0188, 0.0193, 0.0203, 0.0269, 0.0210, 0.0290, 0.0259, 0.0217,\n",
       "          0.0190, 0.0203, 0.0198, 0.0198, 0.0217, 0.0211, 0.0228, 0.0225, 0.0237,\n",
       "          0.0234, 0.0223, 0.0215, 0.0217, 0.0246, 0.0202, 0.0206, 0.0298, 0.0216,\n",
       "          0.0199, 0.0213, 0.0212, 0.0206, 0.0244, 0.0263, 0.0244, 0.0240, 0.0213,\n",
       "          0.0237, 0.0210, 0.0227, 0.0212, 0.0201, 0.0302, 0.0227, 0.0264, 0.0220,\n",
       "          0.0215, 0.0221, 0.0210, 0.0258, 0.0225, 0.0201, 0.0229, 0.0224, 0.0225,\n",
       "          0.0198, 0.0232, 0.0204, 0.0213, 0.0198, 0.0240, 0.0203, 0.0260, 0.0221,\n",
       "          0.0242, 0.0247, 0.0247, 0.0232, 0.0250, 0.0219, 0.0240, 0.0253, 0.0242,\n",
       "          0.0219, 0.0221, 0.0198, 0.0211, 0.0216, 0.0202, 0.0228, 0.0224, 0.0272,\n",
       "          0.0242, 0.0233, 0.0290, 0.0234, 0.0230, 0.0195, 0.0224, 0.0254, 0.0234,\n",
       "          0.0244, 0.0247, 0.0251, 0.0244, 0.0264, 0.0288, 0.0232, 0.0269, 0.0273,\n",
       "          0.0233, 0.0232, 0.0254, 0.0236, 0.0279, 0.0249, 0.0259, 0.0242, 0.0236,\n",
       "          0.0237, 0.0229, 0.0241, 0.0242, 0.0203, 0.0259, 0.0234, 0.0244, 0.0280,\n",
       "          0.0229, 0.0255, 0.0232, 0.0298, 0.0236, 0.0251, 0.0269, 0.0249, 0.0271,\n",
       "          0.0251, 0.0249, 0.0244, 0.0263, 0.0245, 0.0227, 0.0249, 0.0232, 0.0229,\n",
       "          0.0223, 0.0236, 0.0280, 0.0234, 0.0234, 0.0223, 0.0268, 0.0326, 0.0230,\n",
       "          0.0227, 0.0279, 0.0258, 0.0234, 0.0246, 0.0232, 0.0279, 0.0240, 0.0220,\n",
       "          0.0229, 0.0251, 0.0268, 0.0273, 0.0237, 0.0229, 0.0255, 0.0257, 0.0271,\n",
       "          0.0245, 0.0262, 0.0227, 0.0264, 0.0251, 0.0260, 0.0241, 0.0269, 0.0237,\n",
       "          0.0246, 0.0234, 0.0237, 0.0247, 0.0238, 0.0296, 0.0233, 0.0385, 0.0244,\n",
       "          0.0250, 0.0268, 0.0251, 0.0249, 0.0246, 0.0288, 0.0296, 0.0262, 0.0264,\n",
       "          0.0275, 0.0237, 0.0219, 0.0254, 0.0273, 0.0249, 0.0250, 0.0268, 0.0234,\n",
       "          0.0247, 0.0316, 0.0306, 0.0245, 0.0286, 0.0240, 0.0246, 0.0241]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.3.mlp.up_proj.qweight': tensor([[-2043238023,  2007607929,  2056739977,  ...,  2042137480,\n",
       "          -2057717608, -1198954133],\n",
       "         [  898270923, -1970567019, -1737914217,  ..., -2020059975,\n",
       "          -1201433754,   962102104],\n",
       "         [ 1754035850, -1719102106, -1934649434,  ..., -1720346313,\n",
       "          -1723254683,  1735174264],\n",
       "         ...,\n",
       "         [-1469744503, -1751410822,  1995020457,  ..., -1800968059,\n",
       "          -2059897000,  1939375464],\n",
       "         [ 1418348183, -1754887803, -2039895433,  ...,  1468574663,\n",
       "          -1434154665,  2055855061],\n",
       "         [ 1940424886, -2020042374, -1989573227,  ..., -1736682105,\n",
       "          -2034670107,  2026347659]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.12.self_attn.v_proj.bias': tensor([ 0.0059,  0.0029,  0.0400,  ...,  0.0359, -0.0452, -0.0096]),\n",
       " 'llm.model.layers.49.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.27.self_attn.q_proj.scales': tensor([[0.0141, 0.0272, 0.0232,  ..., 0.0230, 0.0251, 0.0269]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.11.self_attn.q_proj.qweight': tensor([[-1734575007, -1835559245,  1806530964,  ..., -1620403866,\n",
       "           1923185561, -1921873049],\n",
       "         [-1502827643, -1667276925,  1604156079,  ...,  2071882315,\n",
       "           1503020436,  1064988277],\n",
       "         [  963860325,   878336861,  1567720593,  ...,  1971280467,\n",
       "           1989692294,  2142462552],\n",
       "         ...,\n",
       "         [-1571903071,  1769179802,  1066037618,  ...,   997691518,\n",
       "          -2075759738,  1686404461],\n",
       "         [-1919711339, -1301521784, -2003658364,  ..., -1302177866,\n",
       "           1938187916, -1500751463],\n",
       "         [-2005435320, -2005098661, -1973125251,  ..., -1939696776,\n",
       "          -1736800898,  1605534606]], dtype=torch.int32),\n",
       " 'llm.model.layers.2.self_attn.v_proj.qweight': tensor([[-1720140918,  1770424951, -1987614614,  ..., -1470584679,\n",
       "          -1753783141,  1737046630],\n",
       "         [ 1788385669,  -895972412, -2019981145,  ...,  1736799623,\n",
       "           2055837287, -1265062776],\n",
       "         [ 1738045573, -1467311719, -2034866057,  ..., -1214683015,\n",
       "          -1670740309,  1987480441],\n",
       "         ...,\n",
       "         [-1763006058, -1700231067,  2040035942,  ..., -1385650552,\n",
       "           2020919689, -2039900567],\n",
       "         [ 2005502122, -1769507209, -2021099401,  ...,  1468438888,\n",
       "          -1752794792, -1752741483],\n",
       "         [-2043447175, -1966622568,  2038069096,  ..., -1654224969,\n",
       "          -1938261595,  1671132792]], dtype=torch.int32),\n",
       " 'llm.model.layers.22.mlp.up_proj.qweight': tensor([[-1973909640, -2055718823, -2021082248,  ...,  1750567019,\n",
       "          -2022218871,  2042350518],\n",
       "         [ 2022218343,  1767339861,  1200134297,  ...,  1785945016,\n",
       "           1755015319,  1184327001],\n",
       "         [ 1989502889, -2057864474, -1485212487,  ..., -1951950681,\n",
       "          -2025551688, -1986505595],\n",
       "         ...,\n",
       "         [-2008386152, -1722112922,  -931746970,  ...,  2007664791,\n",
       "           1467393913,  1988589958],\n",
       "         [-1970575223, -1182050634, -1500743945,  ...,  2038864314,\n",
       "          -1987606377, -1215719035],\n",
       "         [-2052674934, -1415924038, -1485149783,  ..., -1754891606,\n",
       "           1985653145, -1969982312]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.23.layer_norm2.bias': tensor([ 2.2344,  3.1094,  3.1719,  ...,  1.8672,  4.5625, -0.8320]),\n",
       " 'llm.model.layers.48.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.27.self_attn.q_proj.qweight': tensor([[ 1197958599,  1771468714,  1986823830,  ...,  1784278146,\n",
       "           1248430532, -1251382932],\n",
       "         [-1968718186,  1820621944,  2038926283,  ...,  1464506439,\n",
       "           1488557654, -1736800666],\n",
       "         [ -879183973,  1452913528,  1869113733,  ..., -1503889558,\n",
       "          -1720035400,  1687591562],\n",
       "         ...,\n",
       "         [-2018665635, -1253410424,  1739085705,  ..., -1800950632,\n",
       "           1785173402,  2023458711],\n",
       "         [-1232300103, -1485281382,  1753503622,  ..., -1485337219,\n",
       "          -2121565560, -1784968853],\n",
       "         [ 2024237253, -1714120298, -1772578999,  ..., -1989838968,\n",
       "          -1228368488,  1770542663]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.17.layer_norm1.weight': tensor([1.1016, 1.0469, 1.0469,  ..., 0.9961, 0.9727, 1.1172]),\n",
       " 'llm.model.layers.33.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.50.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.4.self_attn.k_proj.qweight': tensor([[ 2056882839,  1952667513,  2004457351,  ...,  -909600376,\n",
       "           -931433079,  1986557351],\n",
       "         [-1986487959,  1720085127,  1515820935,  ..., -1699125672,\n",
       "          -1452710070,  1735817607],\n",
       "         [-1735948184,  1734838440, -1520920425,  ...,  -946383000,\n",
       "           2054581158, -1484163222],\n",
       "         ...,\n",
       "         [-1719236712, -1969510004, -1985574789,  ...,  1770625653,\n",
       "          -1451853669, -1199019899],\n",
       "         [-1470789224,  1490786474,  2021226343,  ...,  2010679450,\n",
       "          -1501992808,  1232500357],\n",
       "         [-1752795000,  1740278201,  1183541366,  ...,  -930719556,\n",
       "           1201313656,  -968792170]], dtype=torch.int32),\n",
       " 'llm.model.layers.22.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.45.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.3.post_attention_layernorm.weight': tensor([0.9336, 0.7773, 0.6445,  ..., 0.6641, 0.8359, 0.1758]),\n",
       " 'vpm.encoder.layers.15.mlp.fc1.qzeros': tensor([[2139062143, 2139062143, 2139062143,  ..., 2139062143, 2139062143,\n",
       "          2139062143]], dtype=torch.int32),\n",
       " 'llm.model.layers.40.self_attn.k_proj.scales': tensor([[0.0143, 0.0173, 0.0148, 0.0241, 0.0223, 0.0154, 0.0176, 0.0120, 0.0197,\n",
       "          0.0130, 0.0215, 0.0333, 0.0189, 0.0189, 0.0154, 0.0251, 0.0201, 0.0289,\n",
       "          0.0229, 0.0199, 0.0203, 0.0210, 0.0206, 0.0241, 0.0249, 0.0221, 0.0216,\n",
       "          0.0161, 0.0219, 0.0253, 0.0212, 0.0251, 0.0150, 0.0217, 0.0113, 0.0165,\n",
       "          0.0223, 0.0152, 0.0227, 0.0199, 0.0206, 0.0177, 0.0198, 0.0212, 0.0136,\n",
       "          0.0260, 0.0176, 0.0189, 0.0190, 0.0221, 0.0177, 0.0247, 0.0224, 0.0221,\n",
       "          0.0238, 0.0228, 0.0217, 0.0197, 0.0225, 0.0275, 0.0281, 0.0230, 0.0216,\n",
       "          0.0217, 0.0101, 0.0120, 0.0141, 0.0127, 0.0174, 0.0148, 0.0163, 0.0178,\n",
       "          0.0212, 0.0224, 0.0206, 0.0169, 0.0220, 0.0237, 0.0156, 0.0237, 0.0221,\n",
       "          0.0305, 0.0228, 0.0232, 0.0289, 0.0346, 0.0223, 0.0238, 0.0268, 0.0341,\n",
       "          0.0309, 0.0294, 0.0367, 0.0263, 0.0414, 0.0279, 0.0123, 0.0146, 0.0172,\n",
       "          0.0137, 0.0139, 0.0188, 0.0120, 0.0177, 0.0131, 0.0186, 0.0169, 0.0130,\n",
       "          0.0195, 0.0255, 0.0173, 0.0216, 0.0178, 0.0309, 0.0194, 0.0241, 0.0202,\n",
       "          0.0247, 0.0216, 0.0280, 0.0277, 0.0251, 0.0309, 0.0281, 0.0276, 0.0315,\n",
       "          0.0283, 0.0296, 0.0083, 0.0086, 0.0082, 0.0094, 0.0113, 0.0146, 0.0114,\n",
       "          0.0107, 0.0139, 0.0100, 0.0090, 0.0120, 0.0152, 0.0132, 0.0146, 0.0181,\n",
       "          0.0171, 0.0157, 0.0237, 0.0147, 0.0174, 0.0312, 0.0269, 0.0458, 0.0458,\n",
       "          0.0396, 0.0458, 0.0359, 0.0511, 0.0435, 0.0320, 0.0367, 0.0100, 0.0083,\n",
       "          0.0085, 0.0109, 0.0092, 0.0102, 0.0135, 0.0113, 0.0110, 0.0092, 0.0152,\n",
       "          0.0133, 0.0153, 0.0112, 0.0207, 0.0184, 0.0136, 0.0197, 0.0211, 0.0197,\n",
       "          0.0221, 0.0230, 0.0262, 0.0471, 0.0338, 0.0409, 0.0424, 0.0414, 0.0323,\n",
       "          0.0417, 0.0310, 0.0315, 0.0119, 0.0095, 0.0124, 0.0094, 0.0146, 0.0195,\n",
       "          0.0146, 0.0157, 0.0142, 0.0242, 0.0157, 0.0165, 0.0176, 0.0198, 0.0230,\n",
       "          0.0225, 0.0185, 0.0223, 0.0233, 0.0188, 0.0228, 0.0249, 0.0221, 0.0230,\n",
       "          0.0234, 0.0201, 0.0320, 0.0206, 0.0259, 0.0193, 0.0279, 0.0236, 0.0495,\n",
       "          0.0211, 0.0116, 0.0105, 0.0177, 0.0119, 0.0139, 0.0152, 0.0139, 0.0206,\n",
       "          0.0161, 0.0212, 0.0193, 0.0156, 0.0159, 0.0233, 0.0193, 0.0229, 0.0253,\n",
       "          0.0185, 0.0247, 0.0225, 0.0204, 0.0262, 0.0275, 0.0217, 0.0302, 0.0199,\n",
       "          0.0264, 0.0258, 0.0269, 0.0213, 0.0190, 0.0242, 0.0158, 0.0181, 0.0254,\n",
       "          0.0174, 0.0254, 0.0190, 0.0315, 0.0237, 0.0314, 0.0163, 0.0281, 0.0250,\n",
       "          0.0220, 0.0399, 0.0296, 0.0293, 0.0327, 0.0315, 0.0329, 0.0275, 0.0160,\n",
       "          0.0312, 0.0346, 0.0375, 0.0311, 0.0233, 0.0266, 0.0284, 0.0419, 0.0357,\n",
       "          0.0155, 0.0245, 0.0181, 0.0154, 0.0238, 0.0177, 0.0275, 0.0251, 0.0262,\n",
       "          0.0159, 0.0302, 0.0230, 0.0354, 0.0333, 0.0272, 0.0365, 0.0264, 0.0290,\n",
       "          0.0296, 0.0268, 0.0329, 0.0399, 0.0206, 0.0279, 0.0306, 0.0383, 0.0158,\n",
       "          0.0171, 0.0357, 0.0306, 0.0307, 0.0307, 0.0150, 0.0090, 0.0113, 0.0103,\n",
       "          0.0106, 0.0132, 0.0124, 0.0152, 0.0150, 0.0100, 0.0141, 0.0160, 0.0213,\n",
       "          0.0157, 0.0184, 0.0194, 0.0151, 0.0204, 0.0259, 0.0236, 0.0284, 0.0236,\n",
       "          0.0262, 0.0269, 0.0306, 0.0171, 0.0250, 0.0271, 0.0370, 0.0399, 0.0305,\n",
       "          0.0362, 0.0089, 0.0123, 0.0111, 0.0098, 0.0133, 0.0128, 0.0120, 0.0180,\n",
       "          0.0144, 0.0161, 0.0161, 0.0178, 0.0210, 0.0128, 0.0182, 0.0229, 0.0201,\n",
       "          0.0276, 0.0151, 0.0245, 0.0249, 0.0286, 0.0227, 0.0272, 0.0331, 0.0401,\n",
       "          0.0293, 0.0289, 0.0404, 0.0336, 0.0346, 0.0380, 0.0223, 0.0149, 0.0107,\n",
       "          0.0207, 0.0184, 0.0137, 0.0173, 0.0201, 0.0111, 0.0208, 0.0182, 0.0185,\n",
       "          0.0186, 0.0255, 0.0166, 0.0224, 0.0301, 0.0228, 0.0188, 0.0188, 0.0245,\n",
       "          0.0195, 0.0275, 0.0208, 0.0227, 0.0251, 0.0383, 0.0202, 0.0195, 0.0215,\n",
       "          0.0211, 0.0217, 0.0163, 0.0173, 0.0176, 0.0169, 0.0213, 0.0163, 0.0190,\n",
       "          0.0191, 0.0207, 0.0190, 0.0134, 0.0201, 0.0224, 0.0207, 0.0188, 0.0216,\n",
       "          0.0178, 0.0210, 0.0181, 0.0220, 0.0191, 0.0198, 0.0198, 0.0247, 0.0211,\n",
       "          0.0193, 0.0118, 0.0236, 0.0202, 0.0219, 0.0228, 0.0194, 0.0135, 0.0120,\n",
       "          0.0098, 0.0100, 0.0138, 0.0105, 0.0111, 0.0128, 0.0118, 0.0104, 0.0128,\n",
       "          0.0169, 0.0180, 0.0186, 0.0171, 0.0212, 0.0238, 0.0247, 0.0211, 0.0250,\n",
       "          0.0310, 0.0272, 0.0336, 0.0336, 0.0456, 0.0323, 0.0346, 0.0341, 0.0443,\n",
       "          0.0370, 0.0383, 0.0453, 0.0102, 0.0109, 0.0100, 0.0143, 0.0098, 0.0124,\n",
       "          0.0148, 0.0138, 0.0127, 0.0156, 0.0164, 0.0177, 0.0159, 0.0173, 0.0184,\n",
       "          0.0305, 0.0184, 0.0341, 0.0262, 0.0286, 0.0306, 0.0273, 0.0411, 0.0336,\n",
       "          0.0375, 0.0283, 0.0417, 0.0357, 0.0440, 0.0393, 0.0401, 0.0396]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.47.mlp.down_proj.scales': tensor([[0.0302, 0.0269, 0.0329,  ..., 0.0247, 0.0271, 0.0365]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.23.self_attn.q_proj.scales': tensor([[0.0007, 0.0008, 0.0007,  ..., 0.0011, 0.0006, 0.0007]]),\n",
       " 'vpm.encoder.layers.3.layer_norm1.weight': tensor([0.7227, 0.4512, 0.2334,  ..., 0.8281, 0.3516, 0.5859]),\n",
       " 'vpm.encoder.layers.24.self_attn.out_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.15.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.25.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.48.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.45.mlp.down_proj.scales': tensor([[0.0297, 0.0285, 0.0263,  ..., 0.0279, 0.0257, 0.0264]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.6.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.13.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.24.self_attn.o_proj.qweight': tensor([[ 2027326102, -1719179129, -1752659831,  ...,  2040031419,\n",
       "          -1703237719,  2005375111],\n",
       "         [-1668708459, -2020963658,  2056816488,  ..., -1467393878,\n",
       "          -1503103610, -1970825353],\n",
       "         [ 1737057588, -1973839973,  2022078584,  ..., -1971753848,\n",
       "          -1199085658, -1739032185],\n",
       "         ...,\n",
       "         [-1468568935,  2073397623, -1483114345,  ..., -1467377015,\n",
       "          -1449419958,  2040174712],\n",
       "         [-2002164071, -1266058857, -1987614568,  ..., -1734698359,\n",
       "           1784194938,  2022152343],\n",
       "         [  900236954, -2006488982, -1753639032,  ..., -2003339367,\n",
       "          -1721264215, -1717929319]], dtype=torch.int32),\n",
       " 'llm.model.layers.22.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.26.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.6.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.0.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.31.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.46.self_attn.o_proj.scales': tensor([[0.0250, 0.0281, 0.0285,  ..., 0.0305, 0.0273, 0.0221]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.7.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.43.self_attn.k_proj.qweight': tensor([[-1733911367, -1734768777,  1753914213,  ...,  1735968824,\n",
       "          -2001229112,  2023196809],\n",
       "         [-2018662311, -1703380634, -2004313995,  ...,  2024307033,\n",
       "          -1520015446, -1483962230],\n",
       "         [ 1436060872, -1503164261, -1988524648,  ...,  2027443863,\n",
       "          -1199015800, -1736931191],\n",
       "         ...,\n",
       "         [ 1218090391, -2071357464, -1718065543,  ..., -1752533336,\n",
       "           2041088424,  2022274984],\n",
       "         [-1197962925,  1753069942, -1752615817,  ...,  -930715254,\n",
       "           1251453816,  1734776472],\n",
       "         [-1771472455,  1753711764, -1975023943,  ..., -2003420248,\n",
       "          -1721198199, -1971877494]], dtype=torch.int32),\n",
       " 'llm.model.layers.30.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.20.mlp.fc1.qweight': tensor([[-1654491794,  2019909266,  1988527514,  ..., -1990237308,\n",
       "           2110814887,  1804500333],\n",
       "         [ 1567134635, -2003735168,  1785828709,  ..., -2019678587,\n",
       "           1769763942,  2058197680],\n",
       "         [ 1986693241,  2065780831,  2022406262,  ...,  2092654942,\n",
       "          -1836665980, -2041476203],\n",
       "         ...,\n",
       "         [-1836543887, -2138682981, -1770945693,  ..., -1553567333,\n",
       "           2055436180,  1655148915],\n",
       "         [ 2020702313, -1886866018,  1803639145,  ...,  1768722807,\n",
       "           1465482926, -1903202683],\n",
       "         [ 2053285793,  2073148782, -2106387157,  ..., -1749529148,\n",
       "           1733054635,  -913013145]], dtype=torch.int32),\n",
       " 'llm.model.layers.17.mlp.up_proj.scales': tensor([[0.0306, 0.0221, 0.0357,  ..., 0.0202, 0.0281, 0.0251]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.36.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.24.self_attn.o_proj.scales': tensor([[0.0251, 0.0290, 0.0284,  ..., 0.0262, 0.0267, 0.0254]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.33.mlp.gate_proj.scales': tensor([[0.0271, 0.0223, 0.0211,  ..., 0.0169, 0.0306, 0.0609]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.13.mlp.fc1.qzeros': tensor([[2139062143, 2139062143, 2139062143,  ..., 2139062143, 2139062143,\n",
       "          2139062143]], dtype=torch.int32),\n",
       " 'llm.model.layers.43.mlp.down_proj.qweight': tensor([[-2003277927, -1720158343,  2005776535,  ..., -1953085562,\n",
       "           -876057977, -1986492567],\n",
       "         [-2005374071, -1463191671,  1988786073,  ...,  2021157992,\n",
       "          -1434810503, -2004449144],\n",
       "         [-1938462087, -1774679689,  2040949159,  ..., -1938401627,\n",
       "           -626404213, -1719109769],\n",
       "         ...,\n",
       "         [ 1717995929,  1285053274, -1737921910,  ..., -1480889482,\n",
       "           2057798982,  1769510794],\n",
       "         [ 2057873548,  1988012905,  -895702902,  ...,  1988933765,\n",
       "           1286038918, -2004260488],\n",
       "         [-1769494165, -1486603909,  2023315621,  ..., -2006407544,\n",
       "           1972873127, -1987479160]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.3.self_attn.out_proj.qweight': tensor([[-1065321316, -2034731105, -1567453089,  ...,  2038707540,\n",
       "           1754046588, -2140035128],\n",
       "         [-2121178760,  1652718175,  1854044251,  ..., -1887462294,\n",
       "          -1849778293, -2099218343],\n",
       "         [ 1951825550,  -694456420, -1417443411,  ...,  1852670094,\n",
       "          -1371090859,  1634949979],\n",
       "         ...,\n",
       "         [-1598777436,  1703812264,  1751679861,  ..., -1427251604,\n",
       "          -1547996809, -1453561968],\n",
       "         [ 1454140705,  1299219627,  2036441998,  ...,  2088399244,\n",
       "           2115932247,  1534832250],\n",
       "         [ 1921408108,  -896489581,  1584624292,  ...,  1027646334,\n",
       "          -1869040786,   228353047]], dtype=torch.int32),\n",
       " 'llm.model.layers.12.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.8.self_attn.q_proj.bias': tensor([-0.0210, -0.0620, -0.0486,  ..., -1.2266, -0.1206, -0.1631]),\n",
       " 'vpm.encoder.layers.7.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.2.self_attn.k_proj.scales': tensor([[0.0155, 0.0162, 0.0150, 0.0167, 0.0180, 0.0211, 0.0194, 0.0181, 0.0169,\n",
       "          0.0173, 0.0145, 0.0198, 0.0174, 0.0193, 0.0158, 0.0228, 0.0234, 0.0197,\n",
       "          0.0471, 0.0201, 0.0839, 0.0221, 0.0227, 0.0253, 0.0539, 0.0237, 0.0249,\n",
       "          0.0238, 0.0259, 0.0254, 0.0275, 0.0375, 0.0171, 0.0148, 0.0198, 0.0146,\n",
       "          0.0172, 0.0147, 0.0193, 0.0137, 0.0178, 0.0156, 0.0167, 0.0159, 0.0113,\n",
       "          0.0191, 0.0190, 0.0211, 0.0167, 0.0173, 0.0250, 0.0219, 0.0211, 0.0224,\n",
       "          0.0271, 0.0232, 0.0531, 0.0264, 0.0237, 0.0266, 0.0264, 0.0290, 0.0264,\n",
       "          0.0225, 0.0219, 0.0169, 0.0156, 0.0172, 0.0167, 0.0184, 0.0168, 0.0194,\n",
       "          0.0171, 0.0197, 0.0189, 0.0208, 0.0229, 0.0253, 0.0228, 0.0280, 0.0257,\n",
       "          0.0453, 0.0242, 0.0259, 0.0220, 0.0213, 0.0211, 0.0232, 0.0245, 0.0268,\n",
       "          0.0223, 0.0251, 0.0271, 0.0273, 0.0249, 0.0267, 0.0185, 0.0182, 0.0152,\n",
       "          0.0151, 0.0169, 0.0188, 0.0156, 0.0190, 0.0191, 0.0193, 0.0219, 0.0203,\n",
       "          0.0213, 0.0204, 0.0215, 0.0244, 0.0293, 0.0228, 0.0233, 0.0246, 0.0244,\n",
       "          0.0227, 0.0224, 0.0258, 0.0212, 0.0734, 0.0215, 0.0220, 0.0216, 0.0220,\n",
       "          0.0255, 0.0241, 0.0191, 0.0145, 0.0182, 0.0212, 0.0198, 0.0236, 0.0203,\n",
       "          0.0223, 0.0132, 0.0241, 0.0156, 0.0203, 0.0249, 0.0230, 0.0319, 0.0221,\n",
       "          0.0264, 0.0271, 0.0253, 0.0399, 0.0332, 0.0359, 0.0244, 0.0264, 0.0745,\n",
       "          0.0268, 0.0257, 0.0319, 0.0263, 0.0289, 0.0294, 0.0290, 0.0201, 0.0168,\n",
       "          0.0121, 0.0176, 0.0148, 0.0227, 0.0146, 0.0279, 0.0217, 0.0204, 0.0202,\n",
       "          0.0232, 0.0180, 0.0229, 0.0238, 0.0259, 0.0242, 0.0262, 0.0288, 0.0411,\n",
       "          0.0622, 0.0253, 0.0367, 0.0272, 0.0682, 0.0237, 0.0503, 0.0225, 0.0289,\n",
       "          0.0229, 0.0259, 0.0241, 0.0380, 0.0250, 0.0184, 0.0208, 0.0230, 0.0271,\n",
       "          0.0238, 0.0188, 0.0238, 0.0219, 0.0232, 0.0242, 0.0233, 0.0233, 0.0234,\n",
       "          0.0229, 0.0253, 0.0233, 0.0251, 0.0281, 0.0324, 0.0234, 0.0251, 0.0230,\n",
       "          0.0531, 0.0244, 0.0372, 0.0258, 0.0277, 0.0244, 0.0286, 0.0262, 0.0260,\n",
       "          0.0233, 0.0257, 0.0303, 0.0206, 0.0257, 0.0293, 0.0190, 0.0251, 0.0213,\n",
       "          0.0237, 0.0217, 0.0255, 0.0297, 0.0257, 0.0268, 0.0223, 0.0266, 0.0286,\n",
       "          0.0354, 0.0327, 0.0516, 0.0288, 0.0279, 0.0521, 0.0259, 0.0227, 0.0378,\n",
       "          0.0306, 0.0260, 0.0315, 0.0268, 0.0174, 0.0230, 0.0157, 0.0207, 0.0159,\n",
       "          0.0219, 0.0166, 0.0276, 0.0188, 0.0409, 0.0401, 0.0258, 0.0266, 0.0332,\n",
       "          0.0396, 0.0315, 0.0352, 0.0362, 0.0253, 0.0336, 0.0273, 0.0269, 0.0323,\n",
       "          0.0319, 0.0484, 0.0302, 0.0309, 0.0312, 0.0269, 0.0303, 0.0349, 0.0344,\n",
       "          0.0181, 0.0147, 0.0199, 0.0189, 0.0161, 0.0221, 0.0194, 0.0289, 0.0241,\n",
       "          0.0293, 0.0310, 0.0306, 0.0327, 0.0280, 0.0316, 0.0281, 0.0316, 0.0296,\n",
       "          0.0293, 0.0285, 0.0329, 0.0276, 0.0289, 0.0273, 0.0401, 0.0298, 0.0280,\n",
       "          0.0312, 0.0285, 0.0319, 0.0328, 0.0212, 0.0232, 0.0137, 0.0220, 0.0173,\n",
       "          0.0164, 0.0220, 0.0202, 0.0225, 0.0234, 0.0204, 0.0225, 0.0237, 0.0227,\n",
       "          0.0286, 0.0211, 0.0241, 0.0305, 0.0247, 0.0336, 0.0240, 0.0615, 0.0249,\n",
       "          0.0293, 0.0258, 0.0427, 0.0297, 0.0331, 0.0336, 0.0336, 0.0244, 0.0249,\n",
       "          0.0227, 0.0185, 0.0137, 0.0129, 0.0197, 0.0131, 0.0208, 0.0151, 0.0223,\n",
       "          0.0250, 0.0138, 0.0195, 0.0240, 0.0225, 0.0247, 0.0237, 0.0228, 0.0281,\n",
       "          0.0227, 0.0333, 0.0257, 0.0482, 0.0484, 0.0289, 0.0306, 0.0641, 0.0280,\n",
       "          0.0246, 0.0289, 0.0307, 0.0329, 0.0254, 0.0307, 0.0232, 0.0163, 0.0236,\n",
       "          0.0221, 0.0258, 0.0236, 0.0247, 0.0284, 0.0232, 0.0280, 0.0225, 0.0264,\n",
       "          0.0296, 0.0294, 0.0341, 0.0323, 0.0257, 0.0273, 0.0286, 0.0267, 0.0275,\n",
       "          0.0260, 0.0289, 0.0289, 0.0516, 0.0323, 0.0263, 0.0271, 0.0305, 0.0272,\n",
       "          0.0260, 0.0258, 0.0215, 0.0228, 0.0311, 0.0158, 0.0247, 0.0173, 0.0224,\n",
       "          0.0202, 0.0257, 0.0294, 0.0264, 0.0264, 0.0228, 0.0285, 0.0246, 0.0289,\n",
       "          0.0241, 0.0283, 0.0311, 0.0283, 0.0292, 0.0279, 0.0311, 0.0244, 0.0466,\n",
       "          0.0267, 0.0292, 0.0258, 0.0268, 0.0279, 0.0254, 0.0385, 0.0283, 0.0197,\n",
       "          0.0156, 0.0201, 0.0234, 0.0277, 0.0215, 0.0215, 0.0204, 0.0188, 0.0216,\n",
       "          0.0210, 0.0210, 0.0290, 0.0268, 0.0290, 0.0228, 0.0229, 0.0432, 0.0249,\n",
       "          0.0257, 0.0448, 0.0284, 0.0286, 0.0453, 0.0223, 0.0236, 0.0234, 0.0204,\n",
       "          0.0225, 0.0319, 0.0229, 0.0143, 0.0225, 0.0193, 0.0204, 0.0288, 0.0201,\n",
       "          0.0227, 0.0136, 0.0288, 0.0191, 0.0250, 0.0189, 0.0240, 0.0233, 0.0255,\n",
       "          0.0203, 0.0230, 0.0237, 0.0359, 0.0237, 0.0229, 0.0414, 0.0453, 0.0244,\n",
       "          0.0458, 0.0279, 0.0254, 0.0255, 0.0216, 0.0292, 0.0250, 0.0245]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.49.self_attn.k_proj.scales': tensor([[0.0115, 0.0099, 0.0099, 0.0088, 0.0086, 0.0093, 0.0089, 0.0088, 0.0119,\n",
       "          0.0157, 0.0137, 0.0125, 0.0158, 0.0182, 0.0188, 0.0210, 0.0230, 0.0202,\n",
       "          0.0188, 0.0165, 0.0221, 0.0221, 0.0236, 0.0215, 0.0201, 0.0207, 0.0320,\n",
       "          0.0327, 0.0362, 0.0388, 0.0303, 0.0273, 0.0082, 0.0070, 0.0080, 0.0081,\n",
       "          0.0079, 0.0085, 0.0083, 0.0094, 0.0089, 0.0105, 0.0080, 0.0189, 0.0129,\n",
       "          0.0198, 0.0188, 0.0167, 0.0202, 0.0168, 0.0238, 0.0232, 0.0237, 0.0245,\n",
       "          0.0217, 0.0237, 0.0271, 0.0221, 0.0438, 0.0300, 0.0286, 0.0237, 0.0234,\n",
       "          0.0422, 0.0135, 0.0171, 0.0126, 0.0227, 0.0129, 0.0186, 0.0220, 0.0168,\n",
       "          0.0210, 0.0186, 0.0204, 0.0198, 0.0266, 0.0210, 0.0216, 0.0238, 0.0169,\n",
       "          0.0201, 0.0228, 0.0281, 0.0328, 0.0263, 0.0279, 0.0240, 0.0292, 0.0096,\n",
       "          0.0223, 0.0240, 0.0227, 0.0245, 0.0331, 0.0221, 0.0158, 0.0156, 0.0159,\n",
       "          0.0189, 0.0167, 0.0145, 0.0204, 0.0162, 0.0173, 0.0158, 0.0193, 0.0146,\n",
       "          0.0253, 0.0251, 0.0163, 0.0223, 0.0199, 0.0247, 0.0225, 0.0276, 0.0258,\n",
       "          0.0240, 0.0225, 0.0253, 0.0288, 0.0385, 0.0238, 0.0283, 0.0221, 0.0204,\n",
       "          0.0207, 0.0267, 0.0085, 0.0100, 0.0141, 0.0113, 0.0140, 0.0132, 0.0142,\n",
       "          0.0134, 0.0154, 0.0102, 0.0112, 0.0120, 0.0133, 0.0152, 0.0145, 0.0172,\n",
       "          0.0139, 0.0131, 0.0177, 0.0185, 0.0193, 0.0236, 0.0238, 0.0206, 0.0246,\n",
       "          0.0242, 0.0253, 0.0217, 0.0492, 0.0316, 0.0327, 0.0228, 0.0168, 0.0104,\n",
       "          0.0115, 0.0112, 0.0104, 0.0128, 0.0115, 0.0097, 0.0122, 0.0122, 0.0112,\n",
       "          0.0158, 0.0139, 0.0154, 0.0111, 0.0162, 0.0160, 0.0181, 0.0172, 0.0182,\n",
       "          0.0180, 0.0138, 0.0189, 0.0195, 0.0186, 0.0244, 0.0228, 0.0305, 0.0432,\n",
       "          0.0333, 0.0236, 0.0276, 0.0150, 0.0171, 0.0150, 0.0163, 0.0128, 0.0173,\n",
       "          0.0184, 0.0146, 0.0161, 0.0171, 0.0158, 0.0148, 0.0159, 0.0199, 0.0176,\n",
       "          0.0178, 0.0201, 0.0206, 0.0181, 0.0185, 0.0173, 0.0251, 0.0212, 0.0234,\n",
       "          0.0241, 0.0210, 0.0159, 0.0211, 0.0180, 0.0212, 0.0236, 0.0177, 0.0173,\n",
       "          0.0169, 0.0145, 0.0150, 0.0172, 0.0160, 0.0156, 0.0197, 0.0165, 0.0150,\n",
       "          0.0169, 0.0146, 0.0193, 0.0118, 0.0246, 0.0210, 0.0215, 0.0208, 0.0213,\n",
       "          0.0251, 0.0199, 0.0186, 0.0254, 0.0204, 0.0292, 0.0197, 0.0422, 0.0269,\n",
       "          0.0211, 0.0233, 0.0182, 0.0267, 0.0146, 0.0139, 0.0167, 0.0158, 0.0219,\n",
       "          0.0165, 0.0168, 0.0181, 0.0169, 0.0163, 0.0133, 0.0216, 0.0169, 0.0188,\n",
       "          0.0141, 0.0199, 0.0241, 0.0171, 0.0174, 0.0245, 0.0202, 0.0190, 0.0206,\n",
       "          0.0199, 0.0238, 0.0212, 0.0201, 0.0242, 0.0247, 0.0276, 0.0338, 0.0258,\n",
       "          0.0176, 0.0161, 0.0157, 0.0136, 0.0168, 0.0169, 0.0173, 0.0171, 0.0177,\n",
       "          0.0237, 0.0154, 0.0180, 0.0185, 0.0224, 0.0191, 0.0227, 0.0185, 0.0150,\n",
       "          0.0224, 0.0169, 0.0275, 0.0220, 0.0266, 0.0241, 0.0271, 0.0193, 0.0307,\n",
       "          0.0289, 0.0306, 0.0244, 0.0245, 0.0250, 0.0173, 0.0119, 0.0160, 0.0105,\n",
       "          0.0116, 0.0120, 0.0089, 0.0129, 0.0121, 0.0130, 0.0159, 0.0119, 0.0188,\n",
       "          0.0186, 0.0137, 0.0141, 0.0163, 0.0152, 0.0171, 0.0185, 0.0211, 0.0227,\n",
       "          0.0227, 0.0245, 0.0178, 0.0320, 0.0318, 0.0326, 0.0324, 0.0268, 0.0213,\n",
       "          0.0411, 0.0116, 0.0114, 0.0089, 0.0130, 0.0096, 0.0087, 0.0105, 0.0139,\n",
       "          0.0140, 0.0107, 0.0117, 0.0122, 0.0121, 0.0181, 0.0148, 0.0148, 0.0127,\n",
       "          0.0195, 0.0139, 0.0198, 0.0158, 0.0182, 0.0259, 0.0307, 0.0432, 0.0221,\n",
       "          0.0297, 0.0362, 0.0314, 0.0212, 0.0307, 0.0268, 0.0094, 0.0086, 0.0094,\n",
       "          0.0099, 0.0073, 0.0083, 0.0085, 0.0090, 0.0100, 0.0102, 0.0133, 0.0116,\n",
       "          0.0164, 0.0113, 0.0142, 0.0135, 0.0220, 0.0141, 0.0169, 0.0164, 0.0188,\n",
       "          0.0229, 0.0203, 0.0401, 0.0435, 0.0396, 0.0534, 0.0344, 0.0359, 0.0332,\n",
       "          0.0393, 0.0362, 0.0095, 0.0090, 0.0079, 0.0083, 0.0080, 0.0102, 0.0085,\n",
       "          0.0098, 0.0091, 0.0086, 0.0100, 0.0083, 0.0132, 0.0149, 0.0166, 0.0136,\n",
       "          0.0171, 0.0174, 0.0168, 0.0190, 0.0204, 0.0288, 0.0236, 0.0383, 0.0258,\n",
       "          0.0309, 0.0357, 0.0271, 0.0341, 0.0448, 0.0224, 0.0195, 0.0154, 0.0176,\n",
       "          0.0130, 0.0171, 0.0147, 0.0177, 0.0158, 0.0215, 0.0185, 0.0198, 0.0198,\n",
       "          0.0178, 0.0223, 0.0189, 0.0190, 0.0240, 0.0289, 0.0223, 0.0208, 0.0236,\n",
       "          0.0236, 0.0230, 0.0245, 0.0236, 0.0210, 0.0268, 0.0242, 0.0238, 0.0230,\n",
       "          0.0307, 0.0251, 0.0228, 0.0148, 0.0158, 0.0145, 0.0146, 0.0166, 0.0221,\n",
       "          0.0137, 0.0188, 0.0176, 0.0181, 0.0197, 0.0186, 0.0238, 0.0223, 0.0169,\n",
       "          0.0228, 0.0244, 0.0217, 0.0263, 0.0234, 0.0262, 0.0251, 0.0246, 0.0271,\n",
       "          0.0240, 0.0206, 0.0225, 0.0289, 0.0267, 0.0257, 0.0294, 0.0315]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.21.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.10.self_attn.out_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.41.mlp.up_proj.scales': tensor([[0.0257, 0.0259, 0.0242,  ..., 0.0273, 0.0269, 0.0255]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.12.self_attn.v_proj.scales': tensor([[0.0234, 0.0292, 0.0253, 0.0233, 0.0277, 0.0220, 0.0211, 0.0234, 0.0296,\n",
       "          0.0247, 0.0258, 0.0228, 0.0230, 0.0245, 0.0233, 0.0242, 0.0204, 0.0219,\n",
       "          0.0241, 0.0227, 0.0241, 0.0249, 0.0269, 0.0224, 0.0224, 0.0223, 0.0215,\n",
       "          0.0251, 0.0253, 0.0238, 0.0296, 0.0266, 0.0262, 0.0221, 0.0230, 0.0216,\n",
       "          0.0258, 0.0244, 0.0227, 0.0215, 0.0216, 0.0245, 0.0221, 0.0221, 0.0229,\n",
       "          0.0253, 0.0212, 0.0258, 0.0217, 0.0229, 0.0201, 0.0212, 0.0262, 0.0237,\n",
       "          0.0236, 0.0267, 0.0227, 0.0225, 0.0206, 0.0251, 0.0263, 0.0198, 0.0234,\n",
       "          0.0216, 0.0212, 0.0272, 0.0253, 0.0211, 0.0229, 0.0267, 0.0258, 0.0216,\n",
       "          0.0263, 0.0206, 0.0233, 0.0240, 0.0225, 0.0257, 0.0211, 0.0247, 0.0307,\n",
       "          0.0280, 0.0245, 0.0268, 0.0249, 0.0215, 0.0359, 0.0333, 0.0298, 0.0242,\n",
       "          0.0259, 0.0210, 0.0213, 0.0217, 0.0237, 0.0301, 0.0267, 0.0263, 0.0259,\n",
       "          0.0241, 0.0272, 0.0251, 0.0259, 0.0246, 0.0268, 0.0262, 0.0236, 0.0236,\n",
       "          0.0260, 0.0221, 0.0213, 0.0244, 0.0255, 0.0244, 0.0242, 0.0255, 0.0262,\n",
       "          0.0190, 0.0461, 0.0211, 0.0262, 0.0327, 0.0221, 0.0221, 0.0303, 0.0225,\n",
       "          0.0215, 0.0253, 0.0254, 0.0250, 0.0242, 0.0223, 0.0272, 0.0288, 0.0250,\n",
       "          0.0260, 0.0207, 0.0228, 0.0245, 0.0223, 0.0309, 0.0263, 0.0259, 0.0275,\n",
       "          0.0255, 0.0246, 0.0234, 0.0241, 0.0309, 0.0273, 0.0253, 0.0246, 0.0267,\n",
       "          0.0244, 0.0228, 0.0237, 0.0208, 0.0212, 0.0247, 0.0228, 0.0233, 0.0283,\n",
       "          0.0240, 0.0260, 0.0289, 0.0245, 0.0251, 0.0253, 0.0220, 0.0244, 0.0264,\n",
       "          0.0207, 0.0233, 0.0242, 0.0227, 0.0244, 0.0272, 0.0229, 0.0277, 0.0251,\n",
       "          0.0227, 0.0254, 0.0268, 0.0365, 0.0219, 0.0224, 0.0242, 0.0246, 0.0232,\n",
       "          0.0301, 0.0258, 0.0241, 0.0272, 0.0247, 0.0259, 0.0283, 0.0283, 0.0247,\n",
       "          0.0279, 0.0227, 0.0276, 0.0234, 0.0238, 0.0249, 0.0262, 0.0249, 0.0221,\n",
       "          0.0234, 0.0240, 0.0268, 0.0262, 0.0273, 0.0276, 0.0284, 0.0236, 0.0272,\n",
       "          0.0237, 0.0260, 0.0277, 0.0266, 0.0232, 0.0312, 0.0272, 0.0247, 0.0249,\n",
       "          0.0263, 0.0238, 0.0236, 0.0250, 0.0249, 0.0225, 0.0219, 0.0233, 0.0238,\n",
       "          0.0262, 0.0257, 0.0230, 0.0259, 0.0266, 0.0224, 0.0234, 0.0255, 0.0328,\n",
       "          0.0254, 0.0259, 0.0249, 0.0329, 0.0257, 0.0208, 0.0242, 0.0268, 0.0257,\n",
       "          0.0233, 0.0258, 0.0246, 0.0290, 0.0201, 0.0257, 0.0236, 0.0245, 0.0247,\n",
       "          0.0230, 0.0246, 0.0237, 0.0244, 0.0260, 0.0263, 0.0225, 0.0266, 0.0262,\n",
       "          0.0232, 0.0255, 0.0244, 0.0224, 0.0247, 0.0254, 0.0224, 0.0269, 0.0275,\n",
       "          0.0262, 0.0293, 0.0249, 0.0234, 0.0233, 0.0294, 0.0251, 0.0246, 0.0227,\n",
       "          0.0297, 0.0275, 0.0257, 0.0253, 0.0271, 0.0242, 0.0225, 0.0262, 0.0232,\n",
       "          0.0306, 0.0263, 0.0220, 0.0247, 0.0264, 0.0220, 0.0294, 0.0247, 0.0255,\n",
       "          0.0241, 0.0236, 0.0262, 0.0264, 0.0267, 0.0260, 0.0240, 0.0220, 0.0286,\n",
       "          0.0242, 0.0251, 0.0258, 0.0255, 0.0251, 0.0191, 0.0238, 0.0182, 0.0212,\n",
       "          0.0323, 0.0220, 0.0195, 0.0195, 0.0184, 0.0225, 0.0244, 0.0215, 0.0244,\n",
       "          0.0319, 0.0173, 0.0216, 0.0221, 0.0266, 0.0191, 0.0210, 0.0204, 0.0195,\n",
       "          0.0194, 0.0208, 0.0263, 0.0184, 0.0220, 0.0258, 0.0191, 0.0173, 0.0283,\n",
       "          0.0221, 0.0194, 0.0217, 0.0221, 0.0186, 0.0173, 0.0169, 0.0165, 0.0302,\n",
       "          0.0191, 0.0197, 0.0174, 0.0255, 0.0223, 0.0336, 0.0211, 0.0280, 0.0177,\n",
       "          0.0227, 0.0267, 0.0181, 0.0203, 0.0268, 0.0201, 0.0180, 0.0212, 0.0234,\n",
       "          0.0188, 0.0186, 0.0207, 0.0186, 0.0181, 0.0176, 0.0262, 0.0289, 0.0271,\n",
       "          0.0224, 0.0280, 0.0375, 0.0258, 0.0260, 0.0255, 0.0245, 0.0238, 0.0253,\n",
       "          0.0247, 0.0229, 0.0262, 0.0298, 0.0228, 0.0281, 0.0293, 0.0228, 0.0240,\n",
       "          0.0220, 0.0263, 0.0229, 0.0229, 0.0247, 0.0240, 0.0224, 0.0246, 0.0241,\n",
       "          0.0267, 0.0247, 0.0229, 0.0305, 0.0253, 0.0244, 0.0238, 0.0259, 0.0237,\n",
       "          0.0227, 0.0241, 0.0230, 0.0259, 0.0273, 0.0298, 0.0286, 0.0250, 0.0251,\n",
       "          0.0346, 0.0262, 0.0232, 0.0220, 0.0246, 0.0240, 0.0217, 0.0237, 0.0232,\n",
       "          0.0271, 0.0242, 0.0230, 0.0229, 0.0242, 0.0221, 0.0257, 0.0198, 0.0240,\n",
       "          0.0182, 0.0176, 0.0168, 0.0194, 0.0164, 0.0204, 0.0314, 0.0171, 0.0190,\n",
       "          0.0180, 0.0182, 0.0167, 0.0197, 0.0178, 0.0180, 0.0197, 0.0173, 0.0178,\n",
       "          0.0172, 0.0198, 0.0178, 0.0216, 0.0186, 0.0191, 0.0186, 0.0194, 0.0168,\n",
       "          0.0206, 0.0169, 0.0186, 0.0151, 0.0177, 0.0215, 0.0160, 0.0174, 0.0194,\n",
       "          0.0190, 0.0168, 0.0190, 0.0158, 0.0191, 0.0164, 0.0178, 0.0211, 0.0173,\n",
       "          0.0216, 0.0168, 0.0213, 0.0197, 0.0155, 0.0177, 0.0161, 0.0165, 0.0155,\n",
       "          0.0268, 0.0191, 0.0189, 0.0212, 0.0208, 0.0201, 0.0188, 0.0198]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.42.self_attn.v_proj.scales': tensor([[0.0338, 0.0346, 0.0354, 0.0336, 0.0409, 0.0193, 0.0259, 0.0365, 0.0300,\n",
       "          0.0310, 0.0300, 0.0289, 0.0329, 0.0301, 0.0298, 0.0378, 0.0310, 0.0307,\n",
       "          0.0315, 0.0312, 0.0367, 0.0344, 0.0362, 0.0300, 0.0378, 0.0336, 0.0388,\n",
       "          0.0349, 0.0338, 0.0310, 0.0422, 0.0359, 0.0290, 0.0346, 0.0293, 0.0207,\n",
       "          0.0336, 0.0336, 0.0305, 0.0521, 0.0357, 0.0372, 0.0357, 0.0289, 0.0286,\n",
       "          0.0370, 0.0344, 0.0396, 0.0332, 0.0333, 0.0327, 0.0414, 0.0380, 0.0328,\n",
       "          0.0320, 0.0359, 0.0359, 0.0352, 0.0396, 0.0316, 0.0316, 0.0283, 0.0312,\n",
       "          0.0338, 0.0221, 0.0221, 0.0223, 0.0244, 0.0263, 0.0257, 0.0221, 0.0233,\n",
       "          0.0293, 0.0223, 0.0212, 0.0247, 0.0194, 0.0225, 0.0213, 0.0237, 0.0269,\n",
       "          0.0254, 0.0208, 0.0219, 0.0217, 0.0242, 0.0228, 0.0215, 0.0217, 0.0232,\n",
       "          0.0242, 0.0206, 0.0263, 0.0217, 0.0233, 0.0240, 0.0260, 0.0223, 0.0210,\n",
       "          0.0232, 0.0213, 0.0294, 0.0258, 0.0259, 0.0259, 0.0213, 0.0272, 0.0259,\n",
       "          0.0257, 0.0246, 0.0203, 0.0242, 0.0260, 0.0294, 0.0249, 0.0244, 0.0227,\n",
       "          0.0232, 0.0301, 0.0223, 0.0262, 0.0238, 0.0216, 0.0234, 0.0255, 0.0277,\n",
       "          0.0238, 0.0236, 0.0292, 0.0280, 0.0367, 0.0370, 0.0285, 0.0372, 0.0370,\n",
       "          0.0362, 0.0320, 0.0327, 0.0531, 0.0327, 0.0242, 0.0297, 0.0316, 0.0267,\n",
       "          0.0385, 0.0264, 0.0383, 0.0290, 0.0396, 0.0302, 0.0293, 0.0341, 0.0319,\n",
       "          0.0266, 0.0275, 0.0417, 0.0266, 0.0383, 0.0336, 0.0333, 0.0277, 0.0354,\n",
       "          0.0314, 0.0370, 0.0266, 0.0352, 0.0283, 0.0219, 0.0331, 0.0258, 0.0306,\n",
       "          0.0333, 0.0385, 0.0281, 0.0322, 0.0296, 0.0393, 0.0302, 0.0311, 0.0275,\n",
       "          0.0290, 0.0280, 0.0370, 0.0284, 0.0307, 0.0417, 0.0385, 0.0388, 0.0310,\n",
       "          0.0301, 0.0357, 0.0224, 0.0391, 0.0385, 0.0604, 0.0352, 0.0404, 0.0448,\n",
       "          0.0297, 0.0414, 0.0378, 0.0399, 0.0466, 0.0440, 0.0411, 0.0411, 0.0404,\n",
       "          0.0427, 0.0328, 0.0461, 0.0414, 0.0479, 0.0422, 0.0438, 0.0469, 0.0388,\n",
       "          0.0399, 0.0393, 0.0333, 0.0417, 0.0331, 0.0495, 0.0280, 0.0380, 0.0375,\n",
       "          0.0495, 0.0341, 0.0406, 0.0461, 0.0432, 0.0417, 0.0388, 0.0440, 0.0219,\n",
       "          0.0258, 0.0286, 0.0367, 0.0427, 0.0362, 0.0396, 0.0461, 0.0357, 0.0298,\n",
       "          0.0409, 0.0448, 0.0354, 0.0388, 0.0352, 0.0396, 0.0372, 0.0479, 0.0396,\n",
       "          0.0385, 0.0435, 0.0419, 0.0357, 0.0309, 0.0349, 0.0257, 0.0338, 0.0268,\n",
       "          0.0258, 0.0322, 0.0280, 0.0294, 0.0311, 0.0359, 0.0262, 0.0286, 0.0367,\n",
       "          0.0288, 0.0286, 0.0293, 0.0320, 0.0319, 0.0284, 0.0352, 0.0327, 0.0229,\n",
       "          0.0292, 0.0279, 0.0316, 0.0254, 0.0279, 0.0292, 0.0294, 0.0306, 0.0283,\n",
       "          0.0262, 0.0242, 0.0284, 0.0314, 0.0281, 0.0262, 0.0289, 0.0264, 0.0294,\n",
       "          0.0290, 0.0297, 0.0298, 0.0289, 0.0315, 0.0272, 0.0284, 0.0314, 0.0272,\n",
       "          0.0267, 0.0365, 0.0280, 0.0349, 0.0303, 0.0316, 0.0352, 0.0309, 0.0273,\n",
       "          0.0320, 0.0302, 0.0406, 0.0318, 0.0277, 0.0292, 0.0288, 0.0354, 0.0309,\n",
       "          0.0331, 0.0393, 0.0266, 0.0338, 0.0320, 0.0312, 0.0309, 0.0359, 0.0336,\n",
       "          0.0338, 0.0298, 0.0322, 0.0298, 0.0492, 0.0318, 0.0341, 0.0303, 0.0306,\n",
       "          0.0284, 0.0305, 0.0309, 0.0328, 0.0332, 0.0318, 0.0302, 0.0311, 0.0300,\n",
       "          0.0293, 0.0283, 0.0338, 0.0359, 0.0318, 0.0303, 0.0349, 0.0292, 0.0290,\n",
       "          0.0286, 0.0300, 0.0306, 0.0262, 0.0349, 0.0328, 0.0388, 0.0322, 0.0312,\n",
       "          0.0294, 0.0263, 0.0391, 0.0298, 0.0318, 0.0365, 0.0310, 0.0338, 0.0280,\n",
       "          0.0271, 0.0322, 0.0289, 0.0432, 0.0268, 0.0365, 0.0332, 0.0279, 0.0275,\n",
       "          0.0303, 0.0309, 0.0323, 0.0271, 0.0336, 0.0251, 0.0286, 0.0296, 0.0277,\n",
       "          0.0269, 0.0399, 0.0316, 0.0263, 0.0260, 0.0352, 0.0255, 0.0277, 0.0267,\n",
       "          0.0240, 0.0271, 0.0276, 0.0245, 0.0264, 0.0292, 0.0298, 0.0263, 0.0234,\n",
       "          0.0277, 0.0375, 0.0254, 0.0280, 0.0245, 0.0244, 0.0255, 0.0257, 0.0259,\n",
       "          0.0297, 0.0324, 0.0253, 0.0246, 0.0276, 0.0242, 0.0240, 0.0249, 0.0305,\n",
       "          0.0292, 0.0254, 0.0273, 0.0276, 0.0268, 0.0258, 0.0267, 0.0303, 0.0301,\n",
       "          0.0266, 0.0236, 0.0272, 0.0314, 0.0245, 0.0309, 0.0250, 0.0314, 0.0257,\n",
       "          0.0383, 0.0385, 0.0300, 0.0365, 0.0286, 0.0344, 0.0319, 0.0318, 0.0328,\n",
       "          0.0338, 0.0327, 0.0273, 0.0307, 0.0210, 0.0307, 0.0352, 0.0401, 0.0289,\n",
       "          0.0319, 0.0344, 0.0292, 0.0326, 0.0336, 0.0406, 0.0367, 0.0305, 0.0294,\n",
       "          0.0320, 0.0375, 0.0338, 0.0331, 0.0293, 0.0318, 0.0338, 0.0319, 0.0240,\n",
       "          0.0357, 0.0341, 0.0328, 0.0324, 0.0333, 0.0346, 0.0349, 0.0383, 0.0346,\n",
       "          0.0336, 0.0372, 0.0324, 0.0404, 0.0307, 0.0315, 0.0306, 0.0307, 0.0319,\n",
       "          0.0344, 0.0300, 0.0298, 0.0273, 0.0324, 0.0406, 0.0336, 0.0319]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.18.self_attn.k_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.27.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.19.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.9.self_attn.v_proj.scales': tensor([[0.0008, 0.0010, 0.0009,  ..., 0.0008, 0.0007, 0.0008]]),\n",
       " 'vpm.encoder.layers.0.self_attn.q_proj.qweight': tensor([[ 1905358438, -1415674524,  1937997367,  ...,  1839693465,\n",
       "          -1685026236, -1753121417],\n",
       "         [ 1988855177,  1955627132, -1787593864,  ...,  2039183452,\n",
       "           1686536642,  2105964951],\n",
       "         [ 2023783783, -1972732536,  2107605379,  ..., -1953866107,\n",
       "          -1955166593,  2123073405],\n",
       "         ...,\n",
       "         [-1820161912, -2053992587, -1970767515,  ...,  1617662619,\n",
       "           1753323634, -2104066445],\n",
       "         [ 2053737583, -2106546054, -1921156996,  ..., -1735814012,\n",
       "          -1956025975, -2054980219],\n",
       "         [-2089983569,  2122745202, -2023325045,  ...,  1903471482,\n",
       "          -2006615223,  2123005313]], dtype=torch.int32),\n",
       " 'llm.model.layers.26.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.20.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.26.mlp.down_proj.scales': tensor([[0.0233, 0.0245, 0.0245,  ..., 0.0297, 0.0234, 0.0372]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.1.mlp.fc2.scales': tensor([[0.0010, 0.0009, 0.0009,  ..., 0.0009, 0.0009, 0.0010]]),\n",
       " 'vpm.encoder.layers.7.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.42.mlp.up_proj.qweight': tensor([[-1201190008,  1470662998,  2022148247,  ...,  1733982313,\n",
       "           1483171548, -1969793383],\n",
       "         [-2040105095, -2003994489, -1451984748,  ..., -1752857211,\n",
       "          -2069131655,  1232698521],\n",
       "         [-1618573142, -1470732889,  2022274746,  ..., -1649841082,\n",
       "          -1969584200, -1986492065],\n",
       "         ...,\n",
       "         [-1469422422, -1164416938, -2003466106,  ...,  1974044536,\n",
       "          -2017953399,  -945260423],\n",
       "         [ -897020855, -1502053770, -2022340261,  ...,   931706710,\n",
       "          -1738028700, -2021230986],\n",
       "         [-1987622471,  2005256391,  2061404295,  ..., -1957140632,\n",
       "           2090423732, -2019058472]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.13.self_attn.v_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.43.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.18.mlp.fc1.scales': tensor([[0.0010, 0.0010, 0.0009,  ..., 0.0010, 0.0009, 0.0009]]),\n",
       " 'llm.model.layers.43.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.51.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.8.self_attn.k_proj.scales': tensor([[0.0157, 0.0158, 0.0181, 0.0203, 0.0165, 0.0199, 0.0215, 0.0189, 0.0152,\n",
       "          0.0178, 0.0255, 0.0223, 0.0253, 0.0228, 0.0188, 0.0224, 0.0224, 0.0242,\n",
       "          0.0279, 0.0264, 0.0280, 0.0294, 0.0262, 0.0286, 0.0497, 0.0297, 0.0293,\n",
       "          0.0250, 0.0254, 0.0276, 0.0249, 0.0245, 0.0177, 0.0173, 0.0201, 0.0144,\n",
       "          0.0154, 0.0171, 0.0195, 0.0198, 0.0211, 0.0206, 0.0180, 0.0206, 0.0194,\n",
       "          0.0245, 0.0290, 0.0312, 0.0242, 0.0227, 0.0388, 0.0238, 0.0262, 0.0262,\n",
       "          0.0300, 0.0302, 0.0604, 0.0266, 0.0249, 0.0349, 0.0242, 0.0310, 0.0267,\n",
       "          0.0388, 0.0194, 0.0223, 0.0178, 0.0176, 0.0204, 0.0147, 0.0219, 0.0161,\n",
       "          0.0193, 0.0212, 0.0207, 0.0217, 0.0177, 0.0206, 0.0202, 0.0210, 0.0185,\n",
       "          0.0269, 0.0206, 0.0219, 0.0319, 0.0401, 0.0378, 0.0215, 0.0633, 0.0284,\n",
       "          0.0344, 0.0273, 0.0333, 0.0309, 0.0290, 0.0367, 0.0120, 0.0216, 0.0174,\n",
       "          0.0211, 0.0213, 0.0174, 0.0199, 0.0174, 0.0216, 0.0197, 0.0140, 0.0201,\n",
       "          0.0186, 0.0148, 0.0210, 0.0185, 0.0189, 0.0154, 0.0193, 0.0238, 0.0251,\n",
       "          0.0378, 0.0255, 0.0319, 0.0503, 0.0253, 0.0310, 0.0318, 0.0268, 0.0268,\n",
       "          0.0307, 0.0500, 0.0149, 0.0109, 0.0142, 0.0102, 0.0135, 0.0133, 0.0109,\n",
       "          0.0177, 0.0136, 0.0167, 0.0202, 0.0219, 0.0271, 0.0241, 0.0182, 0.0190,\n",
       "          0.0234, 0.0171, 0.0221, 0.0202, 0.0305, 0.0206, 0.0320, 0.0242, 0.0615,\n",
       "          0.0281, 0.0306, 0.0333, 0.0312, 0.0310, 0.0247, 0.0573, 0.0181, 0.0143,\n",
       "          0.0118, 0.0162, 0.0142, 0.0172, 0.0132, 0.0159, 0.0182, 0.0159, 0.0159,\n",
       "          0.0177, 0.0191, 0.0160, 0.0280, 0.0162, 0.0182, 0.0189, 0.0260, 0.0327,\n",
       "          0.0296, 0.0194, 0.0341, 0.0221, 0.0833, 0.0247, 0.0359, 0.0277, 0.0375,\n",
       "          0.0272, 0.0247, 0.0272, 0.0280, 0.0180, 0.0259, 0.0120, 0.0212, 0.0208,\n",
       "          0.0156, 0.0207, 0.0177, 0.0172, 0.0199, 0.0194, 0.0207, 0.0173, 0.0194,\n",
       "          0.0223, 0.0216, 0.0199, 0.0255, 0.0296, 0.0216, 0.0314, 0.0250, 0.0336,\n",
       "          0.0659, 0.0181, 0.0215, 0.0224, 0.0312, 0.0263, 0.0534, 0.0306, 0.0201,\n",
       "          0.0210, 0.0210, 0.0158, 0.0228, 0.0191, 0.0191, 0.0276, 0.0184, 0.0201,\n",
       "          0.0244, 0.0207, 0.0195, 0.0212, 0.0260, 0.0189, 0.0193, 0.0188, 0.0238,\n",
       "          0.0359, 0.0461, 0.0240, 0.0230, 0.0453, 0.0765, 0.0284, 0.0290, 0.0255,\n",
       "          0.0280, 0.0216, 0.0221, 0.0244, 0.0262, 0.0236, 0.0212, 0.0303, 0.0188,\n",
       "          0.0188, 0.0254, 0.0224, 0.0370, 0.0257, 0.0312, 0.0319, 0.0303, 0.0223,\n",
       "          0.0316, 0.0344, 0.0372, 0.0268, 0.0264, 0.0336, 0.0296, 0.0378, 0.0247,\n",
       "          0.0272, 0.0275, 0.0250, 0.0490, 0.0251, 0.0279, 0.0336, 0.0276, 0.0359,\n",
       "          0.0312, 0.0367, 0.0245, 0.0268, 0.0195, 0.0301, 0.0316, 0.0199, 0.0269,\n",
       "          0.0297, 0.0336, 0.0294, 0.0169, 0.0185, 0.0306, 0.0169, 0.0301, 0.0322,\n",
       "          0.0310, 0.0303, 0.0217, 0.0315, 0.0276, 0.0257, 0.0244, 0.0257, 0.0531,\n",
       "          0.0341, 0.0257, 0.0292, 0.0262, 0.0272, 0.0184, 0.0173, 0.0174, 0.0176,\n",
       "          0.0172, 0.0186, 0.0193, 0.0173, 0.0180, 0.0188, 0.0173, 0.0213, 0.0177,\n",
       "          0.0203, 0.0190, 0.0264, 0.0251, 0.0219, 0.0242, 0.0294, 0.0237, 0.0228,\n",
       "          0.0300, 0.0289, 0.0393, 0.0306, 0.0327, 0.0301, 0.0237, 0.0323, 0.0288,\n",
       "          0.0257, 0.0208, 0.0210, 0.0194, 0.0145, 0.0225, 0.0190, 0.0201, 0.0184,\n",
       "          0.0173, 0.0212, 0.0180, 0.0217, 0.0194, 0.0210, 0.0194, 0.0277, 0.0241,\n",
       "          0.0185, 0.0224, 0.0238, 0.0266, 0.0303, 0.0254, 0.0227, 0.0529, 0.0267,\n",
       "          0.0277, 0.0240, 0.0461, 0.0281, 0.0306, 0.0283, 0.0165, 0.0189, 0.0154,\n",
       "          0.0203, 0.0189, 0.0149, 0.0242, 0.0193, 0.0227, 0.0211, 0.0230, 0.0236,\n",
       "          0.0177, 0.0217, 0.0145, 0.0236, 0.0237, 0.0225, 0.0286, 0.0344, 0.0417,\n",
       "          0.0289, 0.0277, 0.0262, 0.0359, 0.0479, 0.0273, 0.0281, 0.0399, 0.0372,\n",
       "          0.0391, 0.0285, 0.0177, 0.0140, 0.0178, 0.0210, 0.0174, 0.0203, 0.0202,\n",
       "          0.0197, 0.0149, 0.0253, 0.0220, 0.0149, 0.0199, 0.0185, 0.0237, 0.0277,\n",
       "          0.0227, 0.0245, 0.0212, 0.0286, 0.0227, 0.0251, 0.0284, 0.0229, 0.0220,\n",
       "          0.0552, 0.0333, 0.0303, 0.0233, 0.0283, 0.0324, 0.0264, 0.0264, 0.0253,\n",
       "          0.0273, 0.0188, 0.0189, 0.0172, 0.0294, 0.0224, 0.0298, 0.0260, 0.0315,\n",
       "          0.0244, 0.0380, 0.0319, 0.0280, 0.0385, 0.0238, 0.0296, 0.0290, 0.0242,\n",
       "          0.0272, 0.0279, 0.0322, 0.0290, 0.0385, 0.0272, 0.0285, 0.0290, 0.0286,\n",
       "          0.0311, 0.0249, 0.0279, 0.0168, 0.0247, 0.0147, 0.0178, 0.0290, 0.0210,\n",
       "          0.0220, 0.0211, 0.0323, 0.0250, 0.0372, 0.0246, 0.0298, 0.0333, 0.0316,\n",
       "          0.0453, 0.0341, 0.0324, 0.0281, 0.0212, 0.0294, 0.0301, 0.0290, 0.0354,\n",
       "          0.0596, 0.0383, 0.0285, 0.0378, 0.0378, 0.0518, 0.0292, 0.0275]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.10.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.0.mlp.fc2.qweight': tensor([[-1903790724, -2103203959, -2124191891,  ...,  2088272239,\n",
       "           2120117880, -1853979251],\n",
       "         [ 2053347227,   962878342, -2122415473,  ..., -1887012224,\n",
       "          -2035577954, -1719444630],\n",
       "         [ 1937147240, -2139190616, -2140834432,  ..., -2071826306,\n",
       "           1435524728,  2139655305],\n",
       "         ...,\n",
       "         [ 1552058001, -2018549640, -2106229391,  ..., -2069924979,\n",
       "          -1468494410,  2088997490],\n",
       "         [ 2070380654, -2004793973,  1784252561,  ...,  2056555143,\n",
       "           2143067526,  1836093056],\n",
       "         [ 1585426333,  2073597331,  2121889664,  ...,  1635479442,\n",
       "          -2124052603, -1467322240]], dtype=torch.int32),\n",
       " 'llm.model.layers.13.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.25.mlp.up_proj.scales': tensor([[0.0215, 0.0300, 0.0234,  ..., 0.0229, 0.0213, 0.0240]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.49.mlp.gate_proj.scales': tensor([[0.0211, 0.0208, 0.0229,  ..., 0.0215, 0.0283, 0.0208]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.34.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.2.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.17.self_attn.k_proj.scales': tensor([[0.0008, 0.0007, 0.0009,  ..., 0.0009, 0.0009, 0.0009]]),\n",
       " 'vpm.encoder.layers.20.mlp.fc1.scales': tensor([[0.0012, 0.0010, 0.0009,  ..., 0.0011, 0.0010, 0.0011]]),\n",
       " 'llm.model.layers.5.mlp.down_proj.qweight': tensor([[-1751610985, -1751742603,  1436001703,  ..., -2004641431,\n",
       "          -1717069959,  1753844615],\n",
       "         [ 1770613367,  1734761386,  2005502358,  ..., -1501006180,\n",
       "           2004518008,  2006620056],\n",
       "         [-1985309259,  1787267177, -1734769033,  ..., -1737979175,\n",
       "          -1770493590,  2005375577],\n",
       "         ...,\n",
       "         [-2005497467, -1453553004,  1470741364,  ...,   929600919,\n",
       "          -1988793255, -2022012777],\n",
       "         [ -928741207, -1733785688, -1731553130,  ..., -2003342730,\n",
       "           2072410762, -1751606619],\n",
       "         [ 1212774282,  1699326332,  -880523847,  ...,  2005633687,\n",
       "           2071440008, -2020116102]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.26.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.12.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.0.self_attn.out_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.34.mlp.up_proj.scales': tensor([[0.0233, 0.0215, 0.0202,  ..., 0.0268, 0.0253, 0.0241]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.8.self_attn.o_proj.qweight': tensor([[-1968801877, -2019133834, -1450474056,  ...,  1719044473,\n",
       "          -1754827143, -1754686551],\n",
       "         [-2037995448,  1183152521,  1214731926,  ...,   998995592,\n",
       "           1751550357, -1520011643],\n",
       "         [ 1685604776,  -948344181, -1773639781,  ..., -1769374775,\n",
       "           2038852998, -1974958423],\n",
       "         ...,\n",
       "         [-1752676695, -1401313433, -2020496538,  ..., -2035833481,\n",
       "          -1751737401,  -913798521],\n",
       "         [ 1722390648, -2039907992, -2052622406,  ..., -2051123287,\n",
       "           -142231701, -1468540791],\n",
       "         [-1970628219, -1468549209, -2071422325,  ..., -1214662743,\n",
       "           1722309510,  1753839255]], dtype=torch.int32),\n",
       " 'llm.model.layers.30.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.16.mlp.fc2.scales': tensor([[0.0011, 0.0010, 0.0012,  ..., 0.0010, 0.0011, 0.0009]]),\n",
       " 'llm.model.layers.15.mlp.gate_proj.qweight': tensor([[-1686664310, -2022074778,  1180281750,  ..., -1520879706,\n",
       "           -694790747,  2053535656],\n",
       "         [ 1283958873, -1804895301, -1432786551,  ...,  1769633445,\n",
       "          -1970898299, -1972995207],\n",
       "         [ 2002417012, -1466398070,  1988389784,  ...,  -900245622,\n",
       "          -1721275513,  1453894252],\n",
       "         ...,\n",
       "         [-2018977081, -1986688566,  1768589499,  ..., -1805285225,\n",
       "          -1972004762, -1501861740],\n",
       "         [-2035595337,  1232447067, -1968670055,  ...,  1739179638,\n",
       "           1504406183,  2007526540],\n",
       "         [-1185593784, -1768523896, -1985369690,  ..., -1722304393,\n",
       "           1751689092,  1804192390]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.18.mlp.fc2.qweight': tensor([[ 1920694899,  2121887595, -2071094380,  ..., -1633912190,\n",
       "          -1937671602,  1552701293],\n",
       "         [ 1366196642, -1648643923, -1871998562,  ..., -1918668682,\n",
       "           1762105232,  2142087804],\n",
       "         [-2108859810,  1315471977, -1016629898,  ..., -2138662523,\n",
       "          -1886624451, -1536469149],\n",
       "         ...,\n",
       "         [-1381261970,  1871163261, -1802387122,  ..., -1787728021,\n",
       "           1269255837, -1987011431],\n",
       "         [ 1448305527, -1585215840,  1942465362,  ..., -1050034828,\n",
       "          -2087749777, -1973257104],\n",
       "         [-1551725410, -1933471363, -2125170292,  ...,  1986699898,\n",
       "           2023646563, -1936628565]], dtype=torch.int32),\n",
       " 'llm.model.layers.51.self_attn.o_proj.scales': tensor([[0.0246, 0.0273, 0.0259,  ..., 0.0307, 0.0273, 0.0336]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.43.mlp.up_proj.qweight': tensor([[ 2055714677,  2073528759,  1752812132,  ..., -1751536730,\n",
       "           1739159719,  2022217831],\n",
       "         [-1200252314, -1450662490, -1504152200,  ..., -1752601929,\n",
       "           1722389928, -2003330697],\n",
       "         [-1971824295, -1385711465,  2041078120,  ..., -1953928825,\n",
       "          -1734953591, -1969715319],\n",
       "         ...,\n",
       "         [-1416136249, -1702271636, -1702127176,  ...,  2039187080,\n",
       "           -360032649, -1987475048],\n",
       "         [ 2020247205, -2021226569,  -981043764,  ..., -1987491976,\n",
       "           2005292730,  2023327863],\n",
       "         [-1751532952,  2018977416, -2007431317,  ...,  2068081030,\n",
       "           2040837762, -2004317818]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.25.self_attn.v_proj.bias': tensor([ 0.2207,  0.2334,  0.1494,  ..., -0.1455, -0.0071, -0.0165]),\n",
       " 'llm.model.layers.34.self_attn.k_proj.qweight': tensor([[ 1769579896,  1734907979,  1739178889,  ...,  2024249720,\n",
       "          -1463186844, -2023060855],\n",
       "         [-2070181224, -1719236728,  -106657400,  ..., -1956279915,\n",
       "           1666746517, -1231583352],\n",
       "         [-1684302183, -1986294937,  1750820793,  ...,  1804109961,\n",
       "           1989708154, -2021095031],\n",
       "         ...,\n",
       "         [-1467574395, -1753896756, -2070571674,  ..., -1736148826,\n",
       "          -2002422184, -1751611240],\n",
       "         [ 1767213158, -1971926694, -1719302025,  ..., -1735812712,\n",
       "           1787340904, -1720211335],\n",
       "         [-1469416826, -1437042584, -1971749766,  ..., -2025289817,\n",
       "          -1753782904, -1735878535]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.21.self_attn.v_proj.bias': tensor([-0.0442,  0.0267, -0.0062,  ..., -0.0825,  0.0437, -0.0354]),\n",
       " 'vpm.encoder.layers.2.self_attn.q_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.35.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.11.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.23.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.11.self_attn.k_proj.scales': tensor([[0.0151, 0.0190, 0.0211, 0.0204, 0.0178, 0.0190, 0.0177, 0.0259, 0.0201,\n",
       "          0.0264, 0.0216, 0.0201, 0.0195, 0.0240, 0.0215, 0.0249, 0.0236, 0.0223,\n",
       "          0.0229, 0.0264, 0.0300, 0.0245, 0.0302, 0.0568, 0.0281, 0.0273, 0.0560,\n",
       "          0.0283, 0.0326, 0.0242, 0.0302, 0.0197, 0.0201, 0.0174, 0.0160, 0.0158,\n",
       "          0.0211, 0.0185, 0.0201, 0.0208, 0.0174, 0.0169, 0.0223, 0.0213, 0.0203,\n",
       "          0.0232, 0.0176, 0.0216, 0.0240, 0.0169, 0.0242, 0.0257, 0.0344, 0.0236,\n",
       "          0.0257, 0.0236, 0.0249, 0.0253, 0.0602, 0.0288, 0.0230, 0.0359, 0.0315,\n",
       "          0.0292, 0.0176, 0.0195, 0.0159, 0.0194, 0.0204, 0.0180, 0.0161, 0.0184,\n",
       "          0.0163, 0.0136, 0.0184, 0.0178, 0.0184, 0.0210, 0.0172, 0.0212, 0.0242,\n",
       "          0.0240, 0.0217, 0.0250, 0.0276, 0.0244, 0.0223, 0.0352, 0.0352, 0.0242,\n",
       "          0.0263, 0.0242, 0.0213, 0.0272, 0.0341, 0.0266, 0.0169, 0.0163, 0.0164,\n",
       "          0.0191, 0.0181, 0.0185, 0.0156, 0.0162, 0.0164, 0.0184, 0.0180, 0.0158,\n",
       "          0.0194, 0.0217, 0.0206, 0.0180, 0.0266, 0.0253, 0.0238, 0.0298, 0.0293,\n",
       "          0.0445, 0.0300, 0.0255, 0.0693, 0.0237, 0.0246, 0.0227, 0.0247, 0.0294,\n",
       "          0.0229, 0.0292, 0.0241, 0.0217, 0.0229, 0.0306, 0.0298, 0.0326, 0.0241,\n",
       "          0.0359, 0.0250, 0.0309, 0.0327, 0.0297, 0.0272, 0.0296, 0.0204, 0.0271,\n",
       "          0.0288, 0.0283, 0.0318, 0.0297, 0.0271, 0.0247, 0.0234, 0.0314, 0.0453,\n",
       "          0.0298, 0.0414, 0.0314, 0.0271, 0.0254, 0.0326, 0.0438, 0.0229, 0.0229,\n",
       "          0.0309, 0.0277, 0.0217, 0.0246, 0.0190, 0.0225, 0.0193, 0.0251, 0.0401,\n",
       "          0.0195, 0.0255, 0.0273, 0.0311, 0.0305, 0.0357, 0.0264, 0.0195, 0.0230,\n",
       "          0.0267, 0.0289, 0.0263, 0.0286, 0.0331, 0.0309, 0.0272, 0.0254, 0.0290,\n",
       "          0.0289, 0.0285, 0.0354, 0.0165, 0.0176, 0.0195, 0.0193, 0.0152, 0.0215,\n",
       "          0.0185, 0.0216, 0.0216, 0.0168, 0.0198, 0.0153, 0.0215, 0.0201, 0.0237,\n",
       "          0.0236, 0.0227, 0.0212, 0.0284, 0.0228, 0.0220, 0.0365, 0.0268, 0.0326,\n",
       "          0.0245, 0.0542, 0.0260, 0.0269, 0.0372, 0.0279, 0.0319, 0.0450, 0.0193,\n",
       "          0.0171, 0.0198, 0.0297, 0.0188, 0.0213, 0.0212, 0.0185, 0.0207, 0.0191,\n",
       "          0.0188, 0.0251, 0.0195, 0.0178, 0.0198, 0.0217, 0.0220, 0.0191, 0.0262,\n",
       "          0.0275, 0.0365, 0.0301, 0.0430, 0.0306, 0.0272, 0.0745, 0.0477, 0.0296,\n",
       "          0.0279, 0.0375, 0.0568, 0.0288, 0.0165, 0.0155, 0.0169, 0.0156, 0.0173,\n",
       "          0.0184, 0.0139, 0.0159, 0.0150, 0.0178, 0.0221, 0.0171, 0.0174, 0.0161,\n",
       "          0.0208, 0.0199, 0.0181, 0.0232, 0.0236, 0.0296, 0.0555, 0.0189, 0.0249,\n",
       "          0.0246, 0.0461, 0.0578, 0.0207, 0.0267, 0.0306, 0.0253, 0.0273, 0.0315,\n",
       "          0.0206, 0.0168, 0.0189, 0.0164, 0.0176, 0.0172, 0.0201, 0.0197, 0.0216,\n",
       "          0.0156, 0.0176, 0.0193, 0.0213, 0.0193, 0.0303, 0.0142, 0.0262, 0.0186,\n",
       "          0.0227, 0.0236, 0.0372, 0.0228, 0.0305, 0.0630, 0.0232, 0.0682, 0.0242,\n",
       "          0.0258, 0.0288, 0.0233, 0.0765, 0.0383, 0.0198, 0.0167, 0.0193, 0.0169,\n",
       "          0.0198, 0.0223, 0.0215, 0.0180, 0.0237, 0.0178, 0.0207, 0.0223, 0.0208,\n",
       "          0.0204, 0.0285, 0.0159, 0.0217, 0.0236, 0.0296, 0.0271, 0.0272, 0.0275,\n",
       "          0.0280, 0.0273, 0.0385, 0.0289, 0.0290, 0.0305, 0.0247, 0.0404, 0.0207,\n",
       "          0.0523, 0.0154, 0.0199, 0.0190, 0.0219, 0.0199, 0.0155, 0.0242, 0.0141,\n",
       "          0.0219, 0.0207, 0.0207, 0.0199, 0.0250, 0.0193, 0.0228, 0.0237, 0.0228,\n",
       "          0.0281, 0.0251, 0.0385, 0.0268, 0.0242, 0.0258, 0.0229, 0.0672, 0.0302,\n",
       "          0.0268, 0.0227, 0.0331, 0.0288, 0.0296, 0.0276, 0.0126, 0.0221, 0.0217,\n",
       "          0.0171, 0.0306, 0.0269, 0.0250, 0.0276, 0.0181, 0.0220, 0.0232, 0.0233,\n",
       "          0.0177, 0.0229, 0.0290, 0.0194, 0.0262, 0.0221, 0.0220, 0.0195, 0.0220,\n",
       "          0.0234, 0.0206, 0.0221, 0.0227, 0.0448, 0.0259, 0.0253, 0.0380, 0.0203,\n",
       "          0.0292, 0.0221, 0.0393, 0.0148, 0.0181, 0.0171, 0.0221, 0.0297, 0.0301,\n",
       "          0.0275, 0.0181, 0.0262, 0.0279, 0.0336, 0.0259, 0.0247, 0.0251, 0.0258,\n",
       "          0.0232, 0.0307, 0.0228, 0.0190, 0.0253, 0.0241, 0.0259, 0.0238, 0.0223,\n",
       "          0.0477, 0.0241, 0.0332, 0.0544, 0.0213, 0.0273, 0.0212, 0.0136, 0.0144,\n",
       "          0.0121, 0.0116, 0.0143, 0.0141, 0.0154, 0.0109, 0.0143, 0.0137, 0.0158,\n",
       "          0.0174, 0.0145, 0.0151, 0.0120, 0.0174, 0.0220, 0.0178, 0.0217, 0.0290,\n",
       "          0.0259, 0.0228, 0.0290, 0.0365, 0.0264, 0.0391, 0.0336, 0.0328, 0.0414,\n",
       "          0.0393, 0.0341, 0.0307, 0.0126, 0.0150, 0.0118, 0.0118, 0.0162, 0.0141,\n",
       "          0.0158, 0.0140, 0.0150, 0.0148, 0.0125, 0.0137, 0.0113, 0.0188, 0.0145,\n",
       "          0.0182, 0.0176, 0.0164, 0.0185, 0.0219, 0.0393, 0.0456, 0.0267, 0.0301,\n",
       "          0.0396, 0.0362, 0.0349, 0.0349, 0.0435, 0.0372, 0.0344, 0.0362]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.32.mlp.gate_proj.qweight': tensor([[-1804954523, -1751677530,  1817539784,  ..., -1736149368,\n",
       "          -1735801477, -1437172584],\n",
       "         [-1416067977,  1700236393,  1449884858,  ..., -2021161063,\n",
       "          -1970837625, -1987532619],\n",
       "         [-1734892164,  1717982842, -1168668314,  ..., -1734961014,\n",
       "          -1736730248, -2054568088],\n",
       "         ...,\n",
       "         [ 1235925129, -1686407033,  2008508042,  ..., -1987544953,\n",
       "           1523153014, -1750697110],\n",
       "         [ 2023188025, -2004248731,  -932742744,  ...,  2022213514,\n",
       "           2055703675,   965324234],\n",
       "         [-1499101579, -2022008168,  1791588587,  ...,  1500022680,\n",
       "          -1731888219, -1970914633]], dtype=torch.int32),\n",
       " 'llm.model.layers.2.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.2.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.0.self_attn.v_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.13.mlp.down_proj.scales': tensor([[0.0263, 0.0316, 0.0288,  ..., 0.0242, 0.0268, 0.0477]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.24.self_attn.k_proj.qweight': tensor([[ 1213622092, -1450604394,  1446614390,  ...,  2023253621,\n",
       "           1163435626, -1732930678],\n",
       "         [ 1789622679, -1215915624, -2061072744,  ..., -1441433241,\n",
       "          -2024244811, -1988708232],\n",
       "         [ 1990965191, -1434093674,  1856523398,  ..., -1181046439,\n",
       "          -2004310407, -1215915897],\n",
       "         ...,\n",
       "         [  910711461,  2039121798, -1435003478,  ..., -1968481671,\n",
       "           1204119367,   931624856],\n",
       "         [-2020903592,  1970555290,  2022210186,  ..., -1970706244,\n",
       "           -311060087,  1418337144],\n",
       "         [ 2041202517,  1719175547, -2020243096,  ...,  2023197319,\n",
       "           1737004710,  1770747815]], dtype=torch.int32),\n",
       " 'llm.model.layers.25.self_attn.q_proj.qweight': tensor([[-2015708998,  -963277399, -1483297707,  ..., -2052895385,\n",
       "           2020108117, -1200961592],\n",
       "         [ 2008779495,  1752541084, -1136027320,  ..., -1450805401,\n",
       "           1752856183, -1750477707],\n",
       "         [-1514653556, -1419475307,  2056960181,  ..., -1721084486,\n",
       "          -1752717173, -1466398487],\n",
       "         ...,\n",
       "         [ 1732949879, -1951688807, -2004253016,  ..., -1701279623,\n",
       "          -1198872375, -1715901014],\n",
       "         [ -697012665, -2002929575,  2021291688,  ...,  1769354949,\n",
       "          -2004375642, -1517713046],\n",
       "         [-1482135893,  1489815975, -1718121834,  ..., -1179034742,\n",
       "           1466472359, -2006743369]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.11.layer_norm1.weight': tensor([0.7344, 0.7383, 0.8359,  ..., 0.5742, 0.6758, 0.6836]),\n",
       " 'vpm.encoder.layers.10.mlp.fc2.scales': tensor([[0.0010, 0.0009, 0.0010,  ..., 0.0008, 0.0013, 0.0009]]),\n",
       " 'llm.model.layers.6.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.9.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.17.mlp.fc1.scales': tensor([[0.0010, 0.0003, 0.0011,  ..., 0.0010, 0.0019, 0.0008]]),\n",
       " 'llm.model.layers.31.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.lm_head.weight': tensor([[ 0.0125,  0.0469, -0.0106,  ..., -0.0225,  0.0378, -0.0454],\n",
       "         [ 0.0532,  0.0019, -0.0305,  ...,  0.0292, -0.0267, -0.0225],\n",
       "         [ 0.0233,  0.0530, -0.0454,  ...,  0.0383,  0.0009,  0.0231],\n",
       "         ...,\n",
       "         [-0.0022,  0.0151, -0.0479,  ...,  0.0669,  0.0066,  0.0310],\n",
       "         [-0.0425, -0.0179,  0.0162,  ...,  0.0072,  0.0569, -0.0281],\n",
       "         [-0.0039, -0.0140,  0.1050,  ..., -0.0167, -0.0197, -0.0420]]),\n",
       " 'llm.model.layers.27.self_attn.k_proj.qweight': tensor([[ 1738114711,  1755875003,  2022218072,  ...,  2040371849,\n",
       "          -1701275992,  -947620728],\n",
       "         [-1636341576,   946167205,  1804110519,  ..., -1188714060,\n",
       "          -1550354600,  2057795176],\n",
       "         [-1751414902,   966493287,  2136504488,  ..., -2002089813,\n",
       "          -1719108476,  2019249034],\n",
       "         ...,\n",
       "         [ 1753913721, -2073528474,  1470392999,  ...,  1212901817,\n",
       "          -1987607211, -1957341847],\n",
       "         [-1468429944, -2037745526,  1200068501,  ..., -1702073767,\n",
       "          -2004195159, -1987815557],\n",
       "         [-1969710729,  -659400522, -1522894424,  ..., -1735946583,\n",
       "          -1465157750,  1700096650]], dtype=torch.int32),\n",
       " 'llm.model.layers.28.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.16.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.22.layer_norm2.bias': tensor([ 1.4766,  2.0156, -0.9883,  ..., -0.6914,  5.3750,  1.2734]),\n",
       " 'llm.model.layers.6.self_attn.k_proj.scales': tensor([[0.0157, 0.0159, 0.0172, 0.0158, 0.0176, 0.0176, 0.0145, 0.0157, 0.0150,\n",
       "          0.0152, 0.0173, 0.0159, 0.0167, 0.0152, 0.0123, 0.0143, 0.0185, 0.0229,\n",
       "          0.0401, 0.0383, 0.0372, 0.0557, 0.0552, 0.0513, 0.0581, 0.0469, 0.0479,\n",
       "          0.0445, 0.0550, 0.0414, 0.0508, 0.0495, 0.0143, 0.0171, 0.0152, 0.0148,\n",
       "          0.0161, 0.0173, 0.0152, 0.0147, 0.0156, 0.0157, 0.0152, 0.0151, 0.0176,\n",
       "          0.0140, 0.0145, 0.0174, 0.0157, 0.0186, 0.0202, 0.0411, 0.0477, 0.0367,\n",
       "          0.0526, 0.0576, 0.0505, 0.0482, 0.0484, 0.0523, 0.0469, 0.0570, 0.0474,\n",
       "          0.0419, 0.0172, 0.0148, 0.0161, 0.0171, 0.0161, 0.0178, 0.0147, 0.0172,\n",
       "          0.0201, 0.0215, 0.0173, 0.0199, 0.0163, 0.0244, 0.0204, 0.0208, 0.0208,\n",
       "          0.0178, 0.0228, 0.0228, 0.0380, 0.0482, 0.0484, 0.0225, 0.0349, 0.0521,\n",
       "          0.0289, 0.0286, 0.0310, 0.0258, 0.0344, 0.0237, 0.0128, 0.0139, 0.0181,\n",
       "          0.0188, 0.0144, 0.0190, 0.0189, 0.0211, 0.0143, 0.0195, 0.0241, 0.0208,\n",
       "          0.0184, 0.0181, 0.0245, 0.0208, 0.0182, 0.0240, 0.0255, 0.0215, 0.0258,\n",
       "          0.0292, 0.0232, 0.0341, 0.0269, 0.0573, 0.0215, 0.0229, 0.0318, 0.0259,\n",
       "          0.0280, 0.0461, 0.0186, 0.0297, 0.0230, 0.0314, 0.0233, 0.0210, 0.0195,\n",
       "          0.0210, 0.0240, 0.0238, 0.0236, 0.0220, 0.0228, 0.0284, 0.0242, 0.0310,\n",
       "          0.0255, 0.0234, 0.0259, 0.0244, 0.0312, 0.0272, 0.0284, 0.0305, 0.0490,\n",
       "          0.0267, 0.0318, 0.0305, 0.0259, 0.0409, 0.0391, 0.0275, 0.0292, 0.0172,\n",
       "          0.0210, 0.0238, 0.0199, 0.0220, 0.0212, 0.0260, 0.0216, 0.0237, 0.0201,\n",
       "          0.0220, 0.0266, 0.0277, 0.0229, 0.0251, 0.0247, 0.0268, 0.0283, 0.0268,\n",
       "          0.0324, 0.0349, 0.0268, 0.0224, 0.0539, 0.0323, 0.0241, 0.0281, 0.0255,\n",
       "          0.0591, 0.0555, 0.0272, 0.0190, 0.0199, 0.0177, 0.0378, 0.0223, 0.0233,\n",
       "          0.0249, 0.0190, 0.0289, 0.0173, 0.0301, 0.0247, 0.0156, 0.0230, 0.0385,\n",
       "          0.0283, 0.0208, 0.0216, 0.0216, 0.0344, 0.0367, 0.0326, 0.0290, 0.0284,\n",
       "          0.0513, 0.0262, 0.0362, 0.0302, 0.0285, 0.0255, 0.0259, 0.0380, 0.0253,\n",
       "          0.0172, 0.0202, 0.0195, 0.0289, 0.0370, 0.0233, 0.0281, 0.0293, 0.0182,\n",
       "          0.0404, 0.0285, 0.0237, 0.0301, 0.0195, 0.0212, 0.0207, 0.0277, 0.0251,\n",
       "          0.0322, 0.0469, 0.0217, 0.0260, 0.0435, 0.0635, 0.0296, 0.0250, 0.0268,\n",
       "          0.0223, 0.0281, 0.0271, 0.0232, 0.0158, 0.0202, 0.0190, 0.0238, 0.0190,\n",
       "          0.0194, 0.0189, 0.0189, 0.0232, 0.0212, 0.0177, 0.0210, 0.0210, 0.0195,\n",
       "          0.0190, 0.0217, 0.0193, 0.0177, 0.0230, 0.0505, 0.0201, 0.0221, 0.0409,\n",
       "          0.0531, 0.0297, 0.0555, 0.0279, 0.0260, 0.0306, 0.0292, 0.0385, 0.0578,\n",
       "          0.0166, 0.0210, 0.0230, 0.0204, 0.0206, 0.0188, 0.0189, 0.0173, 0.0198,\n",
       "          0.0188, 0.0220, 0.0230, 0.0194, 0.0147, 0.0240, 0.0181, 0.0233, 0.0232,\n",
       "          0.0204, 0.0251, 0.0232, 0.0227, 0.0349, 0.0211, 0.0264, 0.0482, 0.0300,\n",
       "          0.0273, 0.0432, 0.0272, 0.0234, 0.0267, 0.0144, 0.0182, 0.0212, 0.0293,\n",
       "          0.0271, 0.0320, 0.0182, 0.0185, 0.0234, 0.0223, 0.0211, 0.0230, 0.0172,\n",
       "          0.0224, 0.0273, 0.0215, 0.0219, 0.0240, 0.0213, 0.0262, 0.0213, 0.0277,\n",
       "          0.0246, 0.0247, 0.0264, 0.0344, 0.0314, 0.0199, 0.0453, 0.0258, 0.0422,\n",
       "          0.0238, 0.0233, 0.0188, 0.0233, 0.0135, 0.0180, 0.0189, 0.0285, 0.0258,\n",
       "          0.0279, 0.0217, 0.0290, 0.0298, 0.0273, 0.0224, 0.0232, 0.0203, 0.0281,\n",
       "          0.0215, 0.0208, 0.0236, 0.0217, 0.0228, 0.0217, 0.0220, 0.0223, 0.0823,\n",
       "          0.0223, 0.0188, 0.0224, 0.0229, 0.0352, 0.0244, 0.0168, 0.0124, 0.0181,\n",
       "          0.0151, 0.0336, 0.0203, 0.0233, 0.0206, 0.0259, 0.0164, 0.0255, 0.0199,\n",
       "          0.0185, 0.0341, 0.0229, 0.0370, 0.0213, 0.0193, 0.0186, 0.0186, 0.0202,\n",
       "          0.0202, 0.0217, 0.0167, 0.0327, 0.0258, 0.0266, 0.0238, 0.0220, 0.0544,\n",
       "          0.1073, 0.0204, 0.0148, 0.0151, 0.0254, 0.0116, 0.0215, 0.0208, 0.0277,\n",
       "          0.0249, 0.0267, 0.0199, 0.0210, 0.0212, 0.0203, 0.0245, 0.0181, 0.0198,\n",
       "          0.0152, 0.0341, 0.0223, 0.0159, 0.0201, 0.0164, 0.0163, 0.0188, 0.0781,\n",
       "          0.0185, 0.0208, 0.0542, 0.0311, 0.0215, 0.0307, 0.0211, 0.0124, 0.0126,\n",
       "          0.0098, 0.0106, 0.0115, 0.0117, 0.0117, 0.0138, 0.0139, 0.0129, 0.0184,\n",
       "          0.0172, 0.0238, 0.0216, 0.0279, 0.0289, 0.0255, 0.0359, 0.0365, 0.0315,\n",
       "          0.0275, 0.0249, 0.0286, 0.0336, 0.0404, 0.0294, 0.0469, 0.0288, 0.0365,\n",
       "          0.0300, 0.0435, 0.0271, 0.0128, 0.0122, 0.0107, 0.0113, 0.0109, 0.0111,\n",
       "          0.0134, 0.0129, 0.0144, 0.0168, 0.0185, 0.0203, 0.0280, 0.0254, 0.0237,\n",
       "          0.0251, 0.0306, 0.0283, 0.0217, 0.0370, 0.0332, 0.0292, 0.0346, 0.0260,\n",
       "          0.0497, 0.0285, 0.0341, 0.0276, 0.0305, 0.0319, 0.0279, 0.0213]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.22.self_attn.q_proj.bias': tensor([ 0.0549, -0.0825, -0.0417,  ..., -0.1094,  0.1904, -0.0090]),\n",
       " 'llm.model.layers.31.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.6.self_attn.k_proj.scales': tensor([[0.0009, 0.0013, 0.0013,  ..., 0.0008, 0.0006, 0.0007]]),\n",
       " 'llm.model.layers.45.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.0.mlp.fc1.qweight': tensor([[-1733735543, -1925029755,  2133031531,  ...,  1901772150,\n",
       "           1985718669, -1822510519],\n",
       "         [  494576702,  1418631298, -1854767715,  ..., -1721474914,\n",
       "          -1975613341, -1890417802],\n",
       "         [-2143067500, -1803904133, -2072546166,  ..., -2106820472,\n",
       "          -1869904770,  1848338541],\n",
       "         ...,\n",
       "         [ -477916285, -1853916776, -1333104504,  ..., -1469219179,\n",
       "          -2040824408, -1551145593],\n",
       "         [-1986295680, -1838589553,  1732137649,  ...,  1770469254,\n",
       "           1448949950,  1936346550],\n",
       "         [-1503046039, -1468688278, -2104129147,  ..., -1872011157,\n",
       "          -2139324017, -1955634309]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.14.self_attn.k_proj.bias': tensor([ 0.7852,  0.1104,  0.1758,  ..., -0.1484, -0.8789, -0.3086]),\n",
       " 'llm.model.layers.0.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.21.self_attn.o_proj.scales': tensor([[0.0219, 0.0217, 0.0254,  ..., 0.0217, 0.0206, 0.0143]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.28.mlp.gate_proj.qweight': tensor([[-1497983114, -2048350872,  2023258790,  ..., -1368884844,\n",
       "          -2018994297,  2037156714],\n",
       "         [ 1485420199,  1990895243, -1720215383,  ..., -1235760780,\n",
       "          -1702332536,  1787271516],\n",
       "         [-1466525846,  2089461433,  2037012887,  ..., -1413982365,\n",
       "           1753782438, -1549439834],\n",
       "         ...,\n",
       "         [-1115203482, -1955047865, -2020132952,  ..., -2070050375,\n",
       "          -1986356568, -1500996696],\n",
       "         [ 2057673078,  1755928966, -1717855353,  ..., -1698187720,\n",
       "          -1987532408,  1487374488],\n",
       "         [  932542570,  1770558571,  1769576841,  ...,  2073445608,\n",
       "          -2006350185,  1515763111]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.22.self_attn.k_proj.scales': tensor([[0.0009, 0.0008, 0.0008,  ..., 0.0008, 0.0006, 0.0007]]),\n",
       " 'vpm.encoder.layers.24.self_attn.q_proj.bias': tensor([-0.1768, -0.0187,  0.0245,  ...,  0.0140,  0.0557,  0.1523]),\n",
       " 'llm.model.layers.4.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.23.mlp.fc1.qzeros': tensor([[2139062143, 2139062143, 2139062143,  ..., 2139062143, 2139062143,\n",
       "          2139062143]], dtype=torch.int32),\n",
       " 'llm.model.layers.0.mlp.down_proj.scales': tensor([[0.0194, 0.0208, 0.0233,  ..., 0.0180, 0.0215, 0.0430]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.15.mlp.fc1.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.4.mlp.fc1.qzeros': tensor([[2139062143, 2139062143, 2139062143,  ..., 2139062143, 2139062143,\n",
       "          2139062143]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.26.layer_norm1.bias': tensor([-0.1191,  0.0767, -0.0381,  ...,  0.1533, -0.0859, -0.0388]),\n",
       " 'llm.model.layers.13.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.18.self_attn.out_proj.bias': tensor([ 0.0062, -0.2832,  0.1069,  ...,  0.2139,  0.0503, -0.0500]),\n",
       " 'vpm.encoder.layers.10.self_attn.k_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.29.self_attn.q_proj.scales': tensor([[0.0266, 0.0253, 0.0198,  ..., 0.0242, 0.0259, 0.0267]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.2.self_attn.out_proj.qweight': tensor([[-2009592446,  1719700337, -1690315415,  ..., -1887262281,\n",
       "            443587774, -2103872465],\n",
       "         [-1349883203, -2056493006, -1822182239,  ..., -2090561901,\n",
       "           1791784014,  1836602780],\n",
       "         [  845858436, -1602203522, -2120188064,  ..., -1809540761,\n",
       "          -1855221874, -1751896482],\n",
       "         ...,\n",
       "         [ 1769499532,  1801611131, -1401254813,  ...,  1555203761,\n",
       "           1216648577, -1920956785],\n",
       "         [ 2140117113, -1706452605,  2070636137,  ..., -1952158342,\n",
       "          -1966963081,  2055317347],\n",
       "         [-1935767410,  2073393837, -1399478390,  ...,  1872393592,\n",
       "           2006736847, -1991394971]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.20.mlp.fc2.qweight': tensor([[ 2104979042,  1586990910, -1769503574,  ...,  1066962574,\n",
       "           1952879218, -1565954442],\n",
       "         [-2040042939, -1801933420, -2139325841,  ..., -2036820902,\n",
       "          -1972339008, -1665825935],\n",
       "         [-2105907001,  1686917291, -1852680341,  ...,  2006296728,\n",
       "          -1569287766, -2073585070],\n",
       "         ...,\n",
       "         [ 1319989356, -1552181355, -1806468443,  ..., -2121892686,\n",
       "           2087039279, -2120653201],\n",
       "         [ 1652521597,  1365676455,  2108200293,  ..., -1568041062,\n",
       "           2141027989,  1604807022],\n",
       "         [ 1753183615,  2070889344,  1149985665,  ..., -2120579201,\n",
       "           1908497534, -1970316672]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.20.mlp.fc1.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.18.mlp.down_proj.qweight': tensor([[-1971820933,  2039327354,  1437047464,  ..., -1466677643,\n",
       "           1770564712,  1720161912],\n",
       "         [-1733850966, -1651202440,  2024377463,  ...,  1503366779,\n",
       "          -1936238938,  1785239960],\n",
       "         [-2005292436, -2037938044,  2009426567,  ..., -1968618373,\n",
       "          -1719031959, -1987598201],\n",
       "         ...,\n",
       "         [ 1520080777,  1448781960, -1402506392,  ...,  1701550183,\n",
       "          -2040095641, -1784051356],\n",
       "         [-2036893593,  2054653800, -2018940036,  ..., -2019177802,\n",
       "           1755678810,  1754838663],\n",
       "         [ -694572966,  1219929898, -1769563750,  ...,  1485269622,\n",
       "          -1521190841, -1198946422]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.22.mlp.fc2.qweight': tensor([[-1819979384, -1637888916, -1637197174,  ...,  1451655997,\n",
       "          -1886142497,  2003725389],\n",
       "         [-1803585174,  1919059607, -2122743661,  ...,  1921870218,\n",
       "           2090377084,  1270263171],\n",
       "         [ 2137042270, -1804506763,  2084859026,  ..., -1769505952,\n",
       "           1788757861,  1956554380],\n",
       "         ...,\n",
       "         [ 2021681783, -1954508670,  1669365920,  ..., -1065329527,\n",
       "           1635820338, -1501529456],\n",
       "         [ 1700152961, -1300394848, -1704355503,  ...,  1460306600,\n",
       "          -1689280096, -1984476504],\n",
       "         [ 1704095877, -2138419332,  1920820585,  ..., -1181184174,\n",
       "            999528617, -2036682862]], dtype=torch.int32),\n",
       " 'llm.model.layers.21.self_attn.q_proj.scales': tensor([[0.0298, 0.0263, 0.0202,  ..., 0.0171, 0.0267, 0.0327]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.19.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.47.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.3.mlp.up_proj.scales': tensor([[0.0220, 0.0285, 0.0260,  ..., 0.0245, 0.0202, 0.0230]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.18.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.47.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.12.post_attention_layernorm.weight': tensor([1.3047, 1.1641, 0.9336,  ..., 0.9375, 1.1875, 0.2070]),\n",
       " 'vpm.encoder.layers.20.layer_norm2.bias': tensor([-2.3750, -2.2500,  1.1406,  ...,  1.4688,  0.0150, -1.4688]),\n",
       " 'llm.model.layers.21.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.23.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.9.self_attn.k_proj.scales': tensor([[0.0298, 0.0204, 0.0161, 0.0290, 0.0230, 0.0206, 0.0246, 0.0262, 0.0154,\n",
       "          0.0244, 0.0284, 0.0177, 0.0263, 0.0194, 0.0283, 0.0198, 0.0217, 0.0262,\n",
       "          0.0237, 0.0247, 0.0268, 0.0284, 0.0221, 0.0249, 0.0262, 0.0594, 0.0263,\n",
       "          0.0383, 0.0285, 0.0288, 0.0269, 0.0238, 0.0201, 0.0169, 0.0221, 0.0241,\n",
       "          0.0312, 0.0250, 0.0216, 0.0236, 0.0210, 0.0289, 0.0257, 0.0255, 0.0241,\n",
       "          0.0302, 0.0251, 0.0311, 0.0346, 0.0259, 0.0211, 0.0372, 0.0277, 0.0267,\n",
       "          0.0199, 0.0263, 0.0280, 0.0409, 0.0306, 0.0273, 0.0357, 0.0277, 0.0215,\n",
       "          0.0448, 0.0323, 0.0184, 0.0165, 0.0171, 0.0225, 0.0206, 0.0206, 0.0148,\n",
       "          0.0188, 0.0144, 0.0199, 0.0230, 0.0240, 0.0258, 0.0206, 0.0224, 0.0259,\n",
       "          0.0241, 0.0242, 0.0242, 0.0257, 0.0257, 0.0249, 0.0247, 0.0279, 0.0622,\n",
       "          0.0245, 0.0271, 0.0224, 0.0249, 0.0311, 0.0281, 0.0166, 0.0194, 0.0176,\n",
       "          0.0194, 0.0155, 0.0206, 0.0163, 0.0185, 0.0219, 0.0221, 0.0216, 0.0215,\n",
       "          0.0267, 0.0203, 0.0254, 0.0245, 0.0229, 0.0259, 0.0217, 0.0280, 0.0378,\n",
       "          0.0264, 0.0260, 0.0259, 0.0285, 0.0505, 0.0236, 0.0292, 0.0303, 0.0244,\n",
       "          0.0300, 0.0300, 0.0148, 0.0189, 0.0167, 0.0241, 0.0212, 0.0224, 0.0146,\n",
       "          0.0229, 0.0212, 0.0280, 0.0145, 0.0178, 0.0165, 0.0230, 0.0191, 0.0153,\n",
       "          0.0216, 0.0224, 0.0257, 0.0244, 0.0292, 0.0259, 0.0254, 0.0227, 0.0234,\n",
       "          0.0521, 0.0275, 0.0286, 0.0208, 0.0241, 0.0220, 0.0289, 0.0140, 0.0215,\n",
       "          0.0172, 0.0139, 0.0165, 0.0260, 0.0159, 0.0176, 0.0190, 0.0210, 0.0164,\n",
       "          0.0227, 0.0145, 0.0174, 0.0189, 0.0156, 0.0259, 0.0216, 0.0217, 0.0244,\n",
       "          0.0233, 0.0220, 0.0357, 0.0263, 0.0300, 0.0461, 0.0307, 0.0320, 0.0215,\n",
       "          0.0309, 0.0215, 0.0285, 0.0174, 0.0144, 0.0156, 0.0195, 0.0142, 0.0177,\n",
       "          0.0156, 0.0149, 0.0191, 0.0163, 0.0206, 0.0202, 0.0177, 0.0167, 0.0189,\n",
       "          0.0172, 0.0227, 0.0208, 0.0228, 0.0344, 0.0260, 0.0253, 0.0258, 0.0307,\n",
       "          0.0492, 0.0279, 0.0385, 0.0294, 0.0283, 0.0272, 0.0471, 0.0393, 0.0198,\n",
       "          0.0171, 0.0193, 0.0171, 0.0191, 0.0225, 0.0182, 0.0181, 0.0195, 0.0158,\n",
       "          0.0195, 0.0172, 0.0178, 0.0188, 0.0208, 0.0227, 0.0201, 0.0220, 0.0267,\n",
       "          0.0344, 0.0277, 0.0297, 0.0300, 0.0288, 0.0703, 0.0254, 0.0292, 0.0266,\n",
       "          0.0290, 0.0329, 0.0391, 0.0272, 0.0174, 0.0173, 0.0197, 0.0172, 0.0213,\n",
       "          0.0152, 0.0237, 0.0197, 0.0185, 0.0273, 0.0195, 0.0215, 0.0198, 0.0272,\n",
       "          0.0232, 0.0171, 0.0230, 0.0207, 0.0232, 0.0219, 0.0275, 0.0275, 0.0450,\n",
       "          0.0322, 0.0260, 0.0635, 0.0236, 0.0388, 0.0280, 0.0302, 0.0281, 0.0296,\n",
       "          0.0167, 0.0215, 0.0173, 0.0185, 0.0212, 0.0178, 0.0198, 0.0177, 0.0180,\n",
       "          0.0212, 0.0189, 0.0262, 0.0250, 0.0258, 0.0245, 0.0224, 0.0257, 0.0259,\n",
       "          0.0310, 0.0268, 0.0240, 0.0306, 0.0311, 0.0294, 0.0253, 0.0568, 0.0283,\n",
       "          0.0305, 0.0277, 0.0289, 0.0285, 0.0254, 0.0159, 0.0168, 0.0180, 0.0159,\n",
       "          0.0168, 0.0174, 0.0162, 0.0154, 0.0171, 0.0184, 0.0178, 0.0171, 0.0172,\n",
       "          0.0224, 0.0230, 0.0167, 0.0211, 0.0191, 0.0208, 0.0225, 0.0225, 0.0320,\n",
       "          0.0464, 0.0249, 0.0224, 0.0570, 0.0236, 0.0268, 0.0262, 0.0279, 0.0260,\n",
       "          0.0286, 0.0190, 0.0167, 0.0145, 0.0159, 0.0176, 0.0176, 0.0172, 0.0159,\n",
       "          0.0146, 0.0204, 0.0208, 0.0216, 0.0198, 0.0163, 0.0213, 0.0181, 0.0210,\n",
       "          0.0199, 0.0204, 0.0255, 0.0250, 0.0445, 0.0267, 0.0208, 0.0275, 0.0479,\n",
       "          0.0232, 0.0237, 0.0259, 0.0693, 0.0250, 0.0215, 0.0150, 0.0181, 0.0167,\n",
       "          0.0182, 0.0173, 0.0190, 0.0202, 0.0190, 0.0298, 0.0201, 0.0238, 0.0227,\n",
       "          0.0241, 0.0221, 0.0212, 0.0259, 0.0237, 0.0258, 0.0238, 0.0259, 0.0249,\n",
       "          0.0284, 0.0346, 0.0312, 0.0273, 0.0497, 0.0288, 0.0286, 0.0273, 0.0233,\n",
       "          0.0393, 0.0372, 0.0194, 0.0206, 0.0178, 0.0180, 0.0135, 0.0194, 0.0213,\n",
       "          0.0206, 0.0272, 0.0229, 0.0244, 0.0206, 0.0217, 0.0215, 0.0219, 0.0215,\n",
       "          0.0221, 0.0280, 0.0229, 0.0234, 0.0284, 0.0271, 0.0310, 0.0246, 0.0280,\n",
       "          0.0607, 0.0346, 0.0286, 0.0286, 0.0264, 0.0391, 0.0427, 0.0327, 0.0263,\n",
       "          0.0286, 0.0303, 0.0156, 0.0349, 0.0168, 0.0306, 0.0228, 0.0163, 0.0212,\n",
       "          0.0180, 0.0195, 0.0169, 0.0247, 0.0215, 0.0202, 0.0185, 0.0229, 0.0232,\n",
       "          0.0208, 0.0236, 0.0279, 0.0289, 0.0285, 0.0227, 0.0233, 0.0286, 0.0296,\n",
       "          0.0258, 0.0229, 0.0245, 0.0277, 0.0257, 0.0307, 0.0289, 0.0246, 0.0267,\n",
       "          0.0202, 0.0215, 0.0217, 0.0216, 0.0246, 0.0225, 0.0302, 0.0211, 0.0180,\n",
       "          0.0191, 0.0225, 0.0203, 0.0237, 0.0250, 0.0220, 0.0215, 0.0253, 0.0240,\n",
       "          0.0251, 0.0430, 0.0255, 0.0247, 0.0305, 0.0224, 0.0323, 0.0349]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.7.mlp.fc1.bias': tensor([-1.5078, -2.5625, -1.6172,  ..., -0.9531, -0.7188, -0.9531]),\n",
       " 'llm.model.layers.40.mlp.up_proj.scales': tensor([[0.0309, 0.0199, 0.0207,  ..., 0.0262, 0.0217, 0.0264]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.43.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.10.layer_norm1.weight': tensor([0.6992, 0.7969, 0.8672,  ..., 0.5234, 0.6992, 0.7188]),\n",
       " 'vpm.encoder.layers.7.self_attn.out_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.42.mlp.gate_proj.scales': tensor([[0.0228, 0.0181, 0.0229,  ..., 0.0202, 0.0245, 0.0223]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.7.mlp.up_proj.qweight': tensor([[-1183151736, -2037817752, -2053666185,  ..., -1702463833,\n",
       "           1976203704,   678868968],\n",
       "         [ 1505331093,  2006419623, -1448428886,  ..., -1984198522,\n",
       "           -608663672,  2054584455],\n",
       "         [ 1484158839,  1230665112, -2021235100,  ..., -1752599193,\n",
       "          -2036951689, -1920435853],\n",
       "         ...,\n",
       "         [ 1168541834, -2027337850,  2036189383,  ..., -1668773783,\n",
       "          -1950897351,  2006436233],\n",
       "         [ 2005177977, -1752602456, -1735882874,  ..., -1467438917,\n",
       "          -2037811576, -1752712855],\n",
       "         [ 1740105596,  2076673430, -2089134950,  ...,  1973921880,\n",
       "          -2054530442, -1230477402]], dtype=torch.int32),\n",
       " 'llm.model.layers.31.self_attn.o_proj.scales': tensor([[0.0306, 0.0236, 0.0198,  ..., 0.0189, 0.0228, 0.0160]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.22.self_attn.k_proj.bias': tensor([ 0.5625, -0.5430, -0.4609,  ..., -0.0854, -1.0156, -0.1157]),\n",
       " 'llm.model.layers.32.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.28.mlp.gate_proj.scales': tensor([[0.0300, 0.0206, 0.0399,  ..., 0.0250, 0.0388, 0.0229]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.29.input_layernorm.weight': tensor([1.8828, 1.8125, 0.9219,  ..., 1.1250, 1.9453, 0.1572]),\n",
       " 'vpm.encoder.layers.2.self_attn.v_proj.bias': tensor([ 0.0312, -0.0332,  0.0549,  ...,  0.0103,  0.0043,  0.0137]),\n",
       " 'llm.model.layers.2.mlp.gate_proj.scales': tensor([[0.0193, 0.0263, 0.0283,  ..., 0.0223, 0.0255, 0.0372]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.23.self_attn.k_proj.scales': tensor([[0.0142, 0.0113, 0.0141, 0.0146, 0.0126, 0.0131, 0.0143, 0.0146, 0.0127,\n",
       "          0.0140, 0.0134, 0.0143, 0.0180, 0.0172, 0.0251, 0.0224, 0.0320, 0.0292,\n",
       "          0.0280, 0.0302, 0.0309, 0.0461, 0.0453, 0.0427, 0.0604, 0.0719, 0.0615,\n",
       "          0.0724, 0.0576, 0.0602, 0.0627, 0.0471, 0.0152, 0.0149, 0.0134, 0.0109,\n",
       "          0.0118, 0.0127, 0.0122, 0.0115, 0.0165, 0.0141, 0.0137, 0.0184, 0.0182,\n",
       "          0.0208, 0.0221, 0.0221, 0.0211, 0.0269, 0.0233, 0.0268, 0.0294, 0.0324,\n",
       "          0.0324, 0.0516, 0.0844, 0.0609, 0.0469, 0.0370, 0.0662, 0.0534, 0.0802,\n",
       "          0.0388, 0.0197, 0.0219, 0.0280, 0.0213, 0.0233, 0.0223, 0.0289, 0.0212,\n",
       "          0.0253, 0.0310, 0.0283, 0.0254, 0.0324, 0.0385, 0.0288, 0.0336, 0.0344,\n",
       "          0.0259, 0.0225, 0.0283, 0.0310, 0.0359, 0.0293, 0.0296, 0.0250, 0.0258,\n",
       "          0.0307, 0.0168, 0.0283, 0.0380, 0.0303, 0.0294, 0.0171, 0.0191, 0.0203,\n",
       "          0.0277, 0.0297, 0.0370, 0.0246, 0.0328, 0.0258, 0.0238, 0.0288, 0.0227,\n",
       "          0.0262, 0.0255, 0.0232, 0.0283, 0.0331, 0.0271, 0.0232, 0.0281, 0.0283,\n",
       "          0.0307, 0.0336, 0.0283, 0.0367, 0.0259, 0.0324, 0.0391, 0.0324, 0.0311,\n",
       "          0.0298, 0.0253, 0.0208, 0.0176, 0.0171, 0.0259, 0.0162, 0.0201, 0.0240,\n",
       "          0.0201, 0.0338, 0.0197, 0.0211, 0.0178, 0.0194, 0.0195, 0.0238, 0.0225,\n",
       "          0.0378, 0.0199, 0.0234, 0.0206, 0.0220, 0.0225, 0.0204, 0.0204, 0.0197,\n",
       "          0.0539, 0.0461, 0.0238, 0.0211, 0.0225, 0.0247, 0.0213, 0.0152, 0.0171,\n",
       "          0.0211, 0.0223, 0.0253, 0.0147, 0.0166, 0.0244, 0.0174, 0.0188, 0.0202,\n",
       "          0.0367, 0.0201, 0.0191, 0.0230, 0.0180, 0.0241, 0.0249, 0.0163, 0.0219,\n",
       "          0.0210, 0.0202, 0.0188, 0.0234, 0.0198, 0.0111, 0.0208, 0.0212, 0.0257,\n",
       "          0.0253, 0.0224, 0.0249, 0.0130, 0.0160, 0.0162, 0.0208, 0.0186, 0.0208,\n",
       "          0.0310, 0.0210, 0.0404, 0.0276, 0.0202, 0.0263, 0.0257, 0.0247, 0.0228,\n",
       "          0.0271, 0.0404, 0.0272, 0.0285, 0.0298, 0.0275, 0.0253, 0.0280, 0.0273,\n",
       "          0.0271, 0.0338, 0.0225, 0.0260, 0.0370, 0.0281, 0.0263, 0.0238, 0.0181,\n",
       "          0.0225, 0.0182, 0.0193, 0.0251, 0.0232, 0.0224, 0.0190, 0.0212, 0.0305,\n",
       "          0.0309, 0.0273, 0.0220, 0.0233, 0.0240, 0.0281, 0.0280, 0.0247, 0.0211,\n",
       "          0.0346, 0.0284, 0.0247, 0.0298, 0.0258, 0.0227, 0.0119, 0.0275, 0.0264,\n",
       "          0.0260, 0.0254, 0.0259, 0.0245, 0.0161, 0.0158, 0.0143, 0.0180, 0.0210,\n",
       "          0.0178, 0.0241, 0.0329, 0.0238, 0.0185, 0.0172, 0.0257, 0.0227, 0.0190,\n",
       "          0.0199, 0.0233, 0.0216, 0.0293, 0.0219, 0.0208, 0.0263, 0.0212, 0.0163,\n",
       "          0.0234, 0.0210, 0.0255, 0.0490, 0.0210, 0.0210, 0.0207, 0.0220, 0.0198,\n",
       "          0.0323, 0.0258, 0.0333, 0.0246, 0.0212, 0.0315, 0.0189, 0.0178, 0.0203,\n",
       "          0.0279, 0.0262, 0.0188, 0.0213, 0.0174, 0.0279, 0.0225, 0.0280, 0.0186,\n",
       "          0.0207, 0.0281, 0.0296, 0.0266, 0.0176, 0.0215, 0.0203, 0.0208, 0.0148,\n",
       "          0.0211, 0.0230, 0.0244, 0.0228, 0.0227, 0.0186, 0.0193, 0.0184, 0.0180,\n",
       "          0.0172, 0.0122, 0.0263, 0.0202, 0.0211, 0.0159, 0.0189, 0.0246, 0.0215,\n",
       "          0.0134, 0.0182, 0.0154, 0.0224, 0.0215, 0.0202, 0.0257, 0.0474, 0.0294,\n",
       "          0.0289, 0.0297, 0.0311, 0.0297, 0.0500, 0.0536, 0.0424, 0.0268, 0.0370,\n",
       "          0.0242, 0.0168, 0.0159, 0.0156, 0.0159, 0.0207, 0.0178, 0.0233, 0.0213,\n",
       "          0.0191, 0.0174, 0.0195, 0.0159, 0.0185, 0.0255, 0.0264, 0.0258, 0.0271,\n",
       "          0.0202, 0.0279, 0.0217, 0.0212, 0.0229, 0.0233, 0.0264, 0.0300, 0.0284,\n",
       "          0.0479, 0.0300, 0.0344, 0.0283, 0.0267, 0.0249, 0.0238, 0.0154, 0.0171,\n",
       "          0.0177, 0.0212, 0.0220, 0.0203, 0.0259, 0.0213, 0.0230, 0.0208, 0.0234,\n",
       "          0.0245, 0.0253, 0.0285, 0.0271, 0.0469, 0.0266, 0.0326, 0.0487, 0.0375,\n",
       "          0.0469, 0.0500, 0.0419, 0.0450, 0.0419, 0.0217, 0.0646, 0.0370, 0.0709,\n",
       "          0.0656, 0.0536, 0.0171, 0.0153, 0.0197, 0.0144, 0.0159, 0.0160, 0.0210,\n",
       "          0.0212, 0.0207, 0.0190, 0.0336, 0.0260, 0.0241, 0.0199, 0.0285, 0.0250,\n",
       "          0.0359, 0.0445, 0.0357, 0.0497, 0.0293, 0.0362, 0.0771, 0.0352, 0.0372,\n",
       "          0.0450, 0.0487, 0.0461, 0.0474, 0.0776, 0.0870, 0.0346, 0.0152, 0.0141,\n",
       "          0.0158, 0.0161, 0.0140, 0.0173, 0.0147, 0.0147, 0.0158, 0.0135, 0.0159,\n",
       "          0.0150, 0.0191, 0.0169, 0.0220, 0.0219, 0.0229, 0.0236, 0.0161, 0.0220,\n",
       "          0.0320, 0.0260, 0.0245, 0.0225, 0.0217, 0.0213, 0.0220, 0.0240, 0.0241,\n",
       "          0.0292, 0.0422, 0.0204, 0.0126, 0.0133, 0.0139, 0.0147, 0.0148, 0.0161,\n",
       "          0.0127, 0.0172, 0.0147, 0.0160, 0.0172, 0.0207, 0.0163, 0.0176, 0.0185,\n",
       "          0.0174, 0.0201, 0.0223, 0.0246, 0.0220, 0.0202, 0.0244, 0.0328, 0.0250,\n",
       "          0.0236, 0.0615, 0.0242, 0.0440, 0.0251, 0.0285, 0.0280, 0.0257]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.12.self_attn.o_proj.scales': tensor([[0.0292, 0.0232, 0.0216,  ..., 0.0191, 0.0238, 0.0191]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.21.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.27.mlp.down_proj.scales': tensor([[0.0217, 0.0223, 0.0297,  ..., 0.0303, 0.0258, 0.0267]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.35.self_attn.k_proj.scales': tensor([[0.0154, 0.0152, 0.0178, 0.0191, 0.0212, 0.0178, 0.0213, 0.0219, 0.0236,\n",
       "          0.0257, 0.0166, 0.0212, 0.0296, 0.0233, 0.0254, 0.0271, 0.0208, 0.0198,\n",
       "          0.0240, 0.0232, 0.0306, 0.0245, 0.0300, 0.0277, 0.0329, 0.0604, 0.0293,\n",
       "          0.0268, 0.0365, 0.0344, 0.0271, 0.0247, 0.0189, 0.0169, 0.0203, 0.0167,\n",
       "          0.0177, 0.0169, 0.0220, 0.0184, 0.0149, 0.0215, 0.0280, 0.0240, 0.0188,\n",
       "          0.0241, 0.0229, 0.0156, 0.0280, 0.0212, 0.0246, 0.0233, 0.0253, 0.0246,\n",
       "          0.0268, 0.0230, 0.0306, 0.0115, 0.0238, 0.0228, 0.0316, 0.0267, 0.0264,\n",
       "          0.0260, 0.0137, 0.0142, 0.0135, 0.0202, 0.0146, 0.0163, 0.0173, 0.0202,\n",
       "          0.0178, 0.0232, 0.0195, 0.0150, 0.0158, 0.0213, 0.0210, 0.0143, 0.0208,\n",
       "          0.0158, 0.0250, 0.0275, 0.0227, 0.0289, 0.0289, 0.0290, 0.0272, 0.0333,\n",
       "          0.0323, 0.0296, 0.0245, 0.0280, 0.0285, 0.0276, 0.0159, 0.0142, 0.0169,\n",
       "          0.0217, 0.0147, 0.0208, 0.0213, 0.0186, 0.0246, 0.0233, 0.0206, 0.0267,\n",
       "          0.0195, 0.0238, 0.0234, 0.0262, 0.0245, 0.0266, 0.0294, 0.0268, 0.0207,\n",
       "          0.0232, 0.0326, 0.0293, 0.0301, 0.0225, 0.0241, 0.0399, 0.0266, 0.0297,\n",
       "          0.0267, 0.0266, 0.0146, 0.0146, 0.0137, 0.0118, 0.0130, 0.0136, 0.0173,\n",
       "          0.0160, 0.0127, 0.0142, 0.0115, 0.0176, 0.0190, 0.0159, 0.0156, 0.0234,\n",
       "          0.0206, 0.0245, 0.0244, 0.0220, 0.0249, 0.0223, 0.0237, 0.0296, 0.0288,\n",
       "          0.0328, 0.0284, 0.0283, 0.0309, 0.0293, 0.0293, 0.0438, 0.0121, 0.0110,\n",
       "          0.0109, 0.0126, 0.0139, 0.0146, 0.0153, 0.0139, 0.0166, 0.0213, 0.0223,\n",
       "          0.0173, 0.0128, 0.0173, 0.0145, 0.0202, 0.0178, 0.0236, 0.0237, 0.0225,\n",
       "          0.0273, 0.0207, 0.0303, 0.0296, 0.0338, 0.0312, 0.0344, 0.0306, 0.0281,\n",
       "          0.0331, 0.0370, 0.0322, 0.0344, 0.0249, 0.0158, 0.0182, 0.0135, 0.0280,\n",
       "          0.0157, 0.0204, 0.0229, 0.0208, 0.0194, 0.0254, 0.0234, 0.0219, 0.0198,\n",
       "          0.0173, 0.0245, 0.0219, 0.0257, 0.0247, 0.0286, 0.0186, 0.0223, 0.0220,\n",
       "          0.0195, 0.0102, 0.0201, 0.0221, 0.0240, 0.0234, 0.0204, 0.0251, 0.0327,\n",
       "          0.0253, 0.0172, 0.0354, 0.0153, 0.0216, 0.0137, 0.0188, 0.0193, 0.0161,\n",
       "          0.0303, 0.0149, 0.0194, 0.0250, 0.0247, 0.0136, 0.0207, 0.0262, 0.0161,\n",
       "          0.0238, 0.0232, 0.0220, 0.0204, 0.0230, 0.0219, 0.0370, 0.0208, 0.0223,\n",
       "          0.0213, 0.0210, 0.0203, 0.0234, 0.0140, 0.0118, 0.0130, 0.0103, 0.0148,\n",
       "          0.0125, 0.0149, 0.0141, 0.0152, 0.0190, 0.0150, 0.0152, 0.0172, 0.0163,\n",
       "          0.0184, 0.0194, 0.0245, 0.0271, 0.0212, 0.0292, 0.0244, 0.0260, 0.0357,\n",
       "          0.0341, 0.0432, 0.0406, 0.0406, 0.0333, 0.0352, 0.0448, 0.0344, 0.0344,\n",
       "          0.0098, 0.0116, 0.0119, 0.0113, 0.0125, 0.0133, 0.0171, 0.0130, 0.0172,\n",
       "          0.0145, 0.0169, 0.0157, 0.0157, 0.0258, 0.0157, 0.0225, 0.0204, 0.0375,\n",
       "          0.0199, 0.0258, 0.0266, 0.0352, 0.0259, 0.0311, 0.0213, 0.0378, 0.0414,\n",
       "          0.0383, 0.0370, 0.0495, 0.0399, 0.0370, 0.0138, 0.0094, 0.0105, 0.0131,\n",
       "          0.0103, 0.0120, 0.0100, 0.0109, 0.0164, 0.0123, 0.0115, 0.0121, 0.0167,\n",
       "          0.0161, 0.0134, 0.0219, 0.0198, 0.0203, 0.0182, 0.0263, 0.0266, 0.0213,\n",
       "          0.0307, 0.0323, 0.0267, 0.0267, 0.0331, 0.0306, 0.0289, 0.0294, 0.0411,\n",
       "          0.0328, 0.0100, 0.0110, 0.0117, 0.0100, 0.0107, 0.0109, 0.0118, 0.0130,\n",
       "          0.0147, 0.0140, 0.0171, 0.0119, 0.0142, 0.0152, 0.0178, 0.0160, 0.0227,\n",
       "          0.0152, 0.0195, 0.0176, 0.0323, 0.0194, 0.0236, 0.0230, 0.0296, 0.0570,\n",
       "          0.0277, 0.0281, 0.0346, 0.0217, 0.0385, 0.0288, 0.0186, 0.0137, 0.0253,\n",
       "          0.0206, 0.0188, 0.0257, 0.0269, 0.0168, 0.0188, 0.0367, 0.0164, 0.0276,\n",
       "          0.0401, 0.0290, 0.0163, 0.0266, 0.0224, 0.0284, 0.0404, 0.0286, 0.0336,\n",
       "          0.0263, 0.0311, 0.0277, 0.0318, 0.0294, 0.0260, 0.0269, 0.0331, 0.0401,\n",
       "          0.0280, 0.0277, 0.0180, 0.0238, 0.0171, 0.0210, 0.0156, 0.0203, 0.0266,\n",
       "          0.0189, 0.0320, 0.0244, 0.0247, 0.0315, 0.0165, 0.0255, 0.0246, 0.0349,\n",
       "          0.0271, 0.0258, 0.0247, 0.0298, 0.0300, 0.0349, 0.0284, 0.0341, 0.0315,\n",
       "          0.0259, 0.0329, 0.0352, 0.0279, 0.0286, 0.0254, 0.0266, 0.0142, 0.0126,\n",
       "          0.0133, 0.0128, 0.0153, 0.0212, 0.0150, 0.0195, 0.0197, 0.0185, 0.0213,\n",
       "          0.0172, 0.0332, 0.0230, 0.0245, 0.0316, 0.0237, 0.0341, 0.0338, 0.0241,\n",
       "          0.0336, 0.0346, 0.0393, 0.0346, 0.0329, 0.0346, 0.0322, 0.0365, 0.0296,\n",
       "          0.0427, 0.0349, 0.0341, 0.0152, 0.0141, 0.0130, 0.0163, 0.0132, 0.0172,\n",
       "          0.0223, 0.0207, 0.0177, 0.0215, 0.0224, 0.0206, 0.0269, 0.0259, 0.0333,\n",
       "          0.0269, 0.0349, 0.0309, 0.0331, 0.0257, 0.0336, 0.0333, 0.0401, 0.0523,\n",
       "          0.0393, 0.0399, 0.0461, 0.0324, 0.0341, 0.0331, 0.0414, 0.0365]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.45.self_attn.o_proj.qweight': tensor([[ 1737053576,  2070444457, -1989576359,  ..., -1750497126,\n",
       "           1770481544,  2023266923],\n",
       "         [-1499966039, -1968593015, -1754871930,  ..., -1767470470,\n",
       "          -1518836134, -1771541882],\n",
       "         [-1468434298, -2039973449, -1684436854,  ..., -1499945064,\n",
       "           2055837800, -2003138218],\n",
       "         ...,\n",
       "         [ 2038019449, -1469474952,  1974175368,  ...,  1505004107,\n",
       "          -1722521990, -1518884009],\n",
       "         [-1451783830,  1535744152, -1250396024,  ..., -1720346008,\n",
       "          -2008642393,  1518905221],\n",
       "         [-2013809289, -1682346825,  1787197034,  ..., -1752721321,\n",
       "          -1953917287,  1957005464]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.1.mlp.fc1.qzeros': tensor([[2139062143, 2139062143, 2139062143,  ..., 2139062143, 2139062143,\n",
       "          2139062143]], dtype=torch.int32),\n",
       " 'llm.model.layers.25.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.20.self_attn.k_proj.qweight': tensor([[-1953199992, -1515041676, -1044752524,  ...,  1705223804,\n",
       "           1820033411, -2091021662],\n",
       "         [-2103408238, -1433698704,  1702474421,  ...,  1553499810,\n",
       "           1732347303, -1822454857],\n",
       "         [   -8681565, -2033466758,  2107276951,  ..., -1853185649,\n",
       "           1887015317,  2071244612],\n",
       "         ...,\n",
       "         [ 1837459333,  -998865515,   698374779,  ...,  1904901973,\n",
       "           1834192221, -1936029752],\n",
       "         [-2037350774, -1836670558,  2054903445,  ..., -1672393312,\n",
       "           2086771018, -2072286879],\n",
       "         [-1737592182,  1634165339,  2123916913,  ...,  1699126606,\n",
       "          -2135459420,  1566324320]], dtype=torch.int32),\n",
       " 'llm.model.layers.14.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.35.self_attn.o_proj.qweight': tensor([[ 1485412759, -1971946823, -2038777737,  ..., -1684572008,\n",
       "           1769366635, -1484421257],\n",
       "         [-1770538361, -1449551718,  1737975704,  ...,  1218030236,\n",
       "           2022279607,  2022205355],\n",
       "         [-1768314216, -2036693142,  -864647813,  ..., -1737135724,\n",
       "          -2003347610, -1185049432],\n",
       "         ...,\n",
       "         [-1450748025,  1735092135, -1468372808,  ..., -2089257272,\n",
       "           1436973127,  -947406890],\n",
       "         [ 1734838921, -2037937991, -1484220313,  ..., -1984455895,\n",
       "           2006419850, -1216915305],\n",
       "         [ 1503160470,  1751558297, -2037941655,  ..., -2002487449,\n",
       "           1723311719,  1738959235]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.22.mlp.fc1.qweight': tensor([[-1852988016, -1640839549, -1954446220,  ..., -2137417591,\n",
       "           1902411411, -1970775422],\n",
       "         [-1655743138,  1312447547,  1836095303,  ..., -1987347080,\n",
       "           2060221300, -1431852842],\n",
       "         [-2112725924,  1818987335, -1954513747,  ..., -2035911601,\n",
       "           1784709270,  1534359349],\n",
       "         ...,\n",
       "         [-2106755698,  1886561389, -1988073589,  ..., -2023456857,\n",
       "           1149537956, -1502195610],\n",
       "         [-1484739995, -1736208482,  2119598985,  ...,  1754440610,\n",
       "           1431275169,  1938985869],\n",
       "         [-1722690467,  1150315299, -1988657054,  ...,  2136429648,\n",
       "           1534743405,  1884644415]], dtype=torch.int32),\n",
       " 'llm.model.layers.39.self_attn.v_proj.scales': tensor([[0.0210, 0.0255, 0.0203, 0.0307, 0.0284, 0.0247, 0.0242, 0.0219, 0.0311,\n",
       "          0.0234, 0.0263, 0.0280, 0.0228, 0.0241, 0.0254, 0.0208, 0.0297, 0.0303,\n",
       "          0.0211, 0.0216, 0.0230, 0.0246, 0.0221, 0.0215, 0.0268, 0.0212, 0.0293,\n",
       "          0.0276, 0.0269, 0.0262, 0.0199, 0.0262, 0.0242, 0.0253, 0.0223, 0.0203,\n",
       "          0.0242, 0.0210, 0.0247, 0.0306, 0.0210, 0.0253, 0.0246, 0.0207, 0.0208,\n",
       "          0.0213, 0.0289, 0.0372, 0.0245, 0.0306, 0.0280, 0.0230, 0.0247, 0.0320,\n",
       "          0.0240, 0.0264, 0.0224, 0.0223, 0.0255, 0.0224, 0.0255, 0.0273, 0.0324,\n",
       "          0.0255, 0.0266, 0.0346, 0.0279, 0.0293, 0.0238, 0.0283, 0.0354, 0.0246,\n",
       "          0.0244, 0.0258, 0.0285, 0.0232, 0.0284, 0.0244, 0.0240, 0.0253, 0.0257,\n",
       "          0.0230, 0.0212, 0.0276, 0.0251, 0.0283, 0.0316, 0.0267, 0.0250, 0.0242,\n",
       "          0.0268, 0.0241, 0.0283, 0.0240, 0.0250, 0.0277, 0.0262, 0.0255, 0.0312,\n",
       "          0.0275, 0.0267, 0.0223, 0.0249, 0.0254, 0.0298, 0.0273, 0.0281, 0.0242,\n",
       "          0.0257, 0.0301, 0.0254, 0.0336, 0.0250, 0.0242, 0.0247, 0.0263, 0.0268,\n",
       "          0.0279, 0.0233, 0.0271, 0.0285, 0.0246, 0.0240, 0.0267, 0.0245, 0.0255,\n",
       "          0.0246, 0.0260, 0.0333, 0.0279, 0.0286, 0.0277, 0.0289, 0.0311, 0.0292,\n",
       "          0.0375, 0.0338, 0.0435, 0.0435, 0.0332, 0.0367, 0.0281, 0.0258, 0.0269,\n",
       "          0.0326, 0.0310, 0.0298, 0.0324, 0.0297, 0.0309, 0.0273, 0.0326, 0.0370,\n",
       "          0.0257, 0.0359, 0.0388, 0.0329, 0.0349, 0.0306, 0.0285, 0.0323, 0.0250,\n",
       "          0.0284, 0.0310, 0.0300, 0.0301, 0.0341, 0.0293, 0.0320, 0.0297, 0.0285,\n",
       "          0.0354, 0.0236, 0.0341, 0.0251, 0.0301, 0.0258, 0.0417, 0.0370, 0.0286,\n",
       "          0.0268, 0.0323, 0.0271, 0.0266, 0.0404, 0.0411, 0.0296, 0.0333, 0.0469,\n",
       "          0.0294, 0.0306, 0.0301, 0.0315, 0.0290, 0.0307, 0.0228, 0.0329, 0.0469,\n",
       "          0.0281, 0.0399, 0.0324, 0.0323, 0.0338, 0.0332, 0.0357, 0.0352, 0.0309,\n",
       "          0.0310, 0.0341, 0.0281, 0.0320, 0.0354, 0.0393, 0.0290, 0.0346, 0.0307,\n",
       "          0.0346, 0.0306, 0.0314, 0.0417, 0.0466, 0.0336, 0.0344, 0.0328, 0.0301,\n",
       "          0.0448, 0.0385, 0.0305, 0.0346, 0.0293, 0.0281, 0.0233, 0.0316, 0.0354,\n",
       "          0.0318, 0.0309, 0.0367, 0.0293, 0.0326, 0.0300, 0.0404, 0.0331, 0.0312,\n",
       "          0.0414, 0.0399, 0.0396, 0.0296, 0.0285, 0.0393, 0.0391, 0.0307, 0.0305,\n",
       "          0.0309, 0.0414, 0.0341, 0.0326, 0.0326, 0.0300, 0.0323, 0.0307, 0.0271,\n",
       "          0.0346, 0.0352, 0.0327, 0.0303, 0.0302, 0.0331, 0.0326, 0.0320, 0.0288,\n",
       "          0.0346, 0.0309, 0.0298, 0.0314, 0.0349, 0.0322, 0.0370, 0.0333, 0.0332,\n",
       "          0.0359, 0.0357, 0.0298, 0.0290, 0.0338, 0.0344, 0.0328, 0.0341, 0.0275,\n",
       "          0.0307, 0.0309, 0.0267, 0.0310, 0.0352, 0.0320, 0.0328, 0.0378, 0.0310,\n",
       "          0.0257, 0.0338, 0.0327, 0.0341, 0.0285, 0.0396, 0.0312, 0.0338, 0.0296,\n",
       "          0.0292, 0.0357, 0.0320, 0.0284, 0.0305, 0.0293, 0.0406, 0.0338, 0.0297,\n",
       "          0.0289, 0.0310, 0.0323, 0.0293, 0.0294, 0.0346, 0.0378, 0.0385, 0.0414,\n",
       "          0.0354, 0.0359, 0.0378, 0.0461, 0.0305, 0.0385, 0.0375, 0.0388, 0.0399,\n",
       "          0.0359, 0.0331, 0.0388, 0.0352, 0.0388, 0.0422, 0.0383, 0.0409, 0.0456,\n",
       "          0.0424, 0.0341, 0.0309, 0.0427, 0.0320, 0.0365, 0.0370, 0.0380, 0.0404,\n",
       "          0.0443, 0.0375, 0.0346, 0.0385, 0.0401, 0.0435, 0.0406, 0.0370, 0.0234,\n",
       "          0.0367, 0.0482, 0.0391, 0.0383, 0.0332, 0.0370, 0.0427, 0.0324, 0.0354,\n",
       "          0.0409, 0.0354, 0.0375, 0.0385, 0.0391, 0.0388, 0.0370, 0.0456, 0.0362,\n",
       "          0.0320, 0.0359, 0.0380, 0.0370, 0.0401, 0.0354, 0.0258, 0.0297, 0.0359,\n",
       "          0.0331, 0.0251, 0.0271, 0.0199, 0.0290, 0.0315, 0.0311, 0.0294, 0.0378,\n",
       "          0.0194, 0.0262, 0.0346, 0.0349, 0.0210, 0.0276, 0.0329, 0.0318, 0.0247,\n",
       "          0.0320, 0.0193, 0.0254, 0.0322, 0.0332, 0.0322, 0.0440, 0.0336, 0.0306,\n",
       "          0.0285, 0.0276, 0.0242, 0.0399, 0.0260, 0.0341, 0.0320, 0.0322, 0.0354,\n",
       "          0.0236, 0.0362, 0.0503, 0.0352, 0.0399, 0.0298, 0.0393, 0.0294, 0.0367,\n",
       "          0.0302, 0.0306, 0.0217, 0.0254, 0.0411, 0.0399, 0.0312, 0.0378, 0.0311,\n",
       "          0.0385, 0.0292, 0.0298, 0.0297, 0.0262, 0.0349, 0.0305, 0.0236, 0.0212,\n",
       "          0.0236, 0.0225, 0.0210, 0.0221, 0.0208, 0.0254, 0.0217, 0.0262, 0.0212,\n",
       "          0.0232, 0.0266, 0.0247, 0.0258, 0.0247, 0.0229, 0.0221, 0.0251, 0.0242,\n",
       "          0.0225, 0.0276, 0.0242, 0.0236, 0.0227, 0.0229, 0.0234, 0.0251, 0.0255,\n",
       "          0.0305, 0.0264, 0.0242, 0.0227, 0.0264, 0.0234, 0.0224, 0.0297, 0.0220,\n",
       "          0.0221, 0.0223, 0.0281, 0.0232, 0.0221, 0.0227, 0.0271, 0.0241, 0.0237,\n",
       "          0.0250, 0.0217, 0.0230, 0.0233, 0.0259, 0.0233, 0.0238, 0.0223, 0.0290,\n",
       "          0.0249, 0.0269, 0.0221, 0.0241, 0.0220, 0.0260, 0.0237, 0.0199]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.21.self_attn.k_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.15.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.18.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.26.self_attn.o_proj.qweight': tensor([[-1466602697,  1738110408, -1700172406,  ...,  2039978904,\n",
       "          -1411912538, -1734895450],\n",
       "         [-1434146968, -1739023975, -1705007269,  ...,  1217169284,\n",
       "           1752787128,  2037876854],\n",
       "         [-2001099333,  -946313128, -1445685126,  ...,  1755940729,\n",
       "           1732945547, -1785174137],\n",
       "         ...,\n",
       "         [ 1502984855, -1718262374,  1973921749,  ..., -1953933399,\n",
       "           1753848445, -2041149337],\n",
       "         [ -913598535, -1940339351, -1769233032,  ..., -1150712473,\n",
       "          -1701209945, -1501922920],\n",
       "         [ 1523288742,  -663259287, -1181185641,  ..., -1705535097,\n",
       "           1786360968,  2024567688]], dtype=torch.int32),\n",
       " 'llm.model.layers.8.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.26.self_attn.out_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.20.self_attn.k_proj.scales': tensor([[0.0185, 0.0182, 0.0173, 0.0225, 0.0198, 0.0241, 0.0216, 0.0180, 0.0228,\n",
       "          0.0189, 0.0240, 0.0302, 0.0232, 0.0238, 0.0180, 0.0290, 0.0262, 0.0292,\n",
       "          0.0285, 0.0297, 0.0319, 0.0320, 0.0333, 0.0411, 0.0310, 0.0427, 0.0315,\n",
       "          0.0365, 0.0445, 0.0338, 0.0380, 0.0346, 0.0217, 0.0197, 0.0203, 0.0271,\n",
       "          0.0171, 0.0216, 0.0203, 0.0201, 0.0244, 0.0232, 0.0236, 0.0246, 0.0202,\n",
       "          0.0246, 0.0216, 0.0266, 0.0237, 0.0250, 0.0378, 0.0262, 0.0319, 0.0453,\n",
       "          0.0247, 0.0487, 0.0331, 0.0329, 0.0267, 0.0380, 0.0344, 0.0370, 0.0367,\n",
       "          0.0372, 0.0151, 0.0144, 0.0156, 0.0199, 0.0180, 0.0266, 0.0354, 0.0182,\n",
       "          0.0367, 0.0352, 0.0289, 0.0250, 0.0240, 0.0290, 0.0300, 0.0329, 0.0346,\n",
       "          0.0375, 0.0380, 0.0359, 0.0338, 0.0362, 0.0362, 0.0344, 0.0277, 0.0305,\n",
       "          0.0357, 0.0240, 0.0341, 0.0307, 0.0352, 0.0388, 0.0581, 0.0223, 0.0144,\n",
       "          0.0201, 0.0224, 0.0195, 0.0370, 0.0208, 0.0312, 0.0380, 0.0264, 0.0213,\n",
       "          0.0234, 0.0388, 0.0309, 0.0300, 0.0296, 0.0466, 0.0354, 0.0450, 0.0319,\n",
       "          0.0279, 0.0349, 0.0354, 0.0359, 0.0272, 0.0424, 0.0338, 0.0352, 0.0269,\n",
       "          0.0307, 0.0220, 0.0109, 0.0130, 0.0124, 0.0094, 0.0096, 0.0117, 0.0114,\n",
       "          0.0139, 0.0144, 0.0106, 0.0133, 0.0107, 0.0128, 0.0128, 0.0142, 0.0160,\n",
       "          0.0250, 0.0190, 0.0253, 0.0219, 0.0143, 0.0263, 0.0399, 0.0227, 0.0497,\n",
       "          0.0492, 0.0518, 0.0469, 0.0490, 0.0419, 0.0456, 0.0294, 0.0100, 0.0106,\n",
       "          0.0091, 0.0135, 0.0142, 0.0104, 0.0156, 0.0125, 0.0122, 0.0109, 0.0123,\n",
       "          0.0139, 0.0135, 0.0157, 0.0163, 0.0176, 0.0245, 0.0216, 0.0227, 0.0208,\n",
       "          0.0280, 0.0365, 0.0388, 0.0324, 0.0542, 0.0531, 0.0435, 0.0555, 0.0482,\n",
       "          0.0487, 0.0562, 0.0471, 0.0157, 0.0165, 0.0096, 0.0124, 0.0148, 0.0103,\n",
       "          0.0177, 0.0161, 0.0159, 0.0157, 0.0194, 0.0176, 0.0190, 0.0176, 0.0255,\n",
       "          0.0161, 0.0224, 0.0223, 0.0146, 0.0314, 0.0167, 0.0172, 0.0246, 0.0172,\n",
       "          0.0155, 0.0802, 0.0151, 0.0164, 0.0161, 0.0166, 0.0162, 0.0232, 0.0117,\n",
       "          0.0122, 0.0095, 0.0150, 0.0157, 0.0122, 0.0145, 0.0177, 0.0176, 0.0190,\n",
       "          0.0166, 0.0184, 0.0225, 0.0202, 0.0281, 0.0167, 0.0178, 0.0223, 0.0185,\n",
       "          0.0193, 0.0172, 0.0174, 0.0149, 0.0154, 0.0173, 0.0232, 0.0162, 0.0188,\n",
       "          0.0199, 0.0161, 0.0193, 0.0238, 0.0123, 0.0104, 0.0103, 0.0102, 0.0145,\n",
       "          0.0110, 0.0155, 0.0101, 0.0086, 0.0132, 0.0159, 0.0131, 0.0207, 0.0234,\n",
       "          0.0219, 0.0305, 0.0216, 0.0309, 0.0312, 0.0305, 0.0289, 0.0296, 0.0314,\n",
       "          0.0283, 0.0316, 0.0562, 0.0276, 0.0329, 0.0314, 0.0310, 0.0251, 0.0327,\n",
       "          0.0117, 0.0122, 0.0099, 0.0120, 0.0133, 0.0132, 0.0111, 0.0173, 0.0162,\n",
       "          0.0160, 0.0120, 0.0177, 0.0171, 0.0253, 0.0236, 0.0292, 0.0219, 0.0300,\n",
       "          0.0318, 0.0260, 0.0333, 0.0310, 0.0307, 0.0315, 0.0285, 0.0430, 0.0298,\n",
       "          0.0324, 0.0297, 0.0346, 0.0314, 0.0199, 0.0165, 0.0146, 0.0181, 0.0156,\n",
       "          0.0163, 0.0172, 0.0198, 0.0213, 0.0158, 0.0171, 0.0204, 0.0215, 0.0224,\n",
       "          0.0307, 0.0156, 0.0228, 0.0220, 0.0229, 0.0258, 0.0236, 0.0258, 0.0346,\n",
       "          0.0285, 0.0290, 0.0279, 0.0096, 0.0245, 0.0310, 0.0388, 0.0284, 0.0314,\n",
       "          0.0508, 0.0221, 0.0158, 0.0176, 0.0161, 0.0221, 0.0164, 0.0189, 0.0219,\n",
       "          0.0182, 0.0217, 0.0297, 0.0280, 0.0153, 0.0246, 0.0224, 0.0244, 0.0215,\n",
       "          0.0245, 0.0240, 0.0306, 0.0292, 0.0260, 0.0272, 0.0279, 0.0310, 0.0641,\n",
       "          0.0245, 0.0290, 0.0354, 0.0305, 0.0262, 0.0466, 0.0161, 0.0213, 0.0182,\n",
       "          0.0190, 0.0178, 0.0230, 0.0212, 0.0277, 0.0216, 0.0290, 0.0195, 0.0255,\n",
       "          0.0215, 0.0216, 0.0241, 0.0257, 0.0285, 0.0266, 0.0289, 0.0362, 0.0268,\n",
       "          0.0230, 0.0280, 0.0294, 0.0241, 0.0292, 0.0275, 0.0197, 0.0333, 0.0332,\n",
       "          0.0260, 0.0285, 0.0194, 0.0193, 0.0150, 0.0229, 0.0182, 0.0240, 0.0132,\n",
       "          0.0271, 0.0212, 0.0255, 0.0260, 0.0314, 0.0344, 0.0263, 0.0224, 0.0237,\n",
       "          0.0279, 0.0238, 0.0283, 0.0272, 0.0247, 0.0272, 0.0327, 0.0271, 0.0247,\n",
       "          0.0310, 0.0315, 0.0286, 0.0284, 0.0336, 0.0259, 0.0323, 0.0242, 0.0157,\n",
       "          0.0249, 0.0254, 0.0216, 0.0212, 0.0215, 0.0290, 0.0263, 0.0250, 0.0271,\n",
       "          0.0245, 0.0233, 0.0237, 0.0169, 0.0264, 0.0271, 0.0316, 0.0285, 0.0280,\n",
       "          0.0281, 0.0290, 0.0301, 0.0284, 0.0285, 0.0314, 0.0322, 0.0332, 0.0246,\n",
       "          0.0253, 0.0245, 0.0290, 0.0190, 0.0216, 0.0249, 0.0197, 0.0224, 0.0241,\n",
       "          0.0221, 0.0202, 0.0260, 0.0174, 0.0258, 0.0264, 0.0203, 0.0254, 0.0232,\n",
       "          0.0316, 0.0293, 0.0246, 0.0264, 0.0280, 0.0333, 0.0329, 0.0246, 0.0322,\n",
       "          0.0255, 0.0119, 0.0279, 0.0271, 0.0324, 0.0271, 0.0244, 0.0290]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.0.self_attn.out_proj.bias': tensor([ 0.0081,  0.0111,  0.1865,  ...,  0.0234, -1.0234,  0.0444]),\n",
       " 'llm.model.layers.33.self_attn.k_proj.qweight': tensor([[ 1955109514, -1231517784,  1755809960,  ...,  1755805593,\n",
       "          -2000115542, -1750562086],\n",
       "         [ 1284867704,  1773767273,  2023131014,  ..., -2021094773,\n",
       "           2005567610, -2036902263],\n",
       "         [-2038982986,  1452905877, -1684318550,  ..., -1518839480,\n",
       "          -1721193590,  -948397706],\n",
       "         ...,\n",
       "         [ 1750574489,  1992128921, -1749230951,  ...,  1468565896,\n",
       "           1485346967,  1552311705],\n",
       "         [ 2021242792,  2020973974,  1756670100,  ..., -1215723915,\n",
       "           1720367177, -1752594584],\n",
       "         [ 2075826838, -1486194838,  2038987429,  ..., -1752680041,\n",
       "          -1717139287,  2054842200]], dtype=torch.int32),\n",
       " 'llm.model.layers.51.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.16.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.46.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.17.mlp.fc1.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.21.mlp.down_proj.scales': tensor([[0.0254, 0.0303, 0.0238,  ..., 0.0292, 0.0240, 0.0482]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.28.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.8.self_attn.v_proj.scales': tensor([[0.0208, 0.0237, 0.0247, 0.0221, 0.0229, 0.0262, 0.0221, 0.0206, 0.0223,\n",
       "          0.0224, 0.0217, 0.0233, 0.0233, 0.0217, 0.0210, 0.0229, 0.0219, 0.0224,\n",
       "          0.0204, 0.0253, 0.0238, 0.0193, 0.0225, 0.0245, 0.0198, 0.0249, 0.0254,\n",
       "          0.0220, 0.0312, 0.0203, 0.0212, 0.0254, 0.0242, 0.0253, 0.0272, 0.0202,\n",
       "          0.0229, 0.0213, 0.0217, 0.0232, 0.0272, 0.0219, 0.0267, 0.0206, 0.0234,\n",
       "          0.0217, 0.0224, 0.0223, 0.0232, 0.0213, 0.0233, 0.0204, 0.0225, 0.0269,\n",
       "          0.0212, 0.0228, 0.0202, 0.0236, 0.0279, 0.0186, 0.0215, 0.0219, 0.0233,\n",
       "          0.0212, 0.0263, 0.0279, 0.0263, 0.0233, 0.0311, 0.0223, 0.0251, 0.0310,\n",
       "          0.0240, 0.0232, 0.0260, 0.0266, 0.0245, 0.0237, 0.0264, 0.0242, 0.0294,\n",
       "          0.0217, 0.0286, 0.0234, 0.0264, 0.0242, 0.0275, 0.0225, 0.0255, 0.0251,\n",
       "          0.0283, 0.0238, 0.0245, 0.0227, 0.0273, 0.0267, 0.0247, 0.0253, 0.0250,\n",
       "          0.0283, 0.0259, 0.0251, 0.0241, 0.0260, 0.0263, 0.0240, 0.0279, 0.0240,\n",
       "          0.0253, 0.0241, 0.0262, 0.0247, 0.0228, 0.0228, 0.0250, 0.0297, 0.0219,\n",
       "          0.0258, 0.0245, 0.0221, 0.0289, 0.0264, 0.0260, 0.0253, 0.0237, 0.0260,\n",
       "          0.0268, 0.0259, 0.0230, 0.0250, 0.0249, 0.0275, 0.0223, 0.0236, 0.0254,\n",
       "          0.0269, 0.0277, 0.0263, 0.0229, 0.0250, 0.0318, 0.0301, 0.0263, 0.0303,\n",
       "          0.0267, 0.0249, 0.0228, 0.0319, 0.0215, 0.0259, 0.0284, 0.0273, 0.0269,\n",
       "          0.0269, 0.0237, 0.0293, 0.0269, 0.0255, 0.0247, 0.0228, 0.0211, 0.0254,\n",
       "          0.0229, 0.0245, 0.0251, 0.0244, 0.0262, 0.0268, 0.0300, 0.0236, 0.0251,\n",
       "          0.0268, 0.0266, 0.0228, 0.0232, 0.0250, 0.0249, 0.0242, 0.0251, 0.0220,\n",
       "          0.0271, 0.0241, 0.0217, 0.0253, 0.0312, 0.0241, 0.0254, 0.0223, 0.0302,\n",
       "          0.0212, 0.0268, 0.0258, 0.0188, 0.0238, 0.0215, 0.0215, 0.0220, 0.0251,\n",
       "          0.0199, 0.0213, 0.0220, 0.0233, 0.0262, 0.0203, 0.0232, 0.0215, 0.0230,\n",
       "          0.0250, 0.0221, 0.0240, 0.0191, 0.0193, 0.0232, 0.0202, 0.0238, 0.0190,\n",
       "          0.0203, 0.0197, 0.0202, 0.0224, 0.0221, 0.0233, 0.0233, 0.0251, 0.0227,\n",
       "          0.0198, 0.0208, 0.0207, 0.0190, 0.0213, 0.0208, 0.0181, 0.0224, 0.0220,\n",
       "          0.0212, 0.0232, 0.0228, 0.0216, 0.0188, 0.0207, 0.0293, 0.0228, 0.0237,\n",
       "          0.0206, 0.0186, 0.0240, 0.0193, 0.0194, 0.0229, 0.0195, 0.0206, 0.0186,\n",
       "          0.0210, 0.0195, 0.0221, 0.0195, 0.0251, 0.0234, 0.0249, 0.0227, 0.0233,\n",
       "          0.0242, 0.0220, 0.0247, 0.0208, 0.0246, 0.0223, 0.0259, 0.0230, 0.0233,\n",
       "          0.0249, 0.0219, 0.0211, 0.0227, 0.0199, 0.0273, 0.0245, 0.0221, 0.0228,\n",
       "          0.0229, 0.0249, 0.0279, 0.0206, 0.0289, 0.0244, 0.0262, 0.0212, 0.0193,\n",
       "          0.0195, 0.0229, 0.0208, 0.0279, 0.0212, 0.0217, 0.0293, 0.0249, 0.0223,\n",
       "          0.0245, 0.0203, 0.0223, 0.0236, 0.0227, 0.0275, 0.0228, 0.0210, 0.0251,\n",
       "          0.0223, 0.0229, 0.0207, 0.0258, 0.0225, 0.0228, 0.0225, 0.0250, 0.0237,\n",
       "          0.0225, 0.0237, 0.0234, 0.0216, 0.0320, 0.0202, 0.0211, 0.0220, 0.0258,\n",
       "          0.0273, 0.0255, 0.0254, 0.0246, 0.0220, 0.0234, 0.0240, 0.0250, 0.0213,\n",
       "          0.0244, 0.0211, 0.0228, 0.0244, 0.0208, 0.0272, 0.0203, 0.0258, 0.0275,\n",
       "          0.0262, 0.0284, 0.0213, 0.0206, 0.0267, 0.0249, 0.0216, 0.0245, 0.0203,\n",
       "          0.0197, 0.0253, 0.0247, 0.0241, 0.0237, 0.0211, 0.0227, 0.0216, 0.0245,\n",
       "          0.0230, 0.0244, 0.0220, 0.0254, 0.0263, 0.0260, 0.0219, 0.0230, 0.0241,\n",
       "          0.0223, 0.0233, 0.0201, 0.0258, 0.0247, 0.0236, 0.0215, 0.0219, 0.0212,\n",
       "          0.0227, 0.0223, 0.0232, 0.0215, 0.0211, 0.0217, 0.0319, 0.0268, 0.0229,\n",
       "          0.0224, 0.0296, 0.0273, 0.0284, 0.0253, 0.0217, 0.0246, 0.0246, 0.0257,\n",
       "          0.0259, 0.0314, 0.0230, 0.0251, 0.0249, 0.0263, 0.0250, 0.0296, 0.0255,\n",
       "          0.0262, 0.0285, 0.0251, 0.0289, 0.0268, 0.0275, 0.0290, 0.0225, 0.0230,\n",
       "          0.0269, 0.0255, 0.0232, 0.0224, 0.0249, 0.0237, 0.0289, 0.0233, 0.0245,\n",
       "          0.0255, 0.0297, 0.0307, 0.0279, 0.0240, 0.0251, 0.0229, 0.0240, 0.0257,\n",
       "          0.0272, 0.0245, 0.0276, 0.0298, 0.0281, 0.0249, 0.0241, 0.0219, 0.0288,\n",
       "          0.0253, 0.0272, 0.0272, 0.0269, 0.0263, 0.0233, 0.0280, 0.0234, 0.0293,\n",
       "          0.0208, 0.0250, 0.0311, 0.0255, 0.0221, 0.0232, 0.0319, 0.0250, 0.0223,\n",
       "          0.0237, 0.0318, 0.0328, 0.0383, 0.0284, 0.0275, 0.0262, 0.0324, 0.0255,\n",
       "          0.0255, 0.0249, 0.0277, 0.0241, 0.0289, 0.0310, 0.0211, 0.0240, 0.0241,\n",
       "          0.0271, 0.0247, 0.0245, 0.0223, 0.0215, 0.0238, 0.0240, 0.0245, 0.0233,\n",
       "          0.0264, 0.0254, 0.0216, 0.0217, 0.0253, 0.0206, 0.0285, 0.0234, 0.0246,\n",
       "          0.0301, 0.0233, 0.0238, 0.0292, 0.0229, 0.0223, 0.0262, 0.0269, 0.0257,\n",
       "          0.0247, 0.0284, 0.0237, 0.0262, 0.0238, 0.0240, 0.0273, 0.0286]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.16.layer_norm1.weight': tensor([1.0234, 1.0078, 1.0391,  ..., 0.8555, 0.9844, 0.9961]),\n",
       " 'vpm.encoder.layers.12.mlp.fc2.scales': tensor([[0.0010, 0.0010, 0.0011,  ..., 0.0012, 0.0011, 0.0008]]),\n",
       " 'llm.model.layers.30.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.2.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.23.mlp.down_proj.scales': tensor([[0.0236, 0.0242, 0.0258,  ..., 0.0246, 0.0249, 0.0314]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.29.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.36.self_attn.q_proj.qweight': tensor([[-1819829096, -1735755891, -1466337079,  ...,  -909534791,\n",
       "          -2007524441,  1469565078],\n",
       "         [ 1492748421,  1986361961,  1992066681,  ...,  2024429721,\n",
       "          -1736739196, -1250473882],\n",
       "         [-1488483719, -1166436455,  1218999960,  ..., -1719039913,\n",
       "          -1550284853,  1989782172],\n",
       "         ...,\n",
       "         [-1706444358,  1216063961,  -899966840,  ..., -1199995513,\n",
       "          -1432642360, -1469473929],\n",
       "         [-2005378972, -1721443929, -1736935255,  ..., -1166504521,\n",
       "           1215796839, -1717138548],\n",
       "         [ 2058647450,  1951885193,  1754695544,  ...,  2073663896,\n",
       "          -1986480268, -2005379447]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.26.self_attn.out_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.28.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.6.layer_norm2.weight': tensor([0.7539, 0.7539, 0.7617,  ..., 0.7305, 0.7031, 0.8516]),\n",
       " 'llm.model.layers.47.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.39.mlp.gate_proj.scales': tensor([[0.0242, 0.0189, 0.0229,  ..., 0.0237, 0.0284, 0.0178]],\n",
       "        dtype=torch.float16),\n",
       " 'resampler.proj': tensor([[-4.1016e-02,  1.5625e-02, -5.3223e-02,  ..., -6.9336e-02,\n",
       "          -4.4678e-02,  2.6367e-02],\n",
       "         [-5.3955e-02,  4.5166e-02,  1.1902e-02,  ...,  9.2506e-05,\n",
       "           1.6846e-02,  3.5889e-02],\n",
       "         [ 8.1787e-03, -1.9653e-02,  8.7891e-03,  ...,  1.1780e-02,\n",
       "          -2.2217e-02,  3.1250e-02],\n",
       "         ...,\n",
       "         [ 3.9551e-02, -8.7280e-03,  8.4473e-02,  ...,  5.5176e-02,\n",
       "          -3.3936e-02, -8.7891e-02],\n",
       "         [-1.0986e-02,  2.0905e-03,  3.8300e-03,  ...,  3.7598e-02,\n",
       "          -2.8931e-02, -5.5664e-02],\n",
       "         [-1.4282e-02, -4.9805e-02, -3.8330e-02,  ..., -2.0508e-02,\n",
       "           1.1108e-02, -5.3711e-03]]),\n",
       " 'vpm.encoder.layers.20.self_attn.out_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.45.self_attn.v_proj.scales': tensor([[0.0464, 0.0432, 0.0378, 0.0406, 0.0430, 0.0404, 0.0375, 0.0458, 0.0380,\n",
       "          0.0399, 0.0469, 0.0617, 0.0411, 0.0372, 0.0419, 0.0487, 0.0401, 0.0375,\n",
       "          0.0372, 0.0422, 0.0445, 0.0513, 0.0430, 0.0435, 0.0479, 0.0391, 0.0346,\n",
       "          0.0391, 0.0393, 0.0388, 0.0458, 0.0375, 0.0511, 0.0401, 0.0422, 0.0391,\n",
       "          0.0438, 0.0396, 0.0547, 0.0401, 0.0257, 0.0432, 0.0495, 0.0440, 0.0341,\n",
       "          0.0474, 0.0427, 0.0399, 0.0393, 0.0383, 0.0495, 0.0341, 0.0367, 0.0396,\n",
       "          0.0438, 0.0458, 0.0461, 0.0440, 0.0338, 0.0419, 0.0432, 0.0432, 0.0411,\n",
       "          0.0438, 0.0456, 0.0648, 0.0570, 0.0599, 0.0536, 0.0526, 0.0667, 0.0664,\n",
       "          0.0630, 0.0511, 0.0492, 0.0482, 0.0729, 0.0529, 0.0484, 0.0542, 0.0497,\n",
       "          0.0544, 0.0539, 0.0646, 0.0482, 0.0547, 0.0529, 0.0765, 0.0523, 0.0487,\n",
       "          0.0539, 0.0500, 0.0490, 0.0646, 0.0479, 0.0617, 0.0602, 0.0544, 0.0492,\n",
       "          0.0581, 0.0505, 0.0406, 0.0693, 0.0531, 0.0526, 0.0651, 0.0534, 0.0557,\n",
       "          0.0630, 0.0630, 0.0539, 0.0518, 0.0555, 0.0531, 0.0513, 0.0479, 0.0456,\n",
       "          0.0682, 0.0534, 0.0521, 0.0604, 0.0399, 0.0479, 0.0508, 0.0523, 0.0497,\n",
       "          0.0445, 0.0568, 0.0406, 0.0578, 0.0409, 0.0362, 0.0393, 0.0367, 0.0399,\n",
       "          0.0474, 0.0393, 0.0409, 0.0505, 0.0438, 0.0414, 0.0471, 0.0322, 0.0424,\n",
       "          0.0438, 0.0469, 0.0469, 0.0440, 0.0409, 0.0453, 0.0406, 0.0432, 0.0479,\n",
       "          0.0406, 0.0372, 0.0393, 0.0414, 0.0375, 0.0391, 0.0406, 0.0367, 0.0346,\n",
       "          0.0422, 0.0380, 0.0417, 0.0404, 0.0479, 0.0456, 0.0591, 0.0474, 0.0240,\n",
       "          0.0448, 0.0422, 0.0435, 0.0581, 0.0435, 0.0440, 0.0500, 0.0332, 0.0422,\n",
       "          0.0453, 0.0417, 0.0456, 0.0443, 0.0456, 0.0419, 0.0393, 0.0432, 0.0422,\n",
       "          0.0427, 0.0448, 0.0477, 0.0484, 0.0497, 0.0500, 0.0406, 0.0427, 0.0469,\n",
       "          0.0461, 0.0409, 0.0576, 0.0461, 0.0568, 0.0487, 0.0479, 0.0547, 0.0445,\n",
       "          0.0482, 0.0542, 0.0505, 0.0490, 0.0450, 0.0625, 0.0573, 0.0443, 0.0516,\n",
       "          0.0497, 0.0552, 0.0576, 0.0445, 0.0565, 0.0471, 0.0461, 0.0477, 0.0490,\n",
       "          0.0578, 0.0505, 0.0505, 0.0479, 0.0583, 0.0466, 0.0547, 0.0417, 0.0479,\n",
       "          0.0550, 0.0477, 0.0438, 0.0411, 0.0562, 0.0464, 0.0497, 0.0500, 0.0521,\n",
       "          0.0490, 0.0583, 0.0438, 0.0633, 0.0552, 0.0404, 0.0521, 0.0438, 0.0508,\n",
       "          0.0536, 0.0539, 0.0484, 0.0471, 0.0309, 0.0341, 0.0461, 0.0300, 0.0357,\n",
       "          0.0254, 0.0346, 0.0357, 0.0310, 0.0385, 0.0352, 0.0391, 0.0344, 0.0378,\n",
       "          0.0372, 0.0323, 0.0293, 0.0479, 0.0430, 0.0341, 0.0365, 0.0406, 0.0461,\n",
       "          0.0435, 0.0370, 0.0352, 0.0306, 0.0332, 0.0303, 0.0349, 0.0341, 0.0336,\n",
       "          0.0315, 0.0332, 0.0336, 0.0324, 0.0349, 0.0326, 0.0338, 0.0365, 0.0333,\n",
       "          0.0406, 0.0341, 0.0367, 0.0359, 0.0385, 0.0336, 0.0312, 0.0319, 0.0307,\n",
       "          0.0365, 0.0318, 0.0315, 0.0466, 0.0362, 0.0331, 0.0286, 0.0354, 0.0290,\n",
       "          0.0380, 0.0388, 0.0333, 0.0329, 0.0354, 0.0469, 0.0316, 0.0388, 0.0424,\n",
       "          0.0422, 0.0438, 0.0336, 0.0542, 0.0380, 0.0555, 0.0500, 0.0448, 0.0427,\n",
       "          0.0503, 0.0482, 0.0396, 0.0367, 0.0484, 0.0438, 0.0385, 0.0391, 0.0388,\n",
       "          0.0393, 0.0419, 0.0440, 0.0453, 0.0505, 0.0432, 0.0367, 0.0456, 0.0456,\n",
       "          0.0531, 0.0570, 0.0383, 0.0469, 0.0450, 0.0443, 0.0349, 0.0438, 0.0383,\n",
       "          0.0419, 0.0445, 0.0401, 0.0505, 0.0432, 0.0529, 0.0490, 0.0414, 0.0482,\n",
       "          0.0422, 0.0391, 0.0430, 0.0503, 0.0464, 0.0357, 0.0404, 0.0466, 0.0495,\n",
       "          0.0550, 0.0365, 0.0388, 0.0404, 0.0276, 0.0539, 0.0479, 0.0479, 0.0430,\n",
       "          0.0456, 0.0461, 0.0370, 0.0427, 0.0424, 0.0375, 0.0448, 0.0424, 0.0503,\n",
       "          0.0456, 0.0409, 0.0401, 0.0310, 0.0417, 0.0411, 0.0461, 0.0490, 0.0435,\n",
       "          0.0365, 0.0357, 0.0573, 0.0370, 0.0500, 0.0388, 0.0518, 0.0414, 0.0456,\n",
       "          0.0427, 0.0300, 0.0424, 0.0401, 0.0448, 0.0518, 0.0500, 0.0409, 0.0508,\n",
       "          0.0370, 0.0456, 0.0409, 0.0464, 0.0396, 0.0474, 0.0385, 0.0508, 0.0544,\n",
       "          0.0401, 0.0500, 0.0396, 0.0414, 0.0411, 0.0344, 0.0432, 0.0591, 0.0422,\n",
       "          0.0401, 0.0487, 0.0508, 0.0391, 0.0406, 0.0479, 0.0440, 0.0367, 0.0370,\n",
       "          0.0438, 0.0314, 0.0440, 0.0324, 0.0354, 0.0458, 0.0293, 0.0516, 0.0417,\n",
       "          0.0406, 0.0419, 0.0305, 0.0319, 0.0359, 0.0399, 0.0448, 0.0419, 0.0385,\n",
       "          0.0445, 0.0370, 0.0336, 0.0445, 0.0419, 0.0440, 0.0440, 0.0367, 0.0458,\n",
       "          0.0391, 0.0419, 0.0365, 0.0365, 0.0336, 0.0332, 0.0404, 0.0344, 0.0435,\n",
       "          0.0362, 0.0346, 0.0406, 0.0331, 0.0338, 0.0396, 0.0396, 0.0341, 0.0406,\n",
       "          0.0393, 0.0338, 0.0284, 0.0401, 0.0292, 0.0388, 0.0411, 0.0497, 0.0370,\n",
       "          0.0388, 0.0365, 0.0393, 0.0352, 0.0393, 0.0391, 0.0354, 0.0401]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.1.self_attn.v_proj.scales': tensor([[0.0198, 0.0168, 0.0195, 0.0189, 0.0182, 0.0166, 0.0168, 0.0201, 0.0169,\n",
       "          0.0180, 0.0199, 0.0173, 0.0191, 0.0180, 0.0173, 0.0168, 0.0188, 0.0237,\n",
       "          0.0208, 0.0204, 0.0184, 0.0177, 0.0169, 0.0191, 0.0165, 0.0202, 0.0193,\n",
       "          0.0193, 0.0203, 0.0215, 0.0180, 0.0180, 0.0167, 0.0193, 0.0220, 0.0194,\n",
       "          0.0227, 0.0195, 0.0186, 0.0181, 0.0191, 0.0181, 0.0236, 0.0195, 0.0223,\n",
       "          0.0207, 0.0178, 0.0208, 0.0190, 0.0188, 0.0188, 0.0181, 0.0161, 0.0173,\n",
       "          0.0169, 0.0195, 0.0181, 0.0204, 0.0206, 0.0180, 0.0181, 0.0154, 0.0194,\n",
       "          0.0188, 0.0241, 0.0207, 0.0257, 0.0229, 0.0216, 0.0203, 0.0293, 0.0244,\n",
       "          0.0264, 0.0221, 0.0221, 0.0201, 0.0255, 0.0194, 0.0233, 0.0206, 0.0217,\n",
       "          0.0219, 0.0221, 0.0229, 0.0292, 0.0210, 0.0228, 0.0322, 0.0204, 0.0237,\n",
       "          0.0198, 0.0207, 0.0216, 0.0296, 0.0250, 0.0225, 0.0223, 0.0250, 0.0216,\n",
       "          0.0236, 0.0194, 0.0210, 0.0208, 0.0208, 0.0264, 0.0224, 0.0233, 0.0251,\n",
       "          0.0203, 0.0233, 0.0233, 0.0253, 0.0266, 0.0225, 0.0227, 0.0204, 0.0262,\n",
       "          0.0224, 0.0262, 0.0296, 0.0236, 0.0207, 0.0208, 0.0236, 0.0208, 0.0216,\n",
       "          0.0211, 0.0201, 0.0224, 0.0207, 0.0224, 0.0190, 0.0217, 0.0219, 0.0221,\n",
       "          0.0189, 0.0211, 0.0201, 0.0193, 0.0223, 0.0213, 0.0191, 0.0184, 0.0211,\n",
       "          0.0206, 0.0188, 0.0203, 0.0221, 0.0202, 0.0203, 0.0171, 0.0199, 0.0186,\n",
       "          0.0197, 0.0224, 0.0163, 0.0217, 0.0198, 0.0217, 0.0208, 0.0216, 0.0206,\n",
       "          0.0188, 0.0206, 0.0201, 0.0194, 0.0197, 0.0171, 0.0173, 0.0193, 0.0207,\n",
       "          0.0236, 0.0184, 0.0174, 0.0207, 0.0225, 0.0189, 0.0181, 0.0201, 0.0176,\n",
       "          0.0217, 0.0212, 0.0206, 0.0174, 0.0224, 0.0220, 0.0194, 0.0195, 0.0206,\n",
       "          0.0171, 0.0213, 0.0207, 0.0280, 0.0201, 0.0203, 0.0272, 0.0213, 0.0257,\n",
       "          0.0257, 0.0259, 0.0273, 0.0206, 0.0311, 0.0228, 0.0199, 0.0206, 0.0269,\n",
       "          0.0198, 0.0221, 0.0249, 0.0249, 0.0245, 0.0276, 0.0230, 0.0288, 0.0273,\n",
       "          0.0223, 0.0229, 0.0264, 0.0275, 0.0180, 0.0229, 0.0189, 0.0250, 0.0266,\n",
       "          0.0193, 0.0242, 0.0233, 0.0198, 0.0257, 0.0259, 0.0272, 0.0260, 0.0241,\n",
       "          0.0246, 0.0238, 0.0176, 0.0249, 0.0186, 0.0223, 0.0241, 0.0212, 0.0223,\n",
       "          0.0212, 0.0227, 0.0279, 0.0249, 0.0206, 0.0318, 0.0193, 0.0210, 0.0186,\n",
       "          0.0230, 0.0186, 0.0168, 0.0244, 0.0230, 0.0210, 0.0195, 0.0156, 0.0258,\n",
       "          0.0201, 0.0195, 0.0199, 0.0228, 0.0223, 0.0220, 0.0168, 0.0168, 0.0180,\n",
       "          0.0246, 0.0195, 0.0219, 0.0207, 0.0181, 0.0289, 0.0306, 0.0194, 0.0195,\n",
       "          0.0201, 0.0204, 0.0204, 0.0197, 0.0178, 0.0189, 0.0184, 0.0197, 0.0193,\n",
       "          0.0215, 0.0181, 0.0178, 0.0180, 0.0195, 0.0182, 0.0190, 0.0189, 0.0247,\n",
       "          0.0189, 0.0207, 0.0212, 0.0167, 0.0191, 0.0206, 0.0215, 0.0219, 0.0202,\n",
       "          0.0219, 0.0240, 0.0171, 0.0206, 0.0217, 0.0294, 0.0215, 0.0224, 0.0164,\n",
       "          0.0242, 0.0197, 0.0204, 0.0190, 0.0190, 0.0176, 0.0180, 0.0165, 0.0143,\n",
       "          0.0156, 0.0158, 0.0159, 0.0206, 0.0165, 0.0168, 0.0191, 0.0177, 0.0167,\n",
       "          0.0162, 0.0172, 0.0154, 0.0178, 0.0199, 0.0184, 0.0220, 0.0208, 0.0156,\n",
       "          0.0152, 0.0190, 0.0155, 0.0176, 0.0153, 0.0158, 0.0169, 0.0195, 0.0178,\n",
       "          0.0185, 0.0161, 0.0184, 0.0162, 0.0174, 0.0154, 0.0168, 0.0184, 0.0148,\n",
       "          0.0188, 0.0159, 0.0149, 0.0167, 0.0169, 0.0186, 0.0161, 0.0153, 0.0159,\n",
       "          0.0173, 0.0156, 0.0181, 0.0150, 0.0165, 0.0164, 0.0158, 0.0159, 0.0165,\n",
       "          0.0215, 0.0169, 0.0146, 0.0173, 0.0180, 0.0150, 0.0180, 0.0250, 0.0189,\n",
       "          0.0232, 0.0224, 0.0268, 0.0223, 0.0228, 0.0225, 0.0223, 0.0208, 0.0210,\n",
       "          0.0212, 0.0258, 0.0217, 0.0210, 0.0238, 0.0210, 0.0273, 0.0223, 0.0242,\n",
       "          0.0211, 0.0221, 0.0221, 0.0210, 0.0206, 0.0232, 0.0224, 0.0241, 0.0220,\n",
       "          0.0181, 0.0221, 0.0217, 0.0203, 0.0221, 0.0189, 0.0216, 0.0273, 0.0234,\n",
       "          0.0249, 0.0229, 0.0225, 0.0245, 0.0208, 0.0241, 0.0238, 0.0204, 0.0210,\n",
       "          0.0189, 0.0254, 0.0213, 0.0197, 0.0234, 0.0246, 0.0230, 0.0263, 0.0227,\n",
       "          0.0240, 0.0236, 0.0263, 0.0211, 0.0208, 0.0246, 0.0206, 0.0146, 0.0191,\n",
       "          0.0148, 0.0181, 0.0159, 0.0242, 0.0158, 0.0176, 0.0180, 0.0180, 0.0169,\n",
       "          0.0145, 0.0173, 0.0142, 0.0158, 0.0162, 0.0165, 0.0159, 0.0167, 0.0169,\n",
       "          0.0148, 0.0156, 0.0168, 0.0169, 0.0184, 0.0155, 0.0158, 0.0146, 0.0146,\n",
       "          0.0171, 0.0174, 0.0153, 0.0152, 0.0156, 0.0157, 0.0168, 0.0184, 0.0180,\n",
       "          0.0147, 0.0190, 0.0156, 0.0168, 0.0148, 0.0135, 0.0198, 0.0163, 0.0146,\n",
       "          0.0233, 0.0190, 0.0168, 0.0144, 0.0197, 0.0168, 0.0156, 0.0156, 0.0184,\n",
       "          0.0264, 0.0204, 0.0158, 0.0162, 0.0194, 0.0188, 0.0148, 0.0152]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.37.mlp.down_proj.qweight': tensor([[-2033620892, -1733780860,  1756808826,  ...,  2039842904,\n",
       "           2074433210, -2020247415],\n",
       "         [-2052495017, -1705612125, -1972659576,  ..., -1752671672,\n",
       "          -2070374053, -2004313959],\n",
       "         [ 1436968822,  1786550149, -1919318392,  ...,  1971881782,\n",
       "          -1952732746, -2022204791],\n",
       "         ...,\n",
       "         [ 1183279768, -2035718536, -1990830442,  ..., -1451783034,\n",
       "          -1967481734, -2018932600],\n",
       "         [-1988580472, -2003126922,  1736869514,  ..., -1720219469,\n",
       "          -1671010152,  2022217880],\n",
       "         [ 1501333384,  1822984632, -2002155355,  ..., -1184680055,\n",
       "          -1501071208, -2005293191]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.7.self_attn.out_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.4.self_attn.v_proj.qweight': tensor([[ 1852858456, -2056026993,  1658017702,  ..., -2069852531,\n",
       "           2004583290,  -862482304],\n",
       "         [-1990686588,  -662265765,  1851556295,  ...,  1821666176,\n",
       "           2071888497, -1936297374],\n",
       "         [ 1709736098,   802317162,  1935254961,  ...,   980976813,\n",
       "           2021626003, -1146909556],\n",
       "         ...,\n",
       "         [-1332540805,   849319763,  1900701331,  ..., -1349666962,\n",
       "           1871481486,  1954127490],\n",
       "         [  699167119,  2039181419, -1837939549,  ...,  1666807417,\n",
       "           2071955836,  1919582365],\n",
       "         [  613779818,  2116648110, -1518555199,  ...,  1382126475,\n",
       "          -1972072333, -1737651847]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.17.mlp.fc2.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.30.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.15.mlp.up_proj.qweight': tensor([[-1976075847, -2023257975, -2023250219,  ...,  -662131623,\n",
       "           -962041720,  1955825785],\n",
       "         [-1452959578, -2004329851,  2020178331,  ..., -1705678471,\n",
       "          -1939167351,  -963143820],\n",
       "         [ 1738045079,  2054718824, -1383561080,  ...,  1752724856,\n",
       "          -1464326520,  1235658118],\n",
       "         ...,\n",
       "         [-1984264072,  1769711478, -1773627255,  ..., -1178838632,\n",
       "          -1133282185, -1715967640],\n",
       "         [-2020968314, -1736919413,  1753577079,  ...,  -393508489,\n",
       "            930441383,  1501988714],\n",
       "         [ 1502135146,  -996710261, -1785296746,  ...,  1386698620,\n",
       "           2005567893,   931694008]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.5.self_attn.out_proj.qweight': tensor([[ 1600490123,  1886756152, -1770480531,  ..., -1368095633,\n",
       "          -1130729265,  1298239820],\n",
       "         [-1719695000,   430865291, -1717331804,  ...,  1583844225,\n",
       "          -1186439892,  2035183989],\n",
       "         [-2004052608,  1513451828, -1615496802,  ..., -1703318369,\n",
       "           2069074787,  1447989411],\n",
       "         ...,\n",
       "         [-1502582185,  2039645817,  1033460809,  ...,  1952933271,\n",
       "          -1887518141,  1669484139],\n",
       "         [-1619498791,   943743824, -1095460186,  ..., -1318948997,\n",
       "           1880911453,  -865892939],\n",
       "         [ 1652461446,  1685178206, -1993574320,  ...,  2089138334,\n",
       "           1944149342,  2020833388]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.12.layer_norm1.weight': tensor([0.8203, 0.8203, 0.9414,  ..., 0.6445, 0.7539, 0.7812]),\n",
       " 'vpm.encoder.layers.13.self_attn.out_proj.bias': tensor([ 0.0053,  0.0806,  0.2080,  ..., -0.0183,  0.0126, -0.0505]),\n",
       " 'llm.model.layers.12.self_attn.k_proj.scales': tensor([[0.0190, 0.0190, 0.0185, 0.0241, 0.0206, 0.0225, 0.0212, 0.0232, 0.0230,\n",
       "          0.0195, 0.0249, 0.0236, 0.0305, 0.0305, 0.0242, 0.0224, 0.0238, 0.0267,\n",
       "          0.0357, 0.0300, 0.0401, 0.0338, 0.0307, 0.0315, 0.0285, 0.0604, 0.0320,\n",
       "          0.0319, 0.0310, 0.0341, 0.0312, 0.0388, 0.0255, 0.0208, 0.0212, 0.0212,\n",
       "          0.0161, 0.0204, 0.0159, 0.0201, 0.0228, 0.0242, 0.0326, 0.0266, 0.0297,\n",
       "          0.0225, 0.0292, 0.0329, 0.0266, 0.0346, 0.0300, 0.0272, 0.0269, 0.0380,\n",
       "          0.0346, 0.0276, 0.0307, 0.0787, 0.0320, 0.0289, 0.0388, 0.0328, 0.0285,\n",
       "          0.0285, 0.0260, 0.0245, 0.0197, 0.0169, 0.0189, 0.0254, 0.0184, 0.0263,\n",
       "          0.0207, 0.0244, 0.0254, 0.0249, 0.0212, 0.0285, 0.0233, 0.0255, 0.0240,\n",
       "          0.0208, 0.0259, 0.0225, 0.0273, 0.0266, 0.0238, 0.1042, 0.0272, 0.0238,\n",
       "          0.0555, 0.0388, 0.0292, 0.0262, 0.0251, 0.0247, 0.0238, 0.0215, 0.0224,\n",
       "          0.0201, 0.0267, 0.0193, 0.0253, 0.0253, 0.0228, 0.0336, 0.0245, 0.0273,\n",
       "          0.0245, 0.0232, 0.0281, 0.0266, 0.0249, 0.0266, 0.0225, 0.0242, 0.0303,\n",
       "          0.0427, 0.0264, 0.0417, 0.0266, 0.0271, 0.0245, 0.0264, 0.0302, 0.0228,\n",
       "          0.0230, 0.0253, 0.0215, 0.0207, 0.0198, 0.0220, 0.0201, 0.0249, 0.0208,\n",
       "          0.0177, 0.0201, 0.0237, 0.0236, 0.0259, 0.0240, 0.0236, 0.0241, 0.0273,\n",
       "          0.0236, 0.0328, 0.0236, 0.0238, 0.0302, 0.0301, 0.0273, 0.0268, 0.0505,\n",
       "          0.0301, 0.0354, 0.0362, 0.0311, 0.0263, 0.0354, 0.0305, 0.0207, 0.0228,\n",
       "          0.0244, 0.0210, 0.0216, 0.0207, 0.0190, 0.0224, 0.0180, 0.0249, 0.0165,\n",
       "          0.0275, 0.0206, 0.0207, 0.0198, 0.0220, 0.0220, 0.0241, 0.0388, 0.0419,\n",
       "          0.0285, 0.0303, 0.0292, 0.0249, 0.0859, 0.0536, 0.0482, 0.0359, 0.0280,\n",
       "          0.0396, 0.0333, 0.0290, 0.0233, 0.0217, 0.0245, 0.0232, 0.0273, 0.0303,\n",
       "          0.0329, 0.0266, 0.0240, 0.0320, 0.0292, 0.0286, 0.0241, 0.0253, 0.0272,\n",
       "          0.0238, 0.0320, 0.0294, 0.0263, 0.0300, 0.0240, 0.0277, 0.0312, 0.0268,\n",
       "          0.0385, 0.0693, 0.0315, 0.0370, 0.0466, 0.0349, 0.0492, 0.0620, 0.0269,\n",
       "          0.0264, 0.0285, 0.0232, 0.0297, 0.0300, 0.0284, 0.0247, 0.0246, 0.0260,\n",
       "          0.0258, 0.0327, 0.0246, 0.0258, 0.0255, 0.0262, 0.0310, 0.0277, 0.0269,\n",
       "          0.0323, 0.0422, 0.0338, 0.0264, 0.0409, 0.0289, 0.0565, 0.0370, 0.0294,\n",
       "          0.0401, 0.0417, 0.0385, 0.0354, 0.0188, 0.0197, 0.0277, 0.0178, 0.0233,\n",
       "          0.0204, 0.0185, 0.0267, 0.0233, 0.0195, 0.0331, 0.0169, 0.0238, 0.0185,\n",
       "          0.0298, 0.0271, 0.0269, 0.0293, 0.0307, 0.0424, 0.0354, 0.0280, 0.0324,\n",
       "          0.0581, 0.0466, 0.0309, 0.0622, 0.0357, 0.0312, 0.0311, 0.0280, 0.0385,\n",
       "          0.0193, 0.0216, 0.0246, 0.0204, 0.0224, 0.0201, 0.0219, 0.0193, 0.0276,\n",
       "          0.0194, 0.0263, 0.0233, 0.0303, 0.0272, 0.0246, 0.0262, 0.0149, 0.0288,\n",
       "          0.0276, 0.0326, 0.0316, 0.0288, 0.0393, 0.0672, 0.0294, 0.0283, 0.0495,\n",
       "          0.0385, 0.0326, 0.0320, 0.0255, 0.0487, 0.0336, 0.0241, 0.0167, 0.0210,\n",
       "          0.0197, 0.0174, 0.0242, 0.0177, 0.0185, 0.0344, 0.0277, 0.0215, 0.0331,\n",
       "          0.0171, 0.0263, 0.0277, 0.0306, 0.0293, 0.0249, 0.0272, 0.0260, 0.0241,\n",
       "          0.0247, 0.0249, 0.0289, 0.0240, 0.0630, 0.0251, 0.0237, 0.0260, 0.0550,\n",
       "          0.0474, 0.0138, 0.0210, 0.0171, 0.0316, 0.0212, 0.0207, 0.0260, 0.0203,\n",
       "          0.0182, 0.0253, 0.0275, 0.0191, 0.0298, 0.0236, 0.0229, 0.0262, 0.0279,\n",
       "          0.0233, 0.0260, 0.0262, 0.0236, 0.0188, 0.0273, 0.0234, 0.0238, 0.0244,\n",
       "          0.0479, 0.0249, 0.0246, 0.0267, 0.0253, 0.0254, 0.0165, 0.0181, 0.0195,\n",
       "          0.0150, 0.0213, 0.0220, 0.0275, 0.0195, 0.0260, 0.0250, 0.0338, 0.0234,\n",
       "          0.0253, 0.0281, 0.0302, 0.0300, 0.0255, 0.0302, 0.0309, 0.0289, 0.0279,\n",
       "          0.0359, 0.0307, 0.0273, 0.0298, 0.0562, 0.0277, 0.0448, 0.0272, 0.0315,\n",
       "          0.0281, 0.0272, 0.0267, 0.0257, 0.0372, 0.0195, 0.0258, 0.0182, 0.0232,\n",
       "          0.0203, 0.0247, 0.0220, 0.0273, 0.0245, 0.0251, 0.0267, 0.0219, 0.0315,\n",
       "          0.0269, 0.0201, 0.0383, 0.0271, 0.0310, 0.0267, 0.0474, 0.0263, 0.0273,\n",
       "          0.0771, 0.0279, 0.0542, 0.0309, 0.0290, 0.0279, 0.0294, 0.0257, 0.0324,\n",
       "          0.0133, 0.0224, 0.0188, 0.0236, 0.0238, 0.0244, 0.0212, 0.0224, 0.0180,\n",
       "          0.0212, 0.0262, 0.0223, 0.0223, 0.0251, 0.0182, 0.0275, 0.0194, 0.0208,\n",
       "          0.0210, 0.0244, 0.0233, 0.0229, 0.0917, 0.0186, 0.0285, 0.0698, 0.0479,\n",
       "          0.0375, 0.0223, 0.0202, 0.0148, 0.0168, 0.0238, 0.0228, 0.0158, 0.0249,\n",
       "          0.0216, 0.0157, 0.0203, 0.0190, 0.0229, 0.0219, 0.0160, 0.0223, 0.0202,\n",
       "          0.0285, 0.0234, 0.0322, 0.0221, 0.0224, 0.0228, 0.0197, 0.0225, 0.0238,\n",
       "          0.0714, 0.0242, 0.0182, 0.0197, 0.0547, 0.0191, 0.0414, 0.0208]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.34.self_attn.k_proj.scales': tensor([[0.0133, 0.0132, 0.0127, 0.0126, 0.0127, 0.0105, 0.0113, 0.0159, 0.0121,\n",
       "          0.0123, 0.0136, 0.0168, 0.0185, 0.0181, 0.0207, 0.0229, 0.0249, 0.0247,\n",
       "          0.0310, 0.0440, 0.0469, 0.0445, 0.0477, 0.0536, 0.0643, 0.0565, 0.0687,\n",
       "          0.0401, 0.0560, 0.0565, 0.0450, 0.0378, 0.0111, 0.0135, 0.0128, 0.0119,\n",
       "          0.0132, 0.0140, 0.0142, 0.0132, 0.0140, 0.0148, 0.0148, 0.0159, 0.0143,\n",
       "          0.0157, 0.0176, 0.0169, 0.0229, 0.0303, 0.0318, 0.0281, 0.0276, 0.0672,\n",
       "          0.0309, 0.0578, 0.0438, 0.0771, 0.0399, 0.0518, 0.0450, 0.0760, 0.0432,\n",
       "          0.0497, 0.0191, 0.0204, 0.0185, 0.0267, 0.0215, 0.0156, 0.0225, 0.0202,\n",
       "          0.0303, 0.0250, 0.0251, 0.0217, 0.0281, 0.0284, 0.0328, 0.0328, 0.0182,\n",
       "          0.0338, 0.0300, 0.0307, 0.0305, 0.0292, 0.0267, 0.0338, 0.0302, 0.0128,\n",
       "          0.0277, 0.0326, 0.0289, 0.0327, 0.0322, 0.0312, 0.0195, 0.0180, 0.0163,\n",
       "          0.0276, 0.0195, 0.0238, 0.0262, 0.0177, 0.0247, 0.0169, 0.0259, 0.0204,\n",
       "          0.0271, 0.0150, 0.0275, 0.0277, 0.0249, 0.0318, 0.0254, 0.0281, 0.0372,\n",
       "          0.0264, 0.0303, 0.0310, 0.0301, 0.0370, 0.0311, 0.0310, 0.0249, 0.0354,\n",
       "          0.0294, 0.0338, 0.0159, 0.0208, 0.0174, 0.0167, 0.0220, 0.0232, 0.0147,\n",
       "          0.0223, 0.0210, 0.0146, 0.0268, 0.0324, 0.0290, 0.0213, 0.0268, 0.0271,\n",
       "          0.0329, 0.0240, 0.0322, 0.0262, 0.0300, 0.0306, 0.0302, 0.0332, 0.0234,\n",
       "          0.0305, 0.0380, 0.0331, 0.0336, 0.0362, 0.0362, 0.0318, 0.0199, 0.0150,\n",
       "          0.0152, 0.0191, 0.0154, 0.0206, 0.0324, 0.0188, 0.0228, 0.0346, 0.0240,\n",
       "          0.0158, 0.0302, 0.0292, 0.0283, 0.0208, 0.0336, 0.0316, 0.0260, 0.0309,\n",
       "          0.0309, 0.0307, 0.0326, 0.0293, 0.0490, 0.0354, 0.0359, 0.0327, 0.0370,\n",
       "          0.0406, 0.0367, 0.0352, 0.0237, 0.0221, 0.0255, 0.0180, 0.0232, 0.0198,\n",
       "          0.0199, 0.0208, 0.0215, 0.0168, 0.0234, 0.0232, 0.0141, 0.0232, 0.0232,\n",
       "          0.0233, 0.0263, 0.0174, 0.0221, 0.0306, 0.0229, 0.0311, 0.0237, 0.0284,\n",
       "          0.0289, 0.0211, 0.0216, 0.0219, 0.0253, 0.0233, 0.0232, 0.0229, 0.0173,\n",
       "          0.0215, 0.0208, 0.0147, 0.0186, 0.0208, 0.0174, 0.0191, 0.0206, 0.0259,\n",
       "          0.0223, 0.0254, 0.0230, 0.0228, 0.0280, 0.0258, 0.0223, 0.0269, 0.0289,\n",
       "          0.0244, 0.0269, 0.0198, 0.0244, 0.0254, 0.0255, 0.0255, 0.0245, 0.0276,\n",
       "          0.0362, 0.0234, 0.0280, 0.0269, 0.0202, 0.0219, 0.0193, 0.0237, 0.0199,\n",
       "          0.0236, 0.0190, 0.0233, 0.0213, 0.0303, 0.0311, 0.0338, 0.0212, 0.0322,\n",
       "          0.0310, 0.0213, 0.0283, 0.0307, 0.0281, 0.0254, 0.0257, 0.0352, 0.0328,\n",
       "          0.0296, 0.0294, 0.0318, 0.0253, 0.0279, 0.0346, 0.0357, 0.0314, 0.0271,\n",
       "          0.0206, 0.0167, 0.0138, 0.0257, 0.0166, 0.0290, 0.0173, 0.0255, 0.0284,\n",
       "          0.0300, 0.0193, 0.0306, 0.0300, 0.0322, 0.0332, 0.0238, 0.0290, 0.0322,\n",
       "          0.0202, 0.0283, 0.0318, 0.0319, 0.0307, 0.0286, 0.0259, 0.0293, 0.0311,\n",
       "          0.0320, 0.0300, 0.0336, 0.0289, 0.0338, 0.0173, 0.0172, 0.0168, 0.0173,\n",
       "          0.0172, 0.0180, 0.0232, 0.0143, 0.0260, 0.0155, 0.0221, 0.0135, 0.0233,\n",
       "          0.0207, 0.0178, 0.0237, 0.0237, 0.0242, 0.0318, 0.0269, 0.0280, 0.0277,\n",
       "          0.0250, 0.0289, 0.0275, 0.0378, 0.0292, 0.0329, 0.0272, 0.0289, 0.0314,\n",
       "          0.0474, 0.0176, 0.0180, 0.0151, 0.0168, 0.0177, 0.0210, 0.0228, 0.0220,\n",
       "          0.0166, 0.0255, 0.0195, 0.0233, 0.0220, 0.0271, 0.0233, 0.0258, 0.0176,\n",
       "          0.0246, 0.0224, 0.0225, 0.0297, 0.0324, 0.0289, 0.0292, 0.0260, 0.0138,\n",
       "          0.0432, 0.0324, 0.0375, 0.0268, 0.0247, 0.0260, 0.0180, 0.0124, 0.0118,\n",
       "          0.0133, 0.0152, 0.0156, 0.0198, 0.0148, 0.0201, 0.0247, 0.0184, 0.0141,\n",
       "          0.0207, 0.0254, 0.0153, 0.0258, 0.0195, 0.0203, 0.0234, 0.0217, 0.0224,\n",
       "          0.0223, 0.0207, 0.0211, 0.0271, 0.0236, 0.0323, 0.0244, 0.0232, 0.0207,\n",
       "          0.0297, 0.0228, 0.0143, 0.0108, 0.0115, 0.0133, 0.0181, 0.0149, 0.0185,\n",
       "          0.0152, 0.0225, 0.0148, 0.0210, 0.0224, 0.0194, 0.0240, 0.0143, 0.0246,\n",
       "          0.0148, 0.0262, 0.0193, 0.0171, 0.0202, 0.0203, 0.0193, 0.0230, 0.0160,\n",
       "          0.0210, 0.0367, 0.0234, 0.0234, 0.0211, 0.0250, 0.0228, 0.0329, 0.0160,\n",
       "          0.0269, 0.0193, 0.0267, 0.0242, 0.0126, 0.0242, 0.0359, 0.0271, 0.0323,\n",
       "          0.0146, 0.0220, 0.0177, 0.0241, 0.0359, 0.0255, 0.0357, 0.0301, 0.0333,\n",
       "          0.0333, 0.0354, 0.0109, 0.0191, 0.0388, 0.0316, 0.0336, 0.0427, 0.0367,\n",
       "          0.0267, 0.0289, 0.0284, 0.0199, 0.0211, 0.0290, 0.0181, 0.0152, 0.0130,\n",
       "          0.0266, 0.0229, 0.0152, 0.0314, 0.0293, 0.0257, 0.0234, 0.0314, 0.0283,\n",
       "          0.0229, 0.0237, 0.0316, 0.0346, 0.0404, 0.0324, 0.0312, 0.0176, 0.0307,\n",
       "          0.0305, 0.0352, 0.0163, 0.0328, 0.0320, 0.0357, 0.0302, 0.0552]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.36.self_attn.k_proj.scales': tensor([[0.0167, 0.0139, 0.0140, 0.0135, 0.0156, 0.0177, 0.0194, 0.0118, 0.0178,\n",
       "          0.0174, 0.0133, 0.0189, 0.0210, 0.0237, 0.0159, 0.0277, 0.0228, 0.0213,\n",
       "          0.0253, 0.0269, 0.0269, 0.0259, 0.0306, 0.0245, 0.0401, 0.0578, 0.0807,\n",
       "          0.0776, 0.0745, 0.0917, 0.0526, 0.0332, 0.0135, 0.0184, 0.0184, 0.0159,\n",
       "          0.0184, 0.0162, 0.0161, 0.0167, 0.0165, 0.0230, 0.0224, 0.0241, 0.0225,\n",
       "          0.0255, 0.0213, 0.0257, 0.0238, 0.0257, 0.0246, 0.0213, 0.0230, 0.0333,\n",
       "          0.0286, 0.0359, 0.0406, 0.0896, 0.0591, 0.0740, 0.0393, 0.0607, 0.0424,\n",
       "          0.0662, 0.0105, 0.0111, 0.0115, 0.0114, 0.0124, 0.0154, 0.0137, 0.0115,\n",
       "          0.0161, 0.0101, 0.0128, 0.0122, 0.0122, 0.0158, 0.0123, 0.0271, 0.0172,\n",
       "          0.0391, 0.0244, 0.0206, 0.0328, 0.0292, 0.0259, 0.0411, 0.0372, 0.0539,\n",
       "          0.0461, 0.0411, 0.0380, 0.0388, 0.0497, 0.0411, 0.0120, 0.0104, 0.0133,\n",
       "          0.0111, 0.0113, 0.0152, 0.0116, 0.0131, 0.0129, 0.0160, 0.0123, 0.0178,\n",
       "          0.0158, 0.0132, 0.0193, 0.0219, 0.0213, 0.0264, 0.0267, 0.0251, 0.0290,\n",
       "          0.0396, 0.0284, 0.0289, 0.0427, 0.0456, 0.0438, 0.0635, 0.0440, 0.0464,\n",
       "          0.0409, 0.0399, 0.0136, 0.0111, 0.0190, 0.0102, 0.0137, 0.0117, 0.0158,\n",
       "          0.0161, 0.0124, 0.0188, 0.0143, 0.0215, 0.0174, 0.0188, 0.0198, 0.0211,\n",
       "          0.0220, 0.0220, 0.0237, 0.0210, 0.0185, 0.0204, 0.0268, 0.0236, 0.0212,\n",
       "          0.0201, 0.0240, 0.0346, 0.0234, 0.0357, 0.0372, 0.0281, 0.0094, 0.0145,\n",
       "          0.0180, 0.0111, 0.0167, 0.0128, 0.0135, 0.0178, 0.0144, 0.0201, 0.0163,\n",
       "          0.0171, 0.0212, 0.0204, 0.0262, 0.0237, 0.0294, 0.0212, 0.0225, 0.0284,\n",
       "          0.0288, 0.0227, 0.0236, 0.0206, 0.0244, 0.0224, 0.0250, 0.0255, 0.0255,\n",
       "          0.0244, 0.0233, 0.0269, 0.0143, 0.0128, 0.0128, 0.0128, 0.0124, 0.0176,\n",
       "          0.0193, 0.0138, 0.0148, 0.0128, 0.0186, 0.0138, 0.0193, 0.0197, 0.0161,\n",
       "          0.0212, 0.0203, 0.0180, 0.0230, 0.0207, 0.0217, 0.0211, 0.0221, 0.0221,\n",
       "          0.0255, 0.0133, 0.0260, 0.0262, 0.0306, 0.0404, 0.0320, 0.0456, 0.0136,\n",
       "          0.0152, 0.0145, 0.0124, 0.0147, 0.0180, 0.0121, 0.0172, 0.0194, 0.0234,\n",
       "          0.0190, 0.0229, 0.0137, 0.0219, 0.0198, 0.0197, 0.0234, 0.0184, 0.0296,\n",
       "          0.0267, 0.0225, 0.0283, 0.0264, 0.0219, 0.0246, 0.0440, 0.0271, 0.0294,\n",
       "          0.0264, 0.0505, 0.0326, 0.0316, 0.0125, 0.0122, 0.0149, 0.0133, 0.0156,\n",
       "          0.0136, 0.0135, 0.0141, 0.0135, 0.0180, 0.0172, 0.0178, 0.0186, 0.0150,\n",
       "          0.0245, 0.0228, 0.0221, 0.0253, 0.0255, 0.0247, 0.0312, 0.0272, 0.0309,\n",
       "          0.0305, 0.0319, 0.0281, 0.0288, 0.0322, 0.0322, 0.0357, 0.0275, 0.0370,\n",
       "          0.0146, 0.0101, 0.0110, 0.0162, 0.0124, 0.0164, 0.0134, 0.0275, 0.0151,\n",
       "          0.0142, 0.0134, 0.0120, 0.0207, 0.0171, 0.0188, 0.0223, 0.0162, 0.0279,\n",
       "          0.0260, 0.0286, 0.0219, 0.0268, 0.0320, 0.0290, 0.0253, 0.0327, 0.0283,\n",
       "          0.0298, 0.0298, 0.0314, 0.0399, 0.0479, 0.0495, 0.0357, 0.0114, 0.0155,\n",
       "          0.0159, 0.0199, 0.0148, 0.0174, 0.0176, 0.0203, 0.0139, 0.0195, 0.0184,\n",
       "          0.0182, 0.0251, 0.0138, 0.0178, 0.0146, 0.0208, 0.0176, 0.0186, 0.0202,\n",
       "          0.0191, 0.0223, 0.0190, 0.0199, 0.0188, 0.0180, 0.0185, 0.0177, 0.0186,\n",
       "          0.0180, 0.0161, 0.0181, 0.0161, 0.0266, 0.0132, 0.0186, 0.0262, 0.0213,\n",
       "          0.0195, 0.0157, 0.0181, 0.0194, 0.0142, 0.0164, 0.0189, 0.0172, 0.0163,\n",
       "          0.0163, 0.0171, 0.0167, 0.0190, 0.0208, 0.0161, 0.0180, 0.0208, 0.0163,\n",
       "          0.0372, 0.0269, 0.0215, 0.0220, 0.0198, 0.0284, 0.0100, 0.0123, 0.0118,\n",
       "          0.0096, 0.0090, 0.0097, 0.0105, 0.0096, 0.0118, 0.0104, 0.0094, 0.0138,\n",
       "          0.0133, 0.0156, 0.0166, 0.0201, 0.0253, 0.0232, 0.0245, 0.0247, 0.0266,\n",
       "          0.0247, 0.0314, 0.0297, 0.0375, 0.0362, 0.0440, 0.0432, 0.0375, 0.0534,\n",
       "          0.0396, 0.0326, 0.0100, 0.0092, 0.0096, 0.0107, 0.0092, 0.0120, 0.0102,\n",
       "          0.0100, 0.0115, 0.0109, 0.0118, 0.0134, 0.0158, 0.0174, 0.0184, 0.0227,\n",
       "          0.0217, 0.0254, 0.0346, 0.0240, 0.0268, 0.0333, 0.0316, 0.0293, 0.0456,\n",
       "          0.0367, 0.0474, 0.0349, 0.0521, 0.0385, 0.0568, 0.0319, 0.0148, 0.0120,\n",
       "          0.0161, 0.0166, 0.0139, 0.0141, 0.0158, 0.0161, 0.0168, 0.0172, 0.0229,\n",
       "          0.0169, 0.0182, 0.0219, 0.0206, 0.0212, 0.0199, 0.0207, 0.0227, 0.0244,\n",
       "          0.0237, 0.0250, 0.0301, 0.0305, 0.0221, 0.0332, 0.0281, 0.0328, 0.0372,\n",
       "          0.0283, 0.0286, 0.0448, 0.0147, 0.0152, 0.0144, 0.0145, 0.0151, 0.0158,\n",
       "          0.0135, 0.0161, 0.0151, 0.0156, 0.0141, 0.0178, 0.0197, 0.0180, 0.0223,\n",
       "          0.0194, 0.0203, 0.0251, 0.0257, 0.0229, 0.0276, 0.0246, 0.0234, 0.0288,\n",
       "          0.0303, 0.0275, 0.0503, 0.0301, 0.0312, 0.0456, 0.0372, 0.0357]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.norm.weight': tensor([5.7500, 5.7812, 5.7812,  ..., 5.9688, 5.7500, 4.4062]),\n",
       " 'vpm.encoder.layers.26.mlp.fc2.scales': tensor([[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0009]]),\n",
       " 'llm.model.layers.31.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.47.mlp.down_proj.qweight': tensor([[-1753699689, -1754843271,  2039847335,  ..., -1805028232,\n",
       "          -1503024266, -2001307001],\n",
       "         [-1734952614, -1719039591, -1719109256,  ..., -1971742344,\n",
       "           2023274860, -1702332007],\n",
       "         [-1752656472,  1770568102, -1250396056,  ...,  2052541336,\n",
       "          -2005559931, -1985574776],\n",
       "         ...,\n",
       "         [-1251579735,  1721206392, -1719101561,  ..., -1968597358,\n",
       "          -2021942649, -1717987415],\n",
       "         [ 1719171225, -1735890587,  1751554138,  ..., -1701144983,\n",
       "          -1721206661,  2023192729],\n",
       "         [ 1451718264, -1220123496, -2020903576,  ..., -1650939258,\n",
       "          -2003191671, -1753638761]], dtype=torch.int32),\n",
       " 'llm.model.layers.39.post_attention_layernorm.weight': tensor([1.2734, 1.2656, 0.9453,  ..., 0.9766, 1.4375, 0.2852]),\n",
       " 'llm.model.layers.11.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.5.mlp.down_proj.scales': tensor([[0.0246, 0.0259, 0.0262,  ..., 0.0281, 0.0254, 0.0285]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.46.input_layernorm.weight': tensor([0.7344, 0.6562, 0.5898,  ..., 0.5234, 0.7812, 2.0000]),\n",
       " 'vpm.encoder.layers.6.mlp.fc1.qzeros': tensor([[2139062143, 2139062143, 2139062143,  ..., 2139062143, 2139062143,\n",
       "          2139062143]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.25.self_attn.q_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.4.mlp.fc2.scales': tensor([[0.0010, 0.0012, 0.0009,  ..., 0.0010, 0.0014, 0.0010]]),\n",
       " 'llm.model.layers.27.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.22.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.13.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.12.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.21.mlp.fc2.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.17.self_attn.v_proj.scales': tensor([[0.0333, 0.0204, 0.0262, 0.0267, 0.0294, 0.0258, 0.0242, 0.0211, 0.0236,\n",
       "          0.0247, 0.0213, 0.0220, 0.0250, 0.0275, 0.0230, 0.0260, 0.0285, 0.0208,\n",
       "          0.0244, 0.0234, 0.0240, 0.0257, 0.0255, 0.0225, 0.0233, 0.0217, 0.0232,\n",
       "          0.0203, 0.0289, 0.0240, 0.0232, 0.0254, 0.0253, 0.0219, 0.0237, 0.0279,\n",
       "          0.0259, 0.0292, 0.0238, 0.0272, 0.0259, 0.0254, 0.0233, 0.0211, 0.0241,\n",
       "          0.0234, 0.0253, 0.0230, 0.0289, 0.0240, 0.0228, 0.0228, 0.0254, 0.0272,\n",
       "          0.0283, 0.0255, 0.0269, 0.0227, 0.0238, 0.0228, 0.0253, 0.0220, 0.0224,\n",
       "          0.0262, 0.0230, 0.0305, 0.0251, 0.0264, 0.0242, 0.0221, 0.0210, 0.0251,\n",
       "          0.0228, 0.0327, 0.0264, 0.0245, 0.0233, 0.0242, 0.0300, 0.0257, 0.0244,\n",
       "          0.0223, 0.0285, 0.0234, 0.0257, 0.0221, 0.0233, 0.0210, 0.0260, 0.0223,\n",
       "          0.0233, 0.0264, 0.0251, 0.0224, 0.0263, 0.0271, 0.0229, 0.0217, 0.0336,\n",
       "          0.0268, 0.0251, 0.0244, 0.0223, 0.0227, 0.0225, 0.0233, 0.0249, 0.0234,\n",
       "          0.0250, 0.0283, 0.0267, 0.0219, 0.0259, 0.0258, 0.0250, 0.0221, 0.0247,\n",
       "          0.0257, 0.0267, 0.0258, 0.0212, 0.0264, 0.0276, 0.0259, 0.0213, 0.0217,\n",
       "          0.0249, 0.0258, 0.0227, 0.0203, 0.0236, 0.0213, 0.0184, 0.0269, 0.0212,\n",
       "          0.0221, 0.0195, 0.0227, 0.0241, 0.0211, 0.0232, 0.0245, 0.0203, 0.0236,\n",
       "          0.0251, 0.0303, 0.0233, 0.0204, 0.0203, 0.0233, 0.0280, 0.0227, 0.0199,\n",
       "          0.0216, 0.0221, 0.0213, 0.0229, 0.0213, 0.0215, 0.0229, 0.0259, 0.0217,\n",
       "          0.0206, 0.0204, 0.0238, 0.0203, 0.0224, 0.0230, 0.0216, 0.0208, 0.0210,\n",
       "          0.0213, 0.0253, 0.0210, 0.0228, 0.0202, 0.0221, 0.0241, 0.0223, 0.0215,\n",
       "          0.0234, 0.0220, 0.0223, 0.0220, 0.0220, 0.0289, 0.0173, 0.0211, 0.0229,\n",
       "          0.0273, 0.0208, 0.0240, 0.0260, 0.0329, 0.0292, 0.0443, 0.0581, 0.0362,\n",
       "          0.0443, 0.0305, 0.0258, 0.0357, 0.0319, 0.0414, 0.0273, 0.0346, 0.0310,\n",
       "          0.0349, 0.0427, 0.0469, 0.0341, 0.0578, 0.0448, 0.0406, 0.0312, 0.0359,\n",
       "          0.0245, 0.0301, 0.0349, 0.0445, 0.0344, 0.0346, 0.0438, 0.0284, 0.0383,\n",
       "          0.0341, 0.0332, 0.0367, 0.0458, 0.0338, 0.0399, 0.0359, 0.0438, 0.0370,\n",
       "          0.0225, 0.0581, 0.0320, 0.0362, 0.0375, 0.0453, 0.0352, 0.0385, 0.0290,\n",
       "          0.0495, 0.0316, 0.0417, 0.0414, 0.0249, 0.0419, 0.0349, 0.0511, 0.0314,\n",
       "          0.0362, 0.0273, 0.0288, 0.0414, 0.0259, 0.0238, 0.0255, 0.0242, 0.0204,\n",
       "          0.0212, 0.0232, 0.0228, 0.0207, 0.0237, 0.0254, 0.0263, 0.0236, 0.0228,\n",
       "          0.0240, 0.0220, 0.0232, 0.0246, 0.0241, 0.0255, 0.0233, 0.0293, 0.0245,\n",
       "          0.0217, 0.0215, 0.0249, 0.0237, 0.0251, 0.0227, 0.0221, 0.0227, 0.0224,\n",
       "          0.0193, 0.0240, 0.0240, 0.0279, 0.0257, 0.0288, 0.0246, 0.0237, 0.0232,\n",
       "          0.0285, 0.0207, 0.0220, 0.0240, 0.0213, 0.0293, 0.0217, 0.0288, 0.0228,\n",
       "          0.0206, 0.0238, 0.0219, 0.0247, 0.0238, 0.0227, 0.0234, 0.0228, 0.0249,\n",
       "          0.0240, 0.0207, 0.0197, 0.0229, 0.0230, 0.0223, 0.0223, 0.0254, 0.0217,\n",
       "          0.0232, 0.0217, 0.0237, 0.0227, 0.0301, 0.0241, 0.0208, 0.0227, 0.0204,\n",
       "          0.0241, 0.0221, 0.0246, 0.0219, 0.0236, 0.0233, 0.0213, 0.0220, 0.0221,\n",
       "          0.0224, 0.0202, 0.0237, 0.0230, 0.0212, 0.0210, 0.0212, 0.0225, 0.0215,\n",
       "          0.0236, 0.0206, 0.0236, 0.0202, 0.0228, 0.0259, 0.0202, 0.0227, 0.0245,\n",
       "          0.0206, 0.0293, 0.0236, 0.0300, 0.0220, 0.0240, 0.0240, 0.0198, 0.0198,\n",
       "          0.0207, 0.0203, 0.0240, 0.0246, 0.0220, 0.0212, 0.0195, 0.0204, 0.0244,\n",
       "          0.0207, 0.0263, 0.0241, 0.0230, 0.0195, 0.0223, 0.0240, 0.0283, 0.0242,\n",
       "          0.0203, 0.0249, 0.0215, 0.0216, 0.0220, 0.0221, 0.0245, 0.0240, 0.0204,\n",
       "          0.0199, 0.0262, 0.0259, 0.0220, 0.0234, 0.0219, 0.0219, 0.0191, 0.0228,\n",
       "          0.0255, 0.0229, 0.0244, 0.0203, 0.0203, 0.0236, 0.0250, 0.0260, 0.0212,\n",
       "          0.0262, 0.0254, 0.0211, 0.0198, 0.0213, 0.0241, 0.0233, 0.0220, 0.0232,\n",
       "          0.0232, 0.0236, 0.0211, 0.0292, 0.0310, 0.0221, 0.0197, 0.0219, 0.0217,\n",
       "          0.0224, 0.0213, 0.0224, 0.0224, 0.0202, 0.0238, 0.0224, 0.0204, 0.0250,\n",
       "          0.0201, 0.0221, 0.0215, 0.0275, 0.0207, 0.0217, 0.0233, 0.0182, 0.0206,\n",
       "          0.0202, 0.0225, 0.0194, 0.0185, 0.0227, 0.0257, 0.0203, 0.0184, 0.0253,\n",
       "          0.0219, 0.0190, 0.0228, 0.0201, 0.0241, 0.0186, 0.0220, 0.0206, 0.0199,\n",
       "          0.0212, 0.0188, 0.0217, 0.0242, 0.0204, 0.0262, 0.0204, 0.0227, 0.0194,\n",
       "          0.0213, 0.0247, 0.0198, 0.0198, 0.0203, 0.0199, 0.0201, 0.0225, 0.0245,\n",
       "          0.0195, 0.0229, 0.0228, 0.0206, 0.0212, 0.0181, 0.0245, 0.0191, 0.0228,\n",
       "          0.0204, 0.0210, 0.0191, 0.0217, 0.0193, 0.0219, 0.0221, 0.0204, 0.0201,\n",
       "          0.0208, 0.0227, 0.0210, 0.0207, 0.0195, 0.0191, 0.0245, 0.0177]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.51.self_attn.k_proj.scales': tensor([[0.0199, 0.0153, 0.0154, 0.0182, 0.0163, 0.0159, 0.0153, 0.0145, 0.0118,\n",
       "          0.0156, 0.0194, 0.0159, 0.0190, 0.0148, 0.0186, 0.0213, 0.0210, 0.0260,\n",
       "          0.0185, 0.0182, 0.0184, 0.0182, 0.0219, 0.0180, 0.0198, 0.0199, 0.0174,\n",
       "          0.0181, 0.0178, 0.0221, 0.0269, 0.0254, 0.0144, 0.0150, 0.0148, 0.0143,\n",
       "          0.0194, 0.0165, 0.0151, 0.0188, 0.0173, 0.0141, 0.0172, 0.0126, 0.0215,\n",
       "          0.0163, 0.0173, 0.0227, 0.0139, 0.0301, 0.0195, 0.0201, 0.0172, 0.0171,\n",
       "          0.0207, 0.0167, 0.0181, 0.0195, 0.0289, 0.0193, 0.0259, 0.0211, 0.0230,\n",
       "          0.0171, 0.0202, 0.0166, 0.0104, 0.0137, 0.0171, 0.0118, 0.0171, 0.0114,\n",
       "          0.0162, 0.0144, 0.0159, 0.0169, 0.0193, 0.0166, 0.0160, 0.0185, 0.0141,\n",
       "          0.0168, 0.0172, 0.0172, 0.0172, 0.0182, 0.0182, 0.0177, 0.0202, 0.0322,\n",
       "          0.0191, 0.0188, 0.0349, 0.0223, 0.0202, 0.0233, 0.0115, 0.0133, 0.0098,\n",
       "          0.0152, 0.0133, 0.0112, 0.0160, 0.0114, 0.0149, 0.0131, 0.0227, 0.0125,\n",
       "          0.0168, 0.0145, 0.0162, 0.0148, 0.0185, 0.0166, 0.0185, 0.0211, 0.0143,\n",
       "          0.0166, 0.0156, 0.0177, 0.0161, 0.0077, 0.0167, 0.0253, 0.0208, 0.0178,\n",
       "          0.0215, 0.0169, 0.0158, 0.0135, 0.0134, 0.0133, 0.0208, 0.0158, 0.0126,\n",
       "          0.0155, 0.0186, 0.0177, 0.0212, 0.0166, 0.0158, 0.0168, 0.0193, 0.0173,\n",
       "          0.0189, 0.0173, 0.0178, 0.0189, 0.0189, 0.0193, 0.0204, 0.0202, 0.0202,\n",
       "          0.0090, 0.0190, 0.0225, 0.0134, 0.0206, 0.0180, 0.0123, 0.0115, 0.0216,\n",
       "          0.0171, 0.0135, 0.0159, 0.0144, 0.0154, 0.0141, 0.0184, 0.0157, 0.0190,\n",
       "          0.0199, 0.0128, 0.0180, 0.0174, 0.0172, 0.0164, 0.0176, 0.0208, 0.0152,\n",
       "          0.0186, 0.0177, 0.0197, 0.0211, 0.0242, 0.0215, 0.0241, 0.0195, 0.0198,\n",
       "          0.0198, 0.0101, 0.0135, 0.0118, 0.0118, 0.0111, 0.0114, 0.0130, 0.0105,\n",
       "          0.0113, 0.0133, 0.0119, 0.0131, 0.0107, 0.0148, 0.0172, 0.0263, 0.0149,\n",
       "          0.0188, 0.0289, 0.0176, 0.0212, 0.0208, 0.0191, 0.0242, 0.0223, 0.0208,\n",
       "          0.0341, 0.0176, 0.0262, 0.0191, 0.0241, 0.0224, 0.0260, 0.0328, 0.0115,\n",
       "          0.0137, 0.0122, 0.0124, 0.0102, 0.0137, 0.0143, 0.0126, 0.0124, 0.0133,\n",
       "          0.0127, 0.0159, 0.0122, 0.0122, 0.0152, 0.0194, 0.0267, 0.0164, 0.0257,\n",
       "          0.0177, 0.0250, 0.0544, 0.0234, 0.0215, 0.0529, 0.0213, 0.0225, 0.0236,\n",
       "          0.0228, 0.0319, 0.0207, 0.0277, 0.0111, 0.0118, 0.0134, 0.0141, 0.0144,\n",
       "          0.0120, 0.0142, 0.0137, 0.0116, 0.0155, 0.0148, 0.0134, 0.0189, 0.0135,\n",
       "          0.0174, 0.0158, 0.0160, 0.0184, 0.0157, 0.0191, 0.0173, 0.0180, 0.0172,\n",
       "          0.0206, 0.0193, 0.0169, 0.0161, 0.0215, 0.0365, 0.0148, 0.0217, 0.0188,\n",
       "          0.0127, 0.0111, 0.0131, 0.0112, 0.0126, 0.0104, 0.0118, 0.0143, 0.0134,\n",
       "          0.0137, 0.0159, 0.0116, 0.0145, 0.0140, 0.0181, 0.0159, 0.0166, 0.0135,\n",
       "          0.0161, 0.0145, 0.0154, 0.0193, 0.0163, 0.0160, 0.0171, 0.0180, 0.0201,\n",
       "          0.0190, 0.0202, 0.0166, 0.0198, 0.0275, 0.0092, 0.0165, 0.0181, 0.0151,\n",
       "          0.0169, 0.0156, 0.0150, 0.0193, 0.0184, 0.0159, 0.0188, 0.0171, 0.0182,\n",
       "          0.0154, 0.0165, 0.0155, 0.0174, 0.0207, 0.0127, 0.0169, 0.0165, 0.0210,\n",
       "          0.0184, 0.0199, 0.0185, 0.0194, 0.0201, 0.0144, 0.0184, 0.0191, 0.0184,\n",
       "          0.0201, 0.0143, 0.0168, 0.0171, 0.0142, 0.0153, 0.0201, 0.0167, 0.0197,\n",
       "          0.0164, 0.0166, 0.0198, 0.0165, 0.0191, 0.0165, 0.0182, 0.0189, 0.0171,\n",
       "          0.0213, 0.0189, 0.0191, 0.0180, 0.0219, 0.0197, 0.0155, 0.0184, 0.0174,\n",
       "          0.0174, 0.0268, 0.0168, 0.0172, 0.0180, 0.0178, 0.0139, 0.0139, 0.0132,\n",
       "          0.0146, 0.0150, 0.0130, 0.0156, 0.0145, 0.0145, 0.0144, 0.0137, 0.0133,\n",
       "          0.0156, 0.0140, 0.0165, 0.0198, 0.0165, 0.0197, 0.0188, 0.0118, 0.0182,\n",
       "          0.0190, 0.0195, 0.0193, 0.0280, 0.0188, 0.0181, 0.0211, 0.0157, 0.0221,\n",
       "          0.0258, 0.0204, 0.0132, 0.0145, 0.0133, 0.0130, 0.0137, 0.0139, 0.0124,\n",
       "          0.0133, 0.0137, 0.0156, 0.0126, 0.0147, 0.0153, 0.0182, 0.0155, 0.0133,\n",
       "          0.0193, 0.0161, 0.0181, 0.0236, 0.0193, 0.0210, 0.0162, 0.0188, 0.0207,\n",
       "          0.0197, 0.0188, 0.0244, 0.0146, 0.0171, 0.0169, 0.0144, 0.0113, 0.0097,\n",
       "          0.0112, 0.0113, 0.0153, 0.0110, 0.0105, 0.0131, 0.0133, 0.0113, 0.0145,\n",
       "          0.0148, 0.0146, 0.0186, 0.0145, 0.0162, 0.0157, 0.0201, 0.0191, 0.0238,\n",
       "          0.0188, 0.0341, 0.0201, 0.0277, 0.0246, 0.0227, 0.0359, 0.0217, 0.0257,\n",
       "          0.0315, 0.0213, 0.0244, 0.0103, 0.0104, 0.0122, 0.0124, 0.0121, 0.0118,\n",
       "          0.0113, 0.0150, 0.0133, 0.0130, 0.0143, 0.0134, 0.0139, 0.0168, 0.0174,\n",
       "          0.0163, 0.0162, 0.0166, 0.0189, 0.0176, 0.0177, 0.0246, 0.0191, 0.0194,\n",
       "          0.0232, 0.0195, 0.0385, 0.0236, 0.0212, 0.0309, 0.0288, 0.0281]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.11.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.20.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.21.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.34.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.50.self_attn.v_proj.scales': tensor([[0.0338, 0.0419, 0.0327, 0.0344, 0.0380, 0.0346, 0.0372, 0.0312, 0.0385,\n",
       "          0.0391, 0.0352, 0.0341, 0.0399, 0.0338, 0.0328, 0.0359, 0.0352, 0.0367,\n",
       "          0.0417, 0.0357, 0.0336, 0.0445, 0.0380, 0.0370, 0.0435, 0.0341, 0.0341,\n",
       "          0.0182, 0.0352, 0.0406, 0.0354, 0.0322, 0.0331, 0.0471, 0.0327, 0.0357,\n",
       "          0.0298, 0.0326, 0.0352, 0.0354, 0.0352, 0.0375, 0.0359, 0.0301, 0.0315,\n",
       "          0.0372, 0.0375, 0.0357, 0.0388, 0.0346, 0.0309, 0.0346, 0.0422, 0.0414,\n",
       "          0.0349, 0.0367, 0.0406, 0.0359, 0.0311, 0.0354, 0.0404, 0.0338, 0.0359,\n",
       "          0.0327, 0.0326, 0.0284, 0.0344, 0.0349, 0.0312, 0.0349, 0.0298, 0.0329,\n",
       "          0.0357, 0.0322, 0.0349, 0.0318, 0.0344, 0.0357, 0.0346, 0.0336, 0.0349,\n",
       "          0.0378, 0.0216, 0.0322, 0.0301, 0.0302, 0.0281, 0.0312, 0.0372, 0.0300,\n",
       "          0.0393, 0.0316, 0.0362, 0.0332, 0.0294, 0.0365, 0.0286, 0.0370, 0.0302,\n",
       "          0.0323, 0.0314, 0.0323, 0.0316, 0.0319, 0.0285, 0.0359, 0.0303, 0.0310,\n",
       "          0.0385, 0.0324, 0.0393, 0.0303, 0.0306, 0.0314, 0.0352, 0.0309, 0.0298,\n",
       "          0.0327, 0.0314, 0.0349, 0.0354, 0.0352, 0.0314, 0.0276, 0.0290, 0.0283,\n",
       "          0.0290, 0.0333, 0.0306, 0.0326, 0.0354, 0.0372, 0.0383, 0.0370, 0.0365,\n",
       "          0.0362, 0.0396, 0.0432, 0.0333, 0.0344, 0.0422, 0.0302, 0.0417, 0.0383,\n",
       "          0.0329, 0.0318, 0.0310, 0.0352, 0.0331, 0.0443, 0.0341, 0.0331, 0.0354,\n",
       "          0.0362, 0.0198, 0.0333, 0.0328, 0.0417, 0.0323, 0.0338, 0.0322, 0.0338,\n",
       "          0.0354, 0.0370, 0.0344, 0.0327, 0.0383, 0.0391, 0.0344, 0.0344, 0.0338,\n",
       "          0.0372, 0.0354, 0.0327, 0.0422, 0.0300, 0.0354, 0.0327, 0.0401, 0.0380,\n",
       "          0.0314, 0.0370, 0.0333, 0.0329, 0.0354, 0.0367, 0.0349, 0.0388, 0.0362,\n",
       "          0.0324, 0.0305, 0.0322, 0.0327, 0.0346, 0.0378, 0.0404, 0.0333, 0.0344,\n",
       "          0.0357, 0.0401, 0.0333, 0.0203, 0.0302, 0.0314, 0.0365, 0.0314, 0.0331,\n",
       "          0.0322, 0.0383, 0.0341, 0.0346, 0.0310, 0.0357, 0.0301, 0.0338, 0.0352,\n",
       "          0.0359, 0.0297, 0.0362, 0.0319, 0.0336, 0.0357, 0.0324, 0.0326, 0.0375,\n",
       "          0.0338, 0.0302, 0.0323, 0.0344, 0.0385, 0.0310, 0.0338, 0.0490, 0.0385,\n",
       "          0.0338, 0.0378, 0.0312, 0.0329, 0.0409, 0.0332, 0.0316, 0.0336, 0.0354,\n",
       "          0.0329, 0.0326, 0.0309, 0.0315, 0.0352, 0.0344, 0.0370, 0.0316, 0.0357,\n",
       "          0.0349, 0.0329, 0.0388, 0.0372, 0.0285, 0.0438, 0.0378, 0.0341, 0.0310,\n",
       "          0.0333, 0.0323, 0.0309, 0.0521, 0.0557, 0.0257, 0.0341, 0.0411, 0.0296,\n",
       "          0.0367, 0.0289, 0.0294, 0.0303, 0.0362, 0.0233, 0.0367, 0.0315, 0.0466,\n",
       "          0.0288, 0.0316, 0.0391, 0.0289, 0.0332, 0.0318, 0.0292, 0.0310, 0.0279,\n",
       "          0.0267, 0.0324, 0.0255, 0.0316, 0.0344, 0.0262, 0.0311, 0.0318, 0.0318,\n",
       "          0.0303, 0.0367, 0.0312, 0.0312, 0.0245, 0.0323, 0.0329, 0.0266, 0.0320,\n",
       "          0.0359, 0.0255, 0.0297, 0.0399, 0.0302, 0.0338, 0.0341, 0.0315, 0.0327,\n",
       "          0.0173, 0.0285, 0.0383, 0.0316, 0.0411, 0.0406, 0.0419, 0.0464, 0.0383,\n",
       "          0.0388, 0.0456, 0.0464, 0.0445, 0.0445, 0.0422, 0.0526, 0.0477, 0.0414,\n",
       "          0.0518, 0.0417, 0.0560, 0.0466, 0.0375, 0.0438, 0.0450, 0.0404, 0.0469,\n",
       "          0.0477, 0.0430, 0.0443, 0.0487, 0.0450, 0.0492, 0.0443, 0.0435, 0.0448,\n",
       "          0.0458, 0.0555, 0.0417, 0.0430, 0.0440, 0.0427, 0.0542, 0.0458, 0.0479,\n",
       "          0.0508, 0.0424, 0.0396, 0.0404, 0.0435, 0.0424, 0.0383, 0.0445, 0.0383,\n",
       "          0.0417, 0.0464, 0.0607, 0.0385, 0.0388, 0.0409, 0.0357, 0.0365, 0.0406,\n",
       "          0.0479, 0.0479, 0.0503, 0.0497, 0.0378, 0.0687, 0.0526, 0.0432, 0.0338,\n",
       "          0.0326, 0.0385, 0.0385, 0.0427, 0.0550, 0.0293, 0.0341, 0.0399, 0.0359,\n",
       "          0.0290, 0.0469, 0.0309, 0.0314, 0.0359, 0.0521, 0.0505, 0.0406, 0.0511,\n",
       "          0.0443, 0.0289, 0.0409, 0.0388, 0.0370, 0.0534, 0.0500, 0.0562, 0.0419,\n",
       "          0.0383, 0.0298, 0.0422, 0.0526, 0.0411, 0.0372, 0.0375, 0.0294, 0.0536,\n",
       "          0.0316, 0.0490, 0.0284, 0.0293, 0.0404, 0.0396, 0.0492, 0.0422, 0.0497,\n",
       "          0.0298, 0.0357, 0.0422, 0.0505, 0.0197, 0.0362, 0.0359, 0.0503, 0.0305,\n",
       "          0.0375, 0.0456, 0.0302, 0.0388, 0.0338, 0.0349, 0.0411, 0.0490, 0.0336,\n",
       "          0.0354, 0.0404, 0.0383, 0.0346, 0.0357, 0.0388, 0.0352, 0.0357, 0.0352,\n",
       "          0.0332, 0.0359, 0.0380, 0.0328, 0.0372, 0.0359, 0.0326, 0.0365, 0.0315,\n",
       "          0.0378, 0.0338, 0.0344, 0.0332, 0.0365, 0.0435, 0.0432, 0.0285, 0.0456,\n",
       "          0.0300, 0.0341, 0.0311, 0.0332, 0.0338, 0.0341, 0.0328, 0.0367, 0.0427,\n",
       "          0.0391, 0.0357, 0.0302, 0.0316, 0.0354, 0.0435, 0.0375, 0.0322, 0.0359,\n",
       "          0.0404, 0.0378, 0.0333, 0.0341, 0.0357, 0.0393, 0.0396, 0.0414, 0.0346,\n",
       "          0.0312, 0.0365, 0.0333, 0.0357, 0.0378, 0.0314, 0.0336, 0.0388]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.1.mlp.down_proj.scales': tensor([[0.0283, 0.0266, 0.0284,  ..., 0.0247, 0.0269, 0.0297]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.35.input_layernorm.weight': tensor([2.6250, 2.4062, 0.7344,  ..., 0.9414, 2.8125, 0.1201]),\n",
       " 'llm.model.layers.29.self_attn.k_proj.qweight': tensor([[-1750555015, -1514624646,  1771465608,  ..., -1769297798,\n",
       "           1466533482,  1467647865],\n",
       "         [-2038016102, -1484224647,  1753782406,  ...,  2028562295,\n",
       "          -2055727416, -1970620265],\n",
       "         [-2038859593, -1449494135, -1969595512,  ..., -1214945913,\n",
       "          -1706391654,   931706292],\n",
       "         ...,\n",
       "         [ 2003544725,  2054978442, -2037741387,  ..., -1735883321,\n",
       "          -1465546839,  2003323017],\n",
       "         [ 1468635018,  1419286407,  1972013496,  ...,  1736943990,\n",
       "          -1449694855,  1535736711],\n",
       "         [-1469409673,  1484363657, -1483233145,  ..., -1753389496,\n",
       "          -1786152072, -1468511817]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.3.layer_norm2.weight': tensor([1.0078, 0.8555, 0.4766,  ..., 1.1562, 0.5469, 0.9883]),\n",
       " 'llm.model.layers.38.self_attn.v_proj.qweight': tensor([[-1517709434, -2003929753,  1787723129,  ..., -1971808422,\n",
       "          -2004379542,  1736014216],\n",
       "         [-1448441977, -1514436440,  1733072985,  ..., -2009565317,\n",
       "          -1403418229, -2018932551],\n",
       "         [ 1752668582,  1758825624, -1971804810,  ..., -1737785990,\n",
       "          -1750435978, -1731679625],\n",
       "         ...,\n",
       "         [ 1970567095, -1467320496,  2005493372,  ..., -1148676186,\n",
       "          -1448376150, -1989966666],\n",
       "         [ 2005318007,  2039838871, -1719318118,  ...,  1735690648,\n",
       "          -1484175735,   982145144],\n",
       "         [ 1481405559, -2001954663,  1756202600,  ..., -1703442039,\n",
       "           2040895590,  2000451465]], dtype=torch.int32),\n",
       " 'llm.model.layers.9.self_attn.q_proj.qweight': tensor([[ 2038985881,  1500018566, -1451456855,  ...,  1770621304,\n",
       "           1466463588, -1734830218],\n",
       "         [ 1203396281,  1990683269, -2055693703,  ..., -2022217305,\n",
       "          -2005428088, -2006271880],\n",
       "         [-1718094007, -1233557188, -1488615769,  ...,  2006615930,\n",
       "           1804048809,  2021099673],\n",
       "         ...,\n",
       "         [  879522427, -2016627575, -1785103751,  ...,  1754879876,\n",
       "          -1787200616,  2006489481],\n",
       "         [-1988584566, -1466603675, -2022074005,  ...,  2057725272,\n",
       "          -1720416135,  2027526568],\n",
       "         [-2019972938,  1473087337, -1721079739,  ..., -1234724985,\n",
       "           -930641272,  1769568408]], dtype=torch.int32),\n",
       " 'llm.model.layers.41.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'vpm.post_layernorm.weight': tensor([1.5547, 1.3750, 1.5078,  ..., 1.5312, 1.6016, 1.3750]),\n",
       " 'llm.model.layers.19.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.38.self_attn.k_proj.qweight': tensor([[-1770341688, -1901627208,  1518045128,  ..., -2004318071,\n",
       "          -1165718953, -2023257946],\n",
       "         [ 1957136777, -2005694554, -1970694028,  ..., -2005362296,\n",
       "          -2020111513, -1986659977],\n",
       "         [ 2057930859, -2003072869,  2003921271,  ..., -2003269481,\n",
       "          -1450670424, -1717069158],\n",
       "         ...,\n",
       "         [-1468437848, -2040095834,  2072599430,  ..., -2004318072,\n",
       "          -1968715638,  2040170920],\n",
       "         [ 2006480550, -1755870314,  2005575845,  ..., -1750562696,\n",
       "          -1450673785, -1717200505],\n",
       "         [ 1769305703, -1431849098, -2022340471,  ..., -1734838136,\n",
       "          -2006541159,  1467389863]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.2.layer_norm1.weight': tensor([ 0.8555,  0.3242, -0.0021,  ...,  0.5000,  0.2168,  0.3965]),\n",
       " 'vpm.encoder.layers.14.mlp.fc1.bias': tensor([ 0.0182, -0.8867, -0.8945,  ..., -0.3887, -0.8320,  0.1406]),\n",
       " 'vpm.encoder.layers.15.self_attn.k_proj.qweight': tensor([[-1568173464, -1749587846,  1715779687,  ..., -1866762446,\n",
       "          -1735109250, -2003208798],\n",
       "         [ -610881339, -1785888878,  1501786711,  ...,  1868088686,\n",
       "           1481473442,  2069784946],\n",
       "         [-1950191484,  1822781836, -1453687165,  ..., -1885443257,\n",
       "          -1685684841, -1655462035],\n",
       "         ...,\n",
       "         [-1970493081, -1436309596,  1784717449,  ..., -1736008824,\n",
       "           1617664078, -1536263771],\n",
       "         [-1853315681, -1655078464,  1804441176,  ...,  1869585000,\n",
       "          -1653906258,  1213766263],\n",
       "         [ 1798925195,  1468307064, -1958456449,  ...,  1884245304,\n",
       "           1921752174, -1885767317]], dtype=torch.int32),\n",
       " 'llm.model.layers.30.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.32.mlp.up_proj.qweight': tensor([[ -628516457, -1736857224, -1165527145,  ...,  2037614728,\n",
       "           1751803528, -1702312074],\n",
       "         [-2057004436,  2053667226, -2038855528,  ..., -2004305515,\n",
       "          -1702405735,  2023271095],\n",
       "         [ 1739036842,  2023262858, -2054706775,  ..., -2006423431,\n",
       "          -1936095613, -1735892343],\n",
       "         ...,\n",
       "         [ 1214875258,  1740217015, -2003191133,  ..., -2003269481,\n",
       "          -1416063320,  2020247462],\n",
       "         [-2039957334,  2022156390,  2037889418,  ...,  2038929240,\n",
       "          -1988655220, -1938245945],\n",
       "         [ 2022148408, -1737910121, -1715890584,  ..., -2004379496,\n",
       "          -1217873991, -1804109398]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.3.self_attn.out_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.17.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.34.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.27.post_attention_layernorm.weight': tensor([1.8047, 2.0156, 0.9648,  ..., 1.0859, 1.9688, 0.1250]),\n",
       " 'llm.model.layers.23.self_attn.o_proj.qweight': tensor([[ 2022275453, -2037073481, -2006226536,  ..., -1718187623,\n",
       "          -1721071988, -1182172792],\n",
       "         [-1986295449,  1719176344, -1987540615,  ...,  1789363882,\n",
       "          -1483110023, -1736931448],\n",
       "         [-1468627558,  1417238857,  1485342615,  ..., -1196910742,\n",
       "          -1719162504,  2022148233],\n",
       "         ...,\n",
       "         [-1165400407,  1534502519, -1702257832,  ...,  2023188133,\n",
       "          -1984415109, -1986553722],\n",
       "         [-1716024919,  2038221240, -1702386329,  ...,  1468503687,\n",
       "          -1452652726, -1719171175],\n",
       "         [-1433896585,  2053609894,  1784043673,  ..., -2022131321,\n",
       "           2088401529, -1736865672]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.1.mlp.fc1.scales': tensor([[9.5358e-04, 1.0876e-03, 1.1781e-07,  ..., 9.5741e-04, 8.8848e-04,\n",
       "          9.5358e-04]]),\n",
       " 'vpm.encoder.layers.18.layer_norm1.bias': tensor([ 0.0962,  0.0023, -0.0171,  ...,  0.0483, -0.1099, -0.1079]),\n",
       " 'llm.model.layers.12.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.25.mlp.down_proj.scales': tensor([[0.0255, 0.0246, 0.0262,  ..., 0.0266, 0.0332, 0.0479]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.7.input_layernorm.weight': tensor([1.1172, 1.1484, 0.6758,  ..., 0.7070, 0.9648, 0.3262]),\n",
       " 'llm.model.layers.32.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.26.self_attn.q_proj.qweight': tensor([[-1817540719, -2038985582,  1417702028,  ..., -1957057157,\n",
       "           1954582876, -1517902494],\n",
       "         [ 1901165705, -1922261116, -1499289491,  ..., -2138804092,\n",
       "          -1485518468,  2139713906],\n",
       "         [ 1516805506, -1920363140, -1906418826,  ...,  -609860768,\n",
       "           1537297510,  1769903989],\n",
       "         ...,\n",
       "         [ 1821550228,  1619232384,  1871080839,  ..., -1587131016,\n",
       "          -1183748492, -2037095237],\n",
       "         [-1467572317, -1820746849, -1346537114,  ..., -2140768627,\n",
       "          -2005579445, -1834779011],\n",
       "         [-1383691644, -2022087577, -2070644675,  ..., -1834898053,\n",
       "          -1164076499, -1769702823]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.14.self_attn.out_proj.bias': tensor([0.0388, 0.1245, 0.0179,  ..., 0.0085, 0.1250, 0.1543]),\n",
       " 'vpm.encoder.layers.24.self_attn.k_proj.bias': tensor([-0.6406,  1.7969, -1.5859,  ..., -0.6953,  0.0157, -1.0938]),\n",
       " 'vpm.encoder.layers.10.self_attn.k_proj.scales': tensor([[0.0007, 0.0010, 0.0008,  ..., 0.0014, 0.0010, 0.0009]]),\n",
       " 'vpm.encoder.layers.3.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.8.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.25.self_attn.k_proj.qweight': tensor([[ 2102955382,  1885769532,  2137160863,  ...,  1619419515,\n",
       "           1282509900, -1923756458],\n",
       "         [ 2102355882,  2004100513,  1990878593,  ..., -1985705120,\n",
       "           1504031350, -1872202038],\n",
       "         [ 1032356228, -1955631973, -1433579108,  ...,  1888775834,\n",
       "           1852086653,  2040553026],\n",
       "         ...,\n",
       "         [ 1887067216, -1465562737, -1950187899,  ..., -1870435699,\n",
       "           1972016569, -1753569626],\n",
       "         [-1161325382,  2005952883,  2072344444,  ..., -1802936396,\n",
       "          -1771214441,  1885842356],\n",
       "         [ 1720756897,  1700497333, -1919858096,  ..., -1871862413,\n",
       "           1669089141,  2021608806]], dtype=torch.int32),\n",
       " 'llm.model.layers.46.self_attn.q_proj.scales': tensor([[0.0152, 0.0124, 0.0139,  ..., 0.0250, 0.0190, 0.0159]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.49.mlp.down_proj.scales': tensor([[0.0245, 0.0301, 0.0286,  ..., 0.0253, 0.0240, 0.0435]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.15.self_attn.v_proj.qweight': tensor([[-2140232815,  2018871699,  1603949713,  ..., -1738750117,\n",
       "           1683452780, -2006668654],\n",
       "         [ -550019173, -1867882594, -1701749420,  ...,  1826851201,\n",
       "           1936234335,   850169435],\n",
       "         [-2022521447,  1956991616,  2018926432,  ...,  1353866147,\n",
       "          -1467841934,  1919249261],\n",
       "         ...,\n",
       "         [-1934143885, -1654886507, -2007200344,  ...,  1710786940,\n",
       "          -1918985377,  1434869647],\n",
       "         [ 2073721479,  1635949682,  2020239470,  ...,   982946719,\n",
       "           1513652100, -1867624009],\n",
       "         [-1741326997,  2005432706, -1821158610,  ...,   849037956,\n",
       "           1532187509,  1803509647]], dtype=torch.int32),\n",
       " 'llm.model.layers.10.self_attn.o_proj.scales': tensor([[0.0172, 0.0227, 0.0208,  ..., 0.0199, 0.0229, 0.0280]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.48.mlp.down_proj.qweight': tensor([[-2038061691,  1454022009,  1503111336,  ...,  -361532278,\n",
       "           -962099079, -2018854794],\n",
       "         [ 1687656587,  2006534747, -1703384692,  ...,  1971885655,\n",
       "           1771473287,  1972991559],\n",
       "         [ 1702390919, -1988855659,  -611804998,  ..., -1217894505,\n",
       "          -1499891607,  2004269179],\n",
       "         ...,\n",
       "         [-1183343992, -1433892995, -1733706869,  ...,  1217043065,\n",
       "          -1768392823, -2040030361],\n",
       "         [-1197824137,  1765058698, -2037086074,  ...,  1774806906,\n",
       "           1535671689, -2018863131],\n",
       "         [-1435806120,  1986549354, -1485268360,  ...,  1992727161,\n",
       "           2006423176, -2003069050]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.20.mlp.fc2.scales': tensor([[0.0014, 0.0011, 0.0012,  ..., 0.0011, 0.0011, 0.0012]]),\n",
       " 'vpm.encoder.layers.8.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.5.layer_norm1.weight': tensor([0.6992, 0.5000, 0.6211,  ..., 0.6797, 0.5156, 0.6797]),\n",
       " 'llm.model.layers.42.self_attn.q_proj.qweight': tensor([[-1451775622,  -895980633,  1468623227,  ..., -1751754184,\n",
       "          -1484219480, -1465406826],\n",
       "         [-1483041125,  1716885351, -1467185043,  ..., -1971730551,\n",
       "          -1450689928,  2073453752],\n",
       "         [-1486194342,  2056747128, -2022275208,  ..., -2037946987,\n",
       "          -1986417590,  -915823016],\n",
       "         ...,\n",
       "         [ 1687705925,  2088212874,  2036811095,  ..., -1550288759,\n",
       "          -1451722872,  1755998104],\n",
       "         [-1735029595, -1181182104,  -947749962,  ..., -1468688501,\n",
       "           2007463511,   930395832],\n",
       "         [-1766274251,  2052478535, -1802982539,  ..., -1990769285,\n",
       "          -1720158298,  2024249943]], dtype=torch.int32),\n",
       " 'llm.model.layers.0.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.8.self_attn.out_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.26.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.15.mlp.up_proj.scales': tensor([[0.0253, 0.0237, 0.0242,  ..., 0.0210, 0.0215, 0.0230]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.21.mlp.gate_proj.qweight': tensor([[-2005366391, -1751488855,  2040104873,  ...,  1250724250,\n",
       "           1148680070,  2005628532],\n",
       "         [-1768454232, -2020824937, -1737000329,  ...,  2022341496,\n",
       "           2006362263,  2001235063],\n",
       "         [ 2022144136,  2071574489,  2056816793,  ..., -2008454263,\n",
       "           2022213770,  1989839530],\n",
       "         ...,\n",
       "         [-1718978423, -1718916519, -2040108377,  ...,  1936283545,\n",
       "          -1452766778,  1938716760],\n",
       "         [-2071357306, -1701136471, -1752590966,  ..., -1750685850,\n",
       "          -1933928092,   985308518],\n",
       "         [ 1736013944,  2005305721, -1969719161,  ..., -1684362618,\n",
       "          -1950841223,  1772706229]], dtype=torch.int32),\n",
       " 'llm.model.layers.43.mlp.gate_proj.qweight': tensor([[ 1756927721, -1737066584,  1486399692,  ..., -1469470856,\n",
       "          -2001107766, -2019006060],\n",
       "         [ -944016533,  1770625195, -1701279083,  ..., -1754556823,\n",
       "           1729858185,  2022088088],\n",
       "         [ 1751611514,   966229352,  2055633780,  ..., -1937209192,\n",
       "           2037872197, -1438077845],\n",
       "         ...,\n",
       "         [-2026288536, -1466460265,  1822878090,  ..., -1785100120,\n",
       "           2069326248,  2003347864],\n",
       "         [ 1789580972,  2023205796, -1971676776,  ..., -1987606359,\n",
       "           1503155590, -1737980311],\n",
       "         [-1724352107, -1400276392, -1735857769,  ..., -1750296966,\n",
       "          -1716877398, -1719027081]], dtype=torch.int32),\n",
       " 'llm.model.layers.49.self_attn.v_proj.qweight': tensor([[-1736009574, -1166501478,  2056619929,  ...,  1487501177,\n",
       "          -1737976938, -1468409189],\n",
       "         [-1985509208, -2004383385,  -913733994,  ...,  1437042616,\n",
       "          -1955035784,  -997816425],\n",
       "         [-1734895223, -1970570857, -2004244858,  ..., -1770489206,\n",
       "          -1753708200,  -967341973],\n",
       "         ...,\n",
       "         [-2036767352,  1787332740,  2007599243,  ..., -1987405432,\n",
       "          -1451788392,  1957117032],\n",
       "         [-1701353081,  2106222471, -2021095286,  ..., -1467447146,\n",
       "           1754892710,  1752869045],\n",
       "         [-1451718280,   930782089, -1198029177,  ..., -1974839127,\n",
       "          -2022152280, -1987799209]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.19.mlp.fc2.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.4.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.40.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.8.self_attn.out_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.42.self_attn.q_proj.scales': tensor([[0.0163, 0.0184, 0.0163,  ..., 0.0271, 0.0329, 0.0272]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.24.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.14.self_attn.v_proj.scales': tensor([[0.0007, 0.0008, 0.0008,  ..., 0.0008, 0.0007, 0.0008]]),\n",
       " 'llm.model.layers.26.self_attn.q_proj.scales': tensor([[0.0172, 0.0210, 0.0171,  ..., 0.0223, 0.0296, 0.0221]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.4.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.36.mlp.gate_proj.qweight': tensor([[-1718109592,  1758046840, -2024572778,  ...,  1499821751,\n",
       "          -2034780249, -1638368154],\n",
       "         [-1196972426, -2004314022,  -946366567,  ..., -2058721403,\n",
       "          -1216686474,  1404597384],\n",
       "         [ 1751414137,  2023279273, -1195795831,  ..., -1466592363,\n",
       "           1766372999,  -642218100],\n",
       "         ...,\n",
       "         [-1751603337, -2056672600,  1974126474,  ..., -2038078262,\n",
       "          -1182300006, -1754744711],\n",
       "         [  951040360,  2022156693,  1503249544,  ...,  1722324823,\n",
       "           1784895833, -1734834536],\n",
       "         [-1986274727, -1448577975, -1755757193,  ...,  1754749064,\n",
       "           1453947017, -1721278840]], dtype=torch.int32),\n",
       " 'vpm.embeddings.patch_embedding.bias': tensor([ 0.0957,  0.1797,  0.0136,  ..., -0.0610,  0.1367,  0.2949]),\n",
       " 'vpm.encoder.layers.5.self_attn.k_proj.scales': tensor([[0.0015, 0.0011, 0.0012,  ..., 0.0006, 0.0009, 0.0018]]),\n",
       " 'llm.model.layers.42.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.11.layer_norm1.bias': tensor([ 0.0094,  0.0132, -0.0012,  ..., -0.0205,  0.0496, -0.0486]),\n",
       " 'llm.model.layers.4.self_attn.v_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.32.self_attn.k_proj.qweight': tensor([[ 1987672713,  2003339159, -1732741222,  ...,  -692549002,\n",
       "          -1703379770, -1486318218],\n",
       "         [-1483175544,  1722062952,  -961706105,  ..., -1747412105,\n",
       "          -1720087693, -1487636360],\n",
       "         [ 2005575802, -1751868791,  1770550681,  ...,  2072545178,\n",
       "          -1718064503,  1234671192],\n",
       "         ...,\n",
       "         [ -896825480, -1183152501, -2034599257,  ..., -2002212665,\n",
       "          -1466524583,  2022139976],\n",
       "         [-2003203688, -1718191976,  1737070713,  ..., -2004502439,\n",
       "          -1986364810, -1736014423],\n",
       "         [-1753913225,  1772513144, -1468290935,  ..., -1958430522,\n",
       "          -2005494634, -1682528619]], dtype=torch.int32),\n",
       " 'llm.model.layers.43.self_attn.v_proj.qweight': tensor([[-1986294920,  1988401272, -2003065448,  ...,  1770690696,\n",
       "           1702398600, -1952946039],\n",
       "         [-1737849001, -1768453817, -1736931208,  ...,  2038990744,\n",
       "          -2002155369, -1735817079],\n",
       "         [ 2022275465, -2036950647, -2005371272,  ...,  2024765048,\n",
       "          -1970632825,  1754503017],\n",
       "         ...,\n",
       "         [-1735747464, -2054649447,  2037880713,  ..., -2023122554,\n",
       "           2052618115, -2003204195],\n",
       "         [ 1737005449,  1753716888,  2005370761,  ...,  2038925448,\n",
       "          -1954052473, -1668769657],\n",
       "         [-2006488952,  2022213767,  -929592456,  ...,  2022148489,\n",
       "           1971816296,  -964069239]], dtype=torch.int32),\n",
       " 'llm.model.layers.43.self_attn.k_proj.scales': tensor([[0.0105, 0.0115, 0.0142, 0.0101, 0.0119, 0.0115, 0.0156, 0.0184, 0.0097,\n",
       "          0.0188, 0.0117, 0.0189, 0.0148, 0.0216, 0.0146, 0.0259, 0.0247, 0.0155,\n",
       "          0.0215, 0.0233, 0.0275, 0.0240, 0.0247, 0.0204, 0.0246, 0.0241, 0.0215,\n",
       "          0.0250, 0.0255, 0.0249, 0.0241, 0.0230, 0.0115, 0.0138, 0.0148, 0.0120,\n",
       "          0.0141, 0.0107, 0.0109, 0.0158, 0.0160, 0.0213, 0.0149, 0.0227, 0.0163,\n",
       "          0.0189, 0.0158, 0.0198, 0.0233, 0.0212, 0.0182, 0.0221, 0.0217, 0.0204,\n",
       "          0.0221, 0.0204, 0.0271, 0.0232, 0.0195, 0.0346, 0.0259, 0.0242, 0.0215,\n",
       "          0.0213, 0.0117, 0.0129, 0.0111, 0.0126, 0.0110, 0.0124, 0.0100, 0.0154,\n",
       "          0.0121, 0.0189, 0.0145, 0.0150, 0.0128, 0.0169, 0.0141, 0.0154, 0.0159,\n",
       "          0.0154, 0.0154, 0.0189, 0.0221, 0.0202, 0.0199, 0.0234, 0.0234, 0.0331,\n",
       "          0.0233, 0.0306, 0.0204, 0.0237, 0.0221, 0.0255, 0.0106, 0.0127, 0.0115,\n",
       "          0.0136, 0.0104, 0.0119, 0.0136, 0.0143, 0.0092, 0.0148, 0.0106, 0.0168,\n",
       "          0.0172, 0.0173, 0.0150, 0.0190, 0.0174, 0.0194, 0.0168, 0.0172, 0.0223,\n",
       "          0.0241, 0.0216, 0.0225, 0.0247, 0.0107, 0.0220, 0.0210, 0.0275, 0.0207,\n",
       "          0.0238, 0.0216, 0.0094, 0.0087, 0.0080, 0.0128, 0.0102, 0.0117, 0.0137,\n",
       "          0.0124, 0.0163, 0.0121, 0.0152, 0.0178, 0.0113, 0.0217, 0.0168, 0.0189,\n",
       "          0.0227, 0.0242, 0.0253, 0.0220, 0.0264, 0.0266, 0.0269, 0.0424, 0.0176,\n",
       "          0.0318, 0.0283, 0.0266, 0.0283, 0.0318, 0.0341, 0.0305, 0.0132, 0.0168,\n",
       "          0.0113, 0.0109, 0.0107, 0.0117, 0.0127, 0.0147, 0.0126, 0.0143, 0.0126,\n",
       "          0.0159, 0.0160, 0.0178, 0.0173, 0.0284, 0.0174, 0.0292, 0.0311, 0.0258,\n",
       "          0.0290, 0.0241, 0.0329, 0.0305, 0.0314, 0.0301, 0.0280, 0.0296, 0.0388,\n",
       "          0.0294, 0.0237, 0.0279, 0.0099, 0.0092, 0.0118, 0.0094, 0.0119, 0.0107,\n",
       "          0.0111, 0.0096, 0.0116, 0.0118, 0.0126, 0.0089, 0.0146, 0.0148, 0.0132,\n",
       "          0.0122, 0.0171, 0.0141, 0.0167, 0.0189, 0.0210, 0.0203, 0.0221, 0.0204,\n",
       "          0.0204, 0.0259, 0.0284, 0.0223, 0.0242, 0.0264, 0.0215, 0.0172, 0.0089,\n",
       "          0.0112, 0.0109, 0.0105, 0.0096, 0.0117, 0.0098, 0.0117, 0.0099, 0.0133,\n",
       "          0.0116, 0.0141, 0.0137, 0.0144, 0.0144, 0.0164, 0.0152, 0.0189, 0.0211,\n",
       "          0.0201, 0.0225, 0.0125, 0.0172, 0.0208, 0.0202, 0.0225, 0.0301, 0.0264,\n",
       "          0.0242, 0.0253, 0.0259, 0.0254, 0.0211, 0.0162, 0.0167, 0.0225, 0.0111,\n",
       "          0.0230, 0.0182, 0.0107, 0.0223, 0.0109, 0.0160, 0.0142, 0.0195, 0.0210,\n",
       "          0.0213, 0.0220, 0.0225, 0.0191, 0.0212, 0.0199, 0.0190, 0.0234, 0.0237,\n",
       "          0.0211, 0.0213, 0.0237, 0.0385, 0.0210, 0.0225, 0.0198, 0.0224, 0.0250,\n",
       "          0.0158, 0.0305, 0.0140, 0.0204, 0.0163, 0.0173, 0.0202, 0.0163, 0.0202,\n",
       "          0.0177, 0.0198, 0.0164, 0.0202, 0.0208, 0.0122, 0.0249, 0.0306, 0.0208,\n",
       "          0.0135, 0.0204, 0.0246, 0.0212, 0.0202, 0.0208, 0.0225, 0.0238, 0.0148,\n",
       "          0.0223, 0.0242, 0.0176, 0.0213, 0.0211, 0.0102, 0.0098, 0.0099, 0.0098,\n",
       "          0.0130, 0.0098, 0.0122, 0.0117, 0.0127, 0.0116, 0.0107, 0.0127, 0.0091,\n",
       "          0.0135, 0.0094, 0.0158, 0.0220, 0.0228, 0.0178, 0.0213, 0.0186, 0.0286,\n",
       "          0.0365, 0.0388, 0.0285, 0.0268, 0.0186, 0.0362, 0.0241, 0.0331, 0.0288,\n",
       "          0.0283, 0.0135, 0.0102, 0.0111, 0.0116, 0.0082, 0.0078, 0.0103, 0.0085,\n",
       "          0.0117, 0.0089, 0.0115, 0.0133, 0.0120, 0.0115, 0.0161, 0.0163, 0.0168,\n",
       "          0.0194, 0.0161, 0.0163, 0.0189, 0.0184, 0.0158, 0.0380, 0.0341, 0.0172,\n",
       "          0.0260, 0.0292, 0.0290, 0.0241, 0.0253, 0.0206, 0.0111, 0.0079, 0.0104,\n",
       "          0.0111, 0.0095, 0.0118, 0.0114, 0.0123, 0.0135, 0.0150, 0.0104, 0.0127,\n",
       "          0.0139, 0.0173, 0.0143, 0.0146, 0.0172, 0.0139, 0.0194, 0.0201, 0.0260,\n",
       "          0.0246, 0.0232, 0.0233, 0.0262, 0.0253, 0.0272, 0.0197, 0.0219, 0.0204,\n",
       "          0.0264, 0.0188, 0.0083, 0.0104, 0.0111, 0.0084, 0.0132, 0.0099, 0.0126,\n",
       "          0.0101, 0.0084, 0.0137, 0.0106, 0.0152, 0.0090, 0.0154, 0.0118, 0.0154,\n",
       "          0.0189, 0.0168, 0.0221, 0.0181, 0.0259, 0.0137, 0.0221, 0.0260, 0.0219,\n",
       "          0.0189, 0.0215, 0.0215, 0.0216, 0.0210, 0.0268, 0.0191, 0.0108, 0.0087,\n",
       "          0.0104, 0.0124, 0.0118, 0.0096, 0.0122, 0.0117, 0.0118, 0.0150, 0.0126,\n",
       "          0.0132, 0.0152, 0.0181, 0.0146, 0.0184, 0.0186, 0.0201, 0.0236, 0.0211,\n",
       "          0.0246, 0.0193, 0.0281, 0.0194, 0.0253, 0.0370, 0.0292, 0.0262, 0.0293,\n",
       "          0.0323, 0.0310, 0.0354, 0.0107, 0.0092, 0.0082, 0.0106, 0.0106, 0.0107,\n",
       "          0.0126, 0.0107, 0.0130, 0.0101, 0.0114, 0.0121, 0.0125, 0.0181, 0.0117,\n",
       "          0.0186, 0.0177, 0.0198, 0.0204, 0.0201, 0.0281, 0.0212, 0.0276, 0.0275,\n",
       "          0.0267, 0.0327, 0.0338, 0.0383, 0.0237, 0.0293, 0.0280, 0.0383]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.19.layer_norm1.weight': tensor([1.2031, 1.1562, 1.2109,  ..., 1.1641, 1.1172, 1.2031]),\n",
       " 'llm.model.layers.4.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.24.self_attn.q_proj.qweight': tensor([[-1887924352, -2088673181,  1650298262,  ...,  1432327304,\n",
       "          -2040291745,  1366091881],\n",
       "         [-1838654310, -1835231637,  2004779886,  ...,  1986888296,\n",
       "            762152831, -1852151369],\n",
       "         [ 1533393037, -2074373951, -1705989240,  ...,  1215061893,\n",
       "          -2006995331, -1833272184],\n",
       "         ...,\n",
       "         [ 1816357513,  1690276778,  1452849314,  ..., -2090569872,\n",
       "          -1419412071, -1048328570],\n",
       "         [-1134083471,  1301304166,  1934594869,  ..., -1784909677,\n",
       "           1665239460,  1178426196],\n",
       "         [ 2138874976,  1935042230,  1429939832,  ..., -2055302770,\n",
       "          -1952417359,  1935624095]], dtype=torch.int32),\n",
       " 'llm.model.layers.45.mlp.gate_proj.scales': tensor([[0.0242, 0.0268, 0.0262,  ..., 0.0215, 0.0220, 0.0206]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.19.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.15.mlp.down_proj.scales': tensor([[0.0263, 0.0241, 0.0234,  ..., 0.0233, 0.0228, 0.0323]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.12.self_attn.q_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.18.self_attn.k_proj.scales': tensor([[0.0207, 0.0163, 0.0229, 0.0168, 0.0191, 0.0220, 0.0188, 0.0251, 0.0212,\n",
       "          0.0232, 0.0199, 0.0249, 0.0238, 0.0250, 0.0171, 0.0237, 0.0306, 0.0272,\n",
       "          0.0279, 0.0258, 0.0244, 0.0812, 0.0298, 0.0792, 0.0776, 0.0594, 0.0482,\n",
       "          0.0290, 0.0290, 0.0311, 0.0318, 0.0714, 0.0241, 0.0219, 0.0264, 0.0211,\n",
       "          0.0180, 0.0293, 0.0169, 0.0240, 0.0172, 0.0273, 0.0247, 0.0279, 0.0162,\n",
       "          0.0289, 0.0362, 0.0302, 0.0221, 0.0289, 0.0230, 0.0241, 0.0276, 0.0482,\n",
       "          0.0411, 0.0280, 0.0484, 0.0284, 0.0823, 0.0341, 0.0401, 0.0372, 0.0279,\n",
       "          0.0396, 0.0314, 0.0314, 0.0163, 0.0132, 0.0159, 0.0213, 0.0215, 0.0152,\n",
       "          0.0242, 0.0210, 0.0249, 0.0233, 0.0221, 0.0275, 0.0199, 0.0262, 0.0237,\n",
       "          0.0310, 0.0307, 0.0297, 0.0272, 0.0221, 0.0249, 0.0312, 0.0341, 0.0326,\n",
       "          0.0456, 0.0244, 0.0266, 0.0306, 0.0302, 0.0293, 0.0207, 0.0145, 0.0146,\n",
       "          0.0163, 0.0173, 0.0186, 0.0203, 0.0178, 0.0199, 0.0242, 0.0210, 0.0227,\n",
       "          0.0228, 0.0219, 0.0266, 0.0288, 0.0238, 0.0250, 0.0220, 0.0262, 0.0310,\n",
       "          0.0332, 0.0296, 0.0259, 0.0329, 0.0271, 0.0354, 0.0246, 0.0273, 0.0280,\n",
       "          0.0236, 0.0232, 0.0240, 0.0198, 0.0185, 0.0310, 0.0171, 0.0298, 0.0267,\n",
       "          0.0262, 0.0191, 0.0264, 0.0257, 0.0249, 0.0225, 0.0259, 0.0229, 0.0260,\n",
       "          0.0300, 0.0316, 0.0320, 0.0288, 0.0311, 0.0300, 0.0251, 0.0232, 0.0314,\n",
       "          0.0461, 0.0280, 0.0280, 0.0301, 0.0263, 0.0262, 0.0268, 0.0157, 0.0194,\n",
       "          0.0135, 0.0208, 0.0207, 0.0236, 0.0195, 0.0253, 0.0233, 0.0246, 0.0232,\n",
       "          0.0262, 0.0223, 0.0273, 0.0279, 0.0266, 0.0332, 0.0246, 0.0286, 0.0292,\n",
       "          0.0238, 0.0296, 0.0260, 0.0273, 0.0318, 0.0490, 0.0281, 0.0315, 0.0293,\n",
       "          0.0293, 0.0281, 0.0284, 0.0220, 0.0240, 0.0161, 0.0225, 0.0229, 0.0190,\n",
       "          0.0225, 0.0174, 0.0230, 0.0237, 0.0210, 0.0305, 0.0197, 0.0269, 0.0292,\n",
       "          0.0259, 0.0258, 0.0266, 0.0303, 0.0281, 0.0202, 0.0283, 0.0236, 0.0262,\n",
       "          0.0172, 0.0249, 0.0258, 0.0210, 0.0249, 0.0188, 0.0245, 0.0206, 0.0177,\n",
       "          0.0169, 0.0193, 0.0238, 0.0260, 0.0172, 0.0169, 0.0206, 0.0202, 0.0328,\n",
       "          0.0221, 0.0263, 0.0234, 0.0241, 0.0300, 0.0227, 0.0229, 0.0230, 0.0272,\n",
       "          0.0264, 0.0254, 0.0284, 0.0233, 0.0276, 0.0286, 0.0266, 0.0233, 0.0152,\n",
       "          0.0297, 0.0266, 0.0232, 0.0260, 0.0223, 0.0225, 0.0178, 0.0150, 0.0213,\n",
       "          0.0204, 0.0290, 0.0203, 0.0272, 0.0285, 0.0215, 0.0289, 0.0286, 0.0253,\n",
       "          0.0279, 0.0247, 0.0245, 0.0354, 0.0188, 0.0284, 0.0246, 0.0315, 0.0288,\n",
       "          0.0272, 0.0259, 0.0406, 0.0293, 0.0267, 0.0269, 0.0264, 0.0283, 0.0338,\n",
       "          0.0195, 0.0208, 0.0146, 0.0153, 0.0206, 0.0178, 0.0233, 0.0234, 0.0262,\n",
       "          0.0277, 0.0221, 0.0257, 0.0245, 0.0319, 0.0255, 0.0253, 0.0284, 0.0250,\n",
       "          0.0303, 0.0344, 0.0283, 0.0269, 0.0284, 0.0242, 0.0247, 0.0417, 0.0257,\n",
       "          0.0267, 0.0275, 0.0233, 0.0262, 0.0276, 0.0161, 0.0164, 0.0152, 0.0208,\n",
       "          0.0171, 0.0198, 0.0207, 0.0208, 0.0240, 0.0181, 0.0227, 0.0241, 0.0290,\n",
       "          0.0217, 0.0275, 0.0220, 0.0268, 0.0310, 0.0272, 0.0264, 0.0289, 0.0296,\n",
       "          0.0290, 0.0251, 0.0272, 0.0404, 0.0318, 0.0283, 0.0294, 0.0319, 0.0281,\n",
       "          0.0349, 0.0181, 0.0184, 0.0189, 0.0215, 0.0143, 0.0320, 0.0224, 0.0257,\n",
       "          0.0301, 0.0228, 0.0233, 0.0244, 0.0238, 0.0245, 0.0255, 0.0276, 0.0253,\n",
       "          0.0281, 0.0306, 0.0438, 0.0247, 0.0272, 0.0264, 0.0264, 0.0224, 0.0453,\n",
       "          0.0242, 0.0254, 0.0244, 0.0298, 0.0272, 0.0314, 0.0148, 0.0213, 0.0181,\n",
       "          0.0191, 0.0159, 0.0203, 0.0164, 0.0273, 0.0238, 0.0173, 0.0193, 0.0202,\n",
       "          0.0227, 0.0206, 0.0208, 0.0236, 0.0228, 0.0260, 0.0237, 0.0236, 0.0297,\n",
       "          0.0202, 0.0215, 0.0238, 0.0227, 0.0354, 0.0375, 0.0266, 0.0279, 0.0220,\n",
       "          0.0216, 0.0284, 0.0156, 0.0204, 0.0146, 0.0237, 0.0144, 0.0212, 0.0186,\n",
       "          0.0220, 0.0174, 0.0194, 0.0267, 0.0232, 0.0220, 0.0188, 0.0244, 0.0303,\n",
       "          0.0210, 0.0242, 0.0197, 0.0247, 0.0280, 0.0233, 0.0262, 0.0271, 0.0458,\n",
       "          0.0375, 0.0333, 0.0338, 0.0225, 0.0258, 0.0284, 0.0297, 0.0290, 0.0233,\n",
       "          0.0259, 0.0221, 0.0241, 0.0202, 0.0236, 0.0210, 0.0318, 0.0241, 0.0288,\n",
       "          0.0253, 0.0242, 0.0263, 0.0203, 0.0283, 0.0259, 0.0244, 0.0288, 0.0275,\n",
       "          0.0293, 0.0267, 0.0300, 0.0249, 0.0272, 0.0290, 0.0259, 0.0328, 0.0320,\n",
       "          0.0327, 0.0285, 0.0292, 0.0216, 0.0223, 0.0159, 0.0298, 0.0208, 0.0225,\n",
       "          0.0260, 0.0272, 0.0322, 0.0241, 0.0269, 0.0267, 0.0275, 0.0286, 0.0229,\n",
       "          0.0273, 0.0284, 0.0259, 0.0294, 0.0293, 0.0255, 0.0263, 0.0290, 0.0283,\n",
       "          0.0302, 0.0264, 0.0228, 0.0310, 0.0448, 0.0253, 0.0301, 0.0283]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.18.mlp.fc1.bias': tensor([-0.1631,  0.0562, -0.6641,  ..., -0.7070, -0.4980, -0.4004]),\n",
       " 'llm.model.layers.42.self_attn.k_proj.qweight': tensor([[-1448635030,  -695711658, -1517644119,  ..., -2021234568,\n",
       "           2004322680, -2022016102],\n",
       "         [-1468704601, -1198293335, -1464432004,  ..., -1988503657,\n",
       "          -1986586488,  -645306406],\n",
       "         [-1164194936,  1753974407,  1212843622,  ..., -2022078311,\n",
       "          -1735886456,  2006477688],\n",
       "         ...,\n",
       "         [-1751599737,  2053671542,  1769450380,  ..., -2004252536,\n",
       "           2037880711, -1500940937],\n",
       "         [ 1216977046,  1434879049,  1469416793,  ..., -1735825016,\n",
       "          -1751676536,  2005571928],\n",
       "         [-1766360678, -1518830983, -1768314982,  ..., -1987676808,\n",
       "          -1987553911, -1450677942]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.18.mlp.fc2.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.7.self_attn.q_proj.scales': tensor([[0.0165, 0.0134, 0.0135,  ..., 0.0253, 0.0279, 0.0277]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.25.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.14.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.13.mlp.fc2.bias': tensor([ 0.0032, -0.1230, -0.0189,  ..., -0.1494,  0.2041,  0.0209]),\n",
       " 'llm.model.layers.42.mlp.gate_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.8.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.51.self_attn.v_proj.qweight': tensor([[-1987553415,  1769576326,  2020239271,  ..., -1986418042,\n",
       "           1469548683, -1971873912],\n",
       "         [-1721006198, -1742173575,  2019920486,  ..., -1722251351,\n",
       "          -1752660085,  2024581513],\n",
       "         [-1164420741,  1183287462, -1803970423,  ...,  2023196808,\n",
       "          -2005100936, -1753712262],\n",
       "         ...,\n",
       "         [ 1764190279,  2021170040, -1733715306,  ..., -2022078593,\n",
       "           -964970069, -2019973022],\n",
       "         [-1719293548, -1735961496, -2004112790,  ...,  2039060584,\n",
       "           2038797675, -1971869577],\n",
       "         [-1485203320, -1751681368, -1707756150,  ...,  1753638999,\n",
       "           -878081400,  1517840264]], dtype=torch.int32),\n",
       " 'llm.model.layers.25.mlp.down_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.17.self_attn.k_proj.bias': tensor([ 0.3535, -0.2676, -0.6953,  ...,  0.4414, -0.0801, -0.7344]),\n",
       " 'llm.model.layers.31.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.4.post_attention_layernorm.weight': tensor([1.0781, 0.8828, 0.6992,  ..., 0.7305, 0.8633, 0.2080]),\n",
       " 'vpm.encoder.layers.25.layer_norm1.bias': tensor([-0.1182, -0.0103,  0.0136,  ...,  0.1582, -0.4727,  0.0454]),\n",
       " 'llm.model.layers.49.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.16.self_attn.out_proj.bias': tensor([ 0.0193, -0.1162,  0.1147,  ...,  0.0815, -0.0122,  0.0640]),\n",
       " 'llm.model.layers.10.post_attention_layernorm.weight': tensor([1.3125, 1.1641, 0.9297,  ..., 0.9883, 1.1406, 0.2129]),\n",
       " 'vpm.encoder.layers.24.self_attn.q_proj.scales': tensor([[0.0008, 0.0007, 0.0007,  ..., 0.0008, 0.0008, 0.0007]]),\n",
       " 'llm.model.layers.33.mlp.down_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.25.mlp.fc1.bias': tensor([ 0.4316,  0.2100,  0.0233,  ..., -0.3730,  0.0864,  0.0693]),\n",
       " 'llm.model.layers.15.mlp.down_proj.qweight': tensor([[-1718114678, -1447720298, -1764400506,  ..., -1950918328,\n",
       "           2020116647, -1735869574],\n",
       "         [ 1986620057,  1467594936, -1737788070,  ...,  1769777338,\n",
       "           1752791963, -1987610233],\n",
       "         [-1484232568, -1967625557, -2055698794,  ..., -2002163848,\n",
       "          -1987352201, -1704417110],\n",
       "         ...,\n",
       "         [ 2021169256, -1969842329,  1468697242,  ..., -1165392490,\n",
       "           1723500920,  1700300424],\n",
       "         [-1956144776, -1971882617, -1938187686,  ...,  -933590696,\n",
       "           1771391097,  2005575289],\n",
       "         [-1735891064,  2019068008,  2042123658,  ...,  2041169830,\n",
       "          -1942509464,  2052565097]], dtype=torch.int32),\n",
       " 'llm.model.layers.3.self_attn.o_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.25.layer_norm1.weight': tensor([1.1406, 1.0312, 1.1328,  ..., 1.0391, 1.0781, 1.0781]),\n",
       " 'llm.model.layers.33.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.39.self_attn.v_proj.qweight': tensor([[ 1720222806, -1465276262, -1179985798,  ...,  2038081625,\n",
       "          -1431607882,  1704765595],\n",
       "         [ 1753708409,  2022213780,  -927508586,  ...,  2021230746,\n",
       "          -1130919063, -1449696359],\n",
       "         [-1515542568,  2018994600, -1936103768,  ...,  1519023287,\n",
       "          -2037799276, -1434093482],\n",
       "         ...,\n",
       "         [ 1771546216, -2023204232, -1685480343,  ...,  1199548777,\n",
       "           2041350026,  1720269991],\n",
       "         [ 1751698040,  -124352408, -1466319977,  ...,  2021112456,\n",
       "          -2004198857, -1268562008],\n",
       "         [-2018023577, -1734752186, -2023188354,  ..., -1668761960,\n",
       "           2055969399, -1731553685]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.19.mlp.fc2.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.19.mlp.fc1.bias': tensor([-0.5078, -0.1387,  0.1787,  ..., -0.4102, -0.7461, -0.3594]),\n",
       " 'vpm.encoder.layers.26.self_attn.q_proj.scales': tensor([[0.0008, 0.0010, 0.0007,  ..., 0.0007, 0.0006, 0.0010]]),\n",
       " 'llm.model.layers.44.self_attn.q_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.9.self_attn.v_proj.qweight': tensor([[-1937391227,  2009034619,  1815581035,  ..., -1885760135,\n",
       "          -1922599793, -1968937860],\n",
       "         [ 2105106514, -2053213539, -1886548095,  ..., -1984925374,\n",
       "           1754370238, -1802535808],\n",
       "         [ 1941598860, -1972801670,  1634699608,  ..., -1955498861,\n",
       "          -2040701513, -2055448501],\n",
       "         ...,\n",
       "         [ 1803120495,  1385522295,   964920453,  ...,  1903917673,\n",
       "           2106282183,  1350088863],\n",
       "         [-2024115061,  -713845881, -2073662086,  ...,  2018806940,\n",
       "           1903073366,  1853504385],\n",
       "         [-1652724328, -1552524454, -1651205724,  ..., -1420128822,\n",
       "           1669323859, -1623290723]], dtype=torch.int32),\n",
       " 'llm.model.layers.16.self_attn.v_proj.qweight': tensor([[ 1285064775, -1502048874,  1823028887,  ..., -1704281893,\n",
       "          -1466336856,  1491945398],\n",
       "         [ -931366517,  1955105411, -1449554854,  ...,  1231779479,\n",
       "           -947418979,  1546025369],\n",
       "         [ 2038814540,   933861239,  2006358340,  ...,  1757059719,\n",
       "           2035849351, -2009614216],\n",
       "         ...,\n",
       "         [ 2039834267, -1719425383, -1969891209,  ..., -1532255111,\n",
       "          -1986624073,  2055833210],\n",
       "         [-1450800748,  1502381629, -1249158537,  ...,  1770355306,\n",
       "          -1719241112, -1719233126],\n",
       "         [ 2041358153, -1784379285, -1967536202,  ..., -1734964841,\n",
       "          -1452689477,  1921418582]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.2.mlp.fc2.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.25.self_attn.v_proj.qweight': tensor([[-1246927472,  2137821828, -1752672859,  ...,  1803653473,\n",
       "           1839604894, -2034209651],\n",
       "         [ 1682936957, -2089633166, -1520930419,  ..., -2001705108,\n",
       "           1537645382, -1819647855],\n",
       "         [-2038128283,  2090495646, -1840754035,  ...,  2139640463,\n",
       "          -2104917114,  2118085777],\n",
       "         ...,\n",
       "         [ 2123452979,  1787271055, -1722384224,  ...,  1552644787,\n",
       "           1252963441,  2006094976],\n",
       "         [ -685652881,  1755752621, -1736148409,  ...,  1649313120,\n",
       "          -2038784135,  1537838727],\n",
       "         [-2127065745, -2052621227, -1215770505,  ...,  1431796872,\n",
       "           1469934670,  1651146128]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.24.self_attn.k_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.40.mlp.up_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.10.mlp.gate_proj.scales': tensor([[0.0236, 0.0212, 0.0310,  ..., 0.0233, 0.0202, 0.0219]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.9.self_attn.o_proj.qweight': tensor([[ -982864953,   697604277,  -717792840,  ..., -1734054744,\n",
       "          -1701414314,  1720088934],\n",
       "         [ 2004322473, -1769564278, -1466255240,  ..., -2057600635,\n",
       "          -1481151608,  -946304632],\n",
       "         [-1199916901, -1220123239, -1969707113,  ...,  1757107875,\n",
       "           2022222166, -1195660421],\n",
       "         ...,\n",
       "         [ 1486247579,  1755944842,  2054797254,  ..., -2054449291,\n",
       "          -1987384742, -2020046233],\n",
       "         [ 1750553992,  2035771305,  1753392761,  ..., -1184450394,\n",
       "           1232570980, -1735747703],\n",
       "         [-1736779115,  1769633419,  2060102295,  ..., -2070435683,\n",
       "           1804056742, -1773488456]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.6.mlp.fc2.bias': tensor([ 0.0618, -0.0586,  0.0913,  ...,  0.0684,  0.0505,  0.1128]),\n",
       " 'llm.model.layers.50.mlp.up_proj.scales': tensor([[0.0223, 0.0221, 0.0310,  ..., 0.0272, 0.0257, 0.0228]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.34.self_attn.k_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071]], dtype=torch.int32),\n",
       " 'llm.model.layers.38.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.6.self_attn.k_proj.qweight': tensor([[ 1771661494, -2005444217, -2022078326,  ...,  1502189418,\n",
       "          -1185445990, -2023188150],\n",
       "         [-2021160571, -2021489226,  1534552422,  ...,  1985386154,\n",
       "           1784251975,  2004526984],\n",
       "         [ 1454922357,  1734945398, -1464378971,  ...,  -677881978,\n",
       "           1972930999, -2022201241],\n",
       "         ...,\n",
       "         [-1464298377, -1969707177,  1790425241,  ...,  -982021000,\n",
       "          -1789294440,  1722317992],\n",
       "         [ 1486578549, -1734704006, -2021038729,  ...,  2022213994,\n",
       "           2021046203, -1774543916],\n",
       "         [ 1161427574,  1716964522,  2003139176,  ..., -1990731417,\n",
       "           1704634757, -1767421527]], dtype=torch.int32),\n",
       " 'llm.model.layers.37.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.36.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'llm.model.layers.16.post_attention_layernorm.weight': tensor([1.4609, 1.3828, 0.9336,  ..., 0.9648, 1.3281, 0.2051]),\n",
       " 'vpm.encoder.layers.9.mlp.fc1.qzeros': tensor([[2139062143, 2139062143, 2139062143,  ..., 2139062143, 2139062143,\n",
       "          2139062143]], dtype=torch.int32),\n",
       " 'llm.model.layers.41.post_attention_layernorm.weight': tensor([1.1875, 1.1875, 0.9961,  ..., 1.0391, 1.3359, 0.4258]),\n",
       " 'llm.model.layers.36.mlp.down_proj.scales': tensor([[0.0254, 0.0266, 0.0280,  ..., 0.0272, 0.0232, 0.0424]],\n",
       "        dtype=torch.float16),\n",
       " 'vpm.encoder.layers.15.self_attn.v_proj.bias': tensor([-0.0016, -0.0011,  0.0167,  ..., -0.0320, -0.0308,  0.0112]),\n",
       " 'llm.model.layers.34.mlp.up_proj.qweight': tensor([[ 2058844804,  2057873610, -1166522521,  ..., -1452763562,\n",
       "           1450613897, -1957005173],\n",
       "         [-1449751946,  2038012280,  2072614744,  ...,  1754822807,\n",
       "          -1483175797, -1485477717],\n",
       "         [ 1518835784, -1431791991,  -381205673,  ..., -1451853208,\n",
       "           1970968936,  2055710805],\n",
       "         ...,\n",
       "         [-2004441207, -1720418921,  1990760757,  ..., -1754752137,\n",
       "          -1468364423, -2005432424],\n",
       "         [ 1786415269, -1936157302, -1986683754,  ..., -1736014201,\n",
       "          -1416127607, -1702046311],\n",
       "         [ 1721146506, -1923561081, -1733986629,  ...,  2073524858,\n",
       "          -1734707080, -1704351929]], dtype=torch.int32),\n",
       " 'llm.model.layers.38.mlp.gate_proj.qweight': tensor([[-2055703613, -1750685286,  1987414377,  ...,  2005383302,\n",
       "          -1737853001,  -979998583],\n",
       "         [-1984334501,   993699973, -1719375481,  ...,  2004306298,\n",
       "           2023265943,  -944204132],\n",
       "         [ -713652918, -1500152424,  1786226823,  ..., -1198036615,\n",
       "           1420461993,  2037688441],\n",
       "         ...,\n",
       "         [ 1718003082,  1517913466, -2003347576,  ..., -1989638249,\n",
       "          -1771607672,  1684578200],\n",
       "         [ -361343384, -1985443431,  1518835382,  ..., -2003134052,\n",
       "           1770358632,  2022332537],\n",
       "         [ 1739229353,  2037877383, -1751606409,  ..., -1701283476,\n",
       "           2057021830,  2006342520]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.15.self_attn.out_proj.qzeros': tensor([[2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143,\n",
       "          2139062143, 2139062143, 2139062143, 2139062143, 2139062143, 2139062143]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.39.mlp.down_proj.scales': tensor([[0.0275, 0.0284, 0.0293,  ..., 0.0301, 0.0233, 0.0464]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.30.self_attn.o_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'vpm.encoder.layers.19.self_attn.q_proj.scales': tensor([[0.0008, 0.0006, 0.0007,  ..., 0.0009, 0.0008, 0.0008]]),\n",
       " 'llm.model.layers.44.self_attn.v_proj.qweight': tensor([[-1721530503, -1701475942, -1735493753,  ..., -1735894664,\n",
       "           1756067193, -1198812102],\n",
       "         [ 2054654022, -1436063593,  1989642344,  ...,  1753123467,\n",
       "           -897218387,  -697858184],\n",
       "         [ 1990945160, -1683339383, -1787135093,  ...,  -395806257,\n",
       "           1689991817, -1485346489],\n",
       "         ...,\n",
       "         [-1735894651, -1164404666, -1199862667,  ..., -1468246601,\n",
       "          -1752381050, -2007393639],\n",
       "         [-1434809992, -1770612611, -1786275736,  ...,  1233609849,\n",
       "          -1448372360, -1720296825],\n",
       "         [-1433107062, -1769322373,  -914724950,  ..., -1484106628,\n",
       "          -1970002278,  1988654744]], dtype=torch.int32),\n",
       " 'llm.model.layers.51.mlp.up_proj.qweight': tensor([[-1735878485, -1989449320, -2056537961,  ...,  1989969611,\n",
       "          -1199994183, -1216973159],\n",
       "         [-1952077673, -1984526213, -2022147703,  ...,  1702345819,\n",
       "          -1972855431, -1217893258],\n",
       "         [-2000173196, -1249339735,  1972987720,  ...,  -932665482,\n",
       "          -1741191000, -1215779992],\n",
       "         ...,\n",
       "         [-1701538391, -1955629687,  2001175928,  ...,  1249290311,\n",
       "           1532463478, -1720220007],\n",
       "         [-1180141207, -1970567012, -2005494169,  ..., -1773767783,\n",
       "          -1484212407, -1988572536],\n",
       "         [ -926435943,  1769265769, -2003396507,  ...,  2023147655,\n",
       "           -964072551, -1921436586]], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.17.self_attn.v_proj.bias': tensor([ 0.0457, -0.0142,  0.0344,  ..., -0.0835,  0.0442, -0.0147]),\n",
       " 'llm.model.layers.21.self_attn.v_proj.qweight': tensor([[ 2020193959, -2004322393,  1218028951,  ...,  2035983688,\n",
       "           1751747239, -1973839421],\n",
       "         [  931432809, -1972008840,  1185778047,  ..., -1768585067,\n",
       "           1485342603,  1788311977],\n",
       "         [-2051437927, -1950849125,  1970763927,  ..., -2007438715,\n",
       "          -2007463305, -2007447915],\n",
       "         ...,\n",
       "         [-1986697817, -2039969895, -1165469802,  ..., -1515563127,\n",
       "          -1985382522, -2019722630],\n",
       "         [-2020857443,  1736870281,  2038929515,  ...,   646216087,\n",
       "           1519159641, -1702111317],\n",
       "         [-1734895179, -1952880490, -1745242266,  ..., -1215652952,\n",
       "          -1969772151, -1735026829]], dtype=torch.int32),\n",
       " 'llm.model.layers.28.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'resampler.ln_kv.weight': tensor([0.8750, 0.9531, 0.9219,  ..., 0.8516, 0.7734, 0.9258]),\n",
       " 'llm.model.layers.48.mlp.up_proj.qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "          2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "        dtype=torch.int32),\n",
       " 'llm.model.layers.51.self_attn.v_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.15.mlp.fc2.scales': tensor([[0.0010, 0.0011, 0.0012,  ..., 0.0009, 0.0010, 0.0009]]),\n",
       " 'llm.model.layers.47.input_layernorm.weight': tensor([0.7070, 0.6406, 0.7227,  ..., 0.5938, 0.7500, 1.9062]),\n",
       " 'vpm.encoder.layers.18.self_attn.out_proj.qweight': tensor([[ 1349671064, -1032862372,  1805555330,  ...,  2003351172,\n",
       "           1902544523, -1401584972],\n",
       "         [ 1348171158,  1971360382,  1917768042,  ..., -1202029665,\n",
       "           1988138426,  1705991040],\n",
       "         [-1766036349,  1233150884,  -832072049,  ...,  1737398157,\n",
       "          -2051845260, -1552967299],\n",
       "         ...,\n",
       "         [ 1702332552, -1870110818, -1886102896,  ..., -2057938067,\n",
       "           1936171142,  1766554230],\n",
       "         [ 1199279974,  1871735432, -1901268601,  ..., -1971164520,\n",
       "           1970577802, -1901426815],\n",
       "         [-2034655119, -1738890351,  2040500650,  ..., -2122619197,\n",
       "           2071431041, -1318285706]], dtype=torch.int32),\n",
       " 'llm.model.layers.30.self_attn.o_proj.scales': tensor([[0.0336, 0.0227, 0.0327,  ..., 0.0215, 0.0327, 0.0193]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.24.self_attn.v_proj.scales': tensor([[0.0212, 0.0206, 0.0225, 0.0213, 0.0241, 0.0217, 0.0221, 0.0217, 0.0225,\n",
       "          0.0186, 0.0190, 0.0217, 0.0230, 0.0257, 0.0286, 0.0224, 0.0240, 0.0216,\n",
       "          0.0217, 0.0234, 0.0247, 0.0221, 0.0208, 0.0208, 0.0221, 0.0227, 0.0203,\n",
       "          0.0217, 0.0213, 0.0221, 0.0238, 0.0232, 0.0237, 0.0204, 0.0258, 0.0250,\n",
       "          0.0220, 0.0224, 0.0241, 0.0242, 0.0220, 0.0210, 0.0227, 0.0225, 0.0242,\n",
       "          0.0285, 0.0197, 0.0237, 0.0238, 0.0247, 0.0206, 0.0215, 0.0228, 0.0215,\n",
       "          0.0268, 0.0202, 0.0202, 0.0234, 0.0221, 0.0254, 0.0245, 0.0217, 0.0195,\n",
       "          0.0255, 0.0225, 0.0224, 0.0230, 0.0255, 0.0212, 0.0236, 0.0242, 0.0370,\n",
       "          0.0262, 0.0259, 0.0227, 0.0237, 0.0213, 0.0224, 0.0255, 0.0244, 0.0255,\n",
       "          0.0229, 0.0267, 0.0266, 0.0244, 0.0237, 0.0260, 0.0207, 0.0240, 0.0227,\n",
       "          0.0242, 0.0230, 0.0259, 0.0242, 0.0229, 0.0302, 0.0229, 0.0242, 0.0236,\n",
       "          0.0227, 0.0242, 0.0216, 0.0255, 0.0236, 0.0242, 0.0271, 0.0227, 0.0211,\n",
       "          0.0234, 0.0234, 0.0247, 0.0276, 0.0236, 0.0241, 0.0219, 0.0229, 0.0284,\n",
       "          0.0211, 0.0236, 0.0203, 0.0213, 0.0275, 0.0220, 0.0204, 0.0244, 0.0224,\n",
       "          0.0263, 0.0232, 0.0188, 0.0224, 0.0217, 0.0185, 0.0219, 0.0208, 0.0189,\n",
       "          0.0216, 0.0211, 0.0195, 0.0181, 0.0177, 0.0217, 0.0195, 0.0229, 0.0199,\n",
       "          0.0195, 0.0189, 0.0195, 0.0190, 0.0204, 0.0199, 0.0172, 0.0233, 0.0193,\n",
       "          0.0206, 0.0197, 0.0194, 0.0195, 0.0220, 0.0225, 0.0181, 0.0193, 0.0195,\n",
       "          0.0193, 0.0213, 0.0186, 0.0206, 0.0219, 0.0223, 0.0213, 0.0199, 0.0198,\n",
       "          0.0180, 0.0198, 0.0194, 0.0199, 0.0213, 0.0267, 0.0228, 0.0229, 0.0206,\n",
       "          0.0186, 0.0202, 0.0185, 0.0186, 0.0197, 0.0180, 0.0217, 0.0177, 0.0219,\n",
       "          0.0184, 0.0185, 0.0206, 0.0323, 0.0362, 0.0354, 0.0280, 0.0290, 0.0365,\n",
       "          0.0417, 0.0430, 0.0396, 0.0324, 0.0267, 0.0383, 0.0341, 0.0357, 0.0344,\n",
       "          0.0344, 0.0352, 0.0290, 0.0298, 0.0333, 0.0359, 0.0352, 0.0372, 0.0414,\n",
       "          0.0336, 0.0349, 0.0312, 0.0409, 0.0320, 0.0324, 0.0380, 0.0344, 0.0359,\n",
       "          0.0365, 0.0341, 0.0336, 0.0370, 0.0391, 0.0367, 0.0283, 0.0336, 0.0419,\n",
       "          0.0341, 0.0362, 0.0302, 0.0401, 0.0378, 0.0440, 0.0310, 0.0217, 0.0380,\n",
       "          0.0370, 0.0297, 0.0344, 0.0336, 0.0378, 0.0303, 0.0388, 0.0404, 0.0372,\n",
       "          0.0422, 0.0322, 0.0338, 0.0359, 0.0229, 0.0197, 0.0208, 0.0221, 0.0186,\n",
       "          0.0227, 0.0217, 0.0242, 0.0312, 0.0223, 0.0246, 0.0262, 0.0217, 0.0266,\n",
       "          0.0236, 0.0217, 0.0249, 0.0250, 0.0271, 0.0237, 0.0242, 0.0247, 0.0223,\n",
       "          0.0272, 0.0219, 0.0285, 0.0232, 0.0233, 0.0215, 0.0237, 0.0233, 0.0224,\n",
       "          0.0220, 0.0297, 0.0294, 0.0229, 0.0203, 0.0188, 0.0225, 0.0213, 0.0253,\n",
       "          0.0238, 0.0228, 0.0250, 0.0228, 0.0249, 0.0207, 0.0249, 0.0232, 0.0314,\n",
       "          0.0249, 0.0263, 0.0257, 0.0216, 0.0233, 0.0251, 0.0219, 0.0230, 0.0254,\n",
       "          0.0213, 0.0247, 0.0253, 0.0260, 0.0224, 0.0268, 0.0255, 0.0296, 0.0250,\n",
       "          0.0327, 0.0237, 0.0227, 0.0255, 0.0236, 0.0245, 0.0263, 0.0230, 0.0250,\n",
       "          0.0257, 0.0234, 0.0269, 0.0223, 0.0219, 0.0219, 0.0258, 0.0254, 0.0244,\n",
       "          0.0300, 0.0212, 0.0206, 0.0221, 0.0247, 0.0251, 0.0286, 0.0220, 0.0255,\n",
       "          0.0228, 0.0215, 0.0258, 0.0238, 0.0263, 0.0233, 0.0250, 0.0333, 0.0262,\n",
       "          0.0213, 0.0259, 0.0277, 0.0288, 0.0237, 0.0225, 0.0233, 0.0292, 0.0204,\n",
       "          0.0238, 0.0244, 0.0238, 0.0204, 0.0244, 0.0306, 0.0217, 0.0220, 0.0267,\n",
       "          0.0293, 0.0229, 0.0245, 0.0251, 0.0331, 0.0273, 0.0241, 0.0303, 0.0257,\n",
       "          0.0237, 0.0298, 0.0242, 0.0228, 0.0275, 0.0240, 0.0204, 0.0250, 0.0232,\n",
       "          0.0230, 0.0292, 0.0202, 0.0215, 0.0224, 0.0224, 0.0254, 0.0279, 0.0238,\n",
       "          0.0310, 0.0238, 0.0251, 0.0262, 0.0223, 0.0228, 0.0234, 0.0229, 0.0228,\n",
       "          0.0242, 0.0224, 0.0251, 0.0216, 0.0236, 0.0266, 0.0254, 0.0220, 0.0262,\n",
       "          0.0240, 0.0247, 0.0247, 0.0213, 0.0208, 0.0249, 0.0241, 0.0223, 0.0320,\n",
       "          0.0230, 0.0305, 0.0212, 0.0217, 0.0223, 0.0198, 0.0236, 0.0207, 0.0233,\n",
       "          0.0220, 0.0221, 0.0241, 0.0275, 0.0254, 0.0259, 0.0229, 0.0216, 0.0210,\n",
       "          0.0215, 0.0195, 0.0198, 0.0244, 0.0199, 0.0193, 0.0234, 0.0193, 0.0206,\n",
       "          0.0217, 0.0207, 0.0202, 0.0224, 0.0206, 0.0203, 0.0197, 0.0244, 0.0190,\n",
       "          0.0227, 0.0201, 0.0202, 0.0203, 0.0182, 0.0240, 0.0193, 0.0180, 0.0224,\n",
       "          0.0206, 0.0241, 0.0191, 0.0208, 0.0237, 0.0197, 0.0201, 0.0240, 0.0225,\n",
       "          0.0197, 0.0211, 0.0176, 0.0206, 0.0204, 0.0185, 0.0212, 0.0186, 0.0203,\n",
       "          0.0188, 0.0211, 0.0193, 0.0221, 0.0193, 0.0198, 0.0193, 0.0178, 0.0189,\n",
       "          0.0201, 0.0194, 0.0208, 0.0225, 0.0199, 0.0215, 0.0191, 0.0191]],\n",
       "        dtype=torch.float16),\n",
       " 'llm.model.layers.32.mlp.gate_proj.g_idx': tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32),\n",
       " 'vpm.encoder.layers.7.mlp.fc2.qweight': tensor([[-1620539529,  1870440060,  1805482347,  ..., -2135185331,\n",
       "          -1620536175, -1283433313],\n",
       "         [-1885887061,  1420586111, -1956932997,  ..., -1718512240,\n",
       "          -1262314342, -2087676018],\n",
       "         [-1785502910,  2139852681, -2094100301,  ...,  1904899468,\n",
       "          -1970045784, -1737457512],\n",
       "         ...,\n",
       "         [-1636169090,  1502644869, -1524132259,  ...,  1499433576,\n",
       "          -1473814376, -1818076527],\n",
       "         [ 1553496965, -2084932512,  1706327987,  ...,  2005567291,\n",
       "           2105371223, -1400414113],\n",
       "         [ 2037695073,  1771786118,  1700037789,  ..., -1786741614,\n",
       "          -1635473552,  1400617623]], dtype=torch.int32),\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "/opt/conda/envs/lmquant/lib/python3.11/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256-llm-w4-pc were not used when initializing MiniCPMV: ['llm.model.layers.0.mlp.down_proj.g_idx', 'llm.model.layers.0.mlp.down_proj.qweight', 'llm.model.layers.0.mlp.down_proj.qzeros', 'llm.model.layers.0.mlp.down_proj.scales', 'llm.model.layers.0.mlp.gate_proj.g_idx', 'llm.model.layers.0.mlp.gate_proj.qweight', 'llm.model.layers.0.mlp.gate_proj.qzeros', 'llm.model.layers.0.mlp.gate_proj.scales', 'llm.model.layers.0.mlp.up_proj.g_idx', 'llm.model.layers.0.mlp.up_proj.qweight', 'llm.model.layers.0.mlp.up_proj.qzeros', 'llm.model.layers.0.mlp.up_proj.scales', 'llm.model.layers.0.self_attn.k_proj.g_idx', 'llm.model.layers.0.self_attn.k_proj.qweight', 'llm.model.layers.0.self_attn.k_proj.qzeros', 'llm.model.layers.0.self_attn.k_proj.scales', 'llm.model.layers.0.self_attn.o_proj.g_idx', 'llm.model.layers.0.self_attn.o_proj.qweight', 'llm.model.layers.0.self_attn.o_proj.qzeros', 'llm.model.layers.0.self_attn.o_proj.scales', 'llm.model.layers.0.self_attn.q_proj.g_idx', 'llm.model.layers.0.self_attn.q_proj.qweight', 'llm.model.layers.0.self_attn.q_proj.qzeros', 'llm.model.layers.0.self_attn.q_proj.scales', 'llm.model.layers.0.self_attn.v_proj.g_idx', 'llm.model.layers.0.self_attn.v_proj.qweight', 'llm.model.layers.0.self_attn.v_proj.qzeros', 'llm.model.layers.0.self_attn.v_proj.scales', 'llm.model.layers.1.mlp.down_proj.g_idx', 'llm.model.layers.1.mlp.down_proj.qweight', 'llm.model.layers.1.mlp.down_proj.qzeros', 'llm.model.layers.1.mlp.down_proj.scales', 'llm.model.layers.1.mlp.gate_proj.g_idx', 'llm.model.layers.1.mlp.gate_proj.qweight', 'llm.model.layers.1.mlp.gate_proj.qzeros', 'llm.model.layers.1.mlp.gate_proj.scales', 'llm.model.layers.1.mlp.up_proj.g_idx', 'llm.model.layers.1.mlp.up_proj.qweight', 'llm.model.layers.1.mlp.up_proj.qzeros', 'llm.model.layers.1.mlp.up_proj.scales', 'llm.model.layers.1.self_attn.k_proj.g_idx', 'llm.model.layers.1.self_attn.k_proj.qweight', 'llm.model.layers.1.self_attn.k_proj.qzeros', 'llm.model.layers.1.self_attn.k_proj.scales', 'llm.model.layers.1.self_attn.o_proj.g_idx', 'llm.model.layers.1.self_attn.o_proj.qweight', 'llm.model.layers.1.self_attn.o_proj.qzeros', 'llm.model.layers.1.self_attn.o_proj.scales', 'llm.model.layers.1.self_attn.q_proj.g_idx', 'llm.model.layers.1.self_attn.q_proj.qweight', 'llm.model.layers.1.self_attn.q_proj.qzeros', 'llm.model.layers.1.self_attn.q_proj.scales', 'llm.model.layers.1.self_attn.v_proj.g_idx', 'llm.model.layers.1.self_attn.v_proj.qweight', 'llm.model.layers.1.self_attn.v_proj.qzeros', 'llm.model.layers.1.self_attn.v_proj.scales', 'llm.model.layers.10.mlp.down_proj.g_idx', 'llm.model.layers.10.mlp.down_proj.qweight', 'llm.model.layers.10.mlp.down_proj.qzeros', 'llm.model.layers.10.mlp.down_proj.scales', 'llm.model.layers.10.mlp.gate_proj.g_idx', 'llm.model.layers.10.mlp.gate_proj.qweight', 'llm.model.layers.10.mlp.gate_proj.qzeros', 'llm.model.layers.10.mlp.gate_proj.scales', 'llm.model.layers.10.mlp.up_proj.g_idx', 'llm.model.layers.10.mlp.up_proj.qweight', 'llm.model.layers.10.mlp.up_proj.qzeros', 'llm.model.layers.10.mlp.up_proj.scales', 'llm.model.layers.10.self_attn.k_proj.g_idx', 'llm.model.layers.10.self_attn.k_proj.qweight', 'llm.model.layers.10.self_attn.k_proj.qzeros', 'llm.model.layers.10.self_attn.k_proj.scales', 'llm.model.layers.10.self_attn.o_proj.g_idx', 'llm.model.layers.10.self_attn.o_proj.qweight', 'llm.model.layers.10.self_attn.o_proj.qzeros', 'llm.model.layers.10.self_attn.o_proj.scales', 'llm.model.layers.10.self_attn.q_proj.g_idx', 'llm.model.layers.10.self_attn.q_proj.qweight', 'llm.model.layers.10.self_attn.q_proj.qzeros', 'llm.model.layers.10.self_attn.q_proj.scales', 'llm.model.layers.10.self_attn.v_proj.g_idx', 'llm.model.layers.10.self_attn.v_proj.qweight', 'llm.model.layers.10.self_attn.v_proj.qzeros', 'llm.model.layers.10.self_attn.v_proj.scales', 'llm.model.layers.11.mlp.down_proj.g_idx', 'llm.model.layers.11.mlp.down_proj.qweight', 'llm.model.layers.11.mlp.down_proj.qzeros', 'llm.model.layers.11.mlp.down_proj.scales', 'llm.model.layers.11.mlp.gate_proj.g_idx', 'llm.model.layers.11.mlp.gate_proj.qweight', 'llm.model.layers.11.mlp.gate_proj.qzeros', 'llm.model.layers.11.mlp.gate_proj.scales', 'llm.model.layers.11.mlp.up_proj.g_idx', 'llm.model.layers.11.mlp.up_proj.qweight', 'llm.model.layers.11.mlp.up_proj.qzeros', 'llm.model.layers.11.mlp.up_proj.scales', 'llm.model.layers.11.self_attn.k_proj.g_idx', 'llm.model.layers.11.self_attn.k_proj.qweight', 'llm.model.layers.11.self_attn.k_proj.qzeros', 'llm.model.layers.11.self_attn.k_proj.scales', 'llm.model.layers.11.self_attn.o_proj.g_idx', 'llm.model.layers.11.self_attn.o_proj.qweight', 'llm.model.layers.11.self_attn.o_proj.qzeros', 'llm.model.layers.11.self_attn.o_proj.scales', 'llm.model.layers.11.self_attn.q_proj.g_idx', 'llm.model.layers.11.self_attn.q_proj.qweight', 'llm.model.layers.11.self_attn.q_proj.qzeros', 'llm.model.layers.11.self_attn.q_proj.scales', 'llm.model.layers.11.self_attn.v_proj.g_idx', 'llm.model.layers.11.self_attn.v_proj.qweight', 'llm.model.layers.11.self_attn.v_proj.qzeros', 'llm.model.layers.11.self_attn.v_proj.scales', 'llm.model.layers.12.mlp.down_proj.g_idx', 'llm.model.layers.12.mlp.down_proj.qweight', 'llm.model.layers.12.mlp.down_proj.qzeros', 'llm.model.layers.12.mlp.down_proj.scales', 'llm.model.layers.12.mlp.gate_proj.g_idx', 'llm.model.layers.12.mlp.gate_proj.qweight', 'llm.model.layers.12.mlp.gate_proj.qzeros', 'llm.model.layers.12.mlp.gate_proj.scales', 'llm.model.layers.12.mlp.up_proj.g_idx', 'llm.model.layers.12.mlp.up_proj.qweight', 'llm.model.layers.12.mlp.up_proj.qzeros', 'llm.model.layers.12.mlp.up_proj.scales', 'llm.model.layers.12.self_attn.k_proj.g_idx', 'llm.model.layers.12.self_attn.k_proj.qweight', 'llm.model.layers.12.self_attn.k_proj.qzeros', 'llm.model.layers.12.self_attn.k_proj.scales', 'llm.model.layers.12.self_attn.o_proj.g_idx', 'llm.model.layers.12.self_attn.o_proj.qweight', 'llm.model.layers.12.self_attn.o_proj.qzeros', 'llm.model.layers.12.self_attn.o_proj.scales', 'llm.model.layers.12.self_attn.q_proj.g_idx', 'llm.model.layers.12.self_attn.q_proj.qweight', 'llm.model.layers.12.self_attn.q_proj.qzeros', 'llm.model.layers.12.self_attn.q_proj.scales', 'llm.model.layers.12.self_attn.v_proj.g_idx', 'llm.model.layers.12.self_attn.v_proj.qweight', 'llm.model.layers.12.self_attn.v_proj.qzeros', 'llm.model.layers.12.self_attn.v_proj.scales', 'llm.model.layers.13.mlp.down_proj.g_idx', 'llm.model.layers.13.mlp.down_proj.qweight', 'llm.model.layers.13.mlp.down_proj.qzeros', 'llm.model.layers.13.mlp.down_proj.scales', 'llm.model.layers.13.mlp.gate_proj.g_idx', 'llm.model.layers.13.mlp.gate_proj.qweight', 'llm.model.layers.13.mlp.gate_proj.qzeros', 'llm.model.layers.13.mlp.gate_proj.scales', 'llm.model.layers.13.mlp.up_proj.g_idx', 'llm.model.layers.13.mlp.up_proj.qweight', 'llm.model.layers.13.mlp.up_proj.qzeros', 'llm.model.layers.13.mlp.up_proj.scales', 'llm.model.layers.13.self_attn.k_proj.g_idx', 'llm.model.layers.13.self_attn.k_proj.qweight', 'llm.model.layers.13.self_attn.k_proj.qzeros', 'llm.model.layers.13.self_attn.k_proj.scales', 'llm.model.layers.13.self_attn.o_proj.g_idx', 'llm.model.layers.13.self_attn.o_proj.qweight', 'llm.model.layers.13.self_attn.o_proj.qzeros', 'llm.model.layers.13.self_attn.o_proj.scales', 'llm.model.layers.13.self_attn.q_proj.g_idx', 'llm.model.layers.13.self_attn.q_proj.qweight', 'llm.model.layers.13.self_attn.q_proj.qzeros', 'llm.model.layers.13.self_attn.q_proj.scales', 'llm.model.layers.13.self_attn.v_proj.g_idx', 'llm.model.layers.13.self_attn.v_proj.qweight', 'llm.model.layers.13.self_attn.v_proj.qzeros', 'llm.model.layers.13.self_attn.v_proj.scales', 'llm.model.layers.14.mlp.down_proj.g_idx', 'llm.model.layers.14.mlp.down_proj.qweight', 'llm.model.layers.14.mlp.down_proj.qzeros', 'llm.model.layers.14.mlp.down_proj.scales', 'llm.model.layers.14.mlp.gate_proj.g_idx', 'llm.model.layers.14.mlp.gate_proj.qweight', 'llm.model.layers.14.mlp.gate_proj.qzeros', 'llm.model.layers.14.mlp.gate_proj.scales', 'llm.model.layers.14.mlp.up_proj.g_idx', 'llm.model.layers.14.mlp.up_proj.qweight', 'llm.model.layers.14.mlp.up_proj.qzeros', 'llm.model.layers.14.mlp.up_proj.scales', 'llm.model.layers.14.self_attn.k_proj.g_idx', 'llm.model.layers.14.self_attn.k_proj.qweight', 'llm.model.layers.14.self_attn.k_proj.qzeros', 'llm.model.layers.14.self_attn.k_proj.scales', 'llm.model.layers.14.self_attn.o_proj.g_idx', 'llm.model.layers.14.self_attn.o_proj.qweight', 'llm.model.layers.14.self_attn.o_proj.qzeros', 'llm.model.layers.14.self_attn.o_proj.scales', 'llm.model.layers.14.self_attn.q_proj.g_idx', 'llm.model.layers.14.self_attn.q_proj.qweight', 'llm.model.layers.14.self_attn.q_proj.qzeros', 'llm.model.layers.14.self_attn.q_proj.scales', 'llm.model.layers.14.self_attn.v_proj.g_idx', 'llm.model.layers.14.self_attn.v_proj.qweight', 'llm.model.layers.14.self_attn.v_proj.qzeros', 'llm.model.layers.14.self_attn.v_proj.scales', 'llm.model.layers.15.mlp.down_proj.g_idx', 'llm.model.layers.15.mlp.down_proj.qweight', 'llm.model.layers.15.mlp.down_proj.qzeros', 'llm.model.layers.15.mlp.down_proj.scales', 'llm.model.layers.15.mlp.gate_proj.g_idx', 'llm.model.layers.15.mlp.gate_proj.qweight', 'llm.model.layers.15.mlp.gate_proj.qzeros', 'llm.model.layers.15.mlp.gate_proj.scales', 'llm.model.layers.15.mlp.up_proj.g_idx', 'llm.model.layers.15.mlp.up_proj.qweight', 'llm.model.layers.15.mlp.up_proj.qzeros', 'llm.model.layers.15.mlp.up_proj.scales', 'llm.model.layers.15.self_attn.k_proj.g_idx', 'llm.model.layers.15.self_attn.k_proj.qweight', 'llm.model.layers.15.self_attn.k_proj.qzeros', 'llm.model.layers.15.self_attn.k_proj.scales', 'llm.model.layers.15.self_attn.o_proj.g_idx', 'llm.model.layers.15.self_attn.o_proj.qweight', 'llm.model.layers.15.self_attn.o_proj.qzeros', 'llm.model.layers.15.self_attn.o_proj.scales', 'llm.model.layers.15.self_attn.q_proj.g_idx', 'llm.model.layers.15.self_attn.q_proj.qweight', 'llm.model.layers.15.self_attn.q_proj.qzeros', 'llm.model.layers.15.self_attn.q_proj.scales', 'llm.model.layers.15.self_attn.v_proj.g_idx', 'llm.model.layers.15.self_attn.v_proj.qweight', 'llm.model.layers.15.self_attn.v_proj.qzeros', 'llm.model.layers.15.self_attn.v_proj.scales', 'llm.model.layers.16.mlp.down_proj.g_idx', 'llm.model.layers.16.mlp.down_proj.qweight', 'llm.model.layers.16.mlp.down_proj.qzeros', 'llm.model.layers.16.mlp.down_proj.scales', 'llm.model.layers.16.mlp.gate_proj.g_idx', 'llm.model.layers.16.mlp.gate_proj.qweight', 'llm.model.layers.16.mlp.gate_proj.qzeros', 'llm.model.layers.16.mlp.gate_proj.scales', 'llm.model.layers.16.mlp.up_proj.g_idx', 'llm.model.layers.16.mlp.up_proj.qweight', 'llm.model.layers.16.mlp.up_proj.qzeros', 'llm.model.layers.16.mlp.up_proj.scales', 'llm.model.layers.16.self_attn.k_proj.g_idx', 'llm.model.layers.16.self_attn.k_proj.qweight', 'llm.model.layers.16.self_attn.k_proj.qzeros', 'llm.model.layers.16.self_attn.k_proj.scales', 'llm.model.layers.16.self_attn.o_proj.g_idx', 'llm.model.layers.16.self_attn.o_proj.qweight', 'llm.model.layers.16.self_attn.o_proj.qzeros', 'llm.model.layers.16.self_attn.o_proj.scales', 'llm.model.layers.16.self_attn.q_proj.g_idx', 'llm.model.layers.16.self_attn.q_proj.qweight', 'llm.model.layers.16.self_attn.q_proj.qzeros', 'llm.model.layers.16.self_attn.q_proj.scales', 'llm.model.layers.16.self_attn.v_proj.g_idx', 'llm.model.layers.16.self_attn.v_proj.qweight', 'llm.model.layers.16.self_attn.v_proj.qzeros', 'llm.model.layers.16.self_attn.v_proj.scales', 'llm.model.layers.17.mlp.down_proj.g_idx', 'llm.model.layers.17.mlp.down_proj.qweight', 'llm.model.layers.17.mlp.down_proj.qzeros', 'llm.model.layers.17.mlp.down_proj.scales', 'llm.model.layers.17.mlp.gate_proj.g_idx', 'llm.model.layers.17.mlp.gate_proj.qweight', 'llm.model.layers.17.mlp.gate_proj.qzeros', 'llm.model.layers.17.mlp.gate_proj.scales', 'llm.model.layers.17.mlp.up_proj.g_idx', 'llm.model.layers.17.mlp.up_proj.qweight', 'llm.model.layers.17.mlp.up_proj.qzeros', 'llm.model.layers.17.mlp.up_proj.scales', 'llm.model.layers.17.self_attn.k_proj.g_idx', 'llm.model.layers.17.self_attn.k_proj.qweight', 'llm.model.layers.17.self_attn.k_proj.qzeros', 'llm.model.layers.17.self_attn.k_proj.scales', 'llm.model.layers.17.self_attn.o_proj.g_idx', 'llm.model.layers.17.self_attn.o_proj.qweight', 'llm.model.layers.17.self_attn.o_proj.qzeros', 'llm.model.layers.17.self_attn.o_proj.scales', 'llm.model.layers.17.self_attn.q_proj.g_idx', 'llm.model.layers.17.self_attn.q_proj.qweight', 'llm.model.layers.17.self_attn.q_proj.qzeros', 'llm.model.layers.17.self_attn.q_proj.scales', 'llm.model.layers.17.self_attn.v_proj.g_idx', 'llm.model.layers.17.self_attn.v_proj.qweight', 'llm.model.layers.17.self_attn.v_proj.qzeros', 'llm.model.layers.17.self_attn.v_proj.scales', 'llm.model.layers.18.mlp.down_proj.g_idx', 'llm.model.layers.18.mlp.down_proj.qweight', 'llm.model.layers.18.mlp.down_proj.qzeros', 'llm.model.layers.18.mlp.down_proj.scales', 'llm.model.layers.18.mlp.gate_proj.g_idx', 'llm.model.layers.18.mlp.gate_proj.qweight', 'llm.model.layers.18.mlp.gate_proj.qzeros', 'llm.model.layers.18.mlp.gate_proj.scales', 'llm.model.layers.18.mlp.up_proj.g_idx', 'llm.model.layers.18.mlp.up_proj.qweight', 'llm.model.layers.18.mlp.up_proj.qzeros', 'llm.model.layers.18.mlp.up_proj.scales', 'llm.model.layers.18.self_attn.k_proj.g_idx', 'llm.model.layers.18.self_attn.k_proj.qweight', 'llm.model.layers.18.self_attn.k_proj.qzeros', 'llm.model.layers.18.self_attn.k_proj.scales', 'llm.model.layers.18.self_attn.o_proj.g_idx', 'llm.model.layers.18.self_attn.o_proj.qweight', 'llm.model.layers.18.self_attn.o_proj.qzeros', 'llm.model.layers.18.self_attn.o_proj.scales', 'llm.model.layers.18.self_attn.q_proj.g_idx', 'llm.model.layers.18.self_attn.q_proj.qweight', 'llm.model.layers.18.self_attn.q_proj.qzeros', 'llm.model.layers.18.self_attn.q_proj.scales', 'llm.model.layers.18.self_attn.v_proj.g_idx', 'llm.model.layers.18.self_attn.v_proj.qweight', 'llm.model.layers.18.self_attn.v_proj.qzeros', 'llm.model.layers.18.self_attn.v_proj.scales', 'llm.model.layers.19.mlp.down_proj.g_idx', 'llm.model.layers.19.mlp.down_proj.qweight', 'llm.model.layers.19.mlp.down_proj.qzeros', 'llm.model.layers.19.mlp.down_proj.scales', 'llm.model.layers.19.mlp.gate_proj.g_idx', 'llm.model.layers.19.mlp.gate_proj.qweight', 'llm.model.layers.19.mlp.gate_proj.qzeros', 'llm.model.layers.19.mlp.gate_proj.scales', 'llm.model.layers.19.mlp.up_proj.g_idx', 'llm.model.layers.19.mlp.up_proj.qweight', 'llm.model.layers.19.mlp.up_proj.qzeros', 'llm.model.layers.19.mlp.up_proj.scales', 'llm.model.layers.19.self_attn.k_proj.g_idx', 'llm.model.layers.19.self_attn.k_proj.qweight', 'llm.model.layers.19.self_attn.k_proj.qzeros', 'llm.model.layers.19.self_attn.k_proj.scales', 'llm.model.layers.19.self_attn.o_proj.g_idx', 'llm.model.layers.19.self_attn.o_proj.qweight', 'llm.model.layers.19.self_attn.o_proj.qzeros', 'llm.model.layers.19.self_attn.o_proj.scales', 'llm.model.layers.19.self_attn.q_proj.g_idx', 'llm.model.layers.19.self_attn.q_proj.qweight', 'llm.model.layers.19.self_attn.q_proj.qzeros', 'llm.model.layers.19.self_attn.q_proj.scales', 'llm.model.layers.19.self_attn.v_proj.g_idx', 'llm.model.layers.19.self_attn.v_proj.qweight', 'llm.model.layers.19.self_attn.v_proj.qzeros', 'llm.model.layers.19.self_attn.v_proj.scales', 'llm.model.layers.2.mlp.down_proj.g_idx', 'llm.model.layers.2.mlp.down_proj.qweight', 'llm.model.layers.2.mlp.down_proj.qzeros', 'llm.model.layers.2.mlp.down_proj.scales', 'llm.model.layers.2.mlp.gate_proj.g_idx', 'llm.model.layers.2.mlp.gate_proj.qweight', 'llm.model.layers.2.mlp.gate_proj.qzeros', 'llm.model.layers.2.mlp.gate_proj.scales', 'llm.model.layers.2.mlp.up_proj.g_idx', 'llm.model.layers.2.mlp.up_proj.qweight', 'llm.model.layers.2.mlp.up_proj.qzeros', 'llm.model.layers.2.mlp.up_proj.scales', 'llm.model.layers.2.self_attn.k_proj.g_idx', 'llm.model.layers.2.self_attn.k_proj.qweight', 'llm.model.layers.2.self_attn.k_proj.qzeros', 'llm.model.layers.2.self_attn.k_proj.scales', 'llm.model.layers.2.self_attn.o_proj.g_idx', 'llm.model.layers.2.self_attn.o_proj.qweight', 'llm.model.layers.2.self_attn.o_proj.qzeros', 'llm.model.layers.2.self_attn.o_proj.scales', 'llm.model.layers.2.self_attn.q_proj.g_idx', 'llm.model.layers.2.self_attn.q_proj.qweight', 'llm.model.layers.2.self_attn.q_proj.qzeros', 'llm.model.layers.2.self_attn.q_proj.scales', 'llm.model.layers.2.self_attn.v_proj.g_idx', 'llm.model.layers.2.self_attn.v_proj.qweight', 'llm.model.layers.2.self_attn.v_proj.qzeros', 'llm.model.layers.2.self_attn.v_proj.scales', 'llm.model.layers.20.mlp.down_proj.g_idx', 'llm.model.layers.20.mlp.down_proj.qweight', 'llm.model.layers.20.mlp.down_proj.qzeros', 'llm.model.layers.20.mlp.down_proj.scales', 'llm.model.layers.20.mlp.gate_proj.g_idx', 'llm.model.layers.20.mlp.gate_proj.qweight', 'llm.model.layers.20.mlp.gate_proj.qzeros', 'llm.model.layers.20.mlp.gate_proj.scales', 'llm.model.layers.20.mlp.up_proj.g_idx', 'llm.model.layers.20.mlp.up_proj.qweight', 'llm.model.layers.20.mlp.up_proj.qzeros', 'llm.model.layers.20.mlp.up_proj.scales', 'llm.model.layers.20.self_attn.k_proj.g_idx', 'llm.model.layers.20.self_attn.k_proj.qweight', 'llm.model.layers.20.self_attn.k_proj.qzeros', 'llm.model.layers.20.self_attn.k_proj.scales', 'llm.model.layers.20.self_attn.o_proj.g_idx', 'llm.model.layers.20.self_attn.o_proj.qweight', 'llm.model.layers.20.self_attn.o_proj.qzeros', 'llm.model.layers.20.self_attn.o_proj.scales', 'llm.model.layers.20.self_attn.q_proj.g_idx', 'llm.model.layers.20.self_attn.q_proj.qweight', 'llm.model.layers.20.self_attn.q_proj.qzeros', 'llm.model.layers.20.self_attn.q_proj.scales', 'llm.model.layers.20.self_attn.v_proj.g_idx', 'llm.model.layers.20.self_attn.v_proj.qweight', 'llm.model.layers.20.self_attn.v_proj.qzeros', 'llm.model.layers.20.self_attn.v_proj.scales', 'llm.model.layers.21.mlp.down_proj.g_idx', 'llm.model.layers.21.mlp.down_proj.qweight', 'llm.model.layers.21.mlp.down_proj.qzeros', 'llm.model.layers.21.mlp.down_proj.scales', 'llm.model.layers.21.mlp.gate_proj.g_idx', 'llm.model.layers.21.mlp.gate_proj.qweight', 'llm.model.layers.21.mlp.gate_proj.qzeros', 'llm.model.layers.21.mlp.gate_proj.scales', 'llm.model.layers.21.mlp.up_proj.g_idx', 'llm.model.layers.21.mlp.up_proj.qweight', 'llm.model.layers.21.mlp.up_proj.qzeros', 'llm.model.layers.21.mlp.up_proj.scales', 'llm.model.layers.21.self_attn.k_proj.g_idx', 'llm.model.layers.21.self_attn.k_proj.qweight', 'llm.model.layers.21.self_attn.k_proj.qzeros', 'llm.model.layers.21.self_attn.k_proj.scales', 'llm.model.layers.21.self_attn.o_proj.g_idx', 'llm.model.layers.21.self_attn.o_proj.qweight', 'llm.model.layers.21.self_attn.o_proj.qzeros', 'llm.model.layers.21.self_attn.o_proj.scales', 'llm.model.layers.21.self_attn.q_proj.g_idx', 'llm.model.layers.21.self_attn.q_proj.qweight', 'llm.model.layers.21.self_attn.q_proj.qzeros', 'llm.model.layers.21.self_attn.q_proj.scales', 'llm.model.layers.21.self_attn.v_proj.g_idx', 'llm.model.layers.21.self_attn.v_proj.qweight', 'llm.model.layers.21.self_attn.v_proj.qzeros', 'llm.model.layers.21.self_attn.v_proj.scales', 'llm.model.layers.22.mlp.down_proj.g_idx', 'llm.model.layers.22.mlp.down_proj.qweight', 'llm.model.layers.22.mlp.down_proj.qzeros', 'llm.model.layers.22.mlp.down_proj.scales', 'llm.model.layers.22.mlp.gate_proj.g_idx', 'llm.model.layers.22.mlp.gate_proj.qweight', 'llm.model.layers.22.mlp.gate_proj.qzeros', 'llm.model.layers.22.mlp.gate_proj.scales', 'llm.model.layers.22.mlp.up_proj.g_idx', 'llm.model.layers.22.mlp.up_proj.qweight', 'llm.model.layers.22.mlp.up_proj.qzeros', 'llm.model.layers.22.mlp.up_proj.scales', 'llm.model.layers.22.self_attn.k_proj.g_idx', 'llm.model.layers.22.self_attn.k_proj.qweight', 'llm.model.layers.22.self_attn.k_proj.qzeros', 'llm.model.layers.22.self_attn.k_proj.scales', 'llm.model.layers.22.self_attn.o_proj.g_idx', 'llm.model.layers.22.self_attn.o_proj.qweight', 'llm.model.layers.22.self_attn.o_proj.qzeros', 'llm.model.layers.22.self_attn.o_proj.scales', 'llm.model.layers.22.self_attn.q_proj.g_idx', 'llm.model.layers.22.self_attn.q_proj.qweight', 'llm.model.layers.22.self_attn.q_proj.qzeros', 'llm.model.layers.22.self_attn.q_proj.scales', 'llm.model.layers.22.self_attn.v_proj.g_idx', 'llm.model.layers.22.self_attn.v_proj.qweight', 'llm.model.layers.22.self_attn.v_proj.qzeros', 'llm.model.layers.22.self_attn.v_proj.scales', 'llm.model.layers.23.mlp.down_proj.g_idx', 'llm.model.layers.23.mlp.down_proj.qweight', 'llm.model.layers.23.mlp.down_proj.qzeros', 'llm.model.layers.23.mlp.down_proj.scales', 'llm.model.layers.23.mlp.gate_proj.g_idx', 'llm.model.layers.23.mlp.gate_proj.qweight', 'llm.model.layers.23.mlp.gate_proj.qzeros', 'llm.model.layers.23.mlp.gate_proj.scales', 'llm.model.layers.23.mlp.up_proj.g_idx', 'llm.model.layers.23.mlp.up_proj.qweight', 'llm.model.layers.23.mlp.up_proj.qzeros', 'llm.model.layers.23.mlp.up_proj.scales', 'llm.model.layers.23.self_attn.k_proj.g_idx', 'llm.model.layers.23.self_attn.k_proj.qweight', 'llm.model.layers.23.self_attn.k_proj.qzeros', 'llm.model.layers.23.self_attn.k_proj.scales', 'llm.model.layers.23.self_attn.o_proj.g_idx', 'llm.model.layers.23.self_attn.o_proj.qweight', 'llm.model.layers.23.self_attn.o_proj.qzeros', 'llm.model.layers.23.self_attn.o_proj.scales', 'llm.model.layers.23.self_attn.q_proj.g_idx', 'llm.model.layers.23.self_attn.q_proj.qweight', 'llm.model.layers.23.self_attn.q_proj.qzeros', 'llm.model.layers.23.self_attn.q_proj.scales', 'llm.model.layers.23.self_attn.v_proj.g_idx', 'llm.model.layers.23.self_attn.v_proj.qweight', 'llm.model.layers.23.self_attn.v_proj.qzeros', 'llm.model.layers.23.self_attn.v_proj.scales', 'llm.model.layers.24.mlp.down_proj.g_idx', 'llm.model.layers.24.mlp.down_proj.qweight', 'llm.model.layers.24.mlp.down_proj.qzeros', 'llm.model.layers.24.mlp.down_proj.scales', 'llm.model.layers.24.mlp.gate_proj.g_idx', 'llm.model.layers.24.mlp.gate_proj.qweight', 'llm.model.layers.24.mlp.gate_proj.qzeros', 'llm.model.layers.24.mlp.gate_proj.scales', 'llm.model.layers.24.mlp.up_proj.g_idx', 'llm.model.layers.24.mlp.up_proj.qweight', 'llm.model.layers.24.mlp.up_proj.qzeros', 'llm.model.layers.24.mlp.up_proj.scales', 'llm.model.layers.24.self_attn.k_proj.g_idx', 'llm.model.layers.24.self_attn.k_proj.qweight', 'llm.model.layers.24.self_attn.k_proj.qzeros', 'llm.model.layers.24.self_attn.k_proj.scales', 'llm.model.layers.24.self_attn.o_proj.g_idx', 'llm.model.layers.24.self_attn.o_proj.qweight', 'llm.model.layers.24.self_attn.o_proj.qzeros', 'llm.model.layers.24.self_attn.o_proj.scales', 'llm.model.layers.24.self_attn.q_proj.g_idx', 'llm.model.layers.24.self_attn.q_proj.qweight', 'llm.model.layers.24.self_attn.q_proj.qzeros', 'llm.model.layers.24.self_attn.q_proj.scales', 'llm.model.layers.24.self_attn.v_proj.g_idx', 'llm.model.layers.24.self_attn.v_proj.qweight', 'llm.model.layers.24.self_attn.v_proj.qzeros', 'llm.model.layers.24.self_attn.v_proj.scales', 'llm.model.layers.25.mlp.down_proj.g_idx', 'llm.model.layers.25.mlp.down_proj.qweight', 'llm.model.layers.25.mlp.down_proj.qzeros', 'llm.model.layers.25.mlp.down_proj.scales', 'llm.model.layers.25.mlp.gate_proj.g_idx', 'llm.model.layers.25.mlp.gate_proj.qweight', 'llm.model.layers.25.mlp.gate_proj.qzeros', 'llm.model.layers.25.mlp.gate_proj.scales', 'llm.model.layers.25.mlp.up_proj.g_idx', 'llm.model.layers.25.mlp.up_proj.qweight', 'llm.model.layers.25.mlp.up_proj.qzeros', 'llm.model.layers.25.mlp.up_proj.scales', 'llm.model.layers.25.self_attn.k_proj.g_idx', 'llm.model.layers.25.self_attn.k_proj.qweight', 'llm.model.layers.25.self_attn.k_proj.qzeros', 'llm.model.layers.25.self_attn.k_proj.scales', 'llm.model.layers.25.self_attn.o_proj.g_idx', 'llm.model.layers.25.self_attn.o_proj.qweight', 'llm.model.layers.25.self_attn.o_proj.qzeros', 'llm.model.layers.25.self_attn.o_proj.scales', 'llm.model.layers.25.self_attn.q_proj.g_idx', 'llm.model.layers.25.self_attn.q_proj.qweight', 'llm.model.layers.25.self_attn.q_proj.qzeros', 'llm.model.layers.25.self_attn.q_proj.scales', 'llm.model.layers.25.self_attn.v_proj.g_idx', 'llm.model.layers.25.self_attn.v_proj.qweight', 'llm.model.layers.25.self_attn.v_proj.qzeros', 'llm.model.layers.25.self_attn.v_proj.scales', 'llm.model.layers.26.mlp.down_proj.g_idx', 'llm.model.layers.26.mlp.down_proj.qweight', 'llm.model.layers.26.mlp.down_proj.qzeros', 'llm.model.layers.26.mlp.down_proj.scales', 'llm.model.layers.26.mlp.gate_proj.g_idx', 'llm.model.layers.26.mlp.gate_proj.qweight', 'llm.model.layers.26.mlp.gate_proj.qzeros', 'llm.model.layers.26.mlp.gate_proj.scales', 'llm.model.layers.26.mlp.up_proj.g_idx', 'llm.model.layers.26.mlp.up_proj.qweight', 'llm.model.layers.26.mlp.up_proj.qzeros', 'llm.model.layers.26.mlp.up_proj.scales', 'llm.model.layers.26.self_attn.k_proj.g_idx', 'llm.model.layers.26.self_attn.k_proj.qweight', 'llm.model.layers.26.self_attn.k_proj.qzeros', 'llm.model.layers.26.self_attn.k_proj.scales', 'llm.model.layers.26.self_attn.o_proj.g_idx', 'llm.model.layers.26.self_attn.o_proj.qweight', 'llm.model.layers.26.self_attn.o_proj.qzeros', 'llm.model.layers.26.self_attn.o_proj.scales', 'llm.model.layers.26.self_attn.q_proj.g_idx', 'llm.model.layers.26.self_attn.q_proj.qweight', 'llm.model.layers.26.self_attn.q_proj.qzeros', 'llm.model.layers.26.self_attn.q_proj.scales', 'llm.model.layers.26.self_attn.v_proj.g_idx', 'llm.model.layers.26.self_attn.v_proj.qweight', 'llm.model.layers.26.self_attn.v_proj.qzeros', 'llm.model.layers.26.self_attn.v_proj.scales', 'llm.model.layers.27.mlp.down_proj.g_idx', 'llm.model.layers.27.mlp.down_proj.qweight', 'llm.model.layers.27.mlp.down_proj.qzeros', 'llm.model.layers.27.mlp.down_proj.scales', 'llm.model.layers.27.mlp.gate_proj.g_idx', 'llm.model.layers.27.mlp.gate_proj.qweight', 'llm.model.layers.27.mlp.gate_proj.qzeros', 'llm.model.layers.27.mlp.gate_proj.scales', 'llm.model.layers.27.mlp.up_proj.g_idx', 'llm.model.layers.27.mlp.up_proj.qweight', 'llm.model.layers.27.mlp.up_proj.qzeros', 'llm.model.layers.27.mlp.up_proj.scales', 'llm.model.layers.27.self_attn.k_proj.g_idx', 'llm.model.layers.27.self_attn.k_proj.qweight', 'llm.model.layers.27.self_attn.k_proj.qzeros', 'llm.model.layers.27.self_attn.k_proj.scales', 'llm.model.layers.27.self_attn.o_proj.g_idx', 'llm.model.layers.27.self_attn.o_proj.qweight', 'llm.model.layers.27.self_attn.o_proj.qzeros', 'llm.model.layers.27.self_attn.o_proj.scales', 'llm.model.layers.27.self_attn.q_proj.g_idx', 'llm.model.layers.27.self_attn.q_proj.qweight', 'llm.model.layers.27.self_attn.q_proj.qzeros', 'llm.model.layers.27.self_attn.q_proj.scales', 'llm.model.layers.27.self_attn.v_proj.g_idx', 'llm.model.layers.27.self_attn.v_proj.qweight', 'llm.model.layers.27.self_attn.v_proj.qzeros', 'llm.model.layers.27.self_attn.v_proj.scales', 'llm.model.layers.28.mlp.down_proj.g_idx', 'llm.model.layers.28.mlp.down_proj.qweight', 'llm.model.layers.28.mlp.down_proj.qzeros', 'llm.model.layers.28.mlp.down_proj.scales', 'llm.model.layers.28.mlp.gate_proj.g_idx', 'llm.model.layers.28.mlp.gate_proj.qweight', 'llm.model.layers.28.mlp.gate_proj.qzeros', 'llm.model.layers.28.mlp.gate_proj.scales', 'llm.model.layers.28.mlp.up_proj.g_idx', 'llm.model.layers.28.mlp.up_proj.qweight', 'llm.model.layers.28.mlp.up_proj.qzeros', 'llm.model.layers.28.mlp.up_proj.scales', 'llm.model.layers.28.self_attn.k_proj.g_idx', 'llm.model.layers.28.self_attn.k_proj.qweight', 'llm.model.layers.28.self_attn.k_proj.qzeros', 'llm.model.layers.28.self_attn.k_proj.scales', 'llm.model.layers.28.self_attn.o_proj.g_idx', 'llm.model.layers.28.self_attn.o_proj.qweight', 'llm.model.layers.28.self_attn.o_proj.qzeros', 'llm.model.layers.28.self_attn.o_proj.scales', 'llm.model.layers.28.self_attn.q_proj.g_idx', 'llm.model.layers.28.self_attn.q_proj.qweight', 'llm.model.layers.28.self_attn.q_proj.qzeros', 'llm.model.layers.28.self_attn.q_proj.scales', 'llm.model.layers.28.self_attn.v_proj.g_idx', 'llm.model.layers.28.self_attn.v_proj.qweight', 'llm.model.layers.28.self_attn.v_proj.qzeros', 'llm.model.layers.28.self_attn.v_proj.scales', 'llm.model.layers.29.mlp.down_proj.g_idx', 'llm.model.layers.29.mlp.down_proj.qweight', 'llm.model.layers.29.mlp.down_proj.qzeros', 'llm.model.layers.29.mlp.down_proj.scales', 'llm.model.layers.29.mlp.gate_proj.g_idx', 'llm.model.layers.29.mlp.gate_proj.qweight', 'llm.model.layers.29.mlp.gate_proj.qzeros', 'llm.model.layers.29.mlp.gate_proj.scales', 'llm.model.layers.29.mlp.up_proj.g_idx', 'llm.model.layers.29.mlp.up_proj.qweight', 'llm.model.layers.29.mlp.up_proj.qzeros', 'llm.model.layers.29.mlp.up_proj.scales', 'llm.model.layers.29.self_attn.k_proj.g_idx', 'llm.model.layers.29.self_attn.k_proj.qweight', 'llm.model.layers.29.self_attn.k_proj.qzeros', 'llm.model.layers.29.self_attn.k_proj.scales', 'llm.model.layers.29.self_attn.o_proj.g_idx', 'llm.model.layers.29.self_attn.o_proj.qweight', 'llm.model.layers.29.self_attn.o_proj.qzeros', 'llm.model.layers.29.self_attn.o_proj.scales', 'llm.model.layers.29.self_attn.q_proj.g_idx', 'llm.model.layers.29.self_attn.q_proj.qweight', 'llm.model.layers.29.self_attn.q_proj.qzeros', 'llm.model.layers.29.self_attn.q_proj.scales', 'llm.model.layers.29.self_attn.v_proj.g_idx', 'llm.model.layers.29.self_attn.v_proj.qweight', 'llm.model.layers.29.self_attn.v_proj.qzeros', 'llm.model.layers.29.self_attn.v_proj.scales', 'llm.model.layers.3.mlp.down_proj.g_idx', 'llm.model.layers.3.mlp.down_proj.qweight', 'llm.model.layers.3.mlp.down_proj.qzeros', 'llm.model.layers.3.mlp.down_proj.scales', 'llm.model.layers.3.mlp.gate_proj.g_idx', 'llm.model.layers.3.mlp.gate_proj.qweight', 'llm.model.layers.3.mlp.gate_proj.qzeros', 'llm.model.layers.3.mlp.gate_proj.scales', 'llm.model.layers.3.mlp.up_proj.g_idx', 'llm.model.layers.3.mlp.up_proj.qweight', 'llm.model.layers.3.mlp.up_proj.qzeros', 'llm.model.layers.3.mlp.up_proj.scales', 'llm.model.layers.3.self_attn.k_proj.g_idx', 'llm.model.layers.3.self_attn.k_proj.qweight', 'llm.model.layers.3.self_attn.k_proj.qzeros', 'llm.model.layers.3.self_attn.k_proj.scales', 'llm.model.layers.3.self_attn.o_proj.g_idx', 'llm.model.layers.3.self_attn.o_proj.qweight', 'llm.model.layers.3.self_attn.o_proj.qzeros', 'llm.model.layers.3.self_attn.o_proj.scales', 'llm.model.layers.3.self_attn.q_proj.g_idx', 'llm.model.layers.3.self_attn.q_proj.qweight', 'llm.model.layers.3.self_attn.q_proj.qzeros', 'llm.model.layers.3.self_attn.q_proj.scales', 'llm.model.layers.3.self_attn.v_proj.g_idx', 'llm.model.layers.3.self_attn.v_proj.qweight', 'llm.model.layers.3.self_attn.v_proj.qzeros', 'llm.model.layers.3.self_attn.v_proj.scales', 'llm.model.layers.30.mlp.down_proj.g_idx', 'llm.model.layers.30.mlp.down_proj.qweight', 'llm.model.layers.30.mlp.down_proj.qzeros', 'llm.model.layers.30.mlp.down_proj.scales', 'llm.model.layers.30.mlp.gate_proj.g_idx', 'llm.model.layers.30.mlp.gate_proj.qweight', 'llm.model.layers.30.mlp.gate_proj.qzeros', 'llm.model.layers.30.mlp.gate_proj.scales', 'llm.model.layers.30.mlp.up_proj.g_idx', 'llm.model.layers.30.mlp.up_proj.qweight', 'llm.model.layers.30.mlp.up_proj.qzeros', 'llm.model.layers.30.mlp.up_proj.scales', 'llm.model.layers.30.self_attn.k_proj.g_idx', 'llm.model.layers.30.self_attn.k_proj.qweight', 'llm.model.layers.30.self_attn.k_proj.qzeros', 'llm.model.layers.30.self_attn.k_proj.scales', 'llm.model.layers.30.self_attn.o_proj.g_idx', 'llm.model.layers.30.self_attn.o_proj.qweight', 'llm.model.layers.30.self_attn.o_proj.qzeros', 'llm.model.layers.30.self_attn.o_proj.scales', 'llm.model.layers.30.self_attn.q_proj.g_idx', 'llm.model.layers.30.self_attn.q_proj.qweight', 'llm.model.layers.30.self_attn.q_proj.qzeros', 'llm.model.layers.30.self_attn.q_proj.scales', 'llm.model.layers.30.self_attn.v_proj.g_idx', 'llm.model.layers.30.self_attn.v_proj.qweight', 'llm.model.layers.30.self_attn.v_proj.qzeros', 'llm.model.layers.30.self_attn.v_proj.scales', 'llm.model.layers.31.mlp.down_proj.g_idx', 'llm.model.layers.31.mlp.down_proj.qweight', 'llm.model.layers.31.mlp.down_proj.qzeros', 'llm.model.layers.31.mlp.down_proj.scales', 'llm.model.layers.31.mlp.gate_proj.g_idx', 'llm.model.layers.31.mlp.gate_proj.qweight', 'llm.model.layers.31.mlp.gate_proj.qzeros', 'llm.model.layers.31.mlp.gate_proj.scales', 'llm.model.layers.31.mlp.up_proj.g_idx', 'llm.model.layers.31.mlp.up_proj.qweight', 'llm.model.layers.31.mlp.up_proj.qzeros', 'llm.model.layers.31.mlp.up_proj.scales', 'llm.model.layers.31.self_attn.k_proj.g_idx', 'llm.model.layers.31.self_attn.k_proj.qweight', 'llm.model.layers.31.self_attn.k_proj.qzeros', 'llm.model.layers.31.self_attn.k_proj.scales', 'llm.model.layers.31.self_attn.o_proj.g_idx', 'llm.model.layers.31.self_attn.o_proj.qweight', 'llm.model.layers.31.self_attn.o_proj.qzeros', 'llm.model.layers.31.self_attn.o_proj.scales', 'llm.model.layers.31.self_attn.q_proj.g_idx', 'llm.model.layers.31.self_attn.q_proj.qweight', 'llm.model.layers.31.self_attn.q_proj.qzeros', 'llm.model.layers.31.self_attn.q_proj.scales', 'llm.model.layers.31.self_attn.v_proj.g_idx', 'llm.model.layers.31.self_attn.v_proj.qweight', 'llm.model.layers.31.self_attn.v_proj.qzeros', 'llm.model.layers.31.self_attn.v_proj.scales', 'llm.model.layers.32.mlp.down_proj.g_idx', 'llm.model.layers.32.mlp.down_proj.qweight', 'llm.model.layers.32.mlp.down_proj.qzeros', 'llm.model.layers.32.mlp.down_proj.scales', 'llm.model.layers.32.mlp.gate_proj.g_idx', 'llm.model.layers.32.mlp.gate_proj.qweight', 'llm.model.layers.32.mlp.gate_proj.qzeros', 'llm.model.layers.32.mlp.gate_proj.scales', 'llm.model.layers.32.mlp.up_proj.g_idx', 'llm.model.layers.32.mlp.up_proj.qweight', 'llm.model.layers.32.mlp.up_proj.qzeros', 'llm.model.layers.32.mlp.up_proj.scales', 'llm.model.layers.32.self_attn.k_proj.g_idx', 'llm.model.layers.32.self_attn.k_proj.qweight', 'llm.model.layers.32.self_attn.k_proj.qzeros', 'llm.model.layers.32.self_attn.k_proj.scales', 'llm.model.layers.32.self_attn.o_proj.g_idx', 'llm.model.layers.32.self_attn.o_proj.qweight', 'llm.model.layers.32.self_attn.o_proj.qzeros', 'llm.model.layers.32.self_attn.o_proj.scales', 'llm.model.layers.32.self_attn.q_proj.g_idx', 'llm.model.layers.32.self_attn.q_proj.qweight', 'llm.model.layers.32.self_attn.q_proj.qzeros', 'llm.model.layers.32.self_attn.q_proj.scales', 'llm.model.layers.32.self_attn.v_proj.g_idx', 'llm.model.layers.32.self_attn.v_proj.qweight', 'llm.model.layers.32.self_attn.v_proj.qzeros', 'llm.model.layers.32.self_attn.v_proj.scales', 'llm.model.layers.33.mlp.down_proj.g_idx', 'llm.model.layers.33.mlp.down_proj.qweight', 'llm.model.layers.33.mlp.down_proj.qzeros', 'llm.model.layers.33.mlp.down_proj.scales', 'llm.model.layers.33.mlp.gate_proj.g_idx', 'llm.model.layers.33.mlp.gate_proj.qweight', 'llm.model.layers.33.mlp.gate_proj.qzeros', 'llm.model.layers.33.mlp.gate_proj.scales', 'llm.model.layers.33.mlp.up_proj.g_idx', 'llm.model.layers.33.mlp.up_proj.qweight', 'llm.model.layers.33.mlp.up_proj.qzeros', 'llm.model.layers.33.mlp.up_proj.scales', 'llm.model.layers.33.self_attn.k_proj.g_idx', 'llm.model.layers.33.self_attn.k_proj.qweight', 'llm.model.layers.33.self_attn.k_proj.qzeros', 'llm.model.layers.33.self_attn.k_proj.scales', 'llm.model.layers.33.self_attn.o_proj.g_idx', 'llm.model.layers.33.self_attn.o_proj.qweight', 'llm.model.layers.33.self_attn.o_proj.qzeros', 'llm.model.layers.33.self_attn.o_proj.scales', 'llm.model.layers.33.self_attn.q_proj.g_idx', 'llm.model.layers.33.self_attn.q_proj.qweight', 'llm.model.layers.33.self_attn.q_proj.qzeros', 'llm.model.layers.33.self_attn.q_proj.scales', 'llm.model.layers.33.self_attn.v_proj.g_idx', 'llm.model.layers.33.self_attn.v_proj.qweight', 'llm.model.layers.33.self_attn.v_proj.qzeros', 'llm.model.layers.33.self_attn.v_proj.scales', 'llm.model.layers.34.mlp.down_proj.g_idx', 'llm.model.layers.34.mlp.down_proj.qweight', 'llm.model.layers.34.mlp.down_proj.qzeros', 'llm.model.layers.34.mlp.down_proj.scales', 'llm.model.layers.34.mlp.gate_proj.g_idx', 'llm.model.layers.34.mlp.gate_proj.qweight', 'llm.model.layers.34.mlp.gate_proj.qzeros', 'llm.model.layers.34.mlp.gate_proj.scales', 'llm.model.layers.34.mlp.up_proj.g_idx', 'llm.model.layers.34.mlp.up_proj.qweight', 'llm.model.layers.34.mlp.up_proj.qzeros', 'llm.model.layers.34.mlp.up_proj.scales', 'llm.model.layers.34.self_attn.k_proj.g_idx', 'llm.model.layers.34.self_attn.k_proj.qweight', 'llm.model.layers.34.self_attn.k_proj.qzeros', 'llm.model.layers.34.self_attn.k_proj.scales', 'llm.model.layers.34.self_attn.o_proj.g_idx', 'llm.model.layers.34.self_attn.o_proj.qweight', 'llm.model.layers.34.self_attn.o_proj.qzeros', 'llm.model.layers.34.self_attn.o_proj.scales', 'llm.model.layers.34.self_attn.q_proj.g_idx', 'llm.model.layers.34.self_attn.q_proj.qweight', 'llm.model.layers.34.self_attn.q_proj.qzeros', 'llm.model.layers.34.self_attn.q_proj.scales', 'llm.model.layers.34.self_attn.v_proj.g_idx', 'llm.model.layers.34.self_attn.v_proj.qweight', 'llm.model.layers.34.self_attn.v_proj.qzeros', 'llm.model.layers.34.self_attn.v_proj.scales', 'llm.model.layers.35.mlp.down_proj.g_idx', 'llm.model.layers.35.mlp.down_proj.qweight', 'llm.model.layers.35.mlp.down_proj.qzeros', 'llm.model.layers.35.mlp.down_proj.scales', 'llm.model.layers.35.mlp.gate_proj.g_idx', 'llm.model.layers.35.mlp.gate_proj.qweight', 'llm.model.layers.35.mlp.gate_proj.qzeros', 'llm.model.layers.35.mlp.gate_proj.scales', 'llm.model.layers.35.mlp.up_proj.g_idx', 'llm.model.layers.35.mlp.up_proj.qweight', 'llm.model.layers.35.mlp.up_proj.qzeros', 'llm.model.layers.35.mlp.up_proj.scales', 'llm.model.layers.35.self_attn.k_proj.g_idx', 'llm.model.layers.35.self_attn.k_proj.qweight', 'llm.model.layers.35.self_attn.k_proj.qzeros', 'llm.model.layers.35.self_attn.k_proj.scales', 'llm.model.layers.35.self_attn.o_proj.g_idx', 'llm.model.layers.35.self_attn.o_proj.qweight', 'llm.model.layers.35.self_attn.o_proj.qzeros', 'llm.model.layers.35.self_attn.o_proj.scales', 'llm.model.layers.35.self_attn.q_proj.g_idx', 'llm.model.layers.35.self_attn.q_proj.qweight', 'llm.model.layers.35.self_attn.q_proj.qzeros', 'llm.model.layers.35.self_attn.q_proj.scales', 'llm.model.layers.35.self_attn.v_proj.g_idx', 'llm.model.layers.35.self_attn.v_proj.qweight', 'llm.model.layers.35.self_attn.v_proj.qzeros', 'llm.model.layers.35.self_attn.v_proj.scales', 'llm.model.layers.36.mlp.down_proj.g_idx', 'llm.model.layers.36.mlp.down_proj.qweight', 'llm.model.layers.36.mlp.down_proj.qzeros', 'llm.model.layers.36.mlp.down_proj.scales', 'llm.model.layers.36.mlp.gate_proj.g_idx', 'llm.model.layers.36.mlp.gate_proj.qweight', 'llm.model.layers.36.mlp.gate_proj.qzeros', 'llm.model.layers.36.mlp.gate_proj.scales', 'llm.model.layers.36.mlp.up_proj.g_idx', 'llm.model.layers.36.mlp.up_proj.qweight', 'llm.model.layers.36.mlp.up_proj.qzeros', 'llm.model.layers.36.mlp.up_proj.scales', 'llm.model.layers.36.self_attn.k_proj.g_idx', 'llm.model.layers.36.self_attn.k_proj.qweight', 'llm.model.layers.36.self_attn.k_proj.qzeros', 'llm.model.layers.36.self_attn.k_proj.scales', 'llm.model.layers.36.self_attn.o_proj.g_idx', 'llm.model.layers.36.self_attn.o_proj.qweight', 'llm.model.layers.36.self_attn.o_proj.qzeros', 'llm.model.layers.36.self_attn.o_proj.scales', 'llm.model.layers.36.self_attn.q_proj.g_idx', 'llm.model.layers.36.self_attn.q_proj.qweight', 'llm.model.layers.36.self_attn.q_proj.qzeros', 'llm.model.layers.36.self_attn.q_proj.scales', 'llm.model.layers.36.self_attn.v_proj.g_idx', 'llm.model.layers.36.self_attn.v_proj.qweight', 'llm.model.layers.36.self_attn.v_proj.qzeros', 'llm.model.layers.36.self_attn.v_proj.scales', 'llm.model.layers.37.mlp.down_proj.g_idx', 'llm.model.layers.37.mlp.down_proj.qweight', 'llm.model.layers.37.mlp.down_proj.qzeros', 'llm.model.layers.37.mlp.down_proj.scales', 'llm.model.layers.37.mlp.gate_proj.g_idx', 'llm.model.layers.37.mlp.gate_proj.qweight', 'llm.model.layers.37.mlp.gate_proj.qzeros', 'llm.model.layers.37.mlp.gate_proj.scales', 'llm.model.layers.37.mlp.up_proj.g_idx', 'llm.model.layers.37.mlp.up_proj.qweight', 'llm.model.layers.37.mlp.up_proj.qzeros', 'llm.model.layers.37.mlp.up_proj.scales', 'llm.model.layers.37.self_attn.k_proj.g_idx', 'llm.model.layers.37.self_attn.k_proj.qweight', 'llm.model.layers.37.self_attn.k_proj.qzeros', 'llm.model.layers.37.self_attn.k_proj.scales', 'llm.model.layers.37.self_attn.o_proj.g_idx', 'llm.model.layers.37.self_attn.o_proj.qweight', 'llm.model.layers.37.self_attn.o_proj.qzeros', 'llm.model.layers.37.self_attn.o_proj.scales', 'llm.model.layers.37.self_attn.q_proj.g_idx', 'llm.model.layers.37.self_attn.q_proj.qweight', 'llm.model.layers.37.self_attn.q_proj.qzeros', 'llm.model.layers.37.self_attn.q_proj.scales', 'llm.model.layers.37.self_attn.v_proj.g_idx', 'llm.model.layers.37.self_attn.v_proj.qweight', 'llm.model.layers.37.self_attn.v_proj.qzeros', 'llm.model.layers.37.self_attn.v_proj.scales', 'llm.model.layers.38.mlp.down_proj.g_idx', 'llm.model.layers.38.mlp.down_proj.qweight', 'llm.model.layers.38.mlp.down_proj.qzeros', 'llm.model.layers.38.mlp.down_proj.scales', 'llm.model.layers.38.mlp.gate_proj.g_idx', 'llm.model.layers.38.mlp.gate_proj.qweight', 'llm.model.layers.38.mlp.gate_proj.qzeros', 'llm.model.layers.38.mlp.gate_proj.scales', 'llm.model.layers.38.mlp.up_proj.g_idx', 'llm.model.layers.38.mlp.up_proj.qweight', 'llm.model.layers.38.mlp.up_proj.qzeros', 'llm.model.layers.38.mlp.up_proj.scales', 'llm.model.layers.38.self_attn.k_proj.g_idx', 'llm.model.layers.38.self_attn.k_proj.qweight', 'llm.model.layers.38.self_attn.k_proj.qzeros', 'llm.model.layers.38.self_attn.k_proj.scales', 'llm.model.layers.38.self_attn.o_proj.g_idx', 'llm.model.layers.38.self_attn.o_proj.qweight', 'llm.model.layers.38.self_attn.o_proj.qzeros', 'llm.model.layers.38.self_attn.o_proj.scales', 'llm.model.layers.38.self_attn.q_proj.g_idx', 'llm.model.layers.38.self_attn.q_proj.qweight', 'llm.model.layers.38.self_attn.q_proj.qzeros', 'llm.model.layers.38.self_attn.q_proj.scales', 'llm.model.layers.38.self_attn.v_proj.g_idx', 'llm.model.layers.38.self_attn.v_proj.qweight', 'llm.model.layers.38.self_attn.v_proj.qzeros', 'llm.model.layers.38.self_attn.v_proj.scales', 'llm.model.layers.39.mlp.down_proj.g_idx', 'llm.model.layers.39.mlp.down_proj.qweight', 'llm.model.layers.39.mlp.down_proj.qzeros', 'llm.model.layers.39.mlp.down_proj.scales', 'llm.model.layers.39.mlp.gate_proj.g_idx', 'llm.model.layers.39.mlp.gate_proj.qweight', 'llm.model.layers.39.mlp.gate_proj.qzeros', 'llm.model.layers.39.mlp.gate_proj.scales', 'llm.model.layers.39.mlp.up_proj.g_idx', 'llm.model.layers.39.mlp.up_proj.qweight', 'llm.model.layers.39.mlp.up_proj.qzeros', 'llm.model.layers.39.mlp.up_proj.scales', 'llm.model.layers.39.self_attn.k_proj.g_idx', 'llm.model.layers.39.self_attn.k_proj.qweight', 'llm.model.layers.39.self_attn.k_proj.qzeros', 'llm.model.layers.39.self_attn.k_proj.scales', 'llm.model.layers.39.self_attn.o_proj.g_idx', 'llm.model.layers.39.self_attn.o_proj.qweight', 'llm.model.layers.39.self_attn.o_proj.qzeros', 'llm.model.layers.39.self_attn.o_proj.scales', 'llm.model.layers.39.self_attn.q_proj.g_idx', 'llm.model.layers.39.self_attn.q_proj.qweight', 'llm.model.layers.39.self_attn.q_proj.qzeros', 'llm.model.layers.39.self_attn.q_proj.scales', 'llm.model.layers.39.self_attn.v_proj.g_idx', 'llm.model.layers.39.self_attn.v_proj.qweight', 'llm.model.layers.39.self_attn.v_proj.qzeros', 'llm.model.layers.39.self_attn.v_proj.scales', 'llm.model.layers.4.mlp.down_proj.g_idx', 'llm.model.layers.4.mlp.down_proj.qweight', 'llm.model.layers.4.mlp.down_proj.qzeros', 'llm.model.layers.4.mlp.down_proj.scales', 'llm.model.layers.4.mlp.gate_proj.g_idx', 'llm.model.layers.4.mlp.gate_proj.qweight', 'llm.model.layers.4.mlp.gate_proj.qzeros', 'llm.model.layers.4.mlp.gate_proj.scales', 'llm.model.layers.4.mlp.up_proj.g_idx', 'llm.model.layers.4.mlp.up_proj.qweight', 'llm.model.layers.4.mlp.up_proj.qzeros', 'llm.model.layers.4.mlp.up_proj.scales', 'llm.model.layers.4.self_attn.k_proj.g_idx', 'llm.model.layers.4.self_attn.k_proj.qweight', 'llm.model.layers.4.self_attn.k_proj.qzeros', 'llm.model.layers.4.self_attn.k_proj.scales', 'llm.model.layers.4.self_attn.o_proj.g_idx', 'llm.model.layers.4.self_attn.o_proj.qweight', 'llm.model.layers.4.self_attn.o_proj.qzeros', 'llm.model.layers.4.self_attn.o_proj.scales', 'llm.model.layers.4.self_attn.q_proj.g_idx', 'llm.model.layers.4.self_attn.q_proj.qweight', 'llm.model.layers.4.self_attn.q_proj.qzeros', 'llm.model.layers.4.self_attn.q_proj.scales', 'llm.model.layers.4.self_attn.v_proj.g_idx', 'llm.model.layers.4.self_attn.v_proj.qweight', 'llm.model.layers.4.self_attn.v_proj.qzeros', 'llm.model.layers.4.self_attn.v_proj.scales', 'llm.model.layers.40.mlp.down_proj.g_idx', 'llm.model.layers.40.mlp.down_proj.qweight', 'llm.model.layers.40.mlp.down_proj.qzeros', 'llm.model.layers.40.mlp.down_proj.scales', 'llm.model.layers.40.mlp.gate_proj.g_idx', 'llm.model.layers.40.mlp.gate_proj.qweight', 'llm.model.layers.40.mlp.gate_proj.qzeros', 'llm.model.layers.40.mlp.gate_proj.scales', 'llm.model.layers.40.mlp.up_proj.g_idx', 'llm.model.layers.40.mlp.up_proj.qweight', 'llm.model.layers.40.mlp.up_proj.qzeros', 'llm.model.layers.40.mlp.up_proj.scales', 'llm.model.layers.40.self_attn.k_proj.g_idx', 'llm.model.layers.40.self_attn.k_proj.qweight', 'llm.model.layers.40.self_attn.k_proj.qzeros', 'llm.model.layers.40.self_attn.k_proj.scales', 'llm.model.layers.40.self_attn.o_proj.g_idx', 'llm.model.layers.40.self_attn.o_proj.qweight', 'llm.model.layers.40.self_attn.o_proj.qzeros', 'llm.model.layers.40.self_attn.o_proj.scales', 'llm.model.layers.40.self_attn.q_proj.g_idx', 'llm.model.layers.40.self_attn.q_proj.qweight', 'llm.model.layers.40.self_attn.q_proj.qzeros', 'llm.model.layers.40.self_attn.q_proj.scales', 'llm.model.layers.40.self_attn.v_proj.g_idx', 'llm.model.layers.40.self_attn.v_proj.qweight', 'llm.model.layers.40.self_attn.v_proj.qzeros', 'llm.model.layers.40.self_attn.v_proj.scales', 'llm.model.layers.41.mlp.down_proj.g_idx', 'llm.model.layers.41.mlp.down_proj.qweight', 'llm.model.layers.41.mlp.down_proj.qzeros', 'llm.model.layers.41.mlp.down_proj.scales', 'llm.model.layers.41.mlp.gate_proj.g_idx', 'llm.model.layers.41.mlp.gate_proj.qweight', 'llm.model.layers.41.mlp.gate_proj.qzeros', 'llm.model.layers.41.mlp.gate_proj.scales', 'llm.model.layers.41.mlp.up_proj.g_idx', 'llm.model.layers.41.mlp.up_proj.qweight', 'llm.model.layers.41.mlp.up_proj.qzeros', 'llm.model.layers.41.mlp.up_proj.scales', 'llm.model.layers.41.self_attn.k_proj.g_idx', 'llm.model.layers.41.self_attn.k_proj.qweight', 'llm.model.layers.41.self_attn.k_proj.qzeros', 'llm.model.layers.41.self_attn.k_proj.scales', 'llm.model.layers.41.self_attn.o_proj.g_idx', 'llm.model.layers.41.self_attn.o_proj.qweight', 'llm.model.layers.41.self_attn.o_proj.qzeros', 'llm.model.layers.41.self_attn.o_proj.scales', 'llm.model.layers.41.self_attn.q_proj.g_idx', 'llm.model.layers.41.self_attn.q_proj.qweight', 'llm.model.layers.41.self_attn.q_proj.qzeros', 'llm.model.layers.41.self_attn.q_proj.scales', 'llm.model.layers.41.self_attn.v_proj.g_idx', 'llm.model.layers.41.self_attn.v_proj.qweight', 'llm.model.layers.41.self_attn.v_proj.qzeros', 'llm.model.layers.41.self_attn.v_proj.scales', 'llm.model.layers.42.mlp.down_proj.g_idx', 'llm.model.layers.42.mlp.down_proj.qweight', 'llm.model.layers.42.mlp.down_proj.qzeros', 'llm.model.layers.42.mlp.down_proj.scales', 'llm.model.layers.42.mlp.gate_proj.g_idx', 'llm.model.layers.42.mlp.gate_proj.qweight', 'llm.model.layers.42.mlp.gate_proj.qzeros', 'llm.model.layers.42.mlp.gate_proj.scales', 'llm.model.layers.42.mlp.up_proj.g_idx', 'llm.model.layers.42.mlp.up_proj.qweight', 'llm.model.layers.42.mlp.up_proj.qzeros', 'llm.model.layers.42.mlp.up_proj.scales', 'llm.model.layers.42.self_attn.k_proj.g_idx', 'llm.model.layers.42.self_attn.k_proj.qweight', 'llm.model.layers.42.self_attn.k_proj.qzeros', 'llm.model.layers.42.self_attn.k_proj.scales', 'llm.model.layers.42.self_attn.o_proj.g_idx', 'llm.model.layers.42.self_attn.o_proj.qweight', 'llm.model.layers.42.self_attn.o_proj.qzeros', 'llm.model.layers.42.self_attn.o_proj.scales', 'llm.model.layers.42.self_attn.q_proj.g_idx', 'llm.model.layers.42.self_attn.q_proj.qweight', 'llm.model.layers.42.self_attn.q_proj.qzeros', 'llm.model.layers.42.self_attn.q_proj.scales', 'llm.model.layers.42.self_attn.v_proj.g_idx', 'llm.model.layers.42.self_attn.v_proj.qweight', 'llm.model.layers.42.self_attn.v_proj.qzeros', 'llm.model.layers.42.self_attn.v_proj.scales', 'llm.model.layers.43.mlp.down_proj.g_idx', 'llm.model.layers.43.mlp.down_proj.qweight', 'llm.model.layers.43.mlp.down_proj.qzeros', 'llm.model.layers.43.mlp.down_proj.scales', 'llm.model.layers.43.mlp.gate_proj.g_idx', 'llm.model.layers.43.mlp.gate_proj.qweight', 'llm.model.layers.43.mlp.gate_proj.qzeros', 'llm.model.layers.43.mlp.gate_proj.scales', 'llm.model.layers.43.mlp.up_proj.g_idx', 'llm.model.layers.43.mlp.up_proj.qweight', 'llm.model.layers.43.mlp.up_proj.qzeros', 'llm.model.layers.43.mlp.up_proj.scales', 'llm.model.layers.43.self_attn.k_proj.g_idx', 'llm.model.layers.43.self_attn.k_proj.qweight', 'llm.model.layers.43.self_attn.k_proj.qzeros', 'llm.model.layers.43.self_attn.k_proj.scales', 'llm.model.layers.43.self_attn.o_proj.g_idx', 'llm.model.layers.43.self_attn.o_proj.qweight', 'llm.model.layers.43.self_attn.o_proj.qzeros', 'llm.model.layers.43.self_attn.o_proj.scales', 'llm.model.layers.43.self_attn.q_proj.g_idx', 'llm.model.layers.43.self_attn.q_proj.qweight', 'llm.model.layers.43.self_attn.q_proj.qzeros', 'llm.model.layers.43.self_attn.q_proj.scales', 'llm.model.layers.43.self_attn.v_proj.g_idx', 'llm.model.layers.43.self_attn.v_proj.qweight', 'llm.model.layers.43.self_attn.v_proj.qzeros', 'llm.model.layers.43.self_attn.v_proj.scales', 'llm.model.layers.44.mlp.down_proj.g_idx', 'llm.model.layers.44.mlp.down_proj.qweight', 'llm.model.layers.44.mlp.down_proj.qzeros', 'llm.model.layers.44.mlp.down_proj.scales', 'llm.model.layers.44.mlp.gate_proj.g_idx', 'llm.model.layers.44.mlp.gate_proj.qweight', 'llm.model.layers.44.mlp.gate_proj.qzeros', 'llm.model.layers.44.mlp.gate_proj.scales', 'llm.model.layers.44.mlp.up_proj.g_idx', 'llm.model.layers.44.mlp.up_proj.qweight', 'llm.model.layers.44.mlp.up_proj.qzeros', 'llm.model.layers.44.mlp.up_proj.scales', 'llm.model.layers.44.self_attn.k_proj.g_idx', 'llm.model.layers.44.self_attn.k_proj.qweight', 'llm.model.layers.44.self_attn.k_proj.qzeros', 'llm.model.layers.44.self_attn.k_proj.scales', 'llm.model.layers.44.self_attn.o_proj.g_idx', 'llm.model.layers.44.self_attn.o_proj.qweight', 'llm.model.layers.44.self_attn.o_proj.qzeros', 'llm.model.layers.44.self_attn.o_proj.scales', 'llm.model.layers.44.self_attn.q_proj.g_idx', 'llm.model.layers.44.self_attn.q_proj.qweight', 'llm.model.layers.44.self_attn.q_proj.qzeros', 'llm.model.layers.44.self_attn.q_proj.scales', 'llm.model.layers.44.self_attn.v_proj.g_idx', 'llm.model.layers.44.self_attn.v_proj.qweight', 'llm.model.layers.44.self_attn.v_proj.qzeros', 'llm.model.layers.44.self_attn.v_proj.scales', 'llm.model.layers.45.mlp.down_proj.g_idx', 'llm.model.layers.45.mlp.down_proj.qweight', 'llm.model.layers.45.mlp.down_proj.qzeros', 'llm.model.layers.45.mlp.down_proj.scales', 'llm.model.layers.45.mlp.gate_proj.g_idx', 'llm.model.layers.45.mlp.gate_proj.qweight', 'llm.model.layers.45.mlp.gate_proj.qzeros', 'llm.model.layers.45.mlp.gate_proj.scales', 'llm.model.layers.45.mlp.up_proj.g_idx', 'llm.model.layers.45.mlp.up_proj.qweight', 'llm.model.layers.45.mlp.up_proj.qzeros', 'llm.model.layers.45.mlp.up_proj.scales', 'llm.model.layers.45.self_attn.k_proj.g_idx', 'llm.model.layers.45.self_attn.k_proj.qweight', 'llm.model.layers.45.self_attn.k_proj.qzeros', 'llm.model.layers.45.self_attn.k_proj.scales', 'llm.model.layers.45.self_attn.o_proj.g_idx', 'llm.model.layers.45.self_attn.o_proj.qweight', 'llm.model.layers.45.self_attn.o_proj.qzeros', 'llm.model.layers.45.self_attn.o_proj.scales', 'llm.model.layers.45.self_attn.q_proj.g_idx', 'llm.model.layers.45.self_attn.q_proj.qweight', 'llm.model.layers.45.self_attn.q_proj.qzeros', 'llm.model.layers.45.self_attn.q_proj.scales', 'llm.model.layers.45.self_attn.v_proj.g_idx', 'llm.model.layers.45.self_attn.v_proj.qweight', 'llm.model.layers.45.self_attn.v_proj.qzeros', 'llm.model.layers.45.self_attn.v_proj.scales', 'llm.model.layers.46.mlp.down_proj.g_idx', 'llm.model.layers.46.mlp.down_proj.qweight', 'llm.model.layers.46.mlp.down_proj.qzeros', 'llm.model.layers.46.mlp.down_proj.scales', 'llm.model.layers.46.mlp.gate_proj.g_idx', 'llm.model.layers.46.mlp.gate_proj.qweight', 'llm.model.layers.46.mlp.gate_proj.qzeros', 'llm.model.layers.46.mlp.gate_proj.scales', 'llm.model.layers.46.mlp.up_proj.g_idx', 'llm.model.layers.46.mlp.up_proj.qweight', 'llm.model.layers.46.mlp.up_proj.qzeros', 'llm.model.layers.46.mlp.up_proj.scales', 'llm.model.layers.46.self_attn.k_proj.g_idx', 'llm.model.layers.46.self_attn.k_proj.qweight', 'llm.model.layers.46.self_attn.k_proj.qzeros', 'llm.model.layers.46.self_attn.k_proj.scales', 'llm.model.layers.46.self_attn.o_proj.g_idx', 'llm.model.layers.46.self_attn.o_proj.qweight', 'llm.model.layers.46.self_attn.o_proj.qzeros', 'llm.model.layers.46.self_attn.o_proj.scales', 'llm.model.layers.46.self_attn.q_proj.g_idx', 'llm.model.layers.46.self_attn.q_proj.qweight', 'llm.model.layers.46.self_attn.q_proj.qzeros', 'llm.model.layers.46.self_attn.q_proj.scales', 'llm.model.layers.46.self_attn.v_proj.g_idx', 'llm.model.layers.46.self_attn.v_proj.qweight', 'llm.model.layers.46.self_attn.v_proj.qzeros', 'llm.model.layers.46.self_attn.v_proj.scales', 'llm.model.layers.47.mlp.down_proj.g_idx', 'llm.model.layers.47.mlp.down_proj.qweight', 'llm.model.layers.47.mlp.down_proj.qzeros', 'llm.model.layers.47.mlp.down_proj.scales', 'llm.model.layers.47.mlp.gate_proj.g_idx', 'llm.model.layers.47.mlp.gate_proj.qweight', 'llm.model.layers.47.mlp.gate_proj.qzeros', 'llm.model.layers.47.mlp.gate_proj.scales', 'llm.model.layers.47.mlp.up_proj.g_idx', 'llm.model.layers.47.mlp.up_proj.qweight', 'llm.model.layers.47.mlp.up_proj.qzeros', 'llm.model.layers.47.mlp.up_proj.scales', 'llm.model.layers.47.self_attn.k_proj.g_idx', 'llm.model.layers.47.self_attn.k_proj.qweight', 'llm.model.layers.47.self_attn.k_proj.qzeros', 'llm.model.layers.47.self_attn.k_proj.scales', 'llm.model.layers.47.self_attn.o_proj.g_idx', 'llm.model.layers.47.self_attn.o_proj.qweight', 'llm.model.layers.47.self_attn.o_proj.qzeros', 'llm.model.layers.47.self_attn.o_proj.scales', 'llm.model.layers.47.self_attn.q_proj.g_idx', 'llm.model.layers.47.self_attn.q_proj.qweight', 'llm.model.layers.47.self_attn.q_proj.qzeros', 'llm.model.layers.47.self_attn.q_proj.scales', 'llm.model.layers.47.self_attn.v_proj.g_idx', 'llm.model.layers.47.self_attn.v_proj.qweight', 'llm.model.layers.47.self_attn.v_proj.qzeros', 'llm.model.layers.47.self_attn.v_proj.scales', 'llm.model.layers.48.mlp.down_proj.g_idx', 'llm.model.layers.48.mlp.down_proj.qweight', 'llm.model.layers.48.mlp.down_proj.qzeros', 'llm.model.layers.48.mlp.down_proj.scales', 'llm.model.layers.48.mlp.gate_proj.g_idx', 'llm.model.layers.48.mlp.gate_proj.qweight', 'llm.model.layers.48.mlp.gate_proj.qzeros', 'llm.model.layers.48.mlp.gate_proj.scales', 'llm.model.layers.48.mlp.up_proj.g_idx', 'llm.model.layers.48.mlp.up_proj.qweight', 'llm.model.layers.48.mlp.up_proj.qzeros', 'llm.model.layers.48.mlp.up_proj.scales', 'llm.model.layers.48.self_attn.k_proj.g_idx', 'llm.model.layers.48.self_attn.k_proj.qweight', 'llm.model.layers.48.self_attn.k_proj.qzeros', 'llm.model.layers.48.self_attn.k_proj.scales', 'llm.model.layers.48.self_attn.o_proj.g_idx', 'llm.model.layers.48.self_attn.o_proj.qweight', 'llm.model.layers.48.self_attn.o_proj.qzeros', 'llm.model.layers.48.self_attn.o_proj.scales', 'llm.model.layers.48.self_attn.q_proj.g_idx', 'llm.model.layers.48.self_attn.q_proj.qweight', 'llm.model.layers.48.self_attn.q_proj.qzeros', 'llm.model.layers.48.self_attn.q_proj.scales', 'llm.model.layers.48.self_attn.v_proj.g_idx', 'llm.model.layers.48.self_attn.v_proj.qweight', 'llm.model.layers.48.self_attn.v_proj.qzeros', 'llm.model.layers.48.self_attn.v_proj.scales', 'llm.model.layers.49.mlp.down_proj.g_idx', 'llm.model.layers.49.mlp.down_proj.qweight', 'llm.model.layers.49.mlp.down_proj.qzeros', 'llm.model.layers.49.mlp.down_proj.scales', 'llm.model.layers.49.mlp.gate_proj.g_idx', 'llm.model.layers.49.mlp.gate_proj.qweight', 'llm.model.layers.49.mlp.gate_proj.qzeros', 'llm.model.layers.49.mlp.gate_proj.scales', 'llm.model.layers.49.mlp.up_proj.g_idx', 'llm.model.layers.49.mlp.up_proj.qweight', 'llm.model.layers.49.mlp.up_proj.qzeros', 'llm.model.layers.49.mlp.up_proj.scales', 'llm.model.layers.49.self_attn.k_proj.g_idx', 'llm.model.layers.49.self_attn.k_proj.qweight', 'llm.model.layers.49.self_attn.k_proj.qzeros', 'llm.model.layers.49.self_attn.k_proj.scales', 'llm.model.layers.49.self_attn.o_proj.g_idx', 'llm.model.layers.49.self_attn.o_proj.qweight', 'llm.model.layers.49.self_attn.o_proj.qzeros', 'llm.model.layers.49.self_attn.o_proj.scales', 'llm.model.layers.49.self_attn.q_proj.g_idx', 'llm.model.layers.49.self_attn.q_proj.qweight', 'llm.model.layers.49.self_attn.q_proj.qzeros', 'llm.model.layers.49.self_attn.q_proj.scales', 'llm.model.layers.49.self_attn.v_proj.g_idx', 'llm.model.layers.49.self_attn.v_proj.qweight', 'llm.model.layers.49.self_attn.v_proj.qzeros', 'llm.model.layers.49.self_attn.v_proj.scales', 'llm.model.layers.5.mlp.down_proj.g_idx', 'llm.model.layers.5.mlp.down_proj.qweight', 'llm.model.layers.5.mlp.down_proj.qzeros', 'llm.model.layers.5.mlp.down_proj.scales', 'llm.model.layers.5.mlp.gate_proj.g_idx', 'llm.model.layers.5.mlp.gate_proj.qweight', 'llm.model.layers.5.mlp.gate_proj.qzeros', 'llm.model.layers.5.mlp.gate_proj.scales', 'llm.model.layers.5.mlp.up_proj.g_idx', 'llm.model.layers.5.mlp.up_proj.qweight', 'llm.model.layers.5.mlp.up_proj.qzeros', 'llm.model.layers.5.mlp.up_proj.scales', 'llm.model.layers.5.self_attn.k_proj.g_idx', 'llm.model.layers.5.self_attn.k_proj.qweight', 'llm.model.layers.5.self_attn.k_proj.qzeros', 'llm.model.layers.5.self_attn.k_proj.scales', 'llm.model.layers.5.self_attn.o_proj.g_idx', 'llm.model.layers.5.self_attn.o_proj.qweight', 'llm.model.layers.5.self_attn.o_proj.qzeros', 'llm.model.layers.5.self_attn.o_proj.scales', 'llm.model.layers.5.self_attn.q_proj.g_idx', 'llm.model.layers.5.self_attn.q_proj.qweight', 'llm.model.layers.5.self_attn.q_proj.qzeros', 'llm.model.layers.5.self_attn.q_proj.scales', 'llm.model.layers.5.self_attn.v_proj.g_idx', 'llm.model.layers.5.self_attn.v_proj.qweight', 'llm.model.layers.5.self_attn.v_proj.qzeros', 'llm.model.layers.5.self_attn.v_proj.scales', 'llm.model.layers.50.mlp.down_proj.g_idx', 'llm.model.layers.50.mlp.down_proj.qweight', 'llm.model.layers.50.mlp.down_proj.qzeros', 'llm.model.layers.50.mlp.down_proj.scales', 'llm.model.layers.50.mlp.gate_proj.g_idx', 'llm.model.layers.50.mlp.gate_proj.qweight', 'llm.model.layers.50.mlp.gate_proj.qzeros', 'llm.model.layers.50.mlp.gate_proj.scales', 'llm.model.layers.50.mlp.up_proj.g_idx', 'llm.model.layers.50.mlp.up_proj.qweight', 'llm.model.layers.50.mlp.up_proj.qzeros', 'llm.model.layers.50.mlp.up_proj.scales', 'llm.model.layers.50.self_attn.k_proj.g_idx', 'llm.model.layers.50.self_attn.k_proj.qweight', 'llm.model.layers.50.self_attn.k_proj.qzeros', 'llm.model.layers.50.self_attn.k_proj.scales', 'llm.model.layers.50.self_attn.o_proj.g_idx', 'llm.model.layers.50.self_attn.o_proj.qweight', 'llm.model.layers.50.self_attn.o_proj.qzeros', 'llm.model.layers.50.self_attn.o_proj.scales', 'llm.model.layers.50.self_attn.q_proj.g_idx', 'llm.model.layers.50.self_attn.q_proj.qweight', 'llm.model.layers.50.self_attn.q_proj.qzeros', 'llm.model.layers.50.self_attn.q_proj.scales', 'llm.model.layers.50.self_attn.v_proj.g_idx', 'llm.model.layers.50.self_attn.v_proj.qweight', 'llm.model.layers.50.self_attn.v_proj.qzeros', 'llm.model.layers.50.self_attn.v_proj.scales', 'llm.model.layers.51.mlp.down_proj.g_idx', 'llm.model.layers.51.mlp.down_proj.qweight', 'llm.model.layers.51.mlp.down_proj.qzeros', 'llm.model.layers.51.mlp.down_proj.scales', 'llm.model.layers.51.mlp.gate_proj.g_idx', 'llm.model.layers.51.mlp.gate_proj.qweight', 'llm.model.layers.51.mlp.gate_proj.qzeros', 'llm.model.layers.51.mlp.gate_proj.scales', 'llm.model.layers.51.mlp.up_proj.g_idx', 'llm.model.layers.51.mlp.up_proj.qweight', 'llm.model.layers.51.mlp.up_proj.qzeros', 'llm.model.layers.51.mlp.up_proj.scales', 'llm.model.layers.51.self_attn.k_proj.g_idx', 'llm.model.layers.51.self_attn.k_proj.qweight', 'llm.model.layers.51.self_attn.k_proj.qzeros', 'llm.model.layers.51.self_attn.k_proj.scales', 'llm.model.layers.51.self_attn.o_proj.g_idx', 'llm.model.layers.51.self_attn.o_proj.qweight', 'llm.model.layers.51.self_attn.o_proj.qzeros', 'llm.model.layers.51.self_attn.o_proj.scales', 'llm.model.layers.51.self_attn.q_proj.g_idx', 'llm.model.layers.51.self_attn.q_proj.qweight', 'llm.model.layers.51.self_attn.q_proj.qzeros', 'llm.model.layers.51.self_attn.q_proj.scales', 'llm.model.layers.51.self_attn.v_proj.g_idx', 'llm.model.layers.51.self_attn.v_proj.qweight', 'llm.model.layers.51.self_attn.v_proj.qzeros', 'llm.model.layers.51.self_attn.v_proj.scales', 'llm.model.layers.6.mlp.down_proj.g_idx', 'llm.model.layers.6.mlp.down_proj.qweight', 'llm.model.layers.6.mlp.down_proj.qzeros', 'llm.model.layers.6.mlp.down_proj.scales', 'llm.model.layers.6.mlp.gate_proj.g_idx', 'llm.model.layers.6.mlp.gate_proj.qweight', 'llm.model.layers.6.mlp.gate_proj.qzeros', 'llm.model.layers.6.mlp.gate_proj.scales', 'llm.model.layers.6.mlp.up_proj.g_idx', 'llm.model.layers.6.mlp.up_proj.qweight', 'llm.model.layers.6.mlp.up_proj.qzeros', 'llm.model.layers.6.mlp.up_proj.scales', 'llm.model.layers.6.self_attn.k_proj.g_idx', 'llm.model.layers.6.self_attn.k_proj.qweight', 'llm.model.layers.6.self_attn.k_proj.qzeros', 'llm.model.layers.6.self_attn.k_proj.scales', 'llm.model.layers.6.self_attn.o_proj.g_idx', 'llm.model.layers.6.self_attn.o_proj.qweight', 'llm.model.layers.6.self_attn.o_proj.qzeros', 'llm.model.layers.6.self_attn.o_proj.scales', 'llm.model.layers.6.self_attn.q_proj.g_idx', 'llm.model.layers.6.self_attn.q_proj.qweight', 'llm.model.layers.6.self_attn.q_proj.qzeros', 'llm.model.layers.6.self_attn.q_proj.scales', 'llm.model.layers.6.self_attn.v_proj.g_idx', 'llm.model.layers.6.self_attn.v_proj.qweight', 'llm.model.layers.6.self_attn.v_proj.qzeros', 'llm.model.layers.6.self_attn.v_proj.scales', 'llm.model.layers.7.mlp.down_proj.g_idx', 'llm.model.layers.7.mlp.down_proj.qweight', 'llm.model.layers.7.mlp.down_proj.qzeros', 'llm.model.layers.7.mlp.down_proj.scales', 'llm.model.layers.7.mlp.gate_proj.g_idx', 'llm.model.layers.7.mlp.gate_proj.qweight', 'llm.model.layers.7.mlp.gate_proj.qzeros', 'llm.model.layers.7.mlp.gate_proj.scales', 'llm.model.layers.7.mlp.up_proj.g_idx', 'llm.model.layers.7.mlp.up_proj.qweight', 'llm.model.layers.7.mlp.up_proj.qzeros', 'llm.model.layers.7.mlp.up_proj.scales', 'llm.model.layers.7.self_attn.k_proj.g_idx', 'llm.model.layers.7.self_attn.k_proj.qweight', 'llm.model.layers.7.self_attn.k_proj.qzeros', 'llm.model.layers.7.self_attn.k_proj.scales', 'llm.model.layers.7.self_attn.o_proj.g_idx', 'llm.model.layers.7.self_attn.o_proj.qweight', 'llm.model.layers.7.self_attn.o_proj.qzeros', 'llm.model.layers.7.self_attn.o_proj.scales', 'llm.model.layers.7.self_attn.q_proj.g_idx', 'llm.model.layers.7.self_attn.q_proj.qweight', 'llm.model.layers.7.self_attn.q_proj.qzeros', 'llm.model.layers.7.self_attn.q_proj.scales', 'llm.model.layers.7.self_attn.v_proj.g_idx', 'llm.model.layers.7.self_attn.v_proj.qweight', 'llm.model.layers.7.self_attn.v_proj.qzeros', 'llm.model.layers.7.self_attn.v_proj.scales', 'llm.model.layers.8.mlp.down_proj.g_idx', 'llm.model.layers.8.mlp.down_proj.qweight', 'llm.model.layers.8.mlp.down_proj.qzeros', 'llm.model.layers.8.mlp.down_proj.scales', 'llm.model.layers.8.mlp.gate_proj.g_idx', 'llm.model.layers.8.mlp.gate_proj.qweight', 'llm.model.layers.8.mlp.gate_proj.qzeros', 'llm.model.layers.8.mlp.gate_proj.scales', 'llm.model.layers.8.mlp.up_proj.g_idx', 'llm.model.layers.8.mlp.up_proj.qweight', 'llm.model.layers.8.mlp.up_proj.qzeros', 'llm.model.layers.8.mlp.up_proj.scales', 'llm.model.layers.8.self_attn.k_proj.g_idx', 'llm.model.layers.8.self_attn.k_proj.qweight', 'llm.model.layers.8.self_attn.k_proj.qzeros', 'llm.model.layers.8.self_attn.k_proj.scales', 'llm.model.layers.8.self_attn.o_proj.g_idx', 'llm.model.layers.8.self_attn.o_proj.qweight', 'llm.model.layers.8.self_attn.o_proj.qzeros', 'llm.model.layers.8.self_attn.o_proj.scales', 'llm.model.layers.8.self_attn.q_proj.g_idx', 'llm.model.layers.8.self_attn.q_proj.qweight', 'llm.model.layers.8.self_attn.q_proj.qzeros', 'llm.model.layers.8.self_attn.q_proj.scales', 'llm.model.layers.8.self_attn.v_proj.g_idx', 'llm.model.layers.8.self_attn.v_proj.qweight', 'llm.model.layers.8.self_attn.v_proj.qzeros', 'llm.model.layers.8.self_attn.v_proj.scales', 'llm.model.layers.9.mlp.down_proj.g_idx', 'llm.model.layers.9.mlp.down_proj.qweight', 'llm.model.layers.9.mlp.down_proj.qzeros', 'llm.model.layers.9.mlp.down_proj.scales', 'llm.model.layers.9.mlp.gate_proj.g_idx', 'llm.model.layers.9.mlp.gate_proj.qweight', 'llm.model.layers.9.mlp.gate_proj.qzeros', 'llm.model.layers.9.mlp.gate_proj.scales', 'llm.model.layers.9.mlp.up_proj.g_idx', 'llm.model.layers.9.mlp.up_proj.qweight', 'llm.model.layers.9.mlp.up_proj.qzeros', 'llm.model.layers.9.mlp.up_proj.scales', 'llm.model.layers.9.self_attn.k_proj.g_idx', 'llm.model.layers.9.self_attn.k_proj.qweight', 'llm.model.layers.9.self_attn.k_proj.qzeros', 'llm.model.layers.9.self_attn.k_proj.scales', 'llm.model.layers.9.self_attn.o_proj.g_idx', 'llm.model.layers.9.self_attn.o_proj.qweight', 'llm.model.layers.9.self_attn.o_proj.qzeros', 'llm.model.layers.9.self_attn.o_proj.scales', 'llm.model.layers.9.self_attn.q_proj.g_idx', 'llm.model.layers.9.self_attn.q_proj.qweight', 'llm.model.layers.9.self_attn.q_proj.qzeros', 'llm.model.layers.9.self_attn.q_proj.scales', 'llm.model.layers.9.self_attn.v_proj.g_idx', 'llm.model.layers.9.self_attn.v_proj.qweight', 'llm.model.layers.9.self_attn.v_proj.qzeros', 'llm.model.layers.9.self_attn.v_proj.scales', 'vpm.encoder.layers.0.mlp.fc1.g_idx', 'vpm.encoder.layers.0.mlp.fc1.qweight', 'vpm.encoder.layers.0.mlp.fc1.qzeros', 'vpm.encoder.layers.0.mlp.fc1.scales', 'vpm.encoder.layers.0.mlp.fc2.g_idx', 'vpm.encoder.layers.0.mlp.fc2.qweight', 'vpm.encoder.layers.0.mlp.fc2.qzeros', 'vpm.encoder.layers.0.mlp.fc2.scales', 'vpm.encoder.layers.0.self_attn.k_proj.g_idx', 'vpm.encoder.layers.0.self_attn.k_proj.qweight', 'vpm.encoder.layers.0.self_attn.k_proj.qzeros', 'vpm.encoder.layers.0.self_attn.k_proj.scales', 'vpm.encoder.layers.0.self_attn.out_proj.g_idx', 'vpm.encoder.layers.0.self_attn.out_proj.qweight', 'vpm.encoder.layers.0.self_attn.out_proj.qzeros', 'vpm.encoder.layers.0.self_attn.out_proj.scales', 'vpm.encoder.layers.0.self_attn.q_proj.g_idx', 'vpm.encoder.layers.0.self_attn.q_proj.qweight', 'vpm.encoder.layers.0.self_attn.q_proj.qzeros', 'vpm.encoder.layers.0.self_attn.q_proj.scales', 'vpm.encoder.layers.0.self_attn.v_proj.g_idx', 'vpm.encoder.layers.0.self_attn.v_proj.qweight', 'vpm.encoder.layers.0.self_attn.v_proj.qzeros', 'vpm.encoder.layers.0.self_attn.v_proj.scales', 'vpm.encoder.layers.1.mlp.fc1.g_idx', 'vpm.encoder.layers.1.mlp.fc1.qweight', 'vpm.encoder.layers.1.mlp.fc1.qzeros', 'vpm.encoder.layers.1.mlp.fc1.scales', 'vpm.encoder.layers.1.mlp.fc2.g_idx', 'vpm.encoder.layers.1.mlp.fc2.qweight', 'vpm.encoder.layers.1.mlp.fc2.qzeros', 'vpm.encoder.layers.1.mlp.fc2.scales', 'vpm.encoder.layers.1.self_attn.k_proj.g_idx', 'vpm.encoder.layers.1.self_attn.k_proj.qweight', 'vpm.encoder.layers.1.self_attn.k_proj.qzeros', 'vpm.encoder.layers.1.self_attn.k_proj.scales', 'vpm.encoder.layers.1.self_attn.out_proj.g_idx', 'vpm.encoder.layers.1.self_attn.out_proj.qweight', 'vpm.encoder.layers.1.self_attn.out_proj.qzeros', 'vpm.encoder.layers.1.self_attn.out_proj.scales', 'vpm.encoder.layers.1.self_attn.q_proj.g_idx', 'vpm.encoder.layers.1.self_attn.q_proj.qweight', 'vpm.encoder.layers.1.self_attn.q_proj.qzeros', 'vpm.encoder.layers.1.self_attn.q_proj.scales', 'vpm.encoder.layers.1.self_attn.v_proj.g_idx', 'vpm.encoder.layers.1.self_attn.v_proj.qweight', 'vpm.encoder.layers.1.self_attn.v_proj.qzeros', 'vpm.encoder.layers.1.self_attn.v_proj.scales', 'vpm.encoder.layers.10.mlp.fc1.g_idx', 'vpm.encoder.layers.10.mlp.fc1.qweight', 'vpm.encoder.layers.10.mlp.fc1.qzeros', 'vpm.encoder.layers.10.mlp.fc1.scales', 'vpm.encoder.layers.10.mlp.fc2.g_idx', 'vpm.encoder.layers.10.mlp.fc2.qweight', 'vpm.encoder.layers.10.mlp.fc2.qzeros', 'vpm.encoder.layers.10.mlp.fc2.scales', 'vpm.encoder.layers.10.self_attn.k_proj.g_idx', 'vpm.encoder.layers.10.self_attn.k_proj.qweight', 'vpm.encoder.layers.10.self_attn.k_proj.qzeros', 'vpm.encoder.layers.10.self_attn.k_proj.scales', 'vpm.encoder.layers.10.self_attn.out_proj.g_idx', 'vpm.encoder.layers.10.self_attn.out_proj.qweight', 'vpm.encoder.layers.10.self_attn.out_proj.qzeros', 'vpm.encoder.layers.10.self_attn.out_proj.scales', 'vpm.encoder.layers.10.self_attn.q_proj.g_idx', 'vpm.encoder.layers.10.self_attn.q_proj.qweight', 'vpm.encoder.layers.10.self_attn.q_proj.qzeros', 'vpm.encoder.layers.10.self_attn.q_proj.scales', 'vpm.encoder.layers.10.self_attn.v_proj.g_idx', 'vpm.encoder.layers.10.self_attn.v_proj.qweight', 'vpm.encoder.layers.10.self_attn.v_proj.qzeros', 'vpm.encoder.layers.10.self_attn.v_proj.scales', 'vpm.encoder.layers.11.mlp.fc1.g_idx', 'vpm.encoder.layers.11.mlp.fc1.qweight', 'vpm.encoder.layers.11.mlp.fc1.qzeros', 'vpm.encoder.layers.11.mlp.fc1.scales', 'vpm.encoder.layers.11.mlp.fc2.g_idx', 'vpm.encoder.layers.11.mlp.fc2.qweight', 'vpm.encoder.layers.11.mlp.fc2.qzeros', 'vpm.encoder.layers.11.mlp.fc2.scales', 'vpm.encoder.layers.11.self_attn.k_proj.g_idx', 'vpm.encoder.layers.11.self_attn.k_proj.qweight', 'vpm.encoder.layers.11.self_attn.k_proj.qzeros', 'vpm.encoder.layers.11.self_attn.k_proj.scales', 'vpm.encoder.layers.11.self_attn.out_proj.g_idx', 'vpm.encoder.layers.11.self_attn.out_proj.qweight', 'vpm.encoder.layers.11.self_attn.out_proj.qzeros', 'vpm.encoder.layers.11.self_attn.out_proj.scales', 'vpm.encoder.layers.11.self_attn.q_proj.g_idx', 'vpm.encoder.layers.11.self_attn.q_proj.qweight', 'vpm.encoder.layers.11.self_attn.q_proj.qzeros', 'vpm.encoder.layers.11.self_attn.q_proj.scales', 'vpm.encoder.layers.11.self_attn.v_proj.g_idx', 'vpm.encoder.layers.11.self_attn.v_proj.qweight', 'vpm.encoder.layers.11.self_attn.v_proj.qzeros', 'vpm.encoder.layers.11.self_attn.v_proj.scales', 'vpm.encoder.layers.12.mlp.fc1.g_idx', 'vpm.encoder.layers.12.mlp.fc1.qweight', 'vpm.encoder.layers.12.mlp.fc1.qzeros', 'vpm.encoder.layers.12.mlp.fc1.scales', 'vpm.encoder.layers.12.mlp.fc2.g_idx', 'vpm.encoder.layers.12.mlp.fc2.qweight', 'vpm.encoder.layers.12.mlp.fc2.qzeros', 'vpm.encoder.layers.12.mlp.fc2.scales', 'vpm.encoder.layers.12.self_attn.k_proj.g_idx', 'vpm.encoder.layers.12.self_attn.k_proj.qweight', 'vpm.encoder.layers.12.self_attn.k_proj.qzeros', 'vpm.encoder.layers.12.self_attn.k_proj.scales', 'vpm.encoder.layers.12.self_attn.out_proj.g_idx', 'vpm.encoder.layers.12.self_attn.out_proj.qweight', 'vpm.encoder.layers.12.self_attn.out_proj.qzeros', 'vpm.encoder.layers.12.self_attn.out_proj.scales', 'vpm.encoder.layers.12.self_attn.q_proj.g_idx', 'vpm.encoder.layers.12.self_attn.q_proj.qweight', 'vpm.encoder.layers.12.self_attn.q_proj.qzeros', 'vpm.encoder.layers.12.self_attn.q_proj.scales', 'vpm.encoder.layers.12.self_attn.v_proj.g_idx', 'vpm.encoder.layers.12.self_attn.v_proj.qweight', 'vpm.encoder.layers.12.self_attn.v_proj.qzeros', 'vpm.encoder.layers.12.self_attn.v_proj.scales', 'vpm.encoder.layers.13.mlp.fc1.g_idx', 'vpm.encoder.layers.13.mlp.fc1.qweight', 'vpm.encoder.layers.13.mlp.fc1.qzeros', 'vpm.encoder.layers.13.mlp.fc1.scales', 'vpm.encoder.layers.13.mlp.fc2.g_idx', 'vpm.encoder.layers.13.mlp.fc2.qweight', 'vpm.encoder.layers.13.mlp.fc2.qzeros', 'vpm.encoder.layers.13.mlp.fc2.scales', 'vpm.encoder.layers.13.self_attn.k_proj.g_idx', 'vpm.encoder.layers.13.self_attn.k_proj.qweight', 'vpm.encoder.layers.13.self_attn.k_proj.qzeros', 'vpm.encoder.layers.13.self_attn.k_proj.scales', 'vpm.encoder.layers.13.self_attn.out_proj.g_idx', 'vpm.encoder.layers.13.self_attn.out_proj.qweight', 'vpm.encoder.layers.13.self_attn.out_proj.qzeros', 'vpm.encoder.layers.13.self_attn.out_proj.scales', 'vpm.encoder.layers.13.self_attn.q_proj.g_idx', 'vpm.encoder.layers.13.self_attn.q_proj.qweight', 'vpm.encoder.layers.13.self_attn.q_proj.qzeros', 'vpm.encoder.layers.13.self_attn.q_proj.scales', 'vpm.encoder.layers.13.self_attn.v_proj.g_idx', 'vpm.encoder.layers.13.self_attn.v_proj.qweight', 'vpm.encoder.layers.13.self_attn.v_proj.qzeros', 'vpm.encoder.layers.13.self_attn.v_proj.scales', 'vpm.encoder.layers.14.mlp.fc1.g_idx', 'vpm.encoder.layers.14.mlp.fc1.qweight', 'vpm.encoder.layers.14.mlp.fc1.qzeros', 'vpm.encoder.layers.14.mlp.fc1.scales', 'vpm.encoder.layers.14.mlp.fc2.g_idx', 'vpm.encoder.layers.14.mlp.fc2.qweight', 'vpm.encoder.layers.14.mlp.fc2.qzeros', 'vpm.encoder.layers.14.mlp.fc2.scales', 'vpm.encoder.layers.14.self_attn.k_proj.g_idx', 'vpm.encoder.layers.14.self_attn.k_proj.qweight', 'vpm.encoder.layers.14.self_attn.k_proj.qzeros', 'vpm.encoder.layers.14.self_attn.k_proj.scales', 'vpm.encoder.layers.14.self_attn.out_proj.g_idx', 'vpm.encoder.layers.14.self_attn.out_proj.qweight', 'vpm.encoder.layers.14.self_attn.out_proj.qzeros', 'vpm.encoder.layers.14.self_attn.out_proj.scales', 'vpm.encoder.layers.14.self_attn.q_proj.g_idx', 'vpm.encoder.layers.14.self_attn.q_proj.qweight', 'vpm.encoder.layers.14.self_attn.q_proj.qzeros', 'vpm.encoder.layers.14.self_attn.q_proj.scales', 'vpm.encoder.layers.14.self_attn.v_proj.g_idx', 'vpm.encoder.layers.14.self_attn.v_proj.qweight', 'vpm.encoder.layers.14.self_attn.v_proj.qzeros', 'vpm.encoder.layers.14.self_attn.v_proj.scales', 'vpm.encoder.layers.15.mlp.fc1.g_idx', 'vpm.encoder.layers.15.mlp.fc1.qweight', 'vpm.encoder.layers.15.mlp.fc1.qzeros', 'vpm.encoder.layers.15.mlp.fc1.scales', 'vpm.encoder.layers.15.mlp.fc2.g_idx', 'vpm.encoder.layers.15.mlp.fc2.qweight', 'vpm.encoder.layers.15.mlp.fc2.qzeros', 'vpm.encoder.layers.15.mlp.fc2.scales', 'vpm.encoder.layers.15.self_attn.k_proj.g_idx', 'vpm.encoder.layers.15.self_attn.k_proj.qweight', 'vpm.encoder.layers.15.self_attn.k_proj.qzeros', 'vpm.encoder.layers.15.self_attn.k_proj.scales', 'vpm.encoder.layers.15.self_attn.out_proj.g_idx', 'vpm.encoder.layers.15.self_attn.out_proj.qweight', 'vpm.encoder.layers.15.self_attn.out_proj.qzeros', 'vpm.encoder.layers.15.self_attn.out_proj.scales', 'vpm.encoder.layers.15.self_attn.q_proj.g_idx', 'vpm.encoder.layers.15.self_attn.q_proj.qweight', 'vpm.encoder.layers.15.self_attn.q_proj.qzeros', 'vpm.encoder.layers.15.self_attn.q_proj.scales', 'vpm.encoder.layers.15.self_attn.v_proj.g_idx', 'vpm.encoder.layers.15.self_attn.v_proj.qweight', 'vpm.encoder.layers.15.self_attn.v_proj.qzeros', 'vpm.encoder.layers.15.self_attn.v_proj.scales', 'vpm.encoder.layers.16.mlp.fc1.g_idx', 'vpm.encoder.layers.16.mlp.fc1.qweight', 'vpm.encoder.layers.16.mlp.fc1.qzeros', 'vpm.encoder.layers.16.mlp.fc1.scales', 'vpm.encoder.layers.16.mlp.fc2.g_idx', 'vpm.encoder.layers.16.mlp.fc2.qweight', 'vpm.encoder.layers.16.mlp.fc2.qzeros', 'vpm.encoder.layers.16.mlp.fc2.scales', 'vpm.encoder.layers.16.self_attn.k_proj.g_idx', 'vpm.encoder.layers.16.self_attn.k_proj.qweight', 'vpm.encoder.layers.16.self_attn.k_proj.qzeros', 'vpm.encoder.layers.16.self_attn.k_proj.scales', 'vpm.encoder.layers.16.self_attn.out_proj.g_idx', 'vpm.encoder.layers.16.self_attn.out_proj.qweight', 'vpm.encoder.layers.16.self_attn.out_proj.qzeros', 'vpm.encoder.layers.16.self_attn.out_proj.scales', 'vpm.encoder.layers.16.self_attn.q_proj.g_idx', 'vpm.encoder.layers.16.self_attn.q_proj.qweight', 'vpm.encoder.layers.16.self_attn.q_proj.qzeros', 'vpm.encoder.layers.16.self_attn.q_proj.scales', 'vpm.encoder.layers.16.self_attn.v_proj.g_idx', 'vpm.encoder.layers.16.self_attn.v_proj.qweight', 'vpm.encoder.layers.16.self_attn.v_proj.qzeros', 'vpm.encoder.layers.16.self_attn.v_proj.scales', 'vpm.encoder.layers.17.mlp.fc1.g_idx', 'vpm.encoder.layers.17.mlp.fc1.qweight', 'vpm.encoder.layers.17.mlp.fc1.qzeros', 'vpm.encoder.layers.17.mlp.fc1.scales', 'vpm.encoder.layers.17.mlp.fc2.g_idx', 'vpm.encoder.layers.17.mlp.fc2.qweight', 'vpm.encoder.layers.17.mlp.fc2.qzeros', 'vpm.encoder.layers.17.mlp.fc2.scales', 'vpm.encoder.layers.17.self_attn.k_proj.g_idx', 'vpm.encoder.layers.17.self_attn.k_proj.qweight', 'vpm.encoder.layers.17.self_attn.k_proj.qzeros', 'vpm.encoder.layers.17.self_attn.k_proj.scales', 'vpm.encoder.layers.17.self_attn.out_proj.g_idx', 'vpm.encoder.layers.17.self_attn.out_proj.qweight', 'vpm.encoder.layers.17.self_attn.out_proj.qzeros', 'vpm.encoder.layers.17.self_attn.out_proj.scales', 'vpm.encoder.layers.17.self_attn.q_proj.g_idx', 'vpm.encoder.layers.17.self_attn.q_proj.qweight', 'vpm.encoder.layers.17.self_attn.q_proj.qzeros', 'vpm.encoder.layers.17.self_attn.q_proj.scales', 'vpm.encoder.layers.17.self_attn.v_proj.g_idx', 'vpm.encoder.layers.17.self_attn.v_proj.qweight', 'vpm.encoder.layers.17.self_attn.v_proj.qzeros', 'vpm.encoder.layers.17.self_attn.v_proj.scales', 'vpm.encoder.layers.18.mlp.fc1.g_idx', 'vpm.encoder.layers.18.mlp.fc1.qweight', 'vpm.encoder.layers.18.mlp.fc1.qzeros', 'vpm.encoder.layers.18.mlp.fc1.scales', 'vpm.encoder.layers.18.mlp.fc2.g_idx', 'vpm.encoder.layers.18.mlp.fc2.qweight', 'vpm.encoder.layers.18.mlp.fc2.qzeros', 'vpm.encoder.layers.18.mlp.fc2.scales', 'vpm.encoder.layers.18.self_attn.k_proj.g_idx', 'vpm.encoder.layers.18.self_attn.k_proj.qweight', 'vpm.encoder.layers.18.self_attn.k_proj.qzeros', 'vpm.encoder.layers.18.self_attn.k_proj.scales', 'vpm.encoder.layers.18.self_attn.out_proj.g_idx', 'vpm.encoder.layers.18.self_attn.out_proj.qweight', 'vpm.encoder.layers.18.self_attn.out_proj.qzeros', 'vpm.encoder.layers.18.self_attn.out_proj.scales', 'vpm.encoder.layers.18.self_attn.q_proj.g_idx', 'vpm.encoder.layers.18.self_attn.q_proj.qweight', 'vpm.encoder.layers.18.self_attn.q_proj.qzeros', 'vpm.encoder.layers.18.self_attn.q_proj.scales', 'vpm.encoder.layers.18.self_attn.v_proj.g_idx', 'vpm.encoder.layers.18.self_attn.v_proj.qweight', 'vpm.encoder.layers.18.self_attn.v_proj.qzeros', 'vpm.encoder.layers.18.self_attn.v_proj.scales', 'vpm.encoder.layers.19.mlp.fc1.g_idx', 'vpm.encoder.layers.19.mlp.fc1.qweight', 'vpm.encoder.layers.19.mlp.fc1.qzeros', 'vpm.encoder.layers.19.mlp.fc1.scales', 'vpm.encoder.layers.19.mlp.fc2.g_idx', 'vpm.encoder.layers.19.mlp.fc2.qweight', 'vpm.encoder.layers.19.mlp.fc2.qzeros', 'vpm.encoder.layers.19.mlp.fc2.scales', 'vpm.encoder.layers.19.self_attn.k_proj.g_idx', 'vpm.encoder.layers.19.self_attn.k_proj.qweight', 'vpm.encoder.layers.19.self_attn.k_proj.qzeros', 'vpm.encoder.layers.19.self_attn.k_proj.scales', 'vpm.encoder.layers.19.self_attn.out_proj.g_idx', 'vpm.encoder.layers.19.self_attn.out_proj.qweight', 'vpm.encoder.layers.19.self_attn.out_proj.qzeros', 'vpm.encoder.layers.19.self_attn.out_proj.scales', 'vpm.encoder.layers.19.self_attn.q_proj.g_idx', 'vpm.encoder.layers.19.self_attn.q_proj.qweight', 'vpm.encoder.layers.19.self_attn.q_proj.qzeros', 'vpm.encoder.layers.19.self_attn.q_proj.scales', 'vpm.encoder.layers.19.self_attn.v_proj.g_idx', 'vpm.encoder.layers.19.self_attn.v_proj.qweight', 'vpm.encoder.layers.19.self_attn.v_proj.qzeros', 'vpm.encoder.layers.19.self_attn.v_proj.scales', 'vpm.encoder.layers.2.mlp.fc1.g_idx', 'vpm.encoder.layers.2.mlp.fc1.qweight', 'vpm.encoder.layers.2.mlp.fc1.qzeros', 'vpm.encoder.layers.2.mlp.fc1.scales', 'vpm.encoder.layers.2.mlp.fc2.g_idx', 'vpm.encoder.layers.2.mlp.fc2.qweight', 'vpm.encoder.layers.2.mlp.fc2.qzeros', 'vpm.encoder.layers.2.mlp.fc2.scales', 'vpm.encoder.layers.2.self_attn.k_proj.g_idx', 'vpm.encoder.layers.2.self_attn.k_proj.qweight', 'vpm.encoder.layers.2.self_attn.k_proj.qzeros', 'vpm.encoder.layers.2.self_attn.k_proj.scales', 'vpm.encoder.layers.2.self_attn.out_proj.g_idx', 'vpm.encoder.layers.2.self_attn.out_proj.qweight', 'vpm.encoder.layers.2.self_attn.out_proj.qzeros', 'vpm.encoder.layers.2.self_attn.out_proj.scales', 'vpm.encoder.layers.2.self_attn.q_proj.g_idx', 'vpm.encoder.layers.2.self_attn.q_proj.qweight', 'vpm.encoder.layers.2.self_attn.q_proj.qzeros', 'vpm.encoder.layers.2.self_attn.q_proj.scales', 'vpm.encoder.layers.2.self_attn.v_proj.g_idx', 'vpm.encoder.layers.2.self_attn.v_proj.qweight', 'vpm.encoder.layers.2.self_attn.v_proj.qzeros', 'vpm.encoder.layers.2.self_attn.v_proj.scales', 'vpm.encoder.layers.20.mlp.fc1.g_idx', 'vpm.encoder.layers.20.mlp.fc1.qweight', 'vpm.encoder.layers.20.mlp.fc1.qzeros', 'vpm.encoder.layers.20.mlp.fc1.scales', 'vpm.encoder.layers.20.mlp.fc2.g_idx', 'vpm.encoder.layers.20.mlp.fc2.qweight', 'vpm.encoder.layers.20.mlp.fc2.qzeros', 'vpm.encoder.layers.20.mlp.fc2.scales', 'vpm.encoder.layers.20.self_attn.k_proj.g_idx', 'vpm.encoder.layers.20.self_attn.k_proj.qweight', 'vpm.encoder.layers.20.self_attn.k_proj.qzeros', 'vpm.encoder.layers.20.self_attn.k_proj.scales', 'vpm.encoder.layers.20.self_attn.out_proj.g_idx', 'vpm.encoder.layers.20.self_attn.out_proj.qweight', 'vpm.encoder.layers.20.self_attn.out_proj.qzeros', 'vpm.encoder.layers.20.self_attn.out_proj.scales', 'vpm.encoder.layers.20.self_attn.q_proj.g_idx', 'vpm.encoder.layers.20.self_attn.q_proj.qweight', 'vpm.encoder.layers.20.self_attn.q_proj.qzeros', 'vpm.encoder.layers.20.self_attn.q_proj.scales', 'vpm.encoder.layers.20.self_attn.v_proj.g_idx', 'vpm.encoder.layers.20.self_attn.v_proj.qweight', 'vpm.encoder.layers.20.self_attn.v_proj.qzeros', 'vpm.encoder.layers.20.self_attn.v_proj.scales', 'vpm.encoder.layers.21.mlp.fc1.g_idx', 'vpm.encoder.layers.21.mlp.fc1.qweight', 'vpm.encoder.layers.21.mlp.fc1.qzeros', 'vpm.encoder.layers.21.mlp.fc1.scales', 'vpm.encoder.layers.21.mlp.fc2.g_idx', 'vpm.encoder.layers.21.mlp.fc2.qweight', 'vpm.encoder.layers.21.mlp.fc2.qzeros', 'vpm.encoder.layers.21.mlp.fc2.scales', 'vpm.encoder.layers.21.self_attn.k_proj.g_idx', 'vpm.encoder.layers.21.self_attn.k_proj.qweight', 'vpm.encoder.layers.21.self_attn.k_proj.qzeros', 'vpm.encoder.layers.21.self_attn.k_proj.scales', 'vpm.encoder.layers.21.self_attn.out_proj.g_idx', 'vpm.encoder.layers.21.self_attn.out_proj.qweight', 'vpm.encoder.layers.21.self_attn.out_proj.qzeros', 'vpm.encoder.layers.21.self_attn.out_proj.scales', 'vpm.encoder.layers.21.self_attn.q_proj.g_idx', 'vpm.encoder.layers.21.self_attn.q_proj.qweight', 'vpm.encoder.layers.21.self_attn.q_proj.qzeros', 'vpm.encoder.layers.21.self_attn.q_proj.scales', 'vpm.encoder.layers.21.self_attn.v_proj.g_idx', 'vpm.encoder.layers.21.self_attn.v_proj.qweight', 'vpm.encoder.layers.21.self_attn.v_proj.qzeros', 'vpm.encoder.layers.21.self_attn.v_proj.scales', 'vpm.encoder.layers.22.mlp.fc1.g_idx', 'vpm.encoder.layers.22.mlp.fc1.qweight', 'vpm.encoder.layers.22.mlp.fc1.qzeros', 'vpm.encoder.layers.22.mlp.fc1.scales', 'vpm.encoder.layers.22.mlp.fc2.g_idx', 'vpm.encoder.layers.22.mlp.fc2.qweight', 'vpm.encoder.layers.22.mlp.fc2.qzeros', 'vpm.encoder.layers.22.mlp.fc2.scales', 'vpm.encoder.layers.22.self_attn.k_proj.g_idx', 'vpm.encoder.layers.22.self_attn.k_proj.qweight', 'vpm.encoder.layers.22.self_attn.k_proj.qzeros', 'vpm.encoder.layers.22.self_attn.k_proj.scales', 'vpm.encoder.layers.22.self_attn.out_proj.g_idx', 'vpm.encoder.layers.22.self_attn.out_proj.qweight', 'vpm.encoder.layers.22.self_attn.out_proj.qzeros', 'vpm.encoder.layers.22.self_attn.out_proj.scales', 'vpm.encoder.layers.22.self_attn.q_proj.g_idx', 'vpm.encoder.layers.22.self_attn.q_proj.qweight', 'vpm.encoder.layers.22.self_attn.q_proj.qzeros', 'vpm.encoder.layers.22.self_attn.q_proj.scales', 'vpm.encoder.layers.22.self_attn.v_proj.g_idx', 'vpm.encoder.layers.22.self_attn.v_proj.qweight', 'vpm.encoder.layers.22.self_attn.v_proj.qzeros', 'vpm.encoder.layers.22.self_attn.v_proj.scales', 'vpm.encoder.layers.23.mlp.fc1.g_idx', 'vpm.encoder.layers.23.mlp.fc1.qweight', 'vpm.encoder.layers.23.mlp.fc1.qzeros', 'vpm.encoder.layers.23.mlp.fc1.scales', 'vpm.encoder.layers.23.mlp.fc2.g_idx', 'vpm.encoder.layers.23.mlp.fc2.qweight', 'vpm.encoder.layers.23.mlp.fc2.qzeros', 'vpm.encoder.layers.23.mlp.fc2.scales', 'vpm.encoder.layers.23.self_attn.k_proj.g_idx', 'vpm.encoder.layers.23.self_attn.k_proj.qweight', 'vpm.encoder.layers.23.self_attn.k_proj.qzeros', 'vpm.encoder.layers.23.self_attn.k_proj.scales', 'vpm.encoder.layers.23.self_attn.out_proj.g_idx', 'vpm.encoder.layers.23.self_attn.out_proj.qweight', 'vpm.encoder.layers.23.self_attn.out_proj.qzeros', 'vpm.encoder.layers.23.self_attn.out_proj.scales', 'vpm.encoder.layers.23.self_attn.q_proj.g_idx', 'vpm.encoder.layers.23.self_attn.q_proj.qweight', 'vpm.encoder.layers.23.self_attn.q_proj.qzeros', 'vpm.encoder.layers.23.self_attn.q_proj.scales', 'vpm.encoder.layers.23.self_attn.v_proj.g_idx', 'vpm.encoder.layers.23.self_attn.v_proj.qweight', 'vpm.encoder.layers.23.self_attn.v_proj.qzeros', 'vpm.encoder.layers.23.self_attn.v_proj.scales', 'vpm.encoder.layers.24.mlp.fc1.g_idx', 'vpm.encoder.layers.24.mlp.fc1.qweight', 'vpm.encoder.layers.24.mlp.fc1.qzeros', 'vpm.encoder.layers.24.mlp.fc1.scales', 'vpm.encoder.layers.24.mlp.fc2.g_idx', 'vpm.encoder.layers.24.mlp.fc2.qweight', 'vpm.encoder.layers.24.mlp.fc2.qzeros', 'vpm.encoder.layers.24.mlp.fc2.scales', 'vpm.encoder.layers.24.self_attn.k_proj.g_idx', 'vpm.encoder.layers.24.self_attn.k_proj.qweight', 'vpm.encoder.layers.24.self_attn.k_proj.qzeros', 'vpm.encoder.layers.24.self_attn.k_proj.scales', 'vpm.encoder.layers.24.self_attn.out_proj.g_idx', 'vpm.encoder.layers.24.self_attn.out_proj.qweight', 'vpm.encoder.layers.24.self_attn.out_proj.qzeros', 'vpm.encoder.layers.24.self_attn.out_proj.scales', 'vpm.encoder.layers.24.self_attn.q_proj.g_idx', 'vpm.encoder.layers.24.self_attn.q_proj.qweight', 'vpm.encoder.layers.24.self_attn.q_proj.qzeros', 'vpm.encoder.layers.24.self_attn.q_proj.scales', 'vpm.encoder.layers.24.self_attn.v_proj.g_idx', 'vpm.encoder.layers.24.self_attn.v_proj.qweight', 'vpm.encoder.layers.24.self_attn.v_proj.qzeros', 'vpm.encoder.layers.24.self_attn.v_proj.scales', 'vpm.encoder.layers.25.mlp.fc1.g_idx', 'vpm.encoder.layers.25.mlp.fc1.qweight', 'vpm.encoder.layers.25.mlp.fc1.qzeros', 'vpm.encoder.layers.25.mlp.fc1.scales', 'vpm.encoder.layers.25.mlp.fc2.g_idx', 'vpm.encoder.layers.25.mlp.fc2.qweight', 'vpm.encoder.layers.25.mlp.fc2.qzeros', 'vpm.encoder.layers.25.mlp.fc2.scales', 'vpm.encoder.layers.25.self_attn.k_proj.g_idx', 'vpm.encoder.layers.25.self_attn.k_proj.qweight', 'vpm.encoder.layers.25.self_attn.k_proj.qzeros', 'vpm.encoder.layers.25.self_attn.k_proj.scales', 'vpm.encoder.layers.25.self_attn.out_proj.g_idx', 'vpm.encoder.layers.25.self_attn.out_proj.qweight', 'vpm.encoder.layers.25.self_attn.out_proj.qzeros', 'vpm.encoder.layers.25.self_attn.out_proj.scales', 'vpm.encoder.layers.25.self_attn.q_proj.g_idx', 'vpm.encoder.layers.25.self_attn.q_proj.qweight', 'vpm.encoder.layers.25.self_attn.q_proj.qzeros', 'vpm.encoder.layers.25.self_attn.q_proj.scales', 'vpm.encoder.layers.25.self_attn.v_proj.g_idx', 'vpm.encoder.layers.25.self_attn.v_proj.qweight', 'vpm.encoder.layers.25.self_attn.v_proj.qzeros', 'vpm.encoder.layers.25.self_attn.v_proj.scales', 'vpm.encoder.layers.26.mlp.fc1.g_idx', 'vpm.encoder.layers.26.mlp.fc1.qweight', 'vpm.encoder.layers.26.mlp.fc1.qzeros', 'vpm.encoder.layers.26.mlp.fc1.scales', 'vpm.encoder.layers.26.mlp.fc2.g_idx', 'vpm.encoder.layers.26.mlp.fc2.qweight', 'vpm.encoder.layers.26.mlp.fc2.qzeros', 'vpm.encoder.layers.26.mlp.fc2.scales', 'vpm.encoder.layers.26.self_attn.k_proj.g_idx', 'vpm.encoder.layers.26.self_attn.k_proj.qweight', 'vpm.encoder.layers.26.self_attn.k_proj.qzeros', 'vpm.encoder.layers.26.self_attn.k_proj.scales', 'vpm.encoder.layers.26.self_attn.out_proj.g_idx', 'vpm.encoder.layers.26.self_attn.out_proj.qweight', 'vpm.encoder.layers.26.self_attn.out_proj.qzeros', 'vpm.encoder.layers.26.self_attn.out_proj.scales', 'vpm.encoder.layers.26.self_attn.q_proj.g_idx', 'vpm.encoder.layers.26.self_attn.q_proj.qweight', 'vpm.encoder.layers.26.self_attn.q_proj.qzeros', 'vpm.encoder.layers.26.self_attn.q_proj.scales', 'vpm.encoder.layers.26.self_attn.v_proj.g_idx', 'vpm.encoder.layers.26.self_attn.v_proj.qweight', 'vpm.encoder.layers.26.self_attn.v_proj.qzeros', 'vpm.encoder.layers.26.self_attn.v_proj.scales', 'vpm.encoder.layers.3.mlp.fc1.g_idx', 'vpm.encoder.layers.3.mlp.fc1.qweight', 'vpm.encoder.layers.3.mlp.fc1.qzeros', 'vpm.encoder.layers.3.mlp.fc1.scales', 'vpm.encoder.layers.3.mlp.fc2.g_idx', 'vpm.encoder.layers.3.mlp.fc2.qweight', 'vpm.encoder.layers.3.mlp.fc2.qzeros', 'vpm.encoder.layers.3.mlp.fc2.scales', 'vpm.encoder.layers.3.self_attn.k_proj.g_idx', 'vpm.encoder.layers.3.self_attn.k_proj.qweight', 'vpm.encoder.layers.3.self_attn.k_proj.qzeros', 'vpm.encoder.layers.3.self_attn.k_proj.scales', 'vpm.encoder.layers.3.self_attn.out_proj.g_idx', 'vpm.encoder.layers.3.self_attn.out_proj.qweight', 'vpm.encoder.layers.3.self_attn.out_proj.qzeros', 'vpm.encoder.layers.3.self_attn.out_proj.scales', 'vpm.encoder.layers.3.self_attn.q_proj.g_idx', 'vpm.encoder.layers.3.self_attn.q_proj.qweight', 'vpm.encoder.layers.3.self_attn.q_proj.qzeros', 'vpm.encoder.layers.3.self_attn.q_proj.scales', 'vpm.encoder.layers.3.self_attn.v_proj.g_idx', 'vpm.encoder.layers.3.self_attn.v_proj.qweight', 'vpm.encoder.layers.3.self_attn.v_proj.qzeros', 'vpm.encoder.layers.3.self_attn.v_proj.scales', 'vpm.encoder.layers.4.mlp.fc1.g_idx', 'vpm.encoder.layers.4.mlp.fc1.qweight', 'vpm.encoder.layers.4.mlp.fc1.qzeros', 'vpm.encoder.layers.4.mlp.fc1.scales', 'vpm.encoder.layers.4.mlp.fc2.g_idx', 'vpm.encoder.layers.4.mlp.fc2.qweight', 'vpm.encoder.layers.4.mlp.fc2.qzeros', 'vpm.encoder.layers.4.mlp.fc2.scales', 'vpm.encoder.layers.4.self_attn.k_proj.g_idx', 'vpm.encoder.layers.4.self_attn.k_proj.qweight', 'vpm.encoder.layers.4.self_attn.k_proj.qzeros', 'vpm.encoder.layers.4.self_attn.k_proj.scales', 'vpm.encoder.layers.4.self_attn.out_proj.g_idx', 'vpm.encoder.layers.4.self_attn.out_proj.qweight', 'vpm.encoder.layers.4.self_attn.out_proj.qzeros', 'vpm.encoder.layers.4.self_attn.out_proj.scales', 'vpm.encoder.layers.4.self_attn.q_proj.g_idx', 'vpm.encoder.layers.4.self_attn.q_proj.qweight', 'vpm.encoder.layers.4.self_attn.q_proj.qzeros', 'vpm.encoder.layers.4.self_attn.q_proj.scales', 'vpm.encoder.layers.4.self_attn.v_proj.g_idx', 'vpm.encoder.layers.4.self_attn.v_proj.qweight', 'vpm.encoder.layers.4.self_attn.v_proj.qzeros', 'vpm.encoder.layers.4.self_attn.v_proj.scales', 'vpm.encoder.layers.5.mlp.fc1.g_idx', 'vpm.encoder.layers.5.mlp.fc1.qweight', 'vpm.encoder.layers.5.mlp.fc1.qzeros', 'vpm.encoder.layers.5.mlp.fc1.scales', 'vpm.encoder.layers.5.mlp.fc2.g_idx', 'vpm.encoder.layers.5.mlp.fc2.qweight', 'vpm.encoder.layers.5.mlp.fc2.qzeros', 'vpm.encoder.layers.5.mlp.fc2.scales', 'vpm.encoder.layers.5.self_attn.k_proj.g_idx', 'vpm.encoder.layers.5.self_attn.k_proj.qweight', 'vpm.encoder.layers.5.self_attn.k_proj.qzeros', 'vpm.encoder.layers.5.self_attn.k_proj.scales', 'vpm.encoder.layers.5.self_attn.out_proj.g_idx', 'vpm.encoder.layers.5.self_attn.out_proj.qweight', 'vpm.encoder.layers.5.self_attn.out_proj.qzeros', 'vpm.encoder.layers.5.self_attn.out_proj.scales', 'vpm.encoder.layers.5.self_attn.q_proj.g_idx', 'vpm.encoder.layers.5.self_attn.q_proj.qweight', 'vpm.encoder.layers.5.self_attn.q_proj.qzeros', 'vpm.encoder.layers.5.self_attn.q_proj.scales', 'vpm.encoder.layers.5.self_attn.v_proj.g_idx', 'vpm.encoder.layers.5.self_attn.v_proj.qweight', 'vpm.encoder.layers.5.self_attn.v_proj.qzeros', 'vpm.encoder.layers.5.self_attn.v_proj.scales', 'vpm.encoder.layers.6.mlp.fc1.g_idx', 'vpm.encoder.layers.6.mlp.fc1.qweight', 'vpm.encoder.layers.6.mlp.fc1.qzeros', 'vpm.encoder.layers.6.mlp.fc1.scales', 'vpm.encoder.layers.6.mlp.fc2.g_idx', 'vpm.encoder.layers.6.mlp.fc2.qweight', 'vpm.encoder.layers.6.mlp.fc2.qzeros', 'vpm.encoder.layers.6.mlp.fc2.scales', 'vpm.encoder.layers.6.self_attn.k_proj.g_idx', 'vpm.encoder.layers.6.self_attn.k_proj.qweight', 'vpm.encoder.layers.6.self_attn.k_proj.qzeros', 'vpm.encoder.layers.6.self_attn.k_proj.scales', 'vpm.encoder.layers.6.self_attn.out_proj.g_idx', 'vpm.encoder.layers.6.self_attn.out_proj.qweight', 'vpm.encoder.layers.6.self_attn.out_proj.qzeros', 'vpm.encoder.layers.6.self_attn.out_proj.scales', 'vpm.encoder.layers.6.self_attn.q_proj.g_idx', 'vpm.encoder.layers.6.self_attn.q_proj.qweight', 'vpm.encoder.layers.6.self_attn.q_proj.qzeros', 'vpm.encoder.layers.6.self_attn.q_proj.scales', 'vpm.encoder.layers.6.self_attn.v_proj.g_idx', 'vpm.encoder.layers.6.self_attn.v_proj.qweight', 'vpm.encoder.layers.6.self_attn.v_proj.qzeros', 'vpm.encoder.layers.6.self_attn.v_proj.scales', 'vpm.encoder.layers.7.mlp.fc1.g_idx', 'vpm.encoder.layers.7.mlp.fc1.qweight', 'vpm.encoder.layers.7.mlp.fc1.qzeros', 'vpm.encoder.layers.7.mlp.fc1.scales', 'vpm.encoder.layers.7.mlp.fc2.g_idx', 'vpm.encoder.layers.7.mlp.fc2.qweight', 'vpm.encoder.layers.7.mlp.fc2.qzeros', 'vpm.encoder.layers.7.mlp.fc2.scales', 'vpm.encoder.layers.7.self_attn.k_proj.g_idx', 'vpm.encoder.layers.7.self_attn.k_proj.qweight', 'vpm.encoder.layers.7.self_attn.k_proj.qzeros', 'vpm.encoder.layers.7.self_attn.k_proj.scales', 'vpm.encoder.layers.7.self_attn.out_proj.g_idx', 'vpm.encoder.layers.7.self_attn.out_proj.qweight', 'vpm.encoder.layers.7.self_attn.out_proj.qzeros', 'vpm.encoder.layers.7.self_attn.out_proj.scales', 'vpm.encoder.layers.7.self_attn.q_proj.g_idx', 'vpm.encoder.layers.7.self_attn.q_proj.qweight', 'vpm.encoder.layers.7.self_attn.q_proj.qzeros', 'vpm.encoder.layers.7.self_attn.q_proj.scales', 'vpm.encoder.layers.7.self_attn.v_proj.g_idx', 'vpm.encoder.layers.7.self_attn.v_proj.qweight', 'vpm.encoder.layers.7.self_attn.v_proj.qzeros', 'vpm.encoder.layers.7.self_attn.v_proj.scales', 'vpm.encoder.layers.8.mlp.fc1.g_idx', 'vpm.encoder.layers.8.mlp.fc1.qweight', 'vpm.encoder.layers.8.mlp.fc1.qzeros', 'vpm.encoder.layers.8.mlp.fc1.scales', 'vpm.encoder.layers.8.mlp.fc2.g_idx', 'vpm.encoder.layers.8.mlp.fc2.qweight', 'vpm.encoder.layers.8.mlp.fc2.qzeros', 'vpm.encoder.layers.8.mlp.fc2.scales', 'vpm.encoder.layers.8.self_attn.k_proj.g_idx', 'vpm.encoder.layers.8.self_attn.k_proj.qweight', 'vpm.encoder.layers.8.self_attn.k_proj.qzeros', 'vpm.encoder.layers.8.self_attn.k_proj.scales', 'vpm.encoder.layers.8.self_attn.out_proj.g_idx', 'vpm.encoder.layers.8.self_attn.out_proj.qweight', 'vpm.encoder.layers.8.self_attn.out_proj.qzeros', 'vpm.encoder.layers.8.self_attn.out_proj.scales', 'vpm.encoder.layers.8.self_attn.q_proj.g_idx', 'vpm.encoder.layers.8.self_attn.q_proj.qweight', 'vpm.encoder.layers.8.self_attn.q_proj.qzeros', 'vpm.encoder.layers.8.self_attn.q_proj.scales', 'vpm.encoder.layers.8.self_attn.v_proj.g_idx', 'vpm.encoder.layers.8.self_attn.v_proj.qweight', 'vpm.encoder.layers.8.self_attn.v_proj.qzeros', 'vpm.encoder.layers.8.self_attn.v_proj.scales', 'vpm.encoder.layers.9.mlp.fc1.g_idx', 'vpm.encoder.layers.9.mlp.fc1.qweight', 'vpm.encoder.layers.9.mlp.fc1.qzeros', 'vpm.encoder.layers.9.mlp.fc1.scales', 'vpm.encoder.layers.9.mlp.fc2.g_idx', 'vpm.encoder.layers.9.mlp.fc2.qweight', 'vpm.encoder.layers.9.mlp.fc2.qzeros', 'vpm.encoder.layers.9.mlp.fc2.scales', 'vpm.encoder.layers.9.self_attn.k_proj.g_idx', 'vpm.encoder.layers.9.self_attn.k_proj.qweight', 'vpm.encoder.layers.9.self_attn.k_proj.qzeros', 'vpm.encoder.layers.9.self_attn.k_proj.scales', 'vpm.encoder.layers.9.self_attn.out_proj.g_idx', 'vpm.encoder.layers.9.self_attn.out_proj.qweight', 'vpm.encoder.layers.9.self_attn.out_proj.qzeros', 'vpm.encoder.layers.9.self_attn.out_proj.scales', 'vpm.encoder.layers.9.self_attn.q_proj.g_idx', 'vpm.encoder.layers.9.self_attn.q_proj.qweight', 'vpm.encoder.layers.9.self_attn.q_proj.qzeros', 'vpm.encoder.layers.9.self_attn.q_proj.scales', 'vpm.encoder.layers.9.self_attn.v_proj.g_idx', 'vpm.encoder.layers.9.self_attn.v_proj.qweight', 'vpm.encoder.layers.9.self_attn.v_proj.qzeros', 'vpm.encoder.layers.9.self_attn.v_proj.scales']\n",
      "- This IS expected if you are initializing MiniCPMV from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MiniCPMV from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MiniCPMV were not initialized from the model checkpoint at /home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256-llm-w4-pc and are newly initialized: ['llm.model.layers.0.mlp.down_proj.weight', 'llm.model.layers.0.mlp.gate_proj.weight', 'llm.model.layers.0.mlp.up_proj.weight', 'llm.model.layers.0.self_attn.k_proj.weight', 'llm.model.layers.0.self_attn.v_proj.weight', 'llm.model.layers.1.mlp.down_proj.weight', 'llm.model.layers.1.mlp.gate_proj.weight', 'llm.model.layers.1.mlp.up_proj.weight', 'llm.model.layers.1.self_attn.k_proj.weight', 'llm.model.layers.1.self_attn.v_proj.weight', 'llm.model.layers.10.mlp.down_proj.weight', 'llm.model.layers.10.mlp.gate_proj.weight', 'llm.model.layers.10.mlp.up_proj.weight', 'llm.model.layers.10.self_attn.k_proj.weight', 'llm.model.layers.10.self_attn.v_proj.weight', 'llm.model.layers.11.mlp.down_proj.weight', 'llm.model.layers.11.mlp.gate_proj.weight', 'llm.model.layers.11.mlp.up_proj.weight', 'llm.model.layers.11.self_attn.k_proj.weight', 'llm.model.layers.11.self_attn.v_proj.weight', 'llm.model.layers.12.mlp.down_proj.weight', 'llm.model.layers.12.mlp.gate_proj.weight', 'llm.model.layers.12.mlp.up_proj.weight', 'llm.model.layers.12.self_attn.k_proj.weight', 'llm.model.layers.12.self_attn.v_proj.weight', 'llm.model.layers.13.mlp.down_proj.weight', 'llm.model.layers.13.mlp.gate_proj.weight', 'llm.model.layers.13.mlp.up_proj.weight', 'llm.model.layers.13.self_attn.k_proj.weight', 'llm.model.layers.13.self_attn.v_proj.weight', 'llm.model.layers.14.mlp.down_proj.weight', 'llm.model.layers.14.mlp.gate_proj.weight', 'llm.model.layers.14.mlp.up_proj.weight', 'llm.model.layers.14.self_attn.k_proj.weight', 'llm.model.layers.14.self_attn.v_proj.weight', 'llm.model.layers.15.mlp.down_proj.weight', 'llm.model.layers.15.mlp.gate_proj.weight', 'llm.model.layers.15.mlp.up_proj.weight', 'llm.model.layers.15.self_attn.k_proj.weight', 'llm.model.layers.15.self_attn.v_proj.weight', 'llm.model.layers.16.mlp.down_proj.weight', 'llm.model.layers.16.mlp.gate_proj.weight', 'llm.model.layers.16.mlp.up_proj.weight', 'llm.model.layers.16.self_attn.k_proj.weight', 'llm.model.layers.16.self_attn.v_proj.weight', 'llm.model.layers.17.mlp.down_proj.weight', 'llm.model.layers.17.mlp.gate_proj.weight', 'llm.model.layers.17.mlp.up_proj.weight', 'llm.model.layers.17.self_attn.k_proj.weight', 'llm.model.layers.17.self_attn.v_proj.weight', 'llm.model.layers.18.mlp.down_proj.weight', 'llm.model.layers.18.mlp.gate_proj.weight', 'llm.model.layers.18.mlp.up_proj.weight', 'llm.model.layers.18.self_attn.k_proj.weight', 'llm.model.layers.18.self_attn.v_proj.weight', 'llm.model.layers.19.mlp.down_proj.weight', 'llm.model.layers.19.mlp.gate_proj.weight', 'llm.model.layers.19.mlp.up_proj.weight', 'llm.model.layers.19.self_attn.k_proj.weight', 'llm.model.layers.19.self_attn.v_proj.weight', 'llm.model.layers.2.mlp.down_proj.weight', 'llm.model.layers.2.mlp.gate_proj.weight', 'llm.model.layers.2.mlp.up_proj.weight', 'llm.model.layers.2.self_attn.k_proj.weight', 'llm.model.layers.2.self_attn.v_proj.weight', 'llm.model.layers.20.mlp.down_proj.weight', 'llm.model.layers.20.mlp.gate_proj.weight', 'llm.model.layers.20.mlp.up_proj.weight', 'llm.model.layers.20.self_attn.k_proj.weight', 'llm.model.layers.20.self_attn.v_proj.weight', 'llm.model.layers.21.mlp.down_proj.weight', 'llm.model.layers.21.mlp.gate_proj.weight', 'llm.model.layers.21.mlp.up_proj.weight', 'llm.model.layers.21.self_attn.k_proj.weight', 'llm.model.layers.21.self_attn.v_proj.weight', 'llm.model.layers.22.mlp.down_proj.weight', 'llm.model.layers.22.mlp.gate_proj.weight', 'llm.model.layers.22.mlp.up_proj.weight', 'llm.model.layers.22.self_attn.k_proj.weight', 'llm.model.layers.22.self_attn.v_proj.weight', 'llm.model.layers.23.mlp.down_proj.weight', 'llm.model.layers.23.mlp.gate_proj.weight', 'llm.model.layers.23.mlp.up_proj.weight', 'llm.model.layers.23.self_attn.k_proj.weight', 'llm.model.layers.23.self_attn.v_proj.weight', 'llm.model.layers.24.mlp.down_proj.weight', 'llm.model.layers.24.mlp.gate_proj.weight', 'llm.model.layers.24.mlp.up_proj.weight', 'llm.model.layers.24.self_attn.k_proj.weight', 'llm.model.layers.24.self_attn.v_proj.weight', 'llm.model.layers.25.mlp.down_proj.weight', 'llm.model.layers.25.mlp.gate_proj.weight', 'llm.model.layers.25.mlp.up_proj.weight', 'llm.model.layers.25.self_attn.k_proj.weight', 'llm.model.layers.25.self_attn.v_proj.weight', 'llm.model.layers.26.mlp.down_proj.weight', 'llm.model.layers.26.mlp.gate_proj.weight', 'llm.model.layers.26.mlp.up_proj.weight', 'llm.model.layers.26.self_attn.k_proj.weight', 'llm.model.layers.26.self_attn.v_proj.weight', 'llm.model.layers.27.mlp.down_proj.weight', 'llm.model.layers.27.mlp.gate_proj.weight', 'llm.model.layers.27.mlp.up_proj.weight', 'llm.model.layers.27.self_attn.k_proj.weight', 'llm.model.layers.27.self_attn.v_proj.weight', 'llm.model.layers.28.mlp.down_proj.weight', 'llm.model.layers.28.mlp.gate_proj.weight', 'llm.model.layers.28.mlp.up_proj.weight', 'llm.model.layers.28.self_attn.k_proj.weight', 'llm.model.layers.28.self_attn.v_proj.weight', 'llm.model.layers.29.mlp.down_proj.weight', 'llm.model.layers.29.mlp.gate_proj.weight', 'llm.model.layers.29.mlp.up_proj.weight', 'llm.model.layers.29.self_attn.k_proj.weight', 'llm.model.layers.29.self_attn.v_proj.weight', 'llm.model.layers.3.mlp.down_proj.weight', 'llm.model.layers.3.mlp.gate_proj.weight', 'llm.model.layers.3.mlp.up_proj.weight', 'llm.model.layers.3.self_attn.k_proj.weight', 'llm.model.layers.3.self_attn.v_proj.weight', 'llm.model.layers.30.mlp.down_proj.weight', 'llm.model.layers.30.mlp.gate_proj.weight', 'llm.model.layers.30.mlp.up_proj.weight', 'llm.model.layers.30.self_attn.k_proj.weight', 'llm.model.layers.30.self_attn.v_proj.weight', 'llm.model.layers.31.mlp.down_proj.weight', 'llm.model.layers.31.mlp.gate_proj.weight', 'llm.model.layers.31.mlp.up_proj.weight', 'llm.model.layers.31.self_attn.k_proj.weight', 'llm.model.layers.31.self_attn.v_proj.weight', 'llm.model.layers.32.mlp.down_proj.weight', 'llm.model.layers.32.mlp.gate_proj.weight', 'llm.model.layers.32.mlp.up_proj.weight', 'llm.model.layers.32.self_attn.k_proj.weight', 'llm.model.layers.32.self_attn.v_proj.weight', 'llm.model.layers.33.mlp.down_proj.weight', 'llm.model.layers.33.mlp.gate_proj.weight', 'llm.model.layers.33.mlp.up_proj.weight', 'llm.model.layers.33.self_attn.k_proj.weight', 'llm.model.layers.33.self_attn.v_proj.weight', 'llm.model.layers.34.mlp.down_proj.weight', 'llm.model.layers.34.mlp.gate_proj.weight', 'llm.model.layers.34.mlp.up_proj.weight', 'llm.model.layers.34.self_attn.k_proj.weight', 'llm.model.layers.34.self_attn.v_proj.weight', 'llm.model.layers.35.mlp.down_proj.weight', 'llm.model.layers.35.mlp.gate_proj.weight', 'llm.model.layers.35.mlp.up_proj.weight', 'llm.model.layers.35.self_attn.k_proj.weight', 'llm.model.layers.35.self_attn.v_proj.weight', 'llm.model.layers.36.mlp.down_proj.weight', 'llm.model.layers.36.mlp.gate_proj.weight', 'llm.model.layers.36.mlp.up_proj.weight', 'llm.model.layers.36.self_attn.k_proj.weight', 'llm.model.layers.36.self_attn.v_proj.weight', 'llm.model.layers.37.mlp.down_proj.weight', 'llm.model.layers.37.mlp.gate_proj.weight', 'llm.model.layers.37.mlp.up_proj.weight', 'llm.model.layers.37.self_attn.k_proj.weight', 'llm.model.layers.37.self_attn.v_proj.weight', 'llm.model.layers.38.mlp.down_proj.weight', 'llm.model.layers.38.mlp.gate_proj.weight', 'llm.model.layers.38.mlp.up_proj.weight', 'llm.model.layers.38.self_attn.k_proj.weight', 'llm.model.layers.38.self_attn.v_proj.weight', 'llm.model.layers.39.mlp.down_proj.weight', 'llm.model.layers.39.mlp.gate_proj.weight', 'llm.model.layers.39.mlp.up_proj.weight', 'llm.model.layers.39.self_attn.k_proj.weight', 'llm.model.layers.39.self_attn.v_proj.weight', 'llm.model.layers.4.mlp.down_proj.weight', 'llm.model.layers.4.mlp.gate_proj.weight', 'llm.model.layers.4.mlp.up_proj.weight', 'llm.model.layers.4.self_attn.k_proj.weight', 'llm.model.layers.4.self_attn.v_proj.weight', 'llm.model.layers.40.mlp.down_proj.weight', 'llm.model.layers.40.mlp.gate_proj.weight', 'llm.model.layers.40.mlp.up_proj.weight', 'llm.model.layers.40.self_attn.k_proj.weight', 'llm.model.layers.40.self_attn.v_proj.weight', 'llm.model.layers.41.mlp.down_proj.weight', 'llm.model.layers.41.mlp.gate_proj.weight', 'llm.model.layers.41.mlp.up_proj.weight', 'llm.model.layers.41.self_attn.k_proj.weight', 'llm.model.layers.41.self_attn.v_proj.weight', 'llm.model.layers.42.mlp.down_proj.weight', 'llm.model.layers.42.mlp.gate_proj.weight', 'llm.model.layers.42.mlp.up_proj.weight', 'llm.model.layers.42.self_attn.k_proj.weight', 'llm.model.layers.42.self_attn.v_proj.weight', 'llm.model.layers.43.mlp.down_proj.weight', 'llm.model.layers.43.mlp.gate_proj.weight', 'llm.model.layers.43.mlp.up_proj.weight', 'llm.model.layers.43.self_attn.k_proj.weight', 'llm.model.layers.43.self_attn.v_proj.weight', 'llm.model.layers.44.mlp.down_proj.weight', 'llm.model.layers.44.mlp.gate_proj.weight', 'llm.model.layers.44.mlp.up_proj.weight', 'llm.model.layers.44.self_attn.k_proj.weight', 'llm.model.layers.44.self_attn.v_proj.weight', 'llm.model.layers.45.mlp.down_proj.weight', 'llm.model.layers.45.mlp.gate_proj.weight', 'llm.model.layers.45.mlp.up_proj.weight', 'llm.model.layers.45.self_attn.k_proj.weight', 'llm.model.layers.45.self_attn.v_proj.weight', 'llm.model.layers.46.mlp.down_proj.weight', 'llm.model.layers.46.mlp.gate_proj.weight', 'llm.model.layers.46.mlp.up_proj.weight', 'llm.model.layers.46.self_attn.k_proj.weight', 'llm.model.layers.46.self_attn.v_proj.weight', 'llm.model.layers.47.mlp.down_proj.weight', 'llm.model.layers.47.mlp.gate_proj.weight', 'llm.model.layers.47.mlp.up_proj.weight', 'llm.model.layers.47.self_attn.k_proj.weight', 'llm.model.layers.47.self_attn.v_proj.weight', 'llm.model.layers.48.mlp.down_proj.weight', 'llm.model.layers.48.mlp.gate_proj.weight', 'llm.model.layers.48.mlp.up_proj.weight', 'llm.model.layers.48.self_attn.k_proj.weight', 'llm.model.layers.48.self_attn.v_proj.weight', 'llm.model.layers.49.mlp.down_proj.weight', 'llm.model.layers.49.mlp.gate_proj.weight', 'llm.model.layers.49.mlp.up_proj.weight', 'llm.model.layers.49.self_attn.k_proj.weight', 'llm.model.layers.49.self_attn.v_proj.weight', 'llm.model.layers.5.mlp.down_proj.weight', 'llm.model.layers.5.mlp.gate_proj.weight', 'llm.model.layers.5.mlp.up_proj.weight', 'llm.model.layers.5.self_attn.k_proj.weight', 'llm.model.layers.5.self_attn.v_proj.weight', 'llm.model.layers.50.mlp.down_proj.weight', 'llm.model.layers.50.mlp.gate_proj.weight', 'llm.model.layers.50.mlp.up_proj.weight', 'llm.model.layers.50.self_attn.k_proj.weight', 'llm.model.layers.50.self_attn.v_proj.weight', 'llm.model.layers.51.mlp.down_proj.weight', 'llm.model.layers.51.mlp.gate_proj.weight', 'llm.model.layers.51.mlp.up_proj.weight', 'llm.model.layers.51.self_attn.k_proj.weight', 'llm.model.layers.51.self_attn.v_proj.weight', 'llm.model.layers.6.mlp.down_proj.weight', 'llm.model.layers.6.mlp.gate_proj.weight', 'llm.model.layers.6.mlp.up_proj.weight', 'llm.model.layers.6.self_attn.k_proj.weight', 'llm.model.layers.6.self_attn.v_proj.weight', 'llm.model.layers.7.mlp.down_proj.weight', 'llm.model.layers.7.mlp.gate_proj.weight', 'llm.model.layers.7.mlp.up_proj.weight', 'llm.model.layers.7.self_attn.k_proj.weight', 'llm.model.layers.7.self_attn.v_proj.weight', 'llm.model.layers.8.mlp.down_proj.weight', 'llm.model.layers.8.mlp.gate_proj.weight', 'llm.model.layers.8.mlp.up_proj.weight', 'llm.model.layers.8.self_attn.k_proj.weight', 'llm.model.layers.8.self_attn.v_proj.weight', 'llm.model.layers.9.mlp.down_proj.weight', 'llm.model.layers.9.mlp.gate_proj.weight', 'llm.model.layers.9.mlp.up_proj.weight', 'llm.model.layers.9.self_attn.k_proj.weight', 'llm.model.layers.9.self_attn.v_proj.weight', 'vpm.encoder.layers.0.mlp.fc1.weight', 'vpm.encoder.layers.0.mlp.fc2.weight', 'vpm.encoder.layers.0.self_attn.k_proj.weight', 'vpm.encoder.layers.0.self_attn.out_proj.weight', 'vpm.encoder.layers.0.self_attn.q_proj.weight', 'vpm.encoder.layers.0.self_attn.v_proj.weight', 'vpm.encoder.layers.1.mlp.fc1.weight', 'vpm.encoder.layers.1.mlp.fc2.weight', 'vpm.encoder.layers.1.self_attn.k_proj.weight', 'vpm.encoder.layers.1.self_attn.out_proj.weight', 'vpm.encoder.layers.1.self_attn.q_proj.weight', 'vpm.encoder.layers.1.self_attn.v_proj.weight', 'vpm.encoder.layers.10.mlp.fc1.weight', 'vpm.encoder.layers.10.mlp.fc2.weight', 'vpm.encoder.layers.10.self_attn.k_proj.weight', 'vpm.encoder.layers.10.self_attn.out_proj.weight', 'vpm.encoder.layers.10.self_attn.q_proj.weight', 'vpm.encoder.layers.10.self_attn.v_proj.weight', 'vpm.encoder.layers.11.mlp.fc1.weight', 'vpm.encoder.layers.11.mlp.fc2.weight', 'vpm.encoder.layers.11.self_attn.k_proj.weight', 'vpm.encoder.layers.11.self_attn.out_proj.weight', 'vpm.encoder.layers.11.self_attn.q_proj.weight', 'vpm.encoder.layers.11.self_attn.v_proj.weight', 'vpm.encoder.layers.12.mlp.fc1.weight', 'vpm.encoder.layers.12.mlp.fc2.weight', 'vpm.encoder.layers.12.self_attn.k_proj.weight', 'vpm.encoder.layers.12.self_attn.out_proj.weight', 'vpm.encoder.layers.12.self_attn.q_proj.weight', 'vpm.encoder.layers.12.self_attn.v_proj.weight', 'vpm.encoder.layers.13.mlp.fc1.weight', 'vpm.encoder.layers.13.mlp.fc2.weight', 'vpm.encoder.layers.13.self_attn.k_proj.weight', 'vpm.encoder.layers.13.self_attn.out_proj.weight', 'vpm.encoder.layers.13.self_attn.q_proj.weight', 'vpm.encoder.layers.13.self_attn.v_proj.weight', 'vpm.encoder.layers.14.mlp.fc1.weight', 'vpm.encoder.layers.14.mlp.fc2.weight', 'vpm.encoder.layers.14.self_attn.k_proj.weight', 'vpm.encoder.layers.14.self_attn.out_proj.weight', 'vpm.encoder.layers.14.self_attn.q_proj.weight', 'vpm.encoder.layers.14.self_attn.v_proj.weight', 'vpm.encoder.layers.15.mlp.fc1.weight', 'vpm.encoder.layers.15.mlp.fc2.weight', 'vpm.encoder.layers.15.self_attn.k_proj.weight', 'vpm.encoder.layers.15.self_attn.out_proj.weight', 'vpm.encoder.layers.15.self_attn.q_proj.weight', 'vpm.encoder.layers.15.self_attn.v_proj.weight', 'vpm.encoder.layers.16.mlp.fc1.weight', 'vpm.encoder.layers.16.mlp.fc2.weight', 'vpm.encoder.layers.16.self_attn.k_proj.weight', 'vpm.encoder.layers.16.self_attn.out_proj.weight', 'vpm.encoder.layers.16.self_attn.q_proj.weight', 'vpm.encoder.layers.16.self_attn.v_proj.weight', 'vpm.encoder.layers.17.mlp.fc1.weight', 'vpm.encoder.layers.17.mlp.fc2.weight', 'vpm.encoder.layers.17.self_attn.k_proj.weight', 'vpm.encoder.layers.17.self_attn.out_proj.weight', 'vpm.encoder.layers.17.self_attn.q_proj.weight', 'vpm.encoder.layers.17.self_attn.v_proj.weight', 'vpm.encoder.layers.18.mlp.fc1.weight', 'vpm.encoder.layers.18.mlp.fc2.weight', 'vpm.encoder.layers.18.self_attn.k_proj.weight', 'vpm.encoder.layers.18.self_attn.out_proj.weight', 'vpm.encoder.layers.18.self_attn.q_proj.weight', 'vpm.encoder.layers.18.self_attn.v_proj.weight', 'vpm.encoder.layers.19.mlp.fc1.weight', 'vpm.encoder.layers.19.mlp.fc2.weight', 'vpm.encoder.layers.19.self_attn.k_proj.weight', 'vpm.encoder.layers.19.self_attn.out_proj.weight', 'vpm.encoder.layers.19.self_attn.q_proj.weight', 'vpm.encoder.layers.19.self_attn.v_proj.weight', 'vpm.encoder.layers.2.mlp.fc1.weight', 'vpm.encoder.layers.2.mlp.fc2.weight', 'vpm.encoder.layers.2.self_attn.k_proj.weight', 'vpm.encoder.layers.2.self_attn.out_proj.weight', 'vpm.encoder.layers.2.self_attn.q_proj.weight', 'vpm.encoder.layers.2.self_attn.v_proj.weight', 'vpm.encoder.layers.20.mlp.fc1.weight', 'vpm.encoder.layers.20.mlp.fc2.weight', 'vpm.encoder.layers.20.self_attn.k_proj.weight', 'vpm.encoder.layers.20.self_attn.out_proj.weight', 'vpm.encoder.layers.20.self_attn.q_proj.weight', 'vpm.encoder.layers.20.self_attn.v_proj.weight', 'vpm.encoder.layers.21.mlp.fc1.weight', 'vpm.encoder.layers.21.mlp.fc2.weight', 'vpm.encoder.layers.21.self_attn.k_proj.weight', 'vpm.encoder.layers.21.self_attn.out_proj.weight', 'vpm.encoder.layers.21.self_attn.q_proj.weight', 'vpm.encoder.layers.21.self_attn.v_proj.weight', 'vpm.encoder.layers.22.mlp.fc1.weight', 'vpm.encoder.layers.22.mlp.fc2.weight', 'vpm.encoder.layers.22.self_attn.k_proj.weight', 'vpm.encoder.layers.22.self_attn.out_proj.weight', 'vpm.encoder.layers.22.self_attn.q_proj.weight', 'vpm.encoder.layers.22.self_attn.v_proj.weight', 'vpm.encoder.layers.23.mlp.fc1.weight', 'vpm.encoder.layers.23.mlp.fc2.weight', 'vpm.encoder.layers.23.self_attn.k_proj.weight', 'vpm.encoder.layers.23.self_attn.out_proj.weight', 'vpm.encoder.layers.23.self_attn.q_proj.weight', 'vpm.encoder.layers.23.self_attn.v_proj.weight', 'vpm.encoder.layers.24.mlp.fc1.weight', 'vpm.encoder.layers.24.mlp.fc2.weight', 'vpm.encoder.layers.24.self_attn.k_proj.weight', 'vpm.encoder.layers.24.self_attn.out_proj.weight', 'vpm.encoder.layers.24.self_attn.q_proj.weight', 'vpm.encoder.layers.24.self_attn.v_proj.weight', 'vpm.encoder.layers.25.mlp.fc1.weight', 'vpm.encoder.layers.25.mlp.fc2.weight', 'vpm.encoder.layers.25.self_attn.k_proj.weight', 'vpm.encoder.layers.25.self_attn.out_proj.weight', 'vpm.encoder.layers.25.self_attn.q_proj.weight', 'vpm.encoder.layers.25.self_attn.v_proj.weight', 'vpm.encoder.layers.26.mlp.fc1.weight', 'vpm.encoder.layers.26.mlp.fc2.weight', 'vpm.encoder.layers.26.self_attn.k_proj.weight', 'vpm.encoder.layers.26.self_attn.out_proj.weight', 'vpm.encoder.layers.26.self_attn.q_proj.weight', 'vpm.encoder.layers.26.self_attn.v_proj.weight', 'vpm.encoder.layers.3.mlp.fc1.weight', 'vpm.encoder.layers.3.mlp.fc2.weight', 'vpm.encoder.layers.3.self_attn.k_proj.weight', 'vpm.encoder.layers.3.self_attn.out_proj.weight', 'vpm.encoder.layers.3.self_attn.q_proj.weight', 'vpm.encoder.layers.3.self_attn.v_proj.weight', 'vpm.encoder.layers.4.mlp.fc1.weight', 'vpm.encoder.layers.4.mlp.fc2.weight', 'vpm.encoder.layers.4.self_attn.k_proj.weight', 'vpm.encoder.layers.4.self_attn.out_proj.weight', 'vpm.encoder.layers.4.self_attn.q_proj.weight', 'vpm.encoder.layers.4.self_attn.v_proj.weight', 'vpm.encoder.layers.5.mlp.fc1.weight', 'vpm.encoder.layers.5.mlp.fc2.weight', 'vpm.encoder.layers.5.self_attn.k_proj.weight', 'vpm.encoder.layers.5.self_attn.out_proj.weight', 'vpm.encoder.layers.5.self_attn.q_proj.weight', 'vpm.encoder.layers.5.self_attn.v_proj.weight', 'vpm.encoder.layers.6.mlp.fc1.weight', 'vpm.encoder.layers.6.mlp.fc2.weight', 'vpm.encoder.layers.6.self_attn.k_proj.weight', 'vpm.encoder.layers.6.self_attn.out_proj.weight', 'vpm.encoder.layers.6.self_attn.q_proj.weight', 'vpm.encoder.layers.6.self_attn.v_proj.weight', 'vpm.encoder.layers.7.mlp.fc1.weight', 'vpm.encoder.layers.7.mlp.fc2.weight', 'vpm.encoder.layers.7.self_attn.k_proj.weight', 'vpm.encoder.layers.7.self_attn.out_proj.weight', 'vpm.encoder.layers.7.self_attn.q_proj.weight', 'vpm.encoder.layers.7.self_attn.v_proj.weight', 'vpm.encoder.layers.8.mlp.fc1.weight', 'vpm.encoder.layers.8.mlp.fc2.weight', 'vpm.encoder.layers.8.self_attn.k_proj.weight', 'vpm.encoder.layers.8.self_attn.out_proj.weight', 'vpm.encoder.layers.8.self_attn.q_proj.weight', 'vpm.encoder.layers.8.self_attn.v_proj.weight', 'vpm.encoder.layers.9.mlp.fc1.weight', 'vpm.encoder.layers.9.mlp.fc2.weight', 'vpm.encoder.layers.9.self_attn.k_proj.weight', 'vpm.encoder.layers.9.self_attn.out_proj.weight', 'vpm.encoder.layers.9.self_attn.q_proj.weight', 'vpm.encoder.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from auto_gptq.modeling.minicpm_new.modeling_minicpmv import MiniCPMV\n",
    "quantized_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256-llm-w4-pc\"\n",
    "model = MiniCPMV.from_pretrained(quantized_model_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch.nn as nn\n",
    "\n",
    "def find_layers(module, layers=None, name=\"\"):\n",
    "    if not layers:\n",
    "        layers = [transformers.pytorch_utils.Conv1D, nn.Conv2d, nn.Linear]\n",
    "    for layer in layers:\n",
    "        if isinstance(module, layer):\n",
    "            return {name: module}\n",
    "    res = {}\n",
    "    for name1, child in module.named_children():\n",
    "        res.update(find_layers(child, layers=layers, name=name + \".\" + name1 if name != \"\" else name1))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm.model.layers.0.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.0.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.0.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.0.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.0.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.0.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.0.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.1.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.1.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.1.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.1.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.1.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.1.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.1.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.2.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.2.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.2.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.2.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.2.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.2.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.2.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.3.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.3.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.3.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.3.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.3.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.3.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.3.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.4.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.4.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.4.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.4.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.4.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.4.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.4.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.5.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.5.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.5.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.5.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.5.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.5.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.5.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.6.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.6.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.6.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.6.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.6.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.6.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.6.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.7.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.7.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.7.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.7.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.7.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.7.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.7.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.8.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.8.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.8.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.8.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.8.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.8.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.8.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.9.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.9.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.9.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.9.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.9.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.9.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.9.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.10.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.10.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.10.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.10.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.10.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.10.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.10.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.11.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.11.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.11.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.11.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.11.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.11.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.11.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.12.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.12.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.12.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.12.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.12.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.12.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.12.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.13.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.13.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.13.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.13.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.13.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.13.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.13.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.14.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.14.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.14.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.14.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.14.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.14.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.14.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.15.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.15.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.15.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.15.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.15.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.15.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.15.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.16.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.16.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.16.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.16.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.16.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.16.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.16.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.17.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.17.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.17.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.17.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.17.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.17.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.17.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.18.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.18.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.18.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.18.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.18.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.18.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.18.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.19.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.19.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.19.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.19.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.19.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.19.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.19.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.20.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.20.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.20.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.20.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.20.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.20.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.20.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.21.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.21.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.21.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.21.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.21.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.21.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.21.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.22.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.22.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.22.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.22.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.22.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.22.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.22.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.23.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.23.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.23.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.23.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.23.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.23.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.23.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.24.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.24.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.24.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.24.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.24.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.24.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.24.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.25.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.25.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.25.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.25.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.25.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.25.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.25.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.26.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.26.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.26.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.26.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.26.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.26.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.26.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.27.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.27.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.27.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.27.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.27.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.27.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.27.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.28.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.28.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.28.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.28.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.28.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.28.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.28.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.29.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.29.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.29.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.29.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.29.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.29.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.29.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.30.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.30.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.30.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.30.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.30.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.30.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.30.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.31.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.31.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.31.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.31.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.31.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.31.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.31.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.32.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.32.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.32.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.32.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.32.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.32.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.32.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.33.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.33.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.33.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.33.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.33.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.33.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.33.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.34.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.34.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.34.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.34.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.34.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.34.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.34.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.35.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.35.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.35.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.35.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.35.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.35.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.35.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.36.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.36.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.36.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.36.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.36.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.36.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.36.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.37.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.37.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.37.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.37.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.37.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.37.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.37.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.38.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.38.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.38.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.38.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.38.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.38.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.38.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.39.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.39.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.39.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.39.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.39.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.39.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.39.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.40.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.40.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.40.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.40.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.40.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.40.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.40.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.41.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.41.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.41.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.41.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.41.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.41.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.41.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.42.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.42.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.42.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.42.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.42.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.42.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.42.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.43.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.43.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.43.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.43.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.43.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.43.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.43.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.44.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.44.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.44.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.44.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.44.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.44.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.44.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.45.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.45.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.45.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.45.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.45.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.45.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.45.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.46.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.46.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.46.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.46.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.46.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.46.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.46.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.47.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.47.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.47.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.47.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.47.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.47.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.47.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.48.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.48.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.48.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.48.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.48.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.48.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.48.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.49.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.49.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.49.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.49.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.49.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.49.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.49.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.50.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.50.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.50.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.50.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.50.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.50.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.50.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.model.layers.51.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.51.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.51.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False),\n",
       " 'llm.model.layers.51.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False),\n",
       " 'llm.model.layers.51.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.51.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False),\n",
       " 'llm.model.layers.51.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False),\n",
       " 'llm.lm_head': Linear(in_features=1536, out_features=73464, bias=False),\n",
       " 'vpm.embeddings.patch_embedding': Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid),\n",
       " 'vpm.encoder.layers.0.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.0.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.0.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.0.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.0.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.0.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.1.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.1.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.1.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.1.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.1.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.1.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.2.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.2.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.2.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.2.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.2.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.2.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.3.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.3.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.3.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.3.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.3.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.3.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.4.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.4.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.4.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.4.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.4.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.4.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.5.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.5.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.5.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.5.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.5.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.5.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.6.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.6.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.6.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.6.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.6.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.6.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.7.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.7.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.7.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.7.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.7.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.7.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.8.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.8.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.8.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.8.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.8.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.8.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.9.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.9.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.9.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.9.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.9.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.9.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.10.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.10.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.10.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.10.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.10.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.10.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.11.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.11.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.11.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.11.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.11.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.11.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.12.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.12.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.12.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.12.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.12.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.12.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.13.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.13.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.13.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.13.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.13.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.13.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.14.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.14.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.14.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.14.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.14.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.14.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.15.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.15.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.15.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.15.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.15.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.15.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.16.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.16.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.16.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.16.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.16.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.16.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.17.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.17.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.17.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.17.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.17.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.17.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.18.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.18.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.18.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.18.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.18.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.18.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.19.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.19.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.19.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.19.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.19.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.19.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.20.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.20.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.20.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.20.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.20.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.20.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.21.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.21.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.21.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.21.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.21.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.21.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.22.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.22.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.22.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.22.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.22.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.22.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.23.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.23.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.23.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.23.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.23.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.23.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.24.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.24.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.24.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.24.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.24.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.24.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.25.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.25.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.25.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.25.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.25.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.25.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.26.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.26.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.26.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.26.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True),\n",
       " 'vpm.encoder.layers.26.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True),\n",
       " 'vpm.encoder.layers.26.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True),\n",
       " 'resampler.kv_proj': Linear(in_features=1152, out_features=1536, bias=False),\n",
       " 'resampler.attn.out_proj': Linear(in_features=1536, out_features=1536, bias=True)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_layers(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取激活参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lmquant/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_ScienceQA(nsamples, seed, seqlen, processor, status):\n",
    "    import torch.nn.functional as F\n",
    "    dataset = datasets.load_from_disk(\"/home/workspace/dataset/ScienceQA-2\")[\"train\"]\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "    rng = random.Random(42)\n",
    "\n",
    "    #数据拆分\n",
    "    if status == 0:\n",
    "        traindataset = []\n",
    "        for index, _data in enumerate(dataset):\n",
    "            prompts_lists = []\n",
    "            input_images_lists = []\n",
    "            promt = _data[\"question\"]\n",
    "            # image_file = _data[\"image\"]\n",
    "            image_file = _data[\"image\"]\n",
    "            if image_file is None:\n",
    "                nsamples = nsamples + 1\n",
    "                continue\n",
    "            else:\n",
    "                image = np.array(image_file)\n",
    "                # image = np.array(image_file.resize((448,  448)))\n",
    "            msgs = [{'role': 'user', 'content': \"(<image>./</image>)\\n\"+ promt}]\n",
    "            prompts_lists.append(processor.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True))\n",
    "            \n",
    "            input_images_lists.append([image])\n",
    "            if index >= nsamples:\n",
    "                break\n",
    "     \n",
    "            inputs = processor(\n",
    "                prompts_lists,\n",
    "                input_images_lists,\n",
    "                max_slice_nums=processor.image_processor.max_slice_nums,\n",
    "                use_image_id=processor.image_processor.use_image_id,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=8192\n",
    "            )\n",
    "\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            pixel_values = inputs[\"pixel_values\"]\n",
    "            image_sizes = inputs[\"image_sizes\"]\n",
    "            image_bound = inputs[\"image_bound\"]\n",
    "            tgt_sizes = inputs[\"tgt_sizes\"]\n",
    "            traindataset.append({\"input_ids\": input_ids, \n",
    "                                    \"attention_mask\": attention_mask,\n",
    "                                    \"pixel_values\": pixel_values,\n",
    "                                    \"image_sizes\": image_sizes,\n",
    "                                    \"image_bound\": image_bound,\n",
    "                                    \"tgt_sizes\": tgt_sizes})\n",
    "    elif status == 1:\n",
    "        traindataset = []\n",
    "        prompts_lists = []\n",
    "        input_images_lists = []\n",
    "        for index, _data in enumerate(dataset):\n",
    "            promt = _data[\"question\"]\n",
    "            image_file = _data[\"image\"]\n",
    "            image = np.array(image_file)\n",
    "            if image_file is None:\n",
    "                nsamples = nsamples+1\n",
    "                continue\n",
    "            msgs = [{'role': 'user', 'content': \"(<image>./</image>)\\n\"+ promt}]\n",
    "            prompts_lists.append(processor.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True))\n",
    "            input_images_lists.append([image])\n",
    "            if index >= nsamples-1:\n",
    "                break\n",
    "     \n",
    "        inputs = processor(\n",
    "            prompts_lists,\n",
    "            input_images_lists,\n",
    "            max_slice_nums=processor.image_processor.max_slice_nums,\n",
    "            use_image_id=processor.image_processor.use_image_id,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=8192\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        pixel_values = inputs[\"pixel_values\"]\n",
    "        image_sizes = inputs[\"image_sizes\"]\n",
    "        image_bound = inputs[\"image_bound\"]\n",
    "        tgt_sizes = inputs[\"tgt_sizes\"]\n",
    "        traindataset.append({\"input_ids\": input_ids, \n",
    "                                \"attention_mask\": attention_mask,\n",
    "                                \"pixel_values\": pixel_values,\n",
    "                                \"image_sizes\": image_sizes,\n",
    "                                \"image_bound\": image_bound,\n",
    "                                \"tgt_sizes\": tgt_sizes})\n",
    "\n",
    "    return traindataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lmquant/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "You are using a model of type minicpmv to instantiate a model of type minicpm. This is not supported for all configurations of models and can yield errors.\n",
      "INFO - Ignoring unknown parameter in the quantization configuration: vit_bits.\n",
      "INFO - Ignoring unknown parameter in the quantization configuration: llm_bits.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "/opt/conda/envs/lmquant/lib/python3.11/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256-llm-w4-pc were not used when initializing MiniCPMV: ['llm.model.layers.0.mlp.down_proj.g_idx', 'llm.model.layers.0.mlp.down_proj.qweight', 'llm.model.layers.0.mlp.down_proj.qzeros', 'llm.model.layers.0.mlp.down_proj.scales', 'llm.model.layers.0.mlp.gate_proj.g_idx', 'llm.model.layers.0.mlp.gate_proj.qweight', 'llm.model.layers.0.mlp.gate_proj.qzeros', 'llm.model.layers.0.mlp.gate_proj.scales', 'llm.model.layers.0.mlp.up_proj.g_idx', 'llm.model.layers.0.mlp.up_proj.qweight', 'llm.model.layers.0.mlp.up_proj.qzeros', 'llm.model.layers.0.mlp.up_proj.scales', 'llm.model.layers.0.self_attn.k_proj.g_idx', 'llm.model.layers.0.self_attn.k_proj.qweight', 'llm.model.layers.0.self_attn.k_proj.qzeros', 'llm.model.layers.0.self_attn.k_proj.scales', 'llm.model.layers.0.self_attn.o_proj.g_idx', 'llm.model.layers.0.self_attn.o_proj.qweight', 'llm.model.layers.0.self_attn.o_proj.qzeros', 'llm.model.layers.0.self_attn.o_proj.scales', 'llm.model.layers.0.self_attn.q_proj.g_idx', 'llm.model.layers.0.self_attn.q_proj.qweight', 'llm.model.layers.0.self_attn.q_proj.qzeros', 'llm.model.layers.0.self_attn.q_proj.scales', 'llm.model.layers.0.self_attn.v_proj.g_idx', 'llm.model.layers.0.self_attn.v_proj.qweight', 'llm.model.layers.0.self_attn.v_proj.qzeros', 'llm.model.layers.0.self_attn.v_proj.scales', 'llm.model.layers.1.mlp.down_proj.g_idx', 'llm.model.layers.1.mlp.down_proj.qweight', 'llm.model.layers.1.mlp.down_proj.qzeros', 'llm.model.layers.1.mlp.down_proj.scales', 'llm.model.layers.1.mlp.gate_proj.g_idx', 'llm.model.layers.1.mlp.gate_proj.qweight', 'llm.model.layers.1.mlp.gate_proj.qzeros', 'llm.model.layers.1.mlp.gate_proj.scales', 'llm.model.layers.1.mlp.up_proj.g_idx', 'llm.model.layers.1.mlp.up_proj.qweight', 'llm.model.layers.1.mlp.up_proj.qzeros', 'llm.model.layers.1.mlp.up_proj.scales', 'llm.model.layers.1.self_attn.k_proj.g_idx', 'llm.model.layers.1.self_attn.k_proj.qweight', 'llm.model.layers.1.self_attn.k_proj.qzeros', 'llm.model.layers.1.self_attn.k_proj.scales', 'llm.model.layers.1.self_attn.o_proj.g_idx', 'llm.model.layers.1.self_attn.o_proj.qweight', 'llm.model.layers.1.self_attn.o_proj.qzeros', 'llm.model.layers.1.self_attn.o_proj.scales', 'llm.model.layers.1.self_attn.q_proj.g_idx', 'llm.model.layers.1.self_attn.q_proj.qweight', 'llm.model.layers.1.self_attn.q_proj.qzeros', 'llm.model.layers.1.self_attn.q_proj.scales', 'llm.model.layers.1.self_attn.v_proj.g_idx', 'llm.model.layers.1.self_attn.v_proj.qweight', 'llm.model.layers.1.self_attn.v_proj.qzeros', 'llm.model.layers.1.self_attn.v_proj.scales', 'llm.model.layers.10.mlp.down_proj.g_idx', 'llm.model.layers.10.mlp.down_proj.qweight', 'llm.model.layers.10.mlp.down_proj.qzeros', 'llm.model.layers.10.mlp.down_proj.scales', 'llm.model.layers.10.mlp.gate_proj.g_idx', 'llm.model.layers.10.mlp.gate_proj.qweight', 'llm.model.layers.10.mlp.gate_proj.qzeros', 'llm.model.layers.10.mlp.gate_proj.scales', 'llm.model.layers.10.mlp.up_proj.g_idx', 'llm.model.layers.10.mlp.up_proj.qweight', 'llm.model.layers.10.mlp.up_proj.qzeros', 'llm.model.layers.10.mlp.up_proj.scales', 'llm.model.layers.10.self_attn.k_proj.g_idx', 'llm.model.layers.10.self_attn.k_proj.qweight', 'llm.model.layers.10.self_attn.k_proj.qzeros', 'llm.model.layers.10.self_attn.k_proj.scales', 'llm.model.layers.10.self_attn.o_proj.g_idx', 'llm.model.layers.10.self_attn.o_proj.qweight', 'llm.model.layers.10.self_attn.o_proj.qzeros', 'llm.model.layers.10.self_attn.o_proj.scales', 'llm.model.layers.10.self_attn.q_proj.g_idx', 'llm.model.layers.10.self_attn.q_proj.qweight', 'llm.model.layers.10.self_attn.q_proj.qzeros', 'llm.model.layers.10.self_attn.q_proj.scales', 'llm.model.layers.10.self_attn.v_proj.g_idx', 'llm.model.layers.10.self_attn.v_proj.qweight', 'llm.model.layers.10.self_attn.v_proj.qzeros', 'llm.model.layers.10.self_attn.v_proj.scales', 'llm.model.layers.11.mlp.down_proj.g_idx', 'llm.model.layers.11.mlp.down_proj.qweight', 'llm.model.layers.11.mlp.down_proj.qzeros', 'llm.model.layers.11.mlp.down_proj.scales', 'llm.model.layers.11.mlp.gate_proj.g_idx', 'llm.model.layers.11.mlp.gate_proj.qweight', 'llm.model.layers.11.mlp.gate_proj.qzeros', 'llm.model.layers.11.mlp.gate_proj.scales', 'llm.model.layers.11.mlp.up_proj.g_idx', 'llm.model.layers.11.mlp.up_proj.qweight', 'llm.model.layers.11.mlp.up_proj.qzeros', 'llm.model.layers.11.mlp.up_proj.scales', 'llm.model.layers.11.self_attn.k_proj.g_idx', 'llm.model.layers.11.self_attn.k_proj.qweight', 'llm.model.layers.11.self_attn.k_proj.qzeros', 'llm.model.layers.11.self_attn.k_proj.scales', 'llm.model.layers.11.self_attn.o_proj.g_idx', 'llm.model.layers.11.self_attn.o_proj.qweight', 'llm.model.layers.11.self_attn.o_proj.qzeros', 'llm.model.layers.11.self_attn.o_proj.scales', 'llm.model.layers.11.self_attn.q_proj.g_idx', 'llm.model.layers.11.self_attn.q_proj.qweight', 'llm.model.layers.11.self_attn.q_proj.qzeros', 'llm.model.layers.11.self_attn.q_proj.scales', 'llm.model.layers.11.self_attn.v_proj.g_idx', 'llm.model.layers.11.self_attn.v_proj.qweight', 'llm.model.layers.11.self_attn.v_proj.qzeros', 'llm.model.layers.11.self_attn.v_proj.scales', 'llm.model.layers.12.mlp.down_proj.g_idx', 'llm.model.layers.12.mlp.down_proj.qweight', 'llm.model.layers.12.mlp.down_proj.qzeros', 'llm.model.layers.12.mlp.down_proj.scales', 'llm.model.layers.12.mlp.gate_proj.g_idx', 'llm.model.layers.12.mlp.gate_proj.qweight', 'llm.model.layers.12.mlp.gate_proj.qzeros', 'llm.model.layers.12.mlp.gate_proj.scales', 'llm.model.layers.12.mlp.up_proj.g_idx', 'llm.model.layers.12.mlp.up_proj.qweight', 'llm.model.layers.12.mlp.up_proj.qzeros', 'llm.model.layers.12.mlp.up_proj.scales', 'llm.model.layers.12.self_attn.k_proj.g_idx', 'llm.model.layers.12.self_attn.k_proj.qweight', 'llm.model.layers.12.self_attn.k_proj.qzeros', 'llm.model.layers.12.self_attn.k_proj.scales', 'llm.model.layers.12.self_attn.o_proj.g_idx', 'llm.model.layers.12.self_attn.o_proj.qweight', 'llm.model.layers.12.self_attn.o_proj.qzeros', 'llm.model.layers.12.self_attn.o_proj.scales', 'llm.model.layers.12.self_attn.q_proj.g_idx', 'llm.model.layers.12.self_attn.q_proj.qweight', 'llm.model.layers.12.self_attn.q_proj.qzeros', 'llm.model.layers.12.self_attn.q_proj.scales', 'llm.model.layers.12.self_attn.v_proj.g_idx', 'llm.model.layers.12.self_attn.v_proj.qweight', 'llm.model.layers.12.self_attn.v_proj.qzeros', 'llm.model.layers.12.self_attn.v_proj.scales', 'llm.model.layers.13.mlp.down_proj.g_idx', 'llm.model.layers.13.mlp.down_proj.qweight', 'llm.model.layers.13.mlp.down_proj.qzeros', 'llm.model.layers.13.mlp.down_proj.scales', 'llm.model.layers.13.mlp.gate_proj.g_idx', 'llm.model.layers.13.mlp.gate_proj.qweight', 'llm.model.layers.13.mlp.gate_proj.qzeros', 'llm.model.layers.13.mlp.gate_proj.scales', 'llm.model.layers.13.mlp.up_proj.g_idx', 'llm.model.layers.13.mlp.up_proj.qweight', 'llm.model.layers.13.mlp.up_proj.qzeros', 'llm.model.layers.13.mlp.up_proj.scales', 'llm.model.layers.13.self_attn.k_proj.g_idx', 'llm.model.layers.13.self_attn.k_proj.qweight', 'llm.model.layers.13.self_attn.k_proj.qzeros', 'llm.model.layers.13.self_attn.k_proj.scales', 'llm.model.layers.13.self_attn.o_proj.g_idx', 'llm.model.layers.13.self_attn.o_proj.qweight', 'llm.model.layers.13.self_attn.o_proj.qzeros', 'llm.model.layers.13.self_attn.o_proj.scales', 'llm.model.layers.13.self_attn.q_proj.g_idx', 'llm.model.layers.13.self_attn.q_proj.qweight', 'llm.model.layers.13.self_attn.q_proj.qzeros', 'llm.model.layers.13.self_attn.q_proj.scales', 'llm.model.layers.13.self_attn.v_proj.g_idx', 'llm.model.layers.13.self_attn.v_proj.qweight', 'llm.model.layers.13.self_attn.v_proj.qzeros', 'llm.model.layers.13.self_attn.v_proj.scales', 'llm.model.layers.14.mlp.down_proj.g_idx', 'llm.model.layers.14.mlp.down_proj.qweight', 'llm.model.layers.14.mlp.down_proj.qzeros', 'llm.model.layers.14.mlp.down_proj.scales', 'llm.model.layers.14.mlp.gate_proj.g_idx', 'llm.model.layers.14.mlp.gate_proj.qweight', 'llm.model.layers.14.mlp.gate_proj.qzeros', 'llm.model.layers.14.mlp.gate_proj.scales', 'llm.model.layers.14.mlp.up_proj.g_idx', 'llm.model.layers.14.mlp.up_proj.qweight', 'llm.model.layers.14.mlp.up_proj.qzeros', 'llm.model.layers.14.mlp.up_proj.scales', 'llm.model.layers.14.self_attn.k_proj.g_idx', 'llm.model.layers.14.self_attn.k_proj.qweight', 'llm.model.layers.14.self_attn.k_proj.qzeros', 'llm.model.layers.14.self_attn.k_proj.scales', 'llm.model.layers.14.self_attn.o_proj.g_idx', 'llm.model.layers.14.self_attn.o_proj.qweight', 'llm.model.layers.14.self_attn.o_proj.qzeros', 'llm.model.layers.14.self_attn.o_proj.scales', 'llm.model.layers.14.self_attn.q_proj.g_idx', 'llm.model.layers.14.self_attn.q_proj.qweight', 'llm.model.layers.14.self_attn.q_proj.qzeros', 'llm.model.layers.14.self_attn.q_proj.scales', 'llm.model.layers.14.self_attn.v_proj.g_idx', 'llm.model.layers.14.self_attn.v_proj.qweight', 'llm.model.layers.14.self_attn.v_proj.qzeros', 'llm.model.layers.14.self_attn.v_proj.scales', 'llm.model.layers.15.mlp.down_proj.g_idx', 'llm.model.layers.15.mlp.down_proj.qweight', 'llm.model.layers.15.mlp.down_proj.qzeros', 'llm.model.layers.15.mlp.down_proj.scales', 'llm.model.layers.15.mlp.gate_proj.g_idx', 'llm.model.layers.15.mlp.gate_proj.qweight', 'llm.model.layers.15.mlp.gate_proj.qzeros', 'llm.model.layers.15.mlp.gate_proj.scales', 'llm.model.layers.15.mlp.up_proj.g_idx', 'llm.model.layers.15.mlp.up_proj.qweight', 'llm.model.layers.15.mlp.up_proj.qzeros', 'llm.model.layers.15.mlp.up_proj.scales', 'llm.model.layers.15.self_attn.k_proj.g_idx', 'llm.model.layers.15.self_attn.k_proj.qweight', 'llm.model.layers.15.self_attn.k_proj.qzeros', 'llm.model.layers.15.self_attn.k_proj.scales', 'llm.model.layers.15.self_attn.o_proj.g_idx', 'llm.model.layers.15.self_attn.o_proj.qweight', 'llm.model.layers.15.self_attn.o_proj.qzeros', 'llm.model.layers.15.self_attn.o_proj.scales', 'llm.model.layers.15.self_attn.q_proj.g_idx', 'llm.model.layers.15.self_attn.q_proj.qweight', 'llm.model.layers.15.self_attn.q_proj.qzeros', 'llm.model.layers.15.self_attn.q_proj.scales', 'llm.model.layers.15.self_attn.v_proj.g_idx', 'llm.model.layers.15.self_attn.v_proj.qweight', 'llm.model.layers.15.self_attn.v_proj.qzeros', 'llm.model.layers.15.self_attn.v_proj.scales', 'llm.model.layers.16.mlp.down_proj.g_idx', 'llm.model.layers.16.mlp.down_proj.qweight', 'llm.model.layers.16.mlp.down_proj.qzeros', 'llm.model.layers.16.mlp.down_proj.scales', 'llm.model.layers.16.mlp.gate_proj.g_idx', 'llm.model.layers.16.mlp.gate_proj.qweight', 'llm.model.layers.16.mlp.gate_proj.qzeros', 'llm.model.layers.16.mlp.gate_proj.scales', 'llm.model.layers.16.mlp.up_proj.g_idx', 'llm.model.layers.16.mlp.up_proj.qweight', 'llm.model.layers.16.mlp.up_proj.qzeros', 'llm.model.layers.16.mlp.up_proj.scales', 'llm.model.layers.16.self_attn.k_proj.g_idx', 'llm.model.layers.16.self_attn.k_proj.qweight', 'llm.model.layers.16.self_attn.k_proj.qzeros', 'llm.model.layers.16.self_attn.k_proj.scales', 'llm.model.layers.16.self_attn.o_proj.g_idx', 'llm.model.layers.16.self_attn.o_proj.qweight', 'llm.model.layers.16.self_attn.o_proj.qzeros', 'llm.model.layers.16.self_attn.o_proj.scales', 'llm.model.layers.16.self_attn.q_proj.g_idx', 'llm.model.layers.16.self_attn.q_proj.qweight', 'llm.model.layers.16.self_attn.q_proj.qzeros', 'llm.model.layers.16.self_attn.q_proj.scales', 'llm.model.layers.16.self_attn.v_proj.g_idx', 'llm.model.layers.16.self_attn.v_proj.qweight', 'llm.model.layers.16.self_attn.v_proj.qzeros', 'llm.model.layers.16.self_attn.v_proj.scales', 'llm.model.layers.17.mlp.down_proj.g_idx', 'llm.model.layers.17.mlp.down_proj.qweight', 'llm.model.layers.17.mlp.down_proj.qzeros', 'llm.model.layers.17.mlp.down_proj.scales', 'llm.model.layers.17.mlp.gate_proj.g_idx', 'llm.model.layers.17.mlp.gate_proj.qweight', 'llm.model.layers.17.mlp.gate_proj.qzeros', 'llm.model.layers.17.mlp.gate_proj.scales', 'llm.model.layers.17.mlp.up_proj.g_idx', 'llm.model.layers.17.mlp.up_proj.qweight', 'llm.model.layers.17.mlp.up_proj.qzeros', 'llm.model.layers.17.mlp.up_proj.scales', 'llm.model.layers.17.self_attn.k_proj.g_idx', 'llm.model.layers.17.self_attn.k_proj.qweight', 'llm.model.layers.17.self_attn.k_proj.qzeros', 'llm.model.layers.17.self_attn.k_proj.scales', 'llm.model.layers.17.self_attn.o_proj.g_idx', 'llm.model.layers.17.self_attn.o_proj.qweight', 'llm.model.layers.17.self_attn.o_proj.qzeros', 'llm.model.layers.17.self_attn.o_proj.scales', 'llm.model.layers.17.self_attn.q_proj.g_idx', 'llm.model.layers.17.self_attn.q_proj.qweight', 'llm.model.layers.17.self_attn.q_proj.qzeros', 'llm.model.layers.17.self_attn.q_proj.scales', 'llm.model.layers.17.self_attn.v_proj.g_idx', 'llm.model.layers.17.self_attn.v_proj.qweight', 'llm.model.layers.17.self_attn.v_proj.qzeros', 'llm.model.layers.17.self_attn.v_proj.scales', 'llm.model.layers.18.mlp.down_proj.g_idx', 'llm.model.layers.18.mlp.down_proj.qweight', 'llm.model.layers.18.mlp.down_proj.qzeros', 'llm.model.layers.18.mlp.down_proj.scales', 'llm.model.layers.18.mlp.gate_proj.g_idx', 'llm.model.layers.18.mlp.gate_proj.qweight', 'llm.model.layers.18.mlp.gate_proj.qzeros', 'llm.model.layers.18.mlp.gate_proj.scales', 'llm.model.layers.18.mlp.up_proj.g_idx', 'llm.model.layers.18.mlp.up_proj.qweight', 'llm.model.layers.18.mlp.up_proj.qzeros', 'llm.model.layers.18.mlp.up_proj.scales', 'llm.model.layers.18.self_attn.k_proj.g_idx', 'llm.model.layers.18.self_attn.k_proj.qweight', 'llm.model.layers.18.self_attn.k_proj.qzeros', 'llm.model.layers.18.self_attn.k_proj.scales', 'llm.model.layers.18.self_attn.o_proj.g_idx', 'llm.model.layers.18.self_attn.o_proj.qweight', 'llm.model.layers.18.self_attn.o_proj.qzeros', 'llm.model.layers.18.self_attn.o_proj.scales', 'llm.model.layers.18.self_attn.q_proj.g_idx', 'llm.model.layers.18.self_attn.q_proj.qweight', 'llm.model.layers.18.self_attn.q_proj.qzeros', 'llm.model.layers.18.self_attn.q_proj.scales', 'llm.model.layers.18.self_attn.v_proj.g_idx', 'llm.model.layers.18.self_attn.v_proj.qweight', 'llm.model.layers.18.self_attn.v_proj.qzeros', 'llm.model.layers.18.self_attn.v_proj.scales', 'llm.model.layers.19.mlp.down_proj.g_idx', 'llm.model.layers.19.mlp.down_proj.qweight', 'llm.model.layers.19.mlp.down_proj.qzeros', 'llm.model.layers.19.mlp.down_proj.scales', 'llm.model.layers.19.mlp.gate_proj.g_idx', 'llm.model.layers.19.mlp.gate_proj.qweight', 'llm.model.layers.19.mlp.gate_proj.qzeros', 'llm.model.layers.19.mlp.gate_proj.scales', 'llm.model.layers.19.mlp.up_proj.g_idx', 'llm.model.layers.19.mlp.up_proj.qweight', 'llm.model.layers.19.mlp.up_proj.qzeros', 'llm.model.layers.19.mlp.up_proj.scales', 'llm.model.layers.19.self_attn.k_proj.g_idx', 'llm.model.layers.19.self_attn.k_proj.qweight', 'llm.model.layers.19.self_attn.k_proj.qzeros', 'llm.model.layers.19.self_attn.k_proj.scales', 'llm.model.layers.19.self_attn.o_proj.g_idx', 'llm.model.layers.19.self_attn.o_proj.qweight', 'llm.model.layers.19.self_attn.o_proj.qzeros', 'llm.model.layers.19.self_attn.o_proj.scales', 'llm.model.layers.19.self_attn.q_proj.g_idx', 'llm.model.layers.19.self_attn.q_proj.qweight', 'llm.model.layers.19.self_attn.q_proj.qzeros', 'llm.model.layers.19.self_attn.q_proj.scales', 'llm.model.layers.19.self_attn.v_proj.g_idx', 'llm.model.layers.19.self_attn.v_proj.qweight', 'llm.model.layers.19.self_attn.v_proj.qzeros', 'llm.model.layers.19.self_attn.v_proj.scales', 'llm.model.layers.2.mlp.down_proj.g_idx', 'llm.model.layers.2.mlp.down_proj.qweight', 'llm.model.layers.2.mlp.down_proj.qzeros', 'llm.model.layers.2.mlp.down_proj.scales', 'llm.model.layers.2.mlp.gate_proj.g_idx', 'llm.model.layers.2.mlp.gate_proj.qweight', 'llm.model.layers.2.mlp.gate_proj.qzeros', 'llm.model.layers.2.mlp.gate_proj.scales', 'llm.model.layers.2.mlp.up_proj.g_idx', 'llm.model.layers.2.mlp.up_proj.qweight', 'llm.model.layers.2.mlp.up_proj.qzeros', 'llm.model.layers.2.mlp.up_proj.scales', 'llm.model.layers.2.self_attn.k_proj.g_idx', 'llm.model.layers.2.self_attn.k_proj.qweight', 'llm.model.layers.2.self_attn.k_proj.qzeros', 'llm.model.layers.2.self_attn.k_proj.scales', 'llm.model.layers.2.self_attn.o_proj.g_idx', 'llm.model.layers.2.self_attn.o_proj.qweight', 'llm.model.layers.2.self_attn.o_proj.qzeros', 'llm.model.layers.2.self_attn.o_proj.scales', 'llm.model.layers.2.self_attn.q_proj.g_idx', 'llm.model.layers.2.self_attn.q_proj.qweight', 'llm.model.layers.2.self_attn.q_proj.qzeros', 'llm.model.layers.2.self_attn.q_proj.scales', 'llm.model.layers.2.self_attn.v_proj.g_idx', 'llm.model.layers.2.self_attn.v_proj.qweight', 'llm.model.layers.2.self_attn.v_proj.qzeros', 'llm.model.layers.2.self_attn.v_proj.scales', 'llm.model.layers.20.mlp.down_proj.g_idx', 'llm.model.layers.20.mlp.down_proj.qweight', 'llm.model.layers.20.mlp.down_proj.qzeros', 'llm.model.layers.20.mlp.down_proj.scales', 'llm.model.layers.20.mlp.gate_proj.g_idx', 'llm.model.layers.20.mlp.gate_proj.qweight', 'llm.model.layers.20.mlp.gate_proj.qzeros', 'llm.model.layers.20.mlp.gate_proj.scales', 'llm.model.layers.20.mlp.up_proj.g_idx', 'llm.model.layers.20.mlp.up_proj.qweight', 'llm.model.layers.20.mlp.up_proj.qzeros', 'llm.model.layers.20.mlp.up_proj.scales', 'llm.model.layers.20.self_attn.k_proj.g_idx', 'llm.model.layers.20.self_attn.k_proj.qweight', 'llm.model.layers.20.self_attn.k_proj.qzeros', 'llm.model.layers.20.self_attn.k_proj.scales', 'llm.model.layers.20.self_attn.o_proj.g_idx', 'llm.model.layers.20.self_attn.o_proj.qweight', 'llm.model.layers.20.self_attn.o_proj.qzeros', 'llm.model.layers.20.self_attn.o_proj.scales', 'llm.model.layers.20.self_attn.q_proj.g_idx', 'llm.model.layers.20.self_attn.q_proj.qweight', 'llm.model.layers.20.self_attn.q_proj.qzeros', 'llm.model.layers.20.self_attn.q_proj.scales', 'llm.model.layers.20.self_attn.v_proj.g_idx', 'llm.model.layers.20.self_attn.v_proj.qweight', 'llm.model.layers.20.self_attn.v_proj.qzeros', 'llm.model.layers.20.self_attn.v_proj.scales', 'llm.model.layers.21.mlp.down_proj.g_idx', 'llm.model.layers.21.mlp.down_proj.qweight', 'llm.model.layers.21.mlp.down_proj.qzeros', 'llm.model.layers.21.mlp.down_proj.scales', 'llm.model.layers.21.mlp.gate_proj.g_idx', 'llm.model.layers.21.mlp.gate_proj.qweight', 'llm.model.layers.21.mlp.gate_proj.qzeros', 'llm.model.layers.21.mlp.gate_proj.scales', 'llm.model.layers.21.mlp.up_proj.g_idx', 'llm.model.layers.21.mlp.up_proj.qweight', 'llm.model.layers.21.mlp.up_proj.qzeros', 'llm.model.layers.21.mlp.up_proj.scales', 'llm.model.layers.21.self_attn.k_proj.g_idx', 'llm.model.layers.21.self_attn.k_proj.qweight', 'llm.model.layers.21.self_attn.k_proj.qzeros', 'llm.model.layers.21.self_attn.k_proj.scales', 'llm.model.layers.21.self_attn.o_proj.g_idx', 'llm.model.layers.21.self_attn.o_proj.qweight', 'llm.model.layers.21.self_attn.o_proj.qzeros', 'llm.model.layers.21.self_attn.o_proj.scales', 'llm.model.layers.21.self_attn.q_proj.g_idx', 'llm.model.layers.21.self_attn.q_proj.qweight', 'llm.model.layers.21.self_attn.q_proj.qzeros', 'llm.model.layers.21.self_attn.q_proj.scales', 'llm.model.layers.21.self_attn.v_proj.g_idx', 'llm.model.layers.21.self_attn.v_proj.qweight', 'llm.model.layers.21.self_attn.v_proj.qzeros', 'llm.model.layers.21.self_attn.v_proj.scales', 'llm.model.layers.22.mlp.down_proj.g_idx', 'llm.model.layers.22.mlp.down_proj.qweight', 'llm.model.layers.22.mlp.down_proj.qzeros', 'llm.model.layers.22.mlp.down_proj.scales', 'llm.model.layers.22.mlp.gate_proj.g_idx', 'llm.model.layers.22.mlp.gate_proj.qweight', 'llm.model.layers.22.mlp.gate_proj.qzeros', 'llm.model.layers.22.mlp.gate_proj.scales', 'llm.model.layers.22.mlp.up_proj.g_idx', 'llm.model.layers.22.mlp.up_proj.qweight', 'llm.model.layers.22.mlp.up_proj.qzeros', 'llm.model.layers.22.mlp.up_proj.scales', 'llm.model.layers.22.self_attn.k_proj.g_idx', 'llm.model.layers.22.self_attn.k_proj.qweight', 'llm.model.layers.22.self_attn.k_proj.qzeros', 'llm.model.layers.22.self_attn.k_proj.scales', 'llm.model.layers.22.self_attn.o_proj.g_idx', 'llm.model.layers.22.self_attn.o_proj.qweight', 'llm.model.layers.22.self_attn.o_proj.qzeros', 'llm.model.layers.22.self_attn.o_proj.scales', 'llm.model.layers.22.self_attn.q_proj.g_idx', 'llm.model.layers.22.self_attn.q_proj.qweight', 'llm.model.layers.22.self_attn.q_proj.qzeros', 'llm.model.layers.22.self_attn.q_proj.scales', 'llm.model.layers.22.self_attn.v_proj.g_idx', 'llm.model.layers.22.self_attn.v_proj.qweight', 'llm.model.layers.22.self_attn.v_proj.qzeros', 'llm.model.layers.22.self_attn.v_proj.scales', 'llm.model.layers.23.mlp.down_proj.g_idx', 'llm.model.layers.23.mlp.down_proj.qweight', 'llm.model.layers.23.mlp.down_proj.qzeros', 'llm.model.layers.23.mlp.down_proj.scales', 'llm.model.layers.23.mlp.gate_proj.g_idx', 'llm.model.layers.23.mlp.gate_proj.qweight', 'llm.model.layers.23.mlp.gate_proj.qzeros', 'llm.model.layers.23.mlp.gate_proj.scales', 'llm.model.layers.23.mlp.up_proj.g_idx', 'llm.model.layers.23.mlp.up_proj.qweight', 'llm.model.layers.23.mlp.up_proj.qzeros', 'llm.model.layers.23.mlp.up_proj.scales', 'llm.model.layers.23.self_attn.k_proj.g_idx', 'llm.model.layers.23.self_attn.k_proj.qweight', 'llm.model.layers.23.self_attn.k_proj.qzeros', 'llm.model.layers.23.self_attn.k_proj.scales', 'llm.model.layers.23.self_attn.o_proj.g_idx', 'llm.model.layers.23.self_attn.o_proj.qweight', 'llm.model.layers.23.self_attn.o_proj.qzeros', 'llm.model.layers.23.self_attn.o_proj.scales', 'llm.model.layers.23.self_attn.q_proj.g_idx', 'llm.model.layers.23.self_attn.q_proj.qweight', 'llm.model.layers.23.self_attn.q_proj.qzeros', 'llm.model.layers.23.self_attn.q_proj.scales', 'llm.model.layers.23.self_attn.v_proj.g_idx', 'llm.model.layers.23.self_attn.v_proj.qweight', 'llm.model.layers.23.self_attn.v_proj.qzeros', 'llm.model.layers.23.self_attn.v_proj.scales', 'llm.model.layers.24.mlp.down_proj.g_idx', 'llm.model.layers.24.mlp.down_proj.qweight', 'llm.model.layers.24.mlp.down_proj.qzeros', 'llm.model.layers.24.mlp.down_proj.scales', 'llm.model.layers.24.mlp.gate_proj.g_idx', 'llm.model.layers.24.mlp.gate_proj.qweight', 'llm.model.layers.24.mlp.gate_proj.qzeros', 'llm.model.layers.24.mlp.gate_proj.scales', 'llm.model.layers.24.mlp.up_proj.g_idx', 'llm.model.layers.24.mlp.up_proj.qweight', 'llm.model.layers.24.mlp.up_proj.qzeros', 'llm.model.layers.24.mlp.up_proj.scales', 'llm.model.layers.24.self_attn.k_proj.g_idx', 'llm.model.layers.24.self_attn.k_proj.qweight', 'llm.model.layers.24.self_attn.k_proj.qzeros', 'llm.model.layers.24.self_attn.k_proj.scales', 'llm.model.layers.24.self_attn.o_proj.g_idx', 'llm.model.layers.24.self_attn.o_proj.qweight', 'llm.model.layers.24.self_attn.o_proj.qzeros', 'llm.model.layers.24.self_attn.o_proj.scales', 'llm.model.layers.24.self_attn.q_proj.g_idx', 'llm.model.layers.24.self_attn.q_proj.qweight', 'llm.model.layers.24.self_attn.q_proj.qzeros', 'llm.model.layers.24.self_attn.q_proj.scales', 'llm.model.layers.24.self_attn.v_proj.g_idx', 'llm.model.layers.24.self_attn.v_proj.qweight', 'llm.model.layers.24.self_attn.v_proj.qzeros', 'llm.model.layers.24.self_attn.v_proj.scales', 'llm.model.layers.25.mlp.down_proj.g_idx', 'llm.model.layers.25.mlp.down_proj.qweight', 'llm.model.layers.25.mlp.down_proj.qzeros', 'llm.model.layers.25.mlp.down_proj.scales', 'llm.model.layers.25.mlp.gate_proj.g_idx', 'llm.model.layers.25.mlp.gate_proj.qweight', 'llm.model.layers.25.mlp.gate_proj.qzeros', 'llm.model.layers.25.mlp.gate_proj.scales', 'llm.model.layers.25.mlp.up_proj.g_idx', 'llm.model.layers.25.mlp.up_proj.qweight', 'llm.model.layers.25.mlp.up_proj.qzeros', 'llm.model.layers.25.mlp.up_proj.scales', 'llm.model.layers.25.self_attn.k_proj.g_idx', 'llm.model.layers.25.self_attn.k_proj.qweight', 'llm.model.layers.25.self_attn.k_proj.qzeros', 'llm.model.layers.25.self_attn.k_proj.scales', 'llm.model.layers.25.self_attn.o_proj.g_idx', 'llm.model.layers.25.self_attn.o_proj.qweight', 'llm.model.layers.25.self_attn.o_proj.qzeros', 'llm.model.layers.25.self_attn.o_proj.scales', 'llm.model.layers.25.self_attn.q_proj.g_idx', 'llm.model.layers.25.self_attn.q_proj.qweight', 'llm.model.layers.25.self_attn.q_proj.qzeros', 'llm.model.layers.25.self_attn.q_proj.scales', 'llm.model.layers.25.self_attn.v_proj.g_idx', 'llm.model.layers.25.self_attn.v_proj.qweight', 'llm.model.layers.25.self_attn.v_proj.qzeros', 'llm.model.layers.25.self_attn.v_proj.scales', 'llm.model.layers.26.mlp.down_proj.g_idx', 'llm.model.layers.26.mlp.down_proj.qweight', 'llm.model.layers.26.mlp.down_proj.qzeros', 'llm.model.layers.26.mlp.down_proj.scales', 'llm.model.layers.26.mlp.gate_proj.g_idx', 'llm.model.layers.26.mlp.gate_proj.qweight', 'llm.model.layers.26.mlp.gate_proj.qzeros', 'llm.model.layers.26.mlp.gate_proj.scales', 'llm.model.layers.26.mlp.up_proj.g_idx', 'llm.model.layers.26.mlp.up_proj.qweight', 'llm.model.layers.26.mlp.up_proj.qzeros', 'llm.model.layers.26.mlp.up_proj.scales', 'llm.model.layers.26.self_attn.k_proj.g_idx', 'llm.model.layers.26.self_attn.k_proj.qweight', 'llm.model.layers.26.self_attn.k_proj.qzeros', 'llm.model.layers.26.self_attn.k_proj.scales', 'llm.model.layers.26.self_attn.o_proj.g_idx', 'llm.model.layers.26.self_attn.o_proj.qweight', 'llm.model.layers.26.self_attn.o_proj.qzeros', 'llm.model.layers.26.self_attn.o_proj.scales', 'llm.model.layers.26.self_attn.q_proj.g_idx', 'llm.model.layers.26.self_attn.q_proj.qweight', 'llm.model.layers.26.self_attn.q_proj.qzeros', 'llm.model.layers.26.self_attn.q_proj.scales', 'llm.model.layers.26.self_attn.v_proj.g_idx', 'llm.model.layers.26.self_attn.v_proj.qweight', 'llm.model.layers.26.self_attn.v_proj.qzeros', 'llm.model.layers.26.self_attn.v_proj.scales', 'llm.model.layers.27.mlp.down_proj.g_idx', 'llm.model.layers.27.mlp.down_proj.qweight', 'llm.model.layers.27.mlp.down_proj.qzeros', 'llm.model.layers.27.mlp.down_proj.scales', 'llm.model.layers.27.mlp.gate_proj.g_idx', 'llm.model.layers.27.mlp.gate_proj.qweight', 'llm.model.layers.27.mlp.gate_proj.qzeros', 'llm.model.layers.27.mlp.gate_proj.scales', 'llm.model.layers.27.mlp.up_proj.g_idx', 'llm.model.layers.27.mlp.up_proj.qweight', 'llm.model.layers.27.mlp.up_proj.qzeros', 'llm.model.layers.27.mlp.up_proj.scales', 'llm.model.layers.27.self_attn.k_proj.g_idx', 'llm.model.layers.27.self_attn.k_proj.qweight', 'llm.model.layers.27.self_attn.k_proj.qzeros', 'llm.model.layers.27.self_attn.k_proj.scales', 'llm.model.layers.27.self_attn.o_proj.g_idx', 'llm.model.layers.27.self_attn.o_proj.qweight', 'llm.model.layers.27.self_attn.o_proj.qzeros', 'llm.model.layers.27.self_attn.o_proj.scales', 'llm.model.layers.27.self_attn.q_proj.g_idx', 'llm.model.layers.27.self_attn.q_proj.qweight', 'llm.model.layers.27.self_attn.q_proj.qzeros', 'llm.model.layers.27.self_attn.q_proj.scales', 'llm.model.layers.27.self_attn.v_proj.g_idx', 'llm.model.layers.27.self_attn.v_proj.qweight', 'llm.model.layers.27.self_attn.v_proj.qzeros', 'llm.model.layers.27.self_attn.v_proj.scales', 'llm.model.layers.28.mlp.down_proj.g_idx', 'llm.model.layers.28.mlp.down_proj.qweight', 'llm.model.layers.28.mlp.down_proj.qzeros', 'llm.model.layers.28.mlp.down_proj.scales', 'llm.model.layers.28.mlp.gate_proj.g_idx', 'llm.model.layers.28.mlp.gate_proj.qweight', 'llm.model.layers.28.mlp.gate_proj.qzeros', 'llm.model.layers.28.mlp.gate_proj.scales', 'llm.model.layers.28.mlp.up_proj.g_idx', 'llm.model.layers.28.mlp.up_proj.qweight', 'llm.model.layers.28.mlp.up_proj.qzeros', 'llm.model.layers.28.mlp.up_proj.scales', 'llm.model.layers.28.self_attn.k_proj.g_idx', 'llm.model.layers.28.self_attn.k_proj.qweight', 'llm.model.layers.28.self_attn.k_proj.qzeros', 'llm.model.layers.28.self_attn.k_proj.scales', 'llm.model.layers.28.self_attn.o_proj.g_idx', 'llm.model.layers.28.self_attn.o_proj.qweight', 'llm.model.layers.28.self_attn.o_proj.qzeros', 'llm.model.layers.28.self_attn.o_proj.scales', 'llm.model.layers.28.self_attn.q_proj.g_idx', 'llm.model.layers.28.self_attn.q_proj.qweight', 'llm.model.layers.28.self_attn.q_proj.qzeros', 'llm.model.layers.28.self_attn.q_proj.scales', 'llm.model.layers.28.self_attn.v_proj.g_idx', 'llm.model.layers.28.self_attn.v_proj.qweight', 'llm.model.layers.28.self_attn.v_proj.qzeros', 'llm.model.layers.28.self_attn.v_proj.scales', 'llm.model.layers.29.mlp.down_proj.g_idx', 'llm.model.layers.29.mlp.down_proj.qweight', 'llm.model.layers.29.mlp.down_proj.qzeros', 'llm.model.layers.29.mlp.down_proj.scales', 'llm.model.layers.29.mlp.gate_proj.g_idx', 'llm.model.layers.29.mlp.gate_proj.qweight', 'llm.model.layers.29.mlp.gate_proj.qzeros', 'llm.model.layers.29.mlp.gate_proj.scales', 'llm.model.layers.29.mlp.up_proj.g_idx', 'llm.model.layers.29.mlp.up_proj.qweight', 'llm.model.layers.29.mlp.up_proj.qzeros', 'llm.model.layers.29.mlp.up_proj.scales', 'llm.model.layers.29.self_attn.k_proj.g_idx', 'llm.model.layers.29.self_attn.k_proj.qweight', 'llm.model.layers.29.self_attn.k_proj.qzeros', 'llm.model.layers.29.self_attn.k_proj.scales', 'llm.model.layers.29.self_attn.o_proj.g_idx', 'llm.model.layers.29.self_attn.o_proj.qweight', 'llm.model.layers.29.self_attn.o_proj.qzeros', 'llm.model.layers.29.self_attn.o_proj.scales', 'llm.model.layers.29.self_attn.q_proj.g_idx', 'llm.model.layers.29.self_attn.q_proj.qweight', 'llm.model.layers.29.self_attn.q_proj.qzeros', 'llm.model.layers.29.self_attn.q_proj.scales', 'llm.model.layers.29.self_attn.v_proj.g_idx', 'llm.model.layers.29.self_attn.v_proj.qweight', 'llm.model.layers.29.self_attn.v_proj.qzeros', 'llm.model.layers.29.self_attn.v_proj.scales', 'llm.model.layers.3.mlp.down_proj.g_idx', 'llm.model.layers.3.mlp.down_proj.qweight', 'llm.model.layers.3.mlp.down_proj.qzeros', 'llm.model.layers.3.mlp.down_proj.scales', 'llm.model.layers.3.mlp.gate_proj.g_idx', 'llm.model.layers.3.mlp.gate_proj.qweight', 'llm.model.layers.3.mlp.gate_proj.qzeros', 'llm.model.layers.3.mlp.gate_proj.scales', 'llm.model.layers.3.mlp.up_proj.g_idx', 'llm.model.layers.3.mlp.up_proj.qweight', 'llm.model.layers.3.mlp.up_proj.qzeros', 'llm.model.layers.3.mlp.up_proj.scales', 'llm.model.layers.3.self_attn.k_proj.g_idx', 'llm.model.layers.3.self_attn.k_proj.qweight', 'llm.model.layers.3.self_attn.k_proj.qzeros', 'llm.model.layers.3.self_attn.k_proj.scales', 'llm.model.layers.3.self_attn.o_proj.g_idx', 'llm.model.layers.3.self_attn.o_proj.qweight', 'llm.model.layers.3.self_attn.o_proj.qzeros', 'llm.model.layers.3.self_attn.o_proj.scales', 'llm.model.layers.3.self_attn.q_proj.g_idx', 'llm.model.layers.3.self_attn.q_proj.qweight', 'llm.model.layers.3.self_attn.q_proj.qzeros', 'llm.model.layers.3.self_attn.q_proj.scales', 'llm.model.layers.3.self_attn.v_proj.g_idx', 'llm.model.layers.3.self_attn.v_proj.qweight', 'llm.model.layers.3.self_attn.v_proj.qzeros', 'llm.model.layers.3.self_attn.v_proj.scales', 'llm.model.layers.30.mlp.down_proj.g_idx', 'llm.model.layers.30.mlp.down_proj.qweight', 'llm.model.layers.30.mlp.down_proj.qzeros', 'llm.model.layers.30.mlp.down_proj.scales', 'llm.model.layers.30.mlp.gate_proj.g_idx', 'llm.model.layers.30.mlp.gate_proj.qweight', 'llm.model.layers.30.mlp.gate_proj.qzeros', 'llm.model.layers.30.mlp.gate_proj.scales', 'llm.model.layers.30.mlp.up_proj.g_idx', 'llm.model.layers.30.mlp.up_proj.qweight', 'llm.model.layers.30.mlp.up_proj.qzeros', 'llm.model.layers.30.mlp.up_proj.scales', 'llm.model.layers.30.self_attn.k_proj.g_idx', 'llm.model.layers.30.self_attn.k_proj.qweight', 'llm.model.layers.30.self_attn.k_proj.qzeros', 'llm.model.layers.30.self_attn.k_proj.scales', 'llm.model.layers.30.self_attn.o_proj.g_idx', 'llm.model.layers.30.self_attn.o_proj.qweight', 'llm.model.layers.30.self_attn.o_proj.qzeros', 'llm.model.layers.30.self_attn.o_proj.scales', 'llm.model.layers.30.self_attn.q_proj.g_idx', 'llm.model.layers.30.self_attn.q_proj.qweight', 'llm.model.layers.30.self_attn.q_proj.qzeros', 'llm.model.layers.30.self_attn.q_proj.scales', 'llm.model.layers.30.self_attn.v_proj.g_idx', 'llm.model.layers.30.self_attn.v_proj.qweight', 'llm.model.layers.30.self_attn.v_proj.qzeros', 'llm.model.layers.30.self_attn.v_proj.scales', 'llm.model.layers.31.mlp.down_proj.g_idx', 'llm.model.layers.31.mlp.down_proj.qweight', 'llm.model.layers.31.mlp.down_proj.qzeros', 'llm.model.layers.31.mlp.down_proj.scales', 'llm.model.layers.31.mlp.gate_proj.g_idx', 'llm.model.layers.31.mlp.gate_proj.qweight', 'llm.model.layers.31.mlp.gate_proj.qzeros', 'llm.model.layers.31.mlp.gate_proj.scales', 'llm.model.layers.31.mlp.up_proj.g_idx', 'llm.model.layers.31.mlp.up_proj.qweight', 'llm.model.layers.31.mlp.up_proj.qzeros', 'llm.model.layers.31.mlp.up_proj.scales', 'llm.model.layers.31.self_attn.k_proj.g_idx', 'llm.model.layers.31.self_attn.k_proj.qweight', 'llm.model.layers.31.self_attn.k_proj.qzeros', 'llm.model.layers.31.self_attn.k_proj.scales', 'llm.model.layers.31.self_attn.o_proj.g_idx', 'llm.model.layers.31.self_attn.o_proj.qweight', 'llm.model.layers.31.self_attn.o_proj.qzeros', 'llm.model.layers.31.self_attn.o_proj.scales', 'llm.model.layers.31.self_attn.q_proj.g_idx', 'llm.model.layers.31.self_attn.q_proj.qweight', 'llm.model.layers.31.self_attn.q_proj.qzeros', 'llm.model.layers.31.self_attn.q_proj.scales', 'llm.model.layers.31.self_attn.v_proj.g_idx', 'llm.model.layers.31.self_attn.v_proj.qweight', 'llm.model.layers.31.self_attn.v_proj.qzeros', 'llm.model.layers.31.self_attn.v_proj.scales', 'llm.model.layers.32.mlp.down_proj.g_idx', 'llm.model.layers.32.mlp.down_proj.qweight', 'llm.model.layers.32.mlp.down_proj.qzeros', 'llm.model.layers.32.mlp.down_proj.scales', 'llm.model.layers.32.mlp.gate_proj.g_idx', 'llm.model.layers.32.mlp.gate_proj.qweight', 'llm.model.layers.32.mlp.gate_proj.qzeros', 'llm.model.layers.32.mlp.gate_proj.scales', 'llm.model.layers.32.mlp.up_proj.g_idx', 'llm.model.layers.32.mlp.up_proj.qweight', 'llm.model.layers.32.mlp.up_proj.qzeros', 'llm.model.layers.32.mlp.up_proj.scales', 'llm.model.layers.32.self_attn.k_proj.g_idx', 'llm.model.layers.32.self_attn.k_proj.qweight', 'llm.model.layers.32.self_attn.k_proj.qzeros', 'llm.model.layers.32.self_attn.k_proj.scales', 'llm.model.layers.32.self_attn.o_proj.g_idx', 'llm.model.layers.32.self_attn.o_proj.qweight', 'llm.model.layers.32.self_attn.o_proj.qzeros', 'llm.model.layers.32.self_attn.o_proj.scales', 'llm.model.layers.32.self_attn.q_proj.g_idx', 'llm.model.layers.32.self_attn.q_proj.qweight', 'llm.model.layers.32.self_attn.q_proj.qzeros', 'llm.model.layers.32.self_attn.q_proj.scales', 'llm.model.layers.32.self_attn.v_proj.g_idx', 'llm.model.layers.32.self_attn.v_proj.qweight', 'llm.model.layers.32.self_attn.v_proj.qzeros', 'llm.model.layers.32.self_attn.v_proj.scales', 'llm.model.layers.33.mlp.down_proj.g_idx', 'llm.model.layers.33.mlp.down_proj.qweight', 'llm.model.layers.33.mlp.down_proj.qzeros', 'llm.model.layers.33.mlp.down_proj.scales', 'llm.model.layers.33.mlp.gate_proj.g_idx', 'llm.model.layers.33.mlp.gate_proj.qweight', 'llm.model.layers.33.mlp.gate_proj.qzeros', 'llm.model.layers.33.mlp.gate_proj.scales', 'llm.model.layers.33.mlp.up_proj.g_idx', 'llm.model.layers.33.mlp.up_proj.qweight', 'llm.model.layers.33.mlp.up_proj.qzeros', 'llm.model.layers.33.mlp.up_proj.scales', 'llm.model.layers.33.self_attn.k_proj.g_idx', 'llm.model.layers.33.self_attn.k_proj.qweight', 'llm.model.layers.33.self_attn.k_proj.qzeros', 'llm.model.layers.33.self_attn.k_proj.scales', 'llm.model.layers.33.self_attn.o_proj.g_idx', 'llm.model.layers.33.self_attn.o_proj.qweight', 'llm.model.layers.33.self_attn.o_proj.qzeros', 'llm.model.layers.33.self_attn.o_proj.scales', 'llm.model.layers.33.self_attn.q_proj.g_idx', 'llm.model.layers.33.self_attn.q_proj.qweight', 'llm.model.layers.33.self_attn.q_proj.qzeros', 'llm.model.layers.33.self_attn.q_proj.scales', 'llm.model.layers.33.self_attn.v_proj.g_idx', 'llm.model.layers.33.self_attn.v_proj.qweight', 'llm.model.layers.33.self_attn.v_proj.qzeros', 'llm.model.layers.33.self_attn.v_proj.scales', 'llm.model.layers.34.mlp.down_proj.g_idx', 'llm.model.layers.34.mlp.down_proj.qweight', 'llm.model.layers.34.mlp.down_proj.qzeros', 'llm.model.layers.34.mlp.down_proj.scales', 'llm.model.layers.34.mlp.gate_proj.g_idx', 'llm.model.layers.34.mlp.gate_proj.qweight', 'llm.model.layers.34.mlp.gate_proj.qzeros', 'llm.model.layers.34.mlp.gate_proj.scales', 'llm.model.layers.34.mlp.up_proj.g_idx', 'llm.model.layers.34.mlp.up_proj.qweight', 'llm.model.layers.34.mlp.up_proj.qzeros', 'llm.model.layers.34.mlp.up_proj.scales', 'llm.model.layers.34.self_attn.k_proj.g_idx', 'llm.model.layers.34.self_attn.k_proj.qweight', 'llm.model.layers.34.self_attn.k_proj.qzeros', 'llm.model.layers.34.self_attn.k_proj.scales', 'llm.model.layers.34.self_attn.o_proj.g_idx', 'llm.model.layers.34.self_attn.o_proj.qweight', 'llm.model.layers.34.self_attn.o_proj.qzeros', 'llm.model.layers.34.self_attn.o_proj.scales', 'llm.model.layers.34.self_attn.q_proj.g_idx', 'llm.model.layers.34.self_attn.q_proj.qweight', 'llm.model.layers.34.self_attn.q_proj.qzeros', 'llm.model.layers.34.self_attn.q_proj.scales', 'llm.model.layers.34.self_attn.v_proj.g_idx', 'llm.model.layers.34.self_attn.v_proj.qweight', 'llm.model.layers.34.self_attn.v_proj.qzeros', 'llm.model.layers.34.self_attn.v_proj.scales', 'llm.model.layers.35.mlp.down_proj.g_idx', 'llm.model.layers.35.mlp.down_proj.qweight', 'llm.model.layers.35.mlp.down_proj.qzeros', 'llm.model.layers.35.mlp.down_proj.scales', 'llm.model.layers.35.mlp.gate_proj.g_idx', 'llm.model.layers.35.mlp.gate_proj.qweight', 'llm.model.layers.35.mlp.gate_proj.qzeros', 'llm.model.layers.35.mlp.gate_proj.scales', 'llm.model.layers.35.mlp.up_proj.g_idx', 'llm.model.layers.35.mlp.up_proj.qweight', 'llm.model.layers.35.mlp.up_proj.qzeros', 'llm.model.layers.35.mlp.up_proj.scales', 'llm.model.layers.35.self_attn.k_proj.g_idx', 'llm.model.layers.35.self_attn.k_proj.qweight', 'llm.model.layers.35.self_attn.k_proj.qzeros', 'llm.model.layers.35.self_attn.k_proj.scales', 'llm.model.layers.35.self_attn.o_proj.g_idx', 'llm.model.layers.35.self_attn.o_proj.qweight', 'llm.model.layers.35.self_attn.o_proj.qzeros', 'llm.model.layers.35.self_attn.o_proj.scales', 'llm.model.layers.35.self_attn.q_proj.g_idx', 'llm.model.layers.35.self_attn.q_proj.qweight', 'llm.model.layers.35.self_attn.q_proj.qzeros', 'llm.model.layers.35.self_attn.q_proj.scales', 'llm.model.layers.35.self_attn.v_proj.g_idx', 'llm.model.layers.35.self_attn.v_proj.qweight', 'llm.model.layers.35.self_attn.v_proj.qzeros', 'llm.model.layers.35.self_attn.v_proj.scales', 'llm.model.layers.36.mlp.down_proj.g_idx', 'llm.model.layers.36.mlp.down_proj.qweight', 'llm.model.layers.36.mlp.down_proj.qzeros', 'llm.model.layers.36.mlp.down_proj.scales', 'llm.model.layers.36.mlp.gate_proj.g_idx', 'llm.model.layers.36.mlp.gate_proj.qweight', 'llm.model.layers.36.mlp.gate_proj.qzeros', 'llm.model.layers.36.mlp.gate_proj.scales', 'llm.model.layers.36.mlp.up_proj.g_idx', 'llm.model.layers.36.mlp.up_proj.qweight', 'llm.model.layers.36.mlp.up_proj.qzeros', 'llm.model.layers.36.mlp.up_proj.scales', 'llm.model.layers.36.self_attn.k_proj.g_idx', 'llm.model.layers.36.self_attn.k_proj.qweight', 'llm.model.layers.36.self_attn.k_proj.qzeros', 'llm.model.layers.36.self_attn.k_proj.scales', 'llm.model.layers.36.self_attn.o_proj.g_idx', 'llm.model.layers.36.self_attn.o_proj.qweight', 'llm.model.layers.36.self_attn.o_proj.qzeros', 'llm.model.layers.36.self_attn.o_proj.scales', 'llm.model.layers.36.self_attn.q_proj.g_idx', 'llm.model.layers.36.self_attn.q_proj.qweight', 'llm.model.layers.36.self_attn.q_proj.qzeros', 'llm.model.layers.36.self_attn.q_proj.scales', 'llm.model.layers.36.self_attn.v_proj.g_idx', 'llm.model.layers.36.self_attn.v_proj.qweight', 'llm.model.layers.36.self_attn.v_proj.qzeros', 'llm.model.layers.36.self_attn.v_proj.scales', 'llm.model.layers.37.mlp.down_proj.g_idx', 'llm.model.layers.37.mlp.down_proj.qweight', 'llm.model.layers.37.mlp.down_proj.qzeros', 'llm.model.layers.37.mlp.down_proj.scales', 'llm.model.layers.37.mlp.gate_proj.g_idx', 'llm.model.layers.37.mlp.gate_proj.qweight', 'llm.model.layers.37.mlp.gate_proj.qzeros', 'llm.model.layers.37.mlp.gate_proj.scales', 'llm.model.layers.37.mlp.up_proj.g_idx', 'llm.model.layers.37.mlp.up_proj.qweight', 'llm.model.layers.37.mlp.up_proj.qzeros', 'llm.model.layers.37.mlp.up_proj.scales', 'llm.model.layers.37.self_attn.k_proj.g_idx', 'llm.model.layers.37.self_attn.k_proj.qweight', 'llm.model.layers.37.self_attn.k_proj.qzeros', 'llm.model.layers.37.self_attn.k_proj.scales', 'llm.model.layers.37.self_attn.o_proj.g_idx', 'llm.model.layers.37.self_attn.o_proj.qweight', 'llm.model.layers.37.self_attn.o_proj.qzeros', 'llm.model.layers.37.self_attn.o_proj.scales', 'llm.model.layers.37.self_attn.q_proj.g_idx', 'llm.model.layers.37.self_attn.q_proj.qweight', 'llm.model.layers.37.self_attn.q_proj.qzeros', 'llm.model.layers.37.self_attn.q_proj.scales', 'llm.model.layers.37.self_attn.v_proj.g_idx', 'llm.model.layers.37.self_attn.v_proj.qweight', 'llm.model.layers.37.self_attn.v_proj.qzeros', 'llm.model.layers.37.self_attn.v_proj.scales', 'llm.model.layers.38.mlp.down_proj.g_idx', 'llm.model.layers.38.mlp.down_proj.qweight', 'llm.model.layers.38.mlp.down_proj.qzeros', 'llm.model.layers.38.mlp.down_proj.scales', 'llm.model.layers.38.mlp.gate_proj.g_idx', 'llm.model.layers.38.mlp.gate_proj.qweight', 'llm.model.layers.38.mlp.gate_proj.qzeros', 'llm.model.layers.38.mlp.gate_proj.scales', 'llm.model.layers.38.mlp.up_proj.g_idx', 'llm.model.layers.38.mlp.up_proj.qweight', 'llm.model.layers.38.mlp.up_proj.qzeros', 'llm.model.layers.38.mlp.up_proj.scales', 'llm.model.layers.38.self_attn.k_proj.g_idx', 'llm.model.layers.38.self_attn.k_proj.qweight', 'llm.model.layers.38.self_attn.k_proj.qzeros', 'llm.model.layers.38.self_attn.k_proj.scales', 'llm.model.layers.38.self_attn.o_proj.g_idx', 'llm.model.layers.38.self_attn.o_proj.qweight', 'llm.model.layers.38.self_attn.o_proj.qzeros', 'llm.model.layers.38.self_attn.o_proj.scales', 'llm.model.layers.38.self_attn.q_proj.g_idx', 'llm.model.layers.38.self_attn.q_proj.qweight', 'llm.model.layers.38.self_attn.q_proj.qzeros', 'llm.model.layers.38.self_attn.q_proj.scales', 'llm.model.layers.38.self_attn.v_proj.g_idx', 'llm.model.layers.38.self_attn.v_proj.qweight', 'llm.model.layers.38.self_attn.v_proj.qzeros', 'llm.model.layers.38.self_attn.v_proj.scales', 'llm.model.layers.39.mlp.down_proj.g_idx', 'llm.model.layers.39.mlp.down_proj.qweight', 'llm.model.layers.39.mlp.down_proj.qzeros', 'llm.model.layers.39.mlp.down_proj.scales', 'llm.model.layers.39.mlp.gate_proj.g_idx', 'llm.model.layers.39.mlp.gate_proj.qweight', 'llm.model.layers.39.mlp.gate_proj.qzeros', 'llm.model.layers.39.mlp.gate_proj.scales', 'llm.model.layers.39.mlp.up_proj.g_idx', 'llm.model.layers.39.mlp.up_proj.qweight', 'llm.model.layers.39.mlp.up_proj.qzeros', 'llm.model.layers.39.mlp.up_proj.scales', 'llm.model.layers.39.self_attn.k_proj.g_idx', 'llm.model.layers.39.self_attn.k_proj.qweight', 'llm.model.layers.39.self_attn.k_proj.qzeros', 'llm.model.layers.39.self_attn.k_proj.scales', 'llm.model.layers.39.self_attn.o_proj.g_idx', 'llm.model.layers.39.self_attn.o_proj.qweight', 'llm.model.layers.39.self_attn.o_proj.qzeros', 'llm.model.layers.39.self_attn.o_proj.scales', 'llm.model.layers.39.self_attn.q_proj.g_idx', 'llm.model.layers.39.self_attn.q_proj.qweight', 'llm.model.layers.39.self_attn.q_proj.qzeros', 'llm.model.layers.39.self_attn.q_proj.scales', 'llm.model.layers.39.self_attn.v_proj.g_idx', 'llm.model.layers.39.self_attn.v_proj.qweight', 'llm.model.layers.39.self_attn.v_proj.qzeros', 'llm.model.layers.39.self_attn.v_proj.scales', 'llm.model.layers.4.mlp.down_proj.g_idx', 'llm.model.layers.4.mlp.down_proj.qweight', 'llm.model.layers.4.mlp.down_proj.qzeros', 'llm.model.layers.4.mlp.down_proj.scales', 'llm.model.layers.4.mlp.gate_proj.g_idx', 'llm.model.layers.4.mlp.gate_proj.qweight', 'llm.model.layers.4.mlp.gate_proj.qzeros', 'llm.model.layers.4.mlp.gate_proj.scales', 'llm.model.layers.4.mlp.up_proj.g_idx', 'llm.model.layers.4.mlp.up_proj.qweight', 'llm.model.layers.4.mlp.up_proj.qzeros', 'llm.model.layers.4.mlp.up_proj.scales', 'llm.model.layers.4.self_attn.k_proj.g_idx', 'llm.model.layers.4.self_attn.k_proj.qweight', 'llm.model.layers.4.self_attn.k_proj.qzeros', 'llm.model.layers.4.self_attn.k_proj.scales', 'llm.model.layers.4.self_attn.o_proj.g_idx', 'llm.model.layers.4.self_attn.o_proj.qweight', 'llm.model.layers.4.self_attn.o_proj.qzeros', 'llm.model.layers.4.self_attn.o_proj.scales', 'llm.model.layers.4.self_attn.q_proj.g_idx', 'llm.model.layers.4.self_attn.q_proj.qweight', 'llm.model.layers.4.self_attn.q_proj.qzeros', 'llm.model.layers.4.self_attn.q_proj.scales', 'llm.model.layers.4.self_attn.v_proj.g_idx', 'llm.model.layers.4.self_attn.v_proj.qweight', 'llm.model.layers.4.self_attn.v_proj.qzeros', 'llm.model.layers.4.self_attn.v_proj.scales', 'llm.model.layers.40.mlp.down_proj.g_idx', 'llm.model.layers.40.mlp.down_proj.qweight', 'llm.model.layers.40.mlp.down_proj.qzeros', 'llm.model.layers.40.mlp.down_proj.scales', 'llm.model.layers.40.mlp.gate_proj.g_idx', 'llm.model.layers.40.mlp.gate_proj.qweight', 'llm.model.layers.40.mlp.gate_proj.qzeros', 'llm.model.layers.40.mlp.gate_proj.scales', 'llm.model.layers.40.mlp.up_proj.g_idx', 'llm.model.layers.40.mlp.up_proj.qweight', 'llm.model.layers.40.mlp.up_proj.qzeros', 'llm.model.layers.40.mlp.up_proj.scales', 'llm.model.layers.40.self_attn.k_proj.g_idx', 'llm.model.layers.40.self_attn.k_proj.qweight', 'llm.model.layers.40.self_attn.k_proj.qzeros', 'llm.model.layers.40.self_attn.k_proj.scales', 'llm.model.layers.40.self_attn.o_proj.g_idx', 'llm.model.layers.40.self_attn.o_proj.qweight', 'llm.model.layers.40.self_attn.o_proj.qzeros', 'llm.model.layers.40.self_attn.o_proj.scales', 'llm.model.layers.40.self_attn.q_proj.g_idx', 'llm.model.layers.40.self_attn.q_proj.qweight', 'llm.model.layers.40.self_attn.q_proj.qzeros', 'llm.model.layers.40.self_attn.q_proj.scales', 'llm.model.layers.40.self_attn.v_proj.g_idx', 'llm.model.layers.40.self_attn.v_proj.qweight', 'llm.model.layers.40.self_attn.v_proj.qzeros', 'llm.model.layers.40.self_attn.v_proj.scales', 'llm.model.layers.41.mlp.down_proj.g_idx', 'llm.model.layers.41.mlp.down_proj.qweight', 'llm.model.layers.41.mlp.down_proj.qzeros', 'llm.model.layers.41.mlp.down_proj.scales', 'llm.model.layers.41.mlp.gate_proj.g_idx', 'llm.model.layers.41.mlp.gate_proj.qweight', 'llm.model.layers.41.mlp.gate_proj.qzeros', 'llm.model.layers.41.mlp.gate_proj.scales', 'llm.model.layers.41.mlp.up_proj.g_idx', 'llm.model.layers.41.mlp.up_proj.qweight', 'llm.model.layers.41.mlp.up_proj.qzeros', 'llm.model.layers.41.mlp.up_proj.scales', 'llm.model.layers.41.self_attn.k_proj.g_idx', 'llm.model.layers.41.self_attn.k_proj.qweight', 'llm.model.layers.41.self_attn.k_proj.qzeros', 'llm.model.layers.41.self_attn.k_proj.scales', 'llm.model.layers.41.self_attn.o_proj.g_idx', 'llm.model.layers.41.self_attn.o_proj.qweight', 'llm.model.layers.41.self_attn.o_proj.qzeros', 'llm.model.layers.41.self_attn.o_proj.scales', 'llm.model.layers.41.self_attn.q_proj.g_idx', 'llm.model.layers.41.self_attn.q_proj.qweight', 'llm.model.layers.41.self_attn.q_proj.qzeros', 'llm.model.layers.41.self_attn.q_proj.scales', 'llm.model.layers.41.self_attn.v_proj.g_idx', 'llm.model.layers.41.self_attn.v_proj.qweight', 'llm.model.layers.41.self_attn.v_proj.qzeros', 'llm.model.layers.41.self_attn.v_proj.scales', 'llm.model.layers.42.mlp.down_proj.g_idx', 'llm.model.layers.42.mlp.down_proj.qweight', 'llm.model.layers.42.mlp.down_proj.qzeros', 'llm.model.layers.42.mlp.down_proj.scales', 'llm.model.layers.42.mlp.gate_proj.g_idx', 'llm.model.layers.42.mlp.gate_proj.qweight', 'llm.model.layers.42.mlp.gate_proj.qzeros', 'llm.model.layers.42.mlp.gate_proj.scales', 'llm.model.layers.42.mlp.up_proj.g_idx', 'llm.model.layers.42.mlp.up_proj.qweight', 'llm.model.layers.42.mlp.up_proj.qzeros', 'llm.model.layers.42.mlp.up_proj.scales', 'llm.model.layers.42.self_attn.k_proj.g_idx', 'llm.model.layers.42.self_attn.k_proj.qweight', 'llm.model.layers.42.self_attn.k_proj.qzeros', 'llm.model.layers.42.self_attn.k_proj.scales', 'llm.model.layers.42.self_attn.o_proj.g_idx', 'llm.model.layers.42.self_attn.o_proj.qweight', 'llm.model.layers.42.self_attn.o_proj.qzeros', 'llm.model.layers.42.self_attn.o_proj.scales', 'llm.model.layers.42.self_attn.q_proj.g_idx', 'llm.model.layers.42.self_attn.q_proj.qweight', 'llm.model.layers.42.self_attn.q_proj.qzeros', 'llm.model.layers.42.self_attn.q_proj.scales', 'llm.model.layers.42.self_attn.v_proj.g_idx', 'llm.model.layers.42.self_attn.v_proj.qweight', 'llm.model.layers.42.self_attn.v_proj.qzeros', 'llm.model.layers.42.self_attn.v_proj.scales', 'llm.model.layers.43.mlp.down_proj.g_idx', 'llm.model.layers.43.mlp.down_proj.qweight', 'llm.model.layers.43.mlp.down_proj.qzeros', 'llm.model.layers.43.mlp.down_proj.scales', 'llm.model.layers.43.mlp.gate_proj.g_idx', 'llm.model.layers.43.mlp.gate_proj.qweight', 'llm.model.layers.43.mlp.gate_proj.qzeros', 'llm.model.layers.43.mlp.gate_proj.scales', 'llm.model.layers.43.mlp.up_proj.g_idx', 'llm.model.layers.43.mlp.up_proj.qweight', 'llm.model.layers.43.mlp.up_proj.qzeros', 'llm.model.layers.43.mlp.up_proj.scales', 'llm.model.layers.43.self_attn.k_proj.g_idx', 'llm.model.layers.43.self_attn.k_proj.qweight', 'llm.model.layers.43.self_attn.k_proj.qzeros', 'llm.model.layers.43.self_attn.k_proj.scales', 'llm.model.layers.43.self_attn.o_proj.g_idx', 'llm.model.layers.43.self_attn.o_proj.qweight', 'llm.model.layers.43.self_attn.o_proj.qzeros', 'llm.model.layers.43.self_attn.o_proj.scales', 'llm.model.layers.43.self_attn.q_proj.g_idx', 'llm.model.layers.43.self_attn.q_proj.qweight', 'llm.model.layers.43.self_attn.q_proj.qzeros', 'llm.model.layers.43.self_attn.q_proj.scales', 'llm.model.layers.43.self_attn.v_proj.g_idx', 'llm.model.layers.43.self_attn.v_proj.qweight', 'llm.model.layers.43.self_attn.v_proj.qzeros', 'llm.model.layers.43.self_attn.v_proj.scales', 'llm.model.layers.44.mlp.down_proj.g_idx', 'llm.model.layers.44.mlp.down_proj.qweight', 'llm.model.layers.44.mlp.down_proj.qzeros', 'llm.model.layers.44.mlp.down_proj.scales', 'llm.model.layers.44.mlp.gate_proj.g_idx', 'llm.model.layers.44.mlp.gate_proj.qweight', 'llm.model.layers.44.mlp.gate_proj.qzeros', 'llm.model.layers.44.mlp.gate_proj.scales', 'llm.model.layers.44.mlp.up_proj.g_idx', 'llm.model.layers.44.mlp.up_proj.qweight', 'llm.model.layers.44.mlp.up_proj.qzeros', 'llm.model.layers.44.mlp.up_proj.scales', 'llm.model.layers.44.self_attn.k_proj.g_idx', 'llm.model.layers.44.self_attn.k_proj.qweight', 'llm.model.layers.44.self_attn.k_proj.qzeros', 'llm.model.layers.44.self_attn.k_proj.scales', 'llm.model.layers.44.self_attn.o_proj.g_idx', 'llm.model.layers.44.self_attn.o_proj.qweight', 'llm.model.layers.44.self_attn.o_proj.qzeros', 'llm.model.layers.44.self_attn.o_proj.scales', 'llm.model.layers.44.self_attn.q_proj.g_idx', 'llm.model.layers.44.self_attn.q_proj.qweight', 'llm.model.layers.44.self_attn.q_proj.qzeros', 'llm.model.layers.44.self_attn.q_proj.scales', 'llm.model.layers.44.self_attn.v_proj.g_idx', 'llm.model.layers.44.self_attn.v_proj.qweight', 'llm.model.layers.44.self_attn.v_proj.qzeros', 'llm.model.layers.44.self_attn.v_proj.scales', 'llm.model.layers.45.mlp.down_proj.g_idx', 'llm.model.layers.45.mlp.down_proj.qweight', 'llm.model.layers.45.mlp.down_proj.qzeros', 'llm.model.layers.45.mlp.down_proj.scales', 'llm.model.layers.45.mlp.gate_proj.g_idx', 'llm.model.layers.45.mlp.gate_proj.qweight', 'llm.model.layers.45.mlp.gate_proj.qzeros', 'llm.model.layers.45.mlp.gate_proj.scales', 'llm.model.layers.45.mlp.up_proj.g_idx', 'llm.model.layers.45.mlp.up_proj.qweight', 'llm.model.layers.45.mlp.up_proj.qzeros', 'llm.model.layers.45.mlp.up_proj.scales', 'llm.model.layers.45.self_attn.k_proj.g_idx', 'llm.model.layers.45.self_attn.k_proj.qweight', 'llm.model.layers.45.self_attn.k_proj.qzeros', 'llm.model.layers.45.self_attn.k_proj.scales', 'llm.model.layers.45.self_attn.o_proj.g_idx', 'llm.model.layers.45.self_attn.o_proj.qweight', 'llm.model.layers.45.self_attn.o_proj.qzeros', 'llm.model.layers.45.self_attn.o_proj.scales', 'llm.model.layers.45.self_attn.q_proj.g_idx', 'llm.model.layers.45.self_attn.q_proj.qweight', 'llm.model.layers.45.self_attn.q_proj.qzeros', 'llm.model.layers.45.self_attn.q_proj.scales', 'llm.model.layers.45.self_attn.v_proj.g_idx', 'llm.model.layers.45.self_attn.v_proj.qweight', 'llm.model.layers.45.self_attn.v_proj.qzeros', 'llm.model.layers.45.self_attn.v_proj.scales', 'llm.model.layers.46.mlp.down_proj.g_idx', 'llm.model.layers.46.mlp.down_proj.qweight', 'llm.model.layers.46.mlp.down_proj.qzeros', 'llm.model.layers.46.mlp.down_proj.scales', 'llm.model.layers.46.mlp.gate_proj.g_idx', 'llm.model.layers.46.mlp.gate_proj.qweight', 'llm.model.layers.46.mlp.gate_proj.qzeros', 'llm.model.layers.46.mlp.gate_proj.scales', 'llm.model.layers.46.mlp.up_proj.g_idx', 'llm.model.layers.46.mlp.up_proj.qweight', 'llm.model.layers.46.mlp.up_proj.qzeros', 'llm.model.layers.46.mlp.up_proj.scales', 'llm.model.layers.46.self_attn.k_proj.g_idx', 'llm.model.layers.46.self_attn.k_proj.qweight', 'llm.model.layers.46.self_attn.k_proj.qzeros', 'llm.model.layers.46.self_attn.k_proj.scales', 'llm.model.layers.46.self_attn.o_proj.g_idx', 'llm.model.layers.46.self_attn.o_proj.qweight', 'llm.model.layers.46.self_attn.o_proj.qzeros', 'llm.model.layers.46.self_attn.o_proj.scales', 'llm.model.layers.46.self_attn.q_proj.g_idx', 'llm.model.layers.46.self_attn.q_proj.qweight', 'llm.model.layers.46.self_attn.q_proj.qzeros', 'llm.model.layers.46.self_attn.q_proj.scales', 'llm.model.layers.46.self_attn.v_proj.g_idx', 'llm.model.layers.46.self_attn.v_proj.qweight', 'llm.model.layers.46.self_attn.v_proj.qzeros', 'llm.model.layers.46.self_attn.v_proj.scales', 'llm.model.layers.47.mlp.down_proj.g_idx', 'llm.model.layers.47.mlp.down_proj.qweight', 'llm.model.layers.47.mlp.down_proj.qzeros', 'llm.model.layers.47.mlp.down_proj.scales', 'llm.model.layers.47.mlp.gate_proj.g_idx', 'llm.model.layers.47.mlp.gate_proj.qweight', 'llm.model.layers.47.mlp.gate_proj.qzeros', 'llm.model.layers.47.mlp.gate_proj.scales', 'llm.model.layers.47.mlp.up_proj.g_idx', 'llm.model.layers.47.mlp.up_proj.qweight', 'llm.model.layers.47.mlp.up_proj.qzeros', 'llm.model.layers.47.mlp.up_proj.scales', 'llm.model.layers.47.self_attn.k_proj.g_idx', 'llm.model.layers.47.self_attn.k_proj.qweight', 'llm.model.layers.47.self_attn.k_proj.qzeros', 'llm.model.layers.47.self_attn.k_proj.scales', 'llm.model.layers.47.self_attn.o_proj.g_idx', 'llm.model.layers.47.self_attn.o_proj.qweight', 'llm.model.layers.47.self_attn.o_proj.qzeros', 'llm.model.layers.47.self_attn.o_proj.scales', 'llm.model.layers.47.self_attn.q_proj.g_idx', 'llm.model.layers.47.self_attn.q_proj.qweight', 'llm.model.layers.47.self_attn.q_proj.qzeros', 'llm.model.layers.47.self_attn.q_proj.scales', 'llm.model.layers.47.self_attn.v_proj.g_idx', 'llm.model.layers.47.self_attn.v_proj.qweight', 'llm.model.layers.47.self_attn.v_proj.qzeros', 'llm.model.layers.47.self_attn.v_proj.scales', 'llm.model.layers.48.mlp.down_proj.g_idx', 'llm.model.layers.48.mlp.down_proj.qweight', 'llm.model.layers.48.mlp.down_proj.qzeros', 'llm.model.layers.48.mlp.down_proj.scales', 'llm.model.layers.48.mlp.gate_proj.g_idx', 'llm.model.layers.48.mlp.gate_proj.qweight', 'llm.model.layers.48.mlp.gate_proj.qzeros', 'llm.model.layers.48.mlp.gate_proj.scales', 'llm.model.layers.48.mlp.up_proj.g_idx', 'llm.model.layers.48.mlp.up_proj.qweight', 'llm.model.layers.48.mlp.up_proj.qzeros', 'llm.model.layers.48.mlp.up_proj.scales', 'llm.model.layers.48.self_attn.k_proj.g_idx', 'llm.model.layers.48.self_attn.k_proj.qweight', 'llm.model.layers.48.self_attn.k_proj.qzeros', 'llm.model.layers.48.self_attn.k_proj.scales', 'llm.model.layers.48.self_attn.o_proj.g_idx', 'llm.model.layers.48.self_attn.o_proj.qweight', 'llm.model.layers.48.self_attn.o_proj.qzeros', 'llm.model.layers.48.self_attn.o_proj.scales', 'llm.model.layers.48.self_attn.q_proj.g_idx', 'llm.model.layers.48.self_attn.q_proj.qweight', 'llm.model.layers.48.self_attn.q_proj.qzeros', 'llm.model.layers.48.self_attn.q_proj.scales', 'llm.model.layers.48.self_attn.v_proj.g_idx', 'llm.model.layers.48.self_attn.v_proj.qweight', 'llm.model.layers.48.self_attn.v_proj.qzeros', 'llm.model.layers.48.self_attn.v_proj.scales', 'llm.model.layers.49.mlp.down_proj.g_idx', 'llm.model.layers.49.mlp.down_proj.qweight', 'llm.model.layers.49.mlp.down_proj.qzeros', 'llm.model.layers.49.mlp.down_proj.scales', 'llm.model.layers.49.mlp.gate_proj.g_idx', 'llm.model.layers.49.mlp.gate_proj.qweight', 'llm.model.layers.49.mlp.gate_proj.qzeros', 'llm.model.layers.49.mlp.gate_proj.scales', 'llm.model.layers.49.mlp.up_proj.g_idx', 'llm.model.layers.49.mlp.up_proj.qweight', 'llm.model.layers.49.mlp.up_proj.qzeros', 'llm.model.layers.49.mlp.up_proj.scales', 'llm.model.layers.49.self_attn.k_proj.g_idx', 'llm.model.layers.49.self_attn.k_proj.qweight', 'llm.model.layers.49.self_attn.k_proj.qzeros', 'llm.model.layers.49.self_attn.k_proj.scales', 'llm.model.layers.49.self_attn.o_proj.g_idx', 'llm.model.layers.49.self_attn.o_proj.qweight', 'llm.model.layers.49.self_attn.o_proj.qzeros', 'llm.model.layers.49.self_attn.o_proj.scales', 'llm.model.layers.49.self_attn.q_proj.g_idx', 'llm.model.layers.49.self_attn.q_proj.qweight', 'llm.model.layers.49.self_attn.q_proj.qzeros', 'llm.model.layers.49.self_attn.q_proj.scales', 'llm.model.layers.49.self_attn.v_proj.g_idx', 'llm.model.layers.49.self_attn.v_proj.qweight', 'llm.model.layers.49.self_attn.v_proj.qzeros', 'llm.model.layers.49.self_attn.v_proj.scales', 'llm.model.layers.5.mlp.down_proj.g_idx', 'llm.model.layers.5.mlp.down_proj.qweight', 'llm.model.layers.5.mlp.down_proj.qzeros', 'llm.model.layers.5.mlp.down_proj.scales', 'llm.model.layers.5.mlp.gate_proj.g_idx', 'llm.model.layers.5.mlp.gate_proj.qweight', 'llm.model.layers.5.mlp.gate_proj.qzeros', 'llm.model.layers.5.mlp.gate_proj.scales', 'llm.model.layers.5.mlp.up_proj.g_idx', 'llm.model.layers.5.mlp.up_proj.qweight', 'llm.model.layers.5.mlp.up_proj.qzeros', 'llm.model.layers.5.mlp.up_proj.scales', 'llm.model.layers.5.self_attn.k_proj.g_idx', 'llm.model.layers.5.self_attn.k_proj.qweight', 'llm.model.layers.5.self_attn.k_proj.qzeros', 'llm.model.layers.5.self_attn.k_proj.scales', 'llm.model.layers.5.self_attn.o_proj.g_idx', 'llm.model.layers.5.self_attn.o_proj.qweight', 'llm.model.layers.5.self_attn.o_proj.qzeros', 'llm.model.layers.5.self_attn.o_proj.scales', 'llm.model.layers.5.self_attn.q_proj.g_idx', 'llm.model.layers.5.self_attn.q_proj.qweight', 'llm.model.layers.5.self_attn.q_proj.qzeros', 'llm.model.layers.5.self_attn.q_proj.scales', 'llm.model.layers.5.self_attn.v_proj.g_idx', 'llm.model.layers.5.self_attn.v_proj.qweight', 'llm.model.layers.5.self_attn.v_proj.qzeros', 'llm.model.layers.5.self_attn.v_proj.scales', 'llm.model.layers.50.mlp.down_proj.g_idx', 'llm.model.layers.50.mlp.down_proj.qweight', 'llm.model.layers.50.mlp.down_proj.qzeros', 'llm.model.layers.50.mlp.down_proj.scales', 'llm.model.layers.50.mlp.gate_proj.g_idx', 'llm.model.layers.50.mlp.gate_proj.qweight', 'llm.model.layers.50.mlp.gate_proj.qzeros', 'llm.model.layers.50.mlp.gate_proj.scales', 'llm.model.layers.50.mlp.up_proj.g_idx', 'llm.model.layers.50.mlp.up_proj.qweight', 'llm.model.layers.50.mlp.up_proj.qzeros', 'llm.model.layers.50.mlp.up_proj.scales', 'llm.model.layers.50.self_attn.k_proj.g_idx', 'llm.model.layers.50.self_attn.k_proj.qweight', 'llm.model.layers.50.self_attn.k_proj.qzeros', 'llm.model.layers.50.self_attn.k_proj.scales', 'llm.model.layers.50.self_attn.o_proj.g_idx', 'llm.model.layers.50.self_attn.o_proj.qweight', 'llm.model.layers.50.self_attn.o_proj.qzeros', 'llm.model.layers.50.self_attn.o_proj.scales', 'llm.model.layers.50.self_attn.q_proj.g_idx', 'llm.model.layers.50.self_attn.q_proj.qweight', 'llm.model.layers.50.self_attn.q_proj.qzeros', 'llm.model.layers.50.self_attn.q_proj.scales', 'llm.model.layers.50.self_attn.v_proj.g_idx', 'llm.model.layers.50.self_attn.v_proj.qweight', 'llm.model.layers.50.self_attn.v_proj.qzeros', 'llm.model.layers.50.self_attn.v_proj.scales', 'llm.model.layers.51.mlp.down_proj.g_idx', 'llm.model.layers.51.mlp.down_proj.qweight', 'llm.model.layers.51.mlp.down_proj.qzeros', 'llm.model.layers.51.mlp.down_proj.scales', 'llm.model.layers.51.mlp.gate_proj.g_idx', 'llm.model.layers.51.mlp.gate_proj.qweight', 'llm.model.layers.51.mlp.gate_proj.qzeros', 'llm.model.layers.51.mlp.gate_proj.scales', 'llm.model.layers.51.mlp.up_proj.g_idx', 'llm.model.layers.51.mlp.up_proj.qweight', 'llm.model.layers.51.mlp.up_proj.qzeros', 'llm.model.layers.51.mlp.up_proj.scales', 'llm.model.layers.51.self_attn.k_proj.g_idx', 'llm.model.layers.51.self_attn.k_proj.qweight', 'llm.model.layers.51.self_attn.k_proj.qzeros', 'llm.model.layers.51.self_attn.k_proj.scales', 'llm.model.layers.51.self_attn.o_proj.g_idx', 'llm.model.layers.51.self_attn.o_proj.qweight', 'llm.model.layers.51.self_attn.o_proj.qzeros', 'llm.model.layers.51.self_attn.o_proj.scales', 'llm.model.layers.51.self_attn.q_proj.g_idx', 'llm.model.layers.51.self_attn.q_proj.qweight', 'llm.model.layers.51.self_attn.q_proj.qzeros', 'llm.model.layers.51.self_attn.q_proj.scales', 'llm.model.layers.51.self_attn.v_proj.g_idx', 'llm.model.layers.51.self_attn.v_proj.qweight', 'llm.model.layers.51.self_attn.v_proj.qzeros', 'llm.model.layers.51.self_attn.v_proj.scales', 'llm.model.layers.6.mlp.down_proj.g_idx', 'llm.model.layers.6.mlp.down_proj.qweight', 'llm.model.layers.6.mlp.down_proj.qzeros', 'llm.model.layers.6.mlp.down_proj.scales', 'llm.model.layers.6.mlp.gate_proj.g_idx', 'llm.model.layers.6.mlp.gate_proj.qweight', 'llm.model.layers.6.mlp.gate_proj.qzeros', 'llm.model.layers.6.mlp.gate_proj.scales', 'llm.model.layers.6.mlp.up_proj.g_idx', 'llm.model.layers.6.mlp.up_proj.qweight', 'llm.model.layers.6.mlp.up_proj.qzeros', 'llm.model.layers.6.mlp.up_proj.scales', 'llm.model.layers.6.self_attn.k_proj.g_idx', 'llm.model.layers.6.self_attn.k_proj.qweight', 'llm.model.layers.6.self_attn.k_proj.qzeros', 'llm.model.layers.6.self_attn.k_proj.scales', 'llm.model.layers.6.self_attn.o_proj.g_idx', 'llm.model.layers.6.self_attn.o_proj.qweight', 'llm.model.layers.6.self_attn.o_proj.qzeros', 'llm.model.layers.6.self_attn.o_proj.scales', 'llm.model.layers.6.self_attn.q_proj.g_idx', 'llm.model.layers.6.self_attn.q_proj.qweight', 'llm.model.layers.6.self_attn.q_proj.qzeros', 'llm.model.layers.6.self_attn.q_proj.scales', 'llm.model.layers.6.self_attn.v_proj.g_idx', 'llm.model.layers.6.self_attn.v_proj.qweight', 'llm.model.layers.6.self_attn.v_proj.qzeros', 'llm.model.layers.6.self_attn.v_proj.scales', 'llm.model.layers.7.mlp.down_proj.g_idx', 'llm.model.layers.7.mlp.down_proj.qweight', 'llm.model.layers.7.mlp.down_proj.qzeros', 'llm.model.layers.7.mlp.down_proj.scales', 'llm.model.layers.7.mlp.gate_proj.g_idx', 'llm.model.layers.7.mlp.gate_proj.qweight', 'llm.model.layers.7.mlp.gate_proj.qzeros', 'llm.model.layers.7.mlp.gate_proj.scales', 'llm.model.layers.7.mlp.up_proj.g_idx', 'llm.model.layers.7.mlp.up_proj.qweight', 'llm.model.layers.7.mlp.up_proj.qzeros', 'llm.model.layers.7.mlp.up_proj.scales', 'llm.model.layers.7.self_attn.k_proj.g_idx', 'llm.model.layers.7.self_attn.k_proj.qweight', 'llm.model.layers.7.self_attn.k_proj.qzeros', 'llm.model.layers.7.self_attn.k_proj.scales', 'llm.model.layers.7.self_attn.o_proj.g_idx', 'llm.model.layers.7.self_attn.o_proj.qweight', 'llm.model.layers.7.self_attn.o_proj.qzeros', 'llm.model.layers.7.self_attn.o_proj.scales', 'llm.model.layers.7.self_attn.q_proj.g_idx', 'llm.model.layers.7.self_attn.q_proj.qweight', 'llm.model.layers.7.self_attn.q_proj.qzeros', 'llm.model.layers.7.self_attn.q_proj.scales', 'llm.model.layers.7.self_attn.v_proj.g_idx', 'llm.model.layers.7.self_attn.v_proj.qweight', 'llm.model.layers.7.self_attn.v_proj.qzeros', 'llm.model.layers.7.self_attn.v_proj.scales', 'llm.model.layers.8.mlp.down_proj.g_idx', 'llm.model.layers.8.mlp.down_proj.qweight', 'llm.model.layers.8.mlp.down_proj.qzeros', 'llm.model.layers.8.mlp.down_proj.scales', 'llm.model.layers.8.mlp.gate_proj.g_idx', 'llm.model.layers.8.mlp.gate_proj.qweight', 'llm.model.layers.8.mlp.gate_proj.qzeros', 'llm.model.layers.8.mlp.gate_proj.scales', 'llm.model.layers.8.mlp.up_proj.g_idx', 'llm.model.layers.8.mlp.up_proj.qweight', 'llm.model.layers.8.mlp.up_proj.qzeros', 'llm.model.layers.8.mlp.up_proj.scales', 'llm.model.layers.8.self_attn.k_proj.g_idx', 'llm.model.layers.8.self_attn.k_proj.qweight', 'llm.model.layers.8.self_attn.k_proj.qzeros', 'llm.model.layers.8.self_attn.k_proj.scales', 'llm.model.layers.8.self_attn.o_proj.g_idx', 'llm.model.layers.8.self_attn.o_proj.qweight', 'llm.model.layers.8.self_attn.o_proj.qzeros', 'llm.model.layers.8.self_attn.o_proj.scales', 'llm.model.layers.8.self_attn.q_proj.g_idx', 'llm.model.layers.8.self_attn.q_proj.qweight', 'llm.model.layers.8.self_attn.q_proj.qzeros', 'llm.model.layers.8.self_attn.q_proj.scales', 'llm.model.layers.8.self_attn.v_proj.g_idx', 'llm.model.layers.8.self_attn.v_proj.qweight', 'llm.model.layers.8.self_attn.v_proj.qzeros', 'llm.model.layers.8.self_attn.v_proj.scales', 'llm.model.layers.9.mlp.down_proj.g_idx', 'llm.model.layers.9.mlp.down_proj.qweight', 'llm.model.layers.9.mlp.down_proj.qzeros', 'llm.model.layers.9.mlp.down_proj.scales', 'llm.model.layers.9.mlp.gate_proj.g_idx', 'llm.model.layers.9.mlp.gate_proj.qweight', 'llm.model.layers.9.mlp.gate_proj.qzeros', 'llm.model.layers.9.mlp.gate_proj.scales', 'llm.model.layers.9.mlp.up_proj.g_idx', 'llm.model.layers.9.mlp.up_proj.qweight', 'llm.model.layers.9.mlp.up_proj.qzeros', 'llm.model.layers.9.mlp.up_proj.scales', 'llm.model.layers.9.self_attn.k_proj.g_idx', 'llm.model.layers.9.self_attn.k_proj.qweight', 'llm.model.layers.9.self_attn.k_proj.qzeros', 'llm.model.layers.9.self_attn.k_proj.scales', 'llm.model.layers.9.self_attn.o_proj.g_idx', 'llm.model.layers.9.self_attn.o_proj.qweight', 'llm.model.layers.9.self_attn.o_proj.qzeros', 'llm.model.layers.9.self_attn.o_proj.scales', 'llm.model.layers.9.self_attn.q_proj.g_idx', 'llm.model.layers.9.self_attn.q_proj.qweight', 'llm.model.layers.9.self_attn.q_proj.qzeros', 'llm.model.layers.9.self_attn.q_proj.scales', 'llm.model.layers.9.self_attn.v_proj.g_idx', 'llm.model.layers.9.self_attn.v_proj.qweight', 'llm.model.layers.9.self_attn.v_proj.qzeros', 'llm.model.layers.9.self_attn.v_proj.scales', 'vpm.encoder.layers.0.mlp.fc1.g_idx', 'vpm.encoder.layers.0.mlp.fc1.qweight', 'vpm.encoder.layers.0.mlp.fc1.qzeros', 'vpm.encoder.layers.0.mlp.fc1.scales', 'vpm.encoder.layers.0.mlp.fc2.g_idx', 'vpm.encoder.layers.0.mlp.fc2.qweight', 'vpm.encoder.layers.0.mlp.fc2.qzeros', 'vpm.encoder.layers.0.mlp.fc2.scales', 'vpm.encoder.layers.0.self_attn.k_proj.g_idx', 'vpm.encoder.layers.0.self_attn.k_proj.qweight', 'vpm.encoder.layers.0.self_attn.k_proj.qzeros', 'vpm.encoder.layers.0.self_attn.k_proj.scales', 'vpm.encoder.layers.0.self_attn.out_proj.g_idx', 'vpm.encoder.layers.0.self_attn.out_proj.qweight', 'vpm.encoder.layers.0.self_attn.out_proj.qzeros', 'vpm.encoder.layers.0.self_attn.out_proj.scales', 'vpm.encoder.layers.0.self_attn.q_proj.g_idx', 'vpm.encoder.layers.0.self_attn.q_proj.qweight', 'vpm.encoder.layers.0.self_attn.q_proj.qzeros', 'vpm.encoder.layers.0.self_attn.q_proj.scales', 'vpm.encoder.layers.0.self_attn.v_proj.g_idx', 'vpm.encoder.layers.0.self_attn.v_proj.qweight', 'vpm.encoder.layers.0.self_attn.v_proj.qzeros', 'vpm.encoder.layers.0.self_attn.v_proj.scales', 'vpm.encoder.layers.1.mlp.fc1.g_idx', 'vpm.encoder.layers.1.mlp.fc1.qweight', 'vpm.encoder.layers.1.mlp.fc1.qzeros', 'vpm.encoder.layers.1.mlp.fc1.scales', 'vpm.encoder.layers.1.mlp.fc2.g_idx', 'vpm.encoder.layers.1.mlp.fc2.qweight', 'vpm.encoder.layers.1.mlp.fc2.qzeros', 'vpm.encoder.layers.1.mlp.fc2.scales', 'vpm.encoder.layers.1.self_attn.k_proj.g_idx', 'vpm.encoder.layers.1.self_attn.k_proj.qweight', 'vpm.encoder.layers.1.self_attn.k_proj.qzeros', 'vpm.encoder.layers.1.self_attn.k_proj.scales', 'vpm.encoder.layers.1.self_attn.out_proj.g_idx', 'vpm.encoder.layers.1.self_attn.out_proj.qweight', 'vpm.encoder.layers.1.self_attn.out_proj.qzeros', 'vpm.encoder.layers.1.self_attn.out_proj.scales', 'vpm.encoder.layers.1.self_attn.q_proj.g_idx', 'vpm.encoder.layers.1.self_attn.q_proj.qweight', 'vpm.encoder.layers.1.self_attn.q_proj.qzeros', 'vpm.encoder.layers.1.self_attn.q_proj.scales', 'vpm.encoder.layers.1.self_attn.v_proj.g_idx', 'vpm.encoder.layers.1.self_attn.v_proj.qweight', 'vpm.encoder.layers.1.self_attn.v_proj.qzeros', 'vpm.encoder.layers.1.self_attn.v_proj.scales', 'vpm.encoder.layers.10.mlp.fc1.g_idx', 'vpm.encoder.layers.10.mlp.fc1.qweight', 'vpm.encoder.layers.10.mlp.fc1.qzeros', 'vpm.encoder.layers.10.mlp.fc1.scales', 'vpm.encoder.layers.10.mlp.fc2.g_idx', 'vpm.encoder.layers.10.mlp.fc2.qweight', 'vpm.encoder.layers.10.mlp.fc2.qzeros', 'vpm.encoder.layers.10.mlp.fc2.scales', 'vpm.encoder.layers.10.self_attn.k_proj.g_idx', 'vpm.encoder.layers.10.self_attn.k_proj.qweight', 'vpm.encoder.layers.10.self_attn.k_proj.qzeros', 'vpm.encoder.layers.10.self_attn.k_proj.scales', 'vpm.encoder.layers.10.self_attn.out_proj.g_idx', 'vpm.encoder.layers.10.self_attn.out_proj.qweight', 'vpm.encoder.layers.10.self_attn.out_proj.qzeros', 'vpm.encoder.layers.10.self_attn.out_proj.scales', 'vpm.encoder.layers.10.self_attn.q_proj.g_idx', 'vpm.encoder.layers.10.self_attn.q_proj.qweight', 'vpm.encoder.layers.10.self_attn.q_proj.qzeros', 'vpm.encoder.layers.10.self_attn.q_proj.scales', 'vpm.encoder.layers.10.self_attn.v_proj.g_idx', 'vpm.encoder.layers.10.self_attn.v_proj.qweight', 'vpm.encoder.layers.10.self_attn.v_proj.qzeros', 'vpm.encoder.layers.10.self_attn.v_proj.scales', 'vpm.encoder.layers.11.mlp.fc1.g_idx', 'vpm.encoder.layers.11.mlp.fc1.qweight', 'vpm.encoder.layers.11.mlp.fc1.qzeros', 'vpm.encoder.layers.11.mlp.fc1.scales', 'vpm.encoder.layers.11.mlp.fc2.g_idx', 'vpm.encoder.layers.11.mlp.fc2.qweight', 'vpm.encoder.layers.11.mlp.fc2.qzeros', 'vpm.encoder.layers.11.mlp.fc2.scales', 'vpm.encoder.layers.11.self_attn.k_proj.g_idx', 'vpm.encoder.layers.11.self_attn.k_proj.qweight', 'vpm.encoder.layers.11.self_attn.k_proj.qzeros', 'vpm.encoder.layers.11.self_attn.k_proj.scales', 'vpm.encoder.layers.11.self_attn.out_proj.g_idx', 'vpm.encoder.layers.11.self_attn.out_proj.qweight', 'vpm.encoder.layers.11.self_attn.out_proj.qzeros', 'vpm.encoder.layers.11.self_attn.out_proj.scales', 'vpm.encoder.layers.11.self_attn.q_proj.g_idx', 'vpm.encoder.layers.11.self_attn.q_proj.qweight', 'vpm.encoder.layers.11.self_attn.q_proj.qzeros', 'vpm.encoder.layers.11.self_attn.q_proj.scales', 'vpm.encoder.layers.11.self_attn.v_proj.g_idx', 'vpm.encoder.layers.11.self_attn.v_proj.qweight', 'vpm.encoder.layers.11.self_attn.v_proj.qzeros', 'vpm.encoder.layers.11.self_attn.v_proj.scales', 'vpm.encoder.layers.12.mlp.fc1.g_idx', 'vpm.encoder.layers.12.mlp.fc1.qweight', 'vpm.encoder.layers.12.mlp.fc1.qzeros', 'vpm.encoder.layers.12.mlp.fc1.scales', 'vpm.encoder.layers.12.mlp.fc2.g_idx', 'vpm.encoder.layers.12.mlp.fc2.qweight', 'vpm.encoder.layers.12.mlp.fc2.qzeros', 'vpm.encoder.layers.12.mlp.fc2.scales', 'vpm.encoder.layers.12.self_attn.k_proj.g_idx', 'vpm.encoder.layers.12.self_attn.k_proj.qweight', 'vpm.encoder.layers.12.self_attn.k_proj.qzeros', 'vpm.encoder.layers.12.self_attn.k_proj.scales', 'vpm.encoder.layers.12.self_attn.out_proj.g_idx', 'vpm.encoder.layers.12.self_attn.out_proj.qweight', 'vpm.encoder.layers.12.self_attn.out_proj.qzeros', 'vpm.encoder.layers.12.self_attn.out_proj.scales', 'vpm.encoder.layers.12.self_attn.q_proj.g_idx', 'vpm.encoder.layers.12.self_attn.q_proj.qweight', 'vpm.encoder.layers.12.self_attn.q_proj.qzeros', 'vpm.encoder.layers.12.self_attn.q_proj.scales', 'vpm.encoder.layers.12.self_attn.v_proj.g_idx', 'vpm.encoder.layers.12.self_attn.v_proj.qweight', 'vpm.encoder.layers.12.self_attn.v_proj.qzeros', 'vpm.encoder.layers.12.self_attn.v_proj.scales', 'vpm.encoder.layers.13.mlp.fc1.g_idx', 'vpm.encoder.layers.13.mlp.fc1.qweight', 'vpm.encoder.layers.13.mlp.fc1.qzeros', 'vpm.encoder.layers.13.mlp.fc1.scales', 'vpm.encoder.layers.13.mlp.fc2.g_idx', 'vpm.encoder.layers.13.mlp.fc2.qweight', 'vpm.encoder.layers.13.mlp.fc2.qzeros', 'vpm.encoder.layers.13.mlp.fc2.scales', 'vpm.encoder.layers.13.self_attn.k_proj.g_idx', 'vpm.encoder.layers.13.self_attn.k_proj.qweight', 'vpm.encoder.layers.13.self_attn.k_proj.qzeros', 'vpm.encoder.layers.13.self_attn.k_proj.scales', 'vpm.encoder.layers.13.self_attn.out_proj.g_idx', 'vpm.encoder.layers.13.self_attn.out_proj.qweight', 'vpm.encoder.layers.13.self_attn.out_proj.qzeros', 'vpm.encoder.layers.13.self_attn.out_proj.scales', 'vpm.encoder.layers.13.self_attn.q_proj.g_idx', 'vpm.encoder.layers.13.self_attn.q_proj.qweight', 'vpm.encoder.layers.13.self_attn.q_proj.qzeros', 'vpm.encoder.layers.13.self_attn.q_proj.scales', 'vpm.encoder.layers.13.self_attn.v_proj.g_idx', 'vpm.encoder.layers.13.self_attn.v_proj.qweight', 'vpm.encoder.layers.13.self_attn.v_proj.qzeros', 'vpm.encoder.layers.13.self_attn.v_proj.scales', 'vpm.encoder.layers.14.mlp.fc1.g_idx', 'vpm.encoder.layers.14.mlp.fc1.qweight', 'vpm.encoder.layers.14.mlp.fc1.qzeros', 'vpm.encoder.layers.14.mlp.fc1.scales', 'vpm.encoder.layers.14.mlp.fc2.g_idx', 'vpm.encoder.layers.14.mlp.fc2.qweight', 'vpm.encoder.layers.14.mlp.fc2.qzeros', 'vpm.encoder.layers.14.mlp.fc2.scales', 'vpm.encoder.layers.14.self_attn.k_proj.g_idx', 'vpm.encoder.layers.14.self_attn.k_proj.qweight', 'vpm.encoder.layers.14.self_attn.k_proj.qzeros', 'vpm.encoder.layers.14.self_attn.k_proj.scales', 'vpm.encoder.layers.14.self_attn.out_proj.g_idx', 'vpm.encoder.layers.14.self_attn.out_proj.qweight', 'vpm.encoder.layers.14.self_attn.out_proj.qzeros', 'vpm.encoder.layers.14.self_attn.out_proj.scales', 'vpm.encoder.layers.14.self_attn.q_proj.g_idx', 'vpm.encoder.layers.14.self_attn.q_proj.qweight', 'vpm.encoder.layers.14.self_attn.q_proj.qzeros', 'vpm.encoder.layers.14.self_attn.q_proj.scales', 'vpm.encoder.layers.14.self_attn.v_proj.g_idx', 'vpm.encoder.layers.14.self_attn.v_proj.qweight', 'vpm.encoder.layers.14.self_attn.v_proj.qzeros', 'vpm.encoder.layers.14.self_attn.v_proj.scales', 'vpm.encoder.layers.15.mlp.fc1.g_idx', 'vpm.encoder.layers.15.mlp.fc1.qweight', 'vpm.encoder.layers.15.mlp.fc1.qzeros', 'vpm.encoder.layers.15.mlp.fc1.scales', 'vpm.encoder.layers.15.mlp.fc2.g_idx', 'vpm.encoder.layers.15.mlp.fc2.qweight', 'vpm.encoder.layers.15.mlp.fc2.qzeros', 'vpm.encoder.layers.15.mlp.fc2.scales', 'vpm.encoder.layers.15.self_attn.k_proj.g_idx', 'vpm.encoder.layers.15.self_attn.k_proj.qweight', 'vpm.encoder.layers.15.self_attn.k_proj.qzeros', 'vpm.encoder.layers.15.self_attn.k_proj.scales', 'vpm.encoder.layers.15.self_attn.out_proj.g_idx', 'vpm.encoder.layers.15.self_attn.out_proj.qweight', 'vpm.encoder.layers.15.self_attn.out_proj.qzeros', 'vpm.encoder.layers.15.self_attn.out_proj.scales', 'vpm.encoder.layers.15.self_attn.q_proj.g_idx', 'vpm.encoder.layers.15.self_attn.q_proj.qweight', 'vpm.encoder.layers.15.self_attn.q_proj.qzeros', 'vpm.encoder.layers.15.self_attn.q_proj.scales', 'vpm.encoder.layers.15.self_attn.v_proj.g_idx', 'vpm.encoder.layers.15.self_attn.v_proj.qweight', 'vpm.encoder.layers.15.self_attn.v_proj.qzeros', 'vpm.encoder.layers.15.self_attn.v_proj.scales', 'vpm.encoder.layers.16.mlp.fc1.g_idx', 'vpm.encoder.layers.16.mlp.fc1.qweight', 'vpm.encoder.layers.16.mlp.fc1.qzeros', 'vpm.encoder.layers.16.mlp.fc1.scales', 'vpm.encoder.layers.16.mlp.fc2.g_idx', 'vpm.encoder.layers.16.mlp.fc2.qweight', 'vpm.encoder.layers.16.mlp.fc2.qzeros', 'vpm.encoder.layers.16.mlp.fc2.scales', 'vpm.encoder.layers.16.self_attn.k_proj.g_idx', 'vpm.encoder.layers.16.self_attn.k_proj.qweight', 'vpm.encoder.layers.16.self_attn.k_proj.qzeros', 'vpm.encoder.layers.16.self_attn.k_proj.scales', 'vpm.encoder.layers.16.self_attn.out_proj.g_idx', 'vpm.encoder.layers.16.self_attn.out_proj.qweight', 'vpm.encoder.layers.16.self_attn.out_proj.qzeros', 'vpm.encoder.layers.16.self_attn.out_proj.scales', 'vpm.encoder.layers.16.self_attn.q_proj.g_idx', 'vpm.encoder.layers.16.self_attn.q_proj.qweight', 'vpm.encoder.layers.16.self_attn.q_proj.qzeros', 'vpm.encoder.layers.16.self_attn.q_proj.scales', 'vpm.encoder.layers.16.self_attn.v_proj.g_idx', 'vpm.encoder.layers.16.self_attn.v_proj.qweight', 'vpm.encoder.layers.16.self_attn.v_proj.qzeros', 'vpm.encoder.layers.16.self_attn.v_proj.scales', 'vpm.encoder.layers.17.mlp.fc1.g_idx', 'vpm.encoder.layers.17.mlp.fc1.qweight', 'vpm.encoder.layers.17.mlp.fc1.qzeros', 'vpm.encoder.layers.17.mlp.fc1.scales', 'vpm.encoder.layers.17.mlp.fc2.g_idx', 'vpm.encoder.layers.17.mlp.fc2.qweight', 'vpm.encoder.layers.17.mlp.fc2.qzeros', 'vpm.encoder.layers.17.mlp.fc2.scales', 'vpm.encoder.layers.17.self_attn.k_proj.g_idx', 'vpm.encoder.layers.17.self_attn.k_proj.qweight', 'vpm.encoder.layers.17.self_attn.k_proj.qzeros', 'vpm.encoder.layers.17.self_attn.k_proj.scales', 'vpm.encoder.layers.17.self_attn.out_proj.g_idx', 'vpm.encoder.layers.17.self_attn.out_proj.qweight', 'vpm.encoder.layers.17.self_attn.out_proj.qzeros', 'vpm.encoder.layers.17.self_attn.out_proj.scales', 'vpm.encoder.layers.17.self_attn.q_proj.g_idx', 'vpm.encoder.layers.17.self_attn.q_proj.qweight', 'vpm.encoder.layers.17.self_attn.q_proj.qzeros', 'vpm.encoder.layers.17.self_attn.q_proj.scales', 'vpm.encoder.layers.17.self_attn.v_proj.g_idx', 'vpm.encoder.layers.17.self_attn.v_proj.qweight', 'vpm.encoder.layers.17.self_attn.v_proj.qzeros', 'vpm.encoder.layers.17.self_attn.v_proj.scales', 'vpm.encoder.layers.18.mlp.fc1.g_idx', 'vpm.encoder.layers.18.mlp.fc1.qweight', 'vpm.encoder.layers.18.mlp.fc1.qzeros', 'vpm.encoder.layers.18.mlp.fc1.scales', 'vpm.encoder.layers.18.mlp.fc2.g_idx', 'vpm.encoder.layers.18.mlp.fc2.qweight', 'vpm.encoder.layers.18.mlp.fc2.qzeros', 'vpm.encoder.layers.18.mlp.fc2.scales', 'vpm.encoder.layers.18.self_attn.k_proj.g_idx', 'vpm.encoder.layers.18.self_attn.k_proj.qweight', 'vpm.encoder.layers.18.self_attn.k_proj.qzeros', 'vpm.encoder.layers.18.self_attn.k_proj.scales', 'vpm.encoder.layers.18.self_attn.out_proj.g_idx', 'vpm.encoder.layers.18.self_attn.out_proj.qweight', 'vpm.encoder.layers.18.self_attn.out_proj.qzeros', 'vpm.encoder.layers.18.self_attn.out_proj.scales', 'vpm.encoder.layers.18.self_attn.q_proj.g_idx', 'vpm.encoder.layers.18.self_attn.q_proj.qweight', 'vpm.encoder.layers.18.self_attn.q_proj.qzeros', 'vpm.encoder.layers.18.self_attn.q_proj.scales', 'vpm.encoder.layers.18.self_attn.v_proj.g_idx', 'vpm.encoder.layers.18.self_attn.v_proj.qweight', 'vpm.encoder.layers.18.self_attn.v_proj.qzeros', 'vpm.encoder.layers.18.self_attn.v_proj.scales', 'vpm.encoder.layers.19.mlp.fc1.g_idx', 'vpm.encoder.layers.19.mlp.fc1.qweight', 'vpm.encoder.layers.19.mlp.fc1.qzeros', 'vpm.encoder.layers.19.mlp.fc1.scales', 'vpm.encoder.layers.19.mlp.fc2.g_idx', 'vpm.encoder.layers.19.mlp.fc2.qweight', 'vpm.encoder.layers.19.mlp.fc2.qzeros', 'vpm.encoder.layers.19.mlp.fc2.scales', 'vpm.encoder.layers.19.self_attn.k_proj.g_idx', 'vpm.encoder.layers.19.self_attn.k_proj.qweight', 'vpm.encoder.layers.19.self_attn.k_proj.qzeros', 'vpm.encoder.layers.19.self_attn.k_proj.scales', 'vpm.encoder.layers.19.self_attn.out_proj.g_idx', 'vpm.encoder.layers.19.self_attn.out_proj.qweight', 'vpm.encoder.layers.19.self_attn.out_proj.qzeros', 'vpm.encoder.layers.19.self_attn.out_proj.scales', 'vpm.encoder.layers.19.self_attn.q_proj.g_idx', 'vpm.encoder.layers.19.self_attn.q_proj.qweight', 'vpm.encoder.layers.19.self_attn.q_proj.qzeros', 'vpm.encoder.layers.19.self_attn.q_proj.scales', 'vpm.encoder.layers.19.self_attn.v_proj.g_idx', 'vpm.encoder.layers.19.self_attn.v_proj.qweight', 'vpm.encoder.layers.19.self_attn.v_proj.qzeros', 'vpm.encoder.layers.19.self_attn.v_proj.scales', 'vpm.encoder.layers.2.mlp.fc1.g_idx', 'vpm.encoder.layers.2.mlp.fc1.qweight', 'vpm.encoder.layers.2.mlp.fc1.qzeros', 'vpm.encoder.layers.2.mlp.fc1.scales', 'vpm.encoder.layers.2.mlp.fc2.g_idx', 'vpm.encoder.layers.2.mlp.fc2.qweight', 'vpm.encoder.layers.2.mlp.fc2.qzeros', 'vpm.encoder.layers.2.mlp.fc2.scales', 'vpm.encoder.layers.2.self_attn.k_proj.g_idx', 'vpm.encoder.layers.2.self_attn.k_proj.qweight', 'vpm.encoder.layers.2.self_attn.k_proj.qzeros', 'vpm.encoder.layers.2.self_attn.k_proj.scales', 'vpm.encoder.layers.2.self_attn.out_proj.g_idx', 'vpm.encoder.layers.2.self_attn.out_proj.qweight', 'vpm.encoder.layers.2.self_attn.out_proj.qzeros', 'vpm.encoder.layers.2.self_attn.out_proj.scales', 'vpm.encoder.layers.2.self_attn.q_proj.g_idx', 'vpm.encoder.layers.2.self_attn.q_proj.qweight', 'vpm.encoder.layers.2.self_attn.q_proj.qzeros', 'vpm.encoder.layers.2.self_attn.q_proj.scales', 'vpm.encoder.layers.2.self_attn.v_proj.g_idx', 'vpm.encoder.layers.2.self_attn.v_proj.qweight', 'vpm.encoder.layers.2.self_attn.v_proj.qzeros', 'vpm.encoder.layers.2.self_attn.v_proj.scales', 'vpm.encoder.layers.20.mlp.fc1.g_idx', 'vpm.encoder.layers.20.mlp.fc1.qweight', 'vpm.encoder.layers.20.mlp.fc1.qzeros', 'vpm.encoder.layers.20.mlp.fc1.scales', 'vpm.encoder.layers.20.mlp.fc2.g_idx', 'vpm.encoder.layers.20.mlp.fc2.qweight', 'vpm.encoder.layers.20.mlp.fc2.qzeros', 'vpm.encoder.layers.20.mlp.fc2.scales', 'vpm.encoder.layers.20.self_attn.k_proj.g_idx', 'vpm.encoder.layers.20.self_attn.k_proj.qweight', 'vpm.encoder.layers.20.self_attn.k_proj.qzeros', 'vpm.encoder.layers.20.self_attn.k_proj.scales', 'vpm.encoder.layers.20.self_attn.out_proj.g_idx', 'vpm.encoder.layers.20.self_attn.out_proj.qweight', 'vpm.encoder.layers.20.self_attn.out_proj.qzeros', 'vpm.encoder.layers.20.self_attn.out_proj.scales', 'vpm.encoder.layers.20.self_attn.q_proj.g_idx', 'vpm.encoder.layers.20.self_attn.q_proj.qweight', 'vpm.encoder.layers.20.self_attn.q_proj.qzeros', 'vpm.encoder.layers.20.self_attn.q_proj.scales', 'vpm.encoder.layers.20.self_attn.v_proj.g_idx', 'vpm.encoder.layers.20.self_attn.v_proj.qweight', 'vpm.encoder.layers.20.self_attn.v_proj.qzeros', 'vpm.encoder.layers.20.self_attn.v_proj.scales', 'vpm.encoder.layers.21.mlp.fc1.g_idx', 'vpm.encoder.layers.21.mlp.fc1.qweight', 'vpm.encoder.layers.21.mlp.fc1.qzeros', 'vpm.encoder.layers.21.mlp.fc1.scales', 'vpm.encoder.layers.21.mlp.fc2.g_idx', 'vpm.encoder.layers.21.mlp.fc2.qweight', 'vpm.encoder.layers.21.mlp.fc2.qzeros', 'vpm.encoder.layers.21.mlp.fc2.scales', 'vpm.encoder.layers.21.self_attn.k_proj.g_idx', 'vpm.encoder.layers.21.self_attn.k_proj.qweight', 'vpm.encoder.layers.21.self_attn.k_proj.qzeros', 'vpm.encoder.layers.21.self_attn.k_proj.scales', 'vpm.encoder.layers.21.self_attn.out_proj.g_idx', 'vpm.encoder.layers.21.self_attn.out_proj.qweight', 'vpm.encoder.layers.21.self_attn.out_proj.qzeros', 'vpm.encoder.layers.21.self_attn.out_proj.scales', 'vpm.encoder.layers.21.self_attn.q_proj.g_idx', 'vpm.encoder.layers.21.self_attn.q_proj.qweight', 'vpm.encoder.layers.21.self_attn.q_proj.qzeros', 'vpm.encoder.layers.21.self_attn.q_proj.scales', 'vpm.encoder.layers.21.self_attn.v_proj.g_idx', 'vpm.encoder.layers.21.self_attn.v_proj.qweight', 'vpm.encoder.layers.21.self_attn.v_proj.qzeros', 'vpm.encoder.layers.21.self_attn.v_proj.scales', 'vpm.encoder.layers.22.mlp.fc1.g_idx', 'vpm.encoder.layers.22.mlp.fc1.qweight', 'vpm.encoder.layers.22.mlp.fc1.qzeros', 'vpm.encoder.layers.22.mlp.fc1.scales', 'vpm.encoder.layers.22.mlp.fc2.g_idx', 'vpm.encoder.layers.22.mlp.fc2.qweight', 'vpm.encoder.layers.22.mlp.fc2.qzeros', 'vpm.encoder.layers.22.mlp.fc2.scales', 'vpm.encoder.layers.22.self_attn.k_proj.g_idx', 'vpm.encoder.layers.22.self_attn.k_proj.qweight', 'vpm.encoder.layers.22.self_attn.k_proj.qzeros', 'vpm.encoder.layers.22.self_attn.k_proj.scales', 'vpm.encoder.layers.22.self_attn.out_proj.g_idx', 'vpm.encoder.layers.22.self_attn.out_proj.qweight', 'vpm.encoder.layers.22.self_attn.out_proj.qzeros', 'vpm.encoder.layers.22.self_attn.out_proj.scales', 'vpm.encoder.layers.22.self_attn.q_proj.g_idx', 'vpm.encoder.layers.22.self_attn.q_proj.qweight', 'vpm.encoder.layers.22.self_attn.q_proj.qzeros', 'vpm.encoder.layers.22.self_attn.q_proj.scales', 'vpm.encoder.layers.22.self_attn.v_proj.g_idx', 'vpm.encoder.layers.22.self_attn.v_proj.qweight', 'vpm.encoder.layers.22.self_attn.v_proj.qzeros', 'vpm.encoder.layers.22.self_attn.v_proj.scales', 'vpm.encoder.layers.23.mlp.fc1.g_idx', 'vpm.encoder.layers.23.mlp.fc1.qweight', 'vpm.encoder.layers.23.mlp.fc1.qzeros', 'vpm.encoder.layers.23.mlp.fc1.scales', 'vpm.encoder.layers.23.mlp.fc2.g_idx', 'vpm.encoder.layers.23.mlp.fc2.qweight', 'vpm.encoder.layers.23.mlp.fc2.qzeros', 'vpm.encoder.layers.23.mlp.fc2.scales', 'vpm.encoder.layers.23.self_attn.k_proj.g_idx', 'vpm.encoder.layers.23.self_attn.k_proj.qweight', 'vpm.encoder.layers.23.self_attn.k_proj.qzeros', 'vpm.encoder.layers.23.self_attn.k_proj.scales', 'vpm.encoder.layers.23.self_attn.out_proj.g_idx', 'vpm.encoder.layers.23.self_attn.out_proj.qweight', 'vpm.encoder.layers.23.self_attn.out_proj.qzeros', 'vpm.encoder.layers.23.self_attn.out_proj.scales', 'vpm.encoder.layers.23.self_attn.q_proj.g_idx', 'vpm.encoder.layers.23.self_attn.q_proj.qweight', 'vpm.encoder.layers.23.self_attn.q_proj.qzeros', 'vpm.encoder.layers.23.self_attn.q_proj.scales', 'vpm.encoder.layers.23.self_attn.v_proj.g_idx', 'vpm.encoder.layers.23.self_attn.v_proj.qweight', 'vpm.encoder.layers.23.self_attn.v_proj.qzeros', 'vpm.encoder.layers.23.self_attn.v_proj.scales', 'vpm.encoder.layers.24.mlp.fc1.g_idx', 'vpm.encoder.layers.24.mlp.fc1.qweight', 'vpm.encoder.layers.24.mlp.fc1.qzeros', 'vpm.encoder.layers.24.mlp.fc1.scales', 'vpm.encoder.layers.24.mlp.fc2.g_idx', 'vpm.encoder.layers.24.mlp.fc2.qweight', 'vpm.encoder.layers.24.mlp.fc2.qzeros', 'vpm.encoder.layers.24.mlp.fc2.scales', 'vpm.encoder.layers.24.self_attn.k_proj.g_idx', 'vpm.encoder.layers.24.self_attn.k_proj.qweight', 'vpm.encoder.layers.24.self_attn.k_proj.qzeros', 'vpm.encoder.layers.24.self_attn.k_proj.scales', 'vpm.encoder.layers.24.self_attn.out_proj.g_idx', 'vpm.encoder.layers.24.self_attn.out_proj.qweight', 'vpm.encoder.layers.24.self_attn.out_proj.qzeros', 'vpm.encoder.layers.24.self_attn.out_proj.scales', 'vpm.encoder.layers.24.self_attn.q_proj.g_idx', 'vpm.encoder.layers.24.self_attn.q_proj.qweight', 'vpm.encoder.layers.24.self_attn.q_proj.qzeros', 'vpm.encoder.layers.24.self_attn.q_proj.scales', 'vpm.encoder.layers.24.self_attn.v_proj.g_idx', 'vpm.encoder.layers.24.self_attn.v_proj.qweight', 'vpm.encoder.layers.24.self_attn.v_proj.qzeros', 'vpm.encoder.layers.24.self_attn.v_proj.scales', 'vpm.encoder.layers.25.mlp.fc1.g_idx', 'vpm.encoder.layers.25.mlp.fc1.qweight', 'vpm.encoder.layers.25.mlp.fc1.qzeros', 'vpm.encoder.layers.25.mlp.fc1.scales', 'vpm.encoder.layers.25.mlp.fc2.g_idx', 'vpm.encoder.layers.25.mlp.fc2.qweight', 'vpm.encoder.layers.25.mlp.fc2.qzeros', 'vpm.encoder.layers.25.mlp.fc2.scales', 'vpm.encoder.layers.25.self_attn.k_proj.g_idx', 'vpm.encoder.layers.25.self_attn.k_proj.qweight', 'vpm.encoder.layers.25.self_attn.k_proj.qzeros', 'vpm.encoder.layers.25.self_attn.k_proj.scales', 'vpm.encoder.layers.25.self_attn.out_proj.g_idx', 'vpm.encoder.layers.25.self_attn.out_proj.qweight', 'vpm.encoder.layers.25.self_attn.out_proj.qzeros', 'vpm.encoder.layers.25.self_attn.out_proj.scales', 'vpm.encoder.layers.25.self_attn.q_proj.g_idx', 'vpm.encoder.layers.25.self_attn.q_proj.qweight', 'vpm.encoder.layers.25.self_attn.q_proj.qzeros', 'vpm.encoder.layers.25.self_attn.q_proj.scales', 'vpm.encoder.layers.25.self_attn.v_proj.g_idx', 'vpm.encoder.layers.25.self_attn.v_proj.qweight', 'vpm.encoder.layers.25.self_attn.v_proj.qzeros', 'vpm.encoder.layers.25.self_attn.v_proj.scales', 'vpm.encoder.layers.26.mlp.fc1.g_idx', 'vpm.encoder.layers.26.mlp.fc1.qweight', 'vpm.encoder.layers.26.mlp.fc1.qzeros', 'vpm.encoder.layers.26.mlp.fc1.scales', 'vpm.encoder.layers.26.mlp.fc2.g_idx', 'vpm.encoder.layers.26.mlp.fc2.qweight', 'vpm.encoder.layers.26.mlp.fc2.qzeros', 'vpm.encoder.layers.26.mlp.fc2.scales', 'vpm.encoder.layers.26.self_attn.k_proj.g_idx', 'vpm.encoder.layers.26.self_attn.k_proj.qweight', 'vpm.encoder.layers.26.self_attn.k_proj.qzeros', 'vpm.encoder.layers.26.self_attn.k_proj.scales', 'vpm.encoder.layers.26.self_attn.out_proj.g_idx', 'vpm.encoder.layers.26.self_attn.out_proj.qweight', 'vpm.encoder.layers.26.self_attn.out_proj.qzeros', 'vpm.encoder.layers.26.self_attn.out_proj.scales', 'vpm.encoder.layers.26.self_attn.q_proj.g_idx', 'vpm.encoder.layers.26.self_attn.q_proj.qweight', 'vpm.encoder.layers.26.self_attn.q_proj.qzeros', 'vpm.encoder.layers.26.self_attn.q_proj.scales', 'vpm.encoder.layers.26.self_attn.v_proj.g_idx', 'vpm.encoder.layers.26.self_attn.v_proj.qweight', 'vpm.encoder.layers.26.self_attn.v_proj.qzeros', 'vpm.encoder.layers.26.self_attn.v_proj.scales', 'vpm.encoder.layers.3.mlp.fc1.g_idx', 'vpm.encoder.layers.3.mlp.fc1.qweight', 'vpm.encoder.layers.3.mlp.fc1.qzeros', 'vpm.encoder.layers.3.mlp.fc1.scales', 'vpm.encoder.layers.3.mlp.fc2.g_idx', 'vpm.encoder.layers.3.mlp.fc2.qweight', 'vpm.encoder.layers.3.mlp.fc2.qzeros', 'vpm.encoder.layers.3.mlp.fc2.scales', 'vpm.encoder.layers.3.self_attn.k_proj.g_idx', 'vpm.encoder.layers.3.self_attn.k_proj.qweight', 'vpm.encoder.layers.3.self_attn.k_proj.qzeros', 'vpm.encoder.layers.3.self_attn.k_proj.scales', 'vpm.encoder.layers.3.self_attn.out_proj.g_idx', 'vpm.encoder.layers.3.self_attn.out_proj.qweight', 'vpm.encoder.layers.3.self_attn.out_proj.qzeros', 'vpm.encoder.layers.3.self_attn.out_proj.scales', 'vpm.encoder.layers.3.self_attn.q_proj.g_idx', 'vpm.encoder.layers.3.self_attn.q_proj.qweight', 'vpm.encoder.layers.3.self_attn.q_proj.qzeros', 'vpm.encoder.layers.3.self_attn.q_proj.scales', 'vpm.encoder.layers.3.self_attn.v_proj.g_idx', 'vpm.encoder.layers.3.self_attn.v_proj.qweight', 'vpm.encoder.layers.3.self_attn.v_proj.qzeros', 'vpm.encoder.layers.3.self_attn.v_proj.scales', 'vpm.encoder.layers.4.mlp.fc1.g_idx', 'vpm.encoder.layers.4.mlp.fc1.qweight', 'vpm.encoder.layers.4.mlp.fc1.qzeros', 'vpm.encoder.layers.4.mlp.fc1.scales', 'vpm.encoder.layers.4.mlp.fc2.g_idx', 'vpm.encoder.layers.4.mlp.fc2.qweight', 'vpm.encoder.layers.4.mlp.fc2.qzeros', 'vpm.encoder.layers.4.mlp.fc2.scales', 'vpm.encoder.layers.4.self_attn.k_proj.g_idx', 'vpm.encoder.layers.4.self_attn.k_proj.qweight', 'vpm.encoder.layers.4.self_attn.k_proj.qzeros', 'vpm.encoder.layers.4.self_attn.k_proj.scales', 'vpm.encoder.layers.4.self_attn.out_proj.g_idx', 'vpm.encoder.layers.4.self_attn.out_proj.qweight', 'vpm.encoder.layers.4.self_attn.out_proj.qzeros', 'vpm.encoder.layers.4.self_attn.out_proj.scales', 'vpm.encoder.layers.4.self_attn.q_proj.g_idx', 'vpm.encoder.layers.4.self_attn.q_proj.qweight', 'vpm.encoder.layers.4.self_attn.q_proj.qzeros', 'vpm.encoder.layers.4.self_attn.q_proj.scales', 'vpm.encoder.layers.4.self_attn.v_proj.g_idx', 'vpm.encoder.layers.4.self_attn.v_proj.qweight', 'vpm.encoder.layers.4.self_attn.v_proj.qzeros', 'vpm.encoder.layers.4.self_attn.v_proj.scales', 'vpm.encoder.layers.5.mlp.fc1.g_idx', 'vpm.encoder.layers.5.mlp.fc1.qweight', 'vpm.encoder.layers.5.mlp.fc1.qzeros', 'vpm.encoder.layers.5.mlp.fc1.scales', 'vpm.encoder.layers.5.mlp.fc2.g_idx', 'vpm.encoder.layers.5.mlp.fc2.qweight', 'vpm.encoder.layers.5.mlp.fc2.qzeros', 'vpm.encoder.layers.5.mlp.fc2.scales', 'vpm.encoder.layers.5.self_attn.k_proj.g_idx', 'vpm.encoder.layers.5.self_attn.k_proj.qweight', 'vpm.encoder.layers.5.self_attn.k_proj.qzeros', 'vpm.encoder.layers.5.self_attn.k_proj.scales', 'vpm.encoder.layers.5.self_attn.out_proj.g_idx', 'vpm.encoder.layers.5.self_attn.out_proj.qweight', 'vpm.encoder.layers.5.self_attn.out_proj.qzeros', 'vpm.encoder.layers.5.self_attn.out_proj.scales', 'vpm.encoder.layers.5.self_attn.q_proj.g_idx', 'vpm.encoder.layers.5.self_attn.q_proj.qweight', 'vpm.encoder.layers.5.self_attn.q_proj.qzeros', 'vpm.encoder.layers.5.self_attn.q_proj.scales', 'vpm.encoder.layers.5.self_attn.v_proj.g_idx', 'vpm.encoder.layers.5.self_attn.v_proj.qweight', 'vpm.encoder.layers.5.self_attn.v_proj.qzeros', 'vpm.encoder.layers.5.self_attn.v_proj.scales', 'vpm.encoder.layers.6.mlp.fc1.g_idx', 'vpm.encoder.layers.6.mlp.fc1.qweight', 'vpm.encoder.layers.6.mlp.fc1.qzeros', 'vpm.encoder.layers.6.mlp.fc1.scales', 'vpm.encoder.layers.6.mlp.fc2.g_idx', 'vpm.encoder.layers.6.mlp.fc2.qweight', 'vpm.encoder.layers.6.mlp.fc2.qzeros', 'vpm.encoder.layers.6.mlp.fc2.scales', 'vpm.encoder.layers.6.self_attn.k_proj.g_idx', 'vpm.encoder.layers.6.self_attn.k_proj.qweight', 'vpm.encoder.layers.6.self_attn.k_proj.qzeros', 'vpm.encoder.layers.6.self_attn.k_proj.scales', 'vpm.encoder.layers.6.self_attn.out_proj.g_idx', 'vpm.encoder.layers.6.self_attn.out_proj.qweight', 'vpm.encoder.layers.6.self_attn.out_proj.qzeros', 'vpm.encoder.layers.6.self_attn.out_proj.scales', 'vpm.encoder.layers.6.self_attn.q_proj.g_idx', 'vpm.encoder.layers.6.self_attn.q_proj.qweight', 'vpm.encoder.layers.6.self_attn.q_proj.qzeros', 'vpm.encoder.layers.6.self_attn.q_proj.scales', 'vpm.encoder.layers.6.self_attn.v_proj.g_idx', 'vpm.encoder.layers.6.self_attn.v_proj.qweight', 'vpm.encoder.layers.6.self_attn.v_proj.qzeros', 'vpm.encoder.layers.6.self_attn.v_proj.scales', 'vpm.encoder.layers.7.mlp.fc1.g_idx', 'vpm.encoder.layers.7.mlp.fc1.qweight', 'vpm.encoder.layers.7.mlp.fc1.qzeros', 'vpm.encoder.layers.7.mlp.fc1.scales', 'vpm.encoder.layers.7.mlp.fc2.g_idx', 'vpm.encoder.layers.7.mlp.fc2.qweight', 'vpm.encoder.layers.7.mlp.fc2.qzeros', 'vpm.encoder.layers.7.mlp.fc2.scales', 'vpm.encoder.layers.7.self_attn.k_proj.g_idx', 'vpm.encoder.layers.7.self_attn.k_proj.qweight', 'vpm.encoder.layers.7.self_attn.k_proj.qzeros', 'vpm.encoder.layers.7.self_attn.k_proj.scales', 'vpm.encoder.layers.7.self_attn.out_proj.g_idx', 'vpm.encoder.layers.7.self_attn.out_proj.qweight', 'vpm.encoder.layers.7.self_attn.out_proj.qzeros', 'vpm.encoder.layers.7.self_attn.out_proj.scales', 'vpm.encoder.layers.7.self_attn.q_proj.g_idx', 'vpm.encoder.layers.7.self_attn.q_proj.qweight', 'vpm.encoder.layers.7.self_attn.q_proj.qzeros', 'vpm.encoder.layers.7.self_attn.q_proj.scales', 'vpm.encoder.layers.7.self_attn.v_proj.g_idx', 'vpm.encoder.layers.7.self_attn.v_proj.qweight', 'vpm.encoder.layers.7.self_attn.v_proj.qzeros', 'vpm.encoder.layers.7.self_attn.v_proj.scales', 'vpm.encoder.layers.8.mlp.fc1.g_idx', 'vpm.encoder.layers.8.mlp.fc1.qweight', 'vpm.encoder.layers.8.mlp.fc1.qzeros', 'vpm.encoder.layers.8.mlp.fc1.scales', 'vpm.encoder.layers.8.mlp.fc2.g_idx', 'vpm.encoder.layers.8.mlp.fc2.qweight', 'vpm.encoder.layers.8.mlp.fc2.qzeros', 'vpm.encoder.layers.8.mlp.fc2.scales', 'vpm.encoder.layers.8.self_attn.k_proj.g_idx', 'vpm.encoder.layers.8.self_attn.k_proj.qweight', 'vpm.encoder.layers.8.self_attn.k_proj.qzeros', 'vpm.encoder.layers.8.self_attn.k_proj.scales', 'vpm.encoder.layers.8.self_attn.out_proj.g_idx', 'vpm.encoder.layers.8.self_attn.out_proj.qweight', 'vpm.encoder.layers.8.self_attn.out_proj.qzeros', 'vpm.encoder.layers.8.self_attn.out_proj.scales', 'vpm.encoder.layers.8.self_attn.q_proj.g_idx', 'vpm.encoder.layers.8.self_attn.q_proj.qweight', 'vpm.encoder.layers.8.self_attn.q_proj.qzeros', 'vpm.encoder.layers.8.self_attn.q_proj.scales', 'vpm.encoder.layers.8.self_attn.v_proj.g_idx', 'vpm.encoder.layers.8.self_attn.v_proj.qweight', 'vpm.encoder.layers.8.self_attn.v_proj.qzeros', 'vpm.encoder.layers.8.self_attn.v_proj.scales', 'vpm.encoder.layers.9.mlp.fc1.g_idx', 'vpm.encoder.layers.9.mlp.fc1.qweight', 'vpm.encoder.layers.9.mlp.fc1.qzeros', 'vpm.encoder.layers.9.mlp.fc1.scales', 'vpm.encoder.layers.9.mlp.fc2.g_idx', 'vpm.encoder.layers.9.mlp.fc2.qweight', 'vpm.encoder.layers.9.mlp.fc2.qzeros', 'vpm.encoder.layers.9.mlp.fc2.scales', 'vpm.encoder.layers.9.self_attn.k_proj.g_idx', 'vpm.encoder.layers.9.self_attn.k_proj.qweight', 'vpm.encoder.layers.9.self_attn.k_proj.qzeros', 'vpm.encoder.layers.9.self_attn.k_proj.scales', 'vpm.encoder.layers.9.self_attn.out_proj.g_idx', 'vpm.encoder.layers.9.self_attn.out_proj.qweight', 'vpm.encoder.layers.9.self_attn.out_proj.qzeros', 'vpm.encoder.layers.9.self_attn.out_proj.scales', 'vpm.encoder.layers.9.self_attn.q_proj.g_idx', 'vpm.encoder.layers.9.self_attn.q_proj.qweight', 'vpm.encoder.layers.9.self_attn.q_proj.qzeros', 'vpm.encoder.layers.9.self_attn.q_proj.scales', 'vpm.encoder.layers.9.self_attn.v_proj.g_idx', 'vpm.encoder.layers.9.self_attn.v_proj.qweight', 'vpm.encoder.layers.9.self_attn.v_proj.qzeros', 'vpm.encoder.layers.9.self_attn.v_proj.scales']\n",
      "- This IS expected if you are initializing MiniCPMV from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MiniCPMV from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MiniCPMV were not initialized from the model checkpoint at /home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256-llm-w4-pc and are newly initialized: ['llm.model.layers.0.mlp.down_proj.weight', 'llm.model.layers.0.mlp.gate_proj.weight', 'llm.model.layers.0.mlp.up_proj.weight', 'llm.model.layers.0.self_attn.k_proj.weight', 'llm.model.layers.0.self_attn.v_proj.weight', 'llm.model.layers.1.mlp.down_proj.weight', 'llm.model.layers.1.mlp.gate_proj.weight', 'llm.model.layers.1.mlp.up_proj.weight', 'llm.model.layers.1.self_attn.k_proj.weight', 'llm.model.layers.1.self_attn.v_proj.weight', 'llm.model.layers.10.mlp.down_proj.weight', 'llm.model.layers.10.mlp.gate_proj.weight', 'llm.model.layers.10.mlp.up_proj.weight', 'llm.model.layers.10.self_attn.k_proj.weight', 'llm.model.layers.10.self_attn.v_proj.weight', 'llm.model.layers.11.mlp.down_proj.weight', 'llm.model.layers.11.mlp.gate_proj.weight', 'llm.model.layers.11.mlp.up_proj.weight', 'llm.model.layers.11.self_attn.k_proj.weight', 'llm.model.layers.11.self_attn.v_proj.weight', 'llm.model.layers.12.mlp.down_proj.weight', 'llm.model.layers.12.mlp.gate_proj.weight', 'llm.model.layers.12.mlp.up_proj.weight', 'llm.model.layers.12.self_attn.k_proj.weight', 'llm.model.layers.12.self_attn.v_proj.weight', 'llm.model.layers.13.mlp.down_proj.weight', 'llm.model.layers.13.mlp.gate_proj.weight', 'llm.model.layers.13.mlp.up_proj.weight', 'llm.model.layers.13.self_attn.k_proj.weight', 'llm.model.layers.13.self_attn.v_proj.weight', 'llm.model.layers.14.mlp.down_proj.weight', 'llm.model.layers.14.mlp.gate_proj.weight', 'llm.model.layers.14.mlp.up_proj.weight', 'llm.model.layers.14.self_attn.k_proj.weight', 'llm.model.layers.14.self_attn.v_proj.weight', 'llm.model.layers.15.mlp.down_proj.weight', 'llm.model.layers.15.mlp.gate_proj.weight', 'llm.model.layers.15.mlp.up_proj.weight', 'llm.model.layers.15.self_attn.k_proj.weight', 'llm.model.layers.15.self_attn.v_proj.weight', 'llm.model.layers.16.mlp.down_proj.weight', 'llm.model.layers.16.mlp.gate_proj.weight', 'llm.model.layers.16.mlp.up_proj.weight', 'llm.model.layers.16.self_attn.k_proj.weight', 'llm.model.layers.16.self_attn.v_proj.weight', 'llm.model.layers.17.mlp.down_proj.weight', 'llm.model.layers.17.mlp.gate_proj.weight', 'llm.model.layers.17.mlp.up_proj.weight', 'llm.model.layers.17.self_attn.k_proj.weight', 'llm.model.layers.17.self_attn.v_proj.weight', 'llm.model.layers.18.mlp.down_proj.weight', 'llm.model.layers.18.mlp.gate_proj.weight', 'llm.model.layers.18.mlp.up_proj.weight', 'llm.model.layers.18.self_attn.k_proj.weight', 'llm.model.layers.18.self_attn.v_proj.weight', 'llm.model.layers.19.mlp.down_proj.weight', 'llm.model.layers.19.mlp.gate_proj.weight', 'llm.model.layers.19.mlp.up_proj.weight', 'llm.model.layers.19.self_attn.k_proj.weight', 'llm.model.layers.19.self_attn.v_proj.weight', 'llm.model.layers.2.mlp.down_proj.weight', 'llm.model.layers.2.mlp.gate_proj.weight', 'llm.model.layers.2.mlp.up_proj.weight', 'llm.model.layers.2.self_attn.k_proj.weight', 'llm.model.layers.2.self_attn.v_proj.weight', 'llm.model.layers.20.mlp.down_proj.weight', 'llm.model.layers.20.mlp.gate_proj.weight', 'llm.model.layers.20.mlp.up_proj.weight', 'llm.model.layers.20.self_attn.k_proj.weight', 'llm.model.layers.20.self_attn.v_proj.weight', 'llm.model.layers.21.mlp.down_proj.weight', 'llm.model.layers.21.mlp.gate_proj.weight', 'llm.model.layers.21.mlp.up_proj.weight', 'llm.model.layers.21.self_attn.k_proj.weight', 'llm.model.layers.21.self_attn.v_proj.weight', 'llm.model.layers.22.mlp.down_proj.weight', 'llm.model.layers.22.mlp.gate_proj.weight', 'llm.model.layers.22.mlp.up_proj.weight', 'llm.model.layers.22.self_attn.k_proj.weight', 'llm.model.layers.22.self_attn.v_proj.weight', 'llm.model.layers.23.mlp.down_proj.weight', 'llm.model.layers.23.mlp.gate_proj.weight', 'llm.model.layers.23.mlp.up_proj.weight', 'llm.model.layers.23.self_attn.k_proj.weight', 'llm.model.layers.23.self_attn.v_proj.weight', 'llm.model.layers.24.mlp.down_proj.weight', 'llm.model.layers.24.mlp.gate_proj.weight', 'llm.model.layers.24.mlp.up_proj.weight', 'llm.model.layers.24.self_attn.k_proj.weight', 'llm.model.layers.24.self_attn.v_proj.weight', 'llm.model.layers.25.mlp.down_proj.weight', 'llm.model.layers.25.mlp.gate_proj.weight', 'llm.model.layers.25.mlp.up_proj.weight', 'llm.model.layers.25.self_attn.k_proj.weight', 'llm.model.layers.25.self_attn.v_proj.weight', 'llm.model.layers.26.mlp.down_proj.weight', 'llm.model.layers.26.mlp.gate_proj.weight', 'llm.model.layers.26.mlp.up_proj.weight', 'llm.model.layers.26.self_attn.k_proj.weight', 'llm.model.layers.26.self_attn.v_proj.weight', 'llm.model.layers.27.mlp.down_proj.weight', 'llm.model.layers.27.mlp.gate_proj.weight', 'llm.model.layers.27.mlp.up_proj.weight', 'llm.model.layers.27.self_attn.k_proj.weight', 'llm.model.layers.27.self_attn.v_proj.weight', 'llm.model.layers.28.mlp.down_proj.weight', 'llm.model.layers.28.mlp.gate_proj.weight', 'llm.model.layers.28.mlp.up_proj.weight', 'llm.model.layers.28.self_attn.k_proj.weight', 'llm.model.layers.28.self_attn.v_proj.weight', 'llm.model.layers.29.mlp.down_proj.weight', 'llm.model.layers.29.mlp.gate_proj.weight', 'llm.model.layers.29.mlp.up_proj.weight', 'llm.model.layers.29.self_attn.k_proj.weight', 'llm.model.layers.29.self_attn.v_proj.weight', 'llm.model.layers.3.mlp.down_proj.weight', 'llm.model.layers.3.mlp.gate_proj.weight', 'llm.model.layers.3.mlp.up_proj.weight', 'llm.model.layers.3.self_attn.k_proj.weight', 'llm.model.layers.3.self_attn.v_proj.weight', 'llm.model.layers.30.mlp.down_proj.weight', 'llm.model.layers.30.mlp.gate_proj.weight', 'llm.model.layers.30.mlp.up_proj.weight', 'llm.model.layers.30.self_attn.k_proj.weight', 'llm.model.layers.30.self_attn.v_proj.weight', 'llm.model.layers.31.mlp.down_proj.weight', 'llm.model.layers.31.mlp.gate_proj.weight', 'llm.model.layers.31.mlp.up_proj.weight', 'llm.model.layers.31.self_attn.k_proj.weight', 'llm.model.layers.31.self_attn.v_proj.weight', 'llm.model.layers.32.mlp.down_proj.weight', 'llm.model.layers.32.mlp.gate_proj.weight', 'llm.model.layers.32.mlp.up_proj.weight', 'llm.model.layers.32.self_attn.k_proj.weight', 'llm.model.layers.32.self_attn.v_proj.weight', 'llm.model.layers.33.mlp.down_proj.weight', 'llm.model.layers.33.mlp.gate_proj.weight', 'llm.model.layers.33.mlp.up_proj.weight', 'llm.model.layers.33.self_attn.k_proj.weight', 'llm.model.layers.33.self_attn.v_proj.weight', 'llm.model.layers.34.mlp.down_proj.weight', 'llm.model.layers.34.mlp.gate_proj.weight', 'llm.model.layers.34.mlp.up_proj.weight', 'llm.model.layers.34.self_attn.k_proj.weight', 'llm.model.layers.34.self_attn.v_proj.weight', 'llm.model.layers.35.mlp.down_proj.weight', 'llm.model.layers.35.mlp.gate_proj.weight', 'llm.model.layers.35.mlp.up_proj.weight', 'llm.model.layers.35.self_attn.k_proj.weight', 'llm.model.layers.35.self_attn.v_proj.weight', 'llm.model.layers.36.mlp.down_proj.weight', 'llm.model.layers.36.mlp.gate_proj.weight', 'llm.model.layers.36.mlp.up_proj.weight', 'llm.model.layers.36.self_attn.k_proj.weight', 'llm.model.layers.36.self_attn.v_proj.weight', 'llm.model.layers.37.mlp.down_proj.weight', 'llm.model.layers.37.mlp.gate_proj.weight', 'llm.model.layers.37.mlp.up_proj.weight', 'llm.model.layers.37.self_attn.k_proj.weight', 'llm.model.layers.37.self_attn.v_proj.weight', 'llm.model.layers.38.mlp.down_proj.weight', 'llm.model.layers.38.mlp.gate_proj.weight', 'llm.model.layers.38.mlp.up_proj.weight', 'llm.model.layers.38.self_attn.k_proj.weight', 'llm.model.layers.38.self_attn.v_proj.weight', 'llm.model.layers.39.mlp.down_proj.weight', 'llm.model.layers.39.mlp.gate_proj.weight', 'llm.model.layers.39.mlp.up_proj.weight', 'llm.model.layers.39.self_attn.k_proj.weight', 'llm.model.layers.39.self_attn.v_proj.weight', 'llm.model.layers.4.mlp.down_proj.weight', 'llm.model.layers.4.mlp.gate_proj.weight', 'llm.model.layers.4.mlp.up_proj.weight', 'llm.model.layers.4.self_attn.k_proj.weight', 'llm.model.layers.4.self_attn.v_proj.weight', 'llm.model.layers.40.mlp.down_proj.weight', 'llm.model.layers.40.mlp.gate_proj.weight', 'llm.model.layers.40.mlp.up_proj.weight', 'llm.model.layers.40.self_attn.k_proj.weight', 'llm.model.layers.40.self_attn.v_proj.weight', 'llm.model.layers.41.mlp.down_proj.weight', 'llm.model.layers.41.mlp.gate_proj.weight', 'llm.model.layers.41.mlp.up_proj.weight', 'llm.model.layers.41.self_attn.k_proj.weight', 'llm.model.layers.41.self_attn.v_proj.weight', 'llm.model.layers.42.mlp.down_proj.weight', 'llm.model.layers.42.mlp.gate_proj.weight', 'llm.model.layers.42.mlp.up_proj.weight', 'llm.model.layers.42.self_attn.k_proj.weight', 'llm.model.layers.42.self_attn.v_proj.weight', 'llm.model.layers.43.mlp.down_proj.weight', 'llm.model.layers.43.mlp.gate_proj.weight', 'llm.model.layers.43.mlp.up_proj.weight', 'llm.model.layers.43.self_attn.k_proj.weight', 'llm.model.layers.43.self_attn.v_proj.weight', 'llm.model.layers.44.mlp.down_proj.weight', 'llm.model.layers.44.mlp.gate_proj.weight', 'llm.model.layers.44.mlp.up_proj.weight', 'llm.model.layers.44.self_attn.k_proj.weight', 'llm.model.layers.44.self_attn.v_proj.weight', 'llm.model.layers.45.mlp.down_proj.weight', 'llm.model.layers.45.mlp.gate_proj.weight', 'llm.model.layers.45.mlp.up_proj.weight', 'llm.model.layers.45.self_attn.k_proj.weight', 'llm.model.layers.45.self_attn.v_proj.weight', 'llm.model.layers.46.mlp.down_proj.weight', 'llm.model.layers.46.mlp.gate_proj.weight', 'llm.model.layers.46.mlp.up_proj.weight', 'llm.model.layers.46.self_attn.k_proj.weight', 'llm.model.layers.46.self_attn.v_proj.weight', 'llm.model.layers.47.mlp.down_proj.weight', 'llm.model.layers.47.mlp.gate_proj.weight', 'llm.model.layers.47.mlp.up_proj.weight', 'llm.model.layers.47.self_attn.k_proj.weight', 'llm.model.layers.47.self_attn.v_proj.weight', 'llm.model.layers.48.mlp.down_proj.weight', 'llm.model.layers.48.mlp.gate_proj.weight', 'llm.model.layers.48.mlp.up_proj.weight', 'llm.model.layers.48.self_attn.k_proj.weight', 'llm.model.layers.48.self_attn.v_proj.weight', 'llm.model.layers.49.mlp.down_proj.weight', 'llm.model.layers.49.mlp.gate_proj.weight', 'llm.model.layers.49.mlp.up_proj.weight', 'llm.model.layers.49.self_attn.k_proj.weight', 'llm.model.layers.49.self_attn.v_proj.weight', 'llm.model.layers.5.mlp.down_proj.weight', 'llm.model.layers.5.mlp.gate_proj.weight', 'llm.model.layers.5.mlp.up_proj.weight', 'llm.model.layers.5.self_attn.k_proj.weight', 'llm.model.layers.5.self_attn.v_proj.weight', 'llm.model.layers.50.mlp.down_proj.weight', 'llm.model.layers.50.mlp.gate_proj.weight', 'llm.model.layers.50.mlp.up_proj.weight', 'llm.model.layers.50.self_attn.k_proj.weight', 'llm.model.layers.50.self_attn.v_proj.weight', 'llm.model.layers.51.mlp.down_proj.weight', 'llm.model.layers.51.mlp.gate_proj.weight', 'llm.model.layers.51.mlp.up_proj.weight', 'llm.model.layers.51.self_attn.k_proj.weight', 'llm.model.layers.51.self_attn.v_proj.weight', 'llm.model.layers.6.mlp.down_proj.weight', 'llm.model.layers.6.mlp.gate_proj.weight', 'llm.model.layers.6.mlp.up_proj.weight', 'llm.model.layers.6.self_attn.k_proj.weight', 'llm.model.layers.6.self_attn.v_proj.weight', 'llm.model.layers.7.mlp.down_proj.weight', 'llm.model.layers.7.mlp.gate_proj.weight', 'llm.model.layers.7.mlp.up_proj.weight', 'llm.model.layers.7.self_attn.k_proj.weight', 'llm.model.layers.7.self_attn.v_proj.weight', 'llm.model.layers.8.mlp.down_proj.weight', 'llm.model.layers.8.mlp.gate_proj.weight', 'llm.model.layers.8.mlp.up_proj.weight', 'llm.model.layers.8.self_attn.k_proj.weight', 'llm.model.layers.8.self_attn.v_proj.weight', 'llm.model.layers.9.mlp.down_proj.weight', 'llm.model.layers.9.mlp.gate_proj.weight', 'llm.model.layers.9.mlp.up_proj.weight', 'llm.model.layers.9.self_attn.k_proj.weight', 'llm.model.layers.9.self_attn.v_proj.weight', 'vpm.encoder.layers.0.mlp.fc1.weight', 'vpm.encoder.layers.0.mlp.fc2.weight', 'vpm.encoder.layers.0.self_attn.k_proj.weight', 'vpm.encoder.layers.0.self_attn.out_proj.weight', 'vpm.encoder.layers.0.self_attn.q_proj.weight', 'vpm.encoder.layers.0.self_attn.v_proj.weight', 'vpm.encoder.layers.1.mlp.fc1.weight', 'vpm.encoder.layers.1.mlp.fc2.weight', 'vpm.encoder.layers.1.self_attn.k_proj.weight', 'vpm.encoder.layers.1.self_attn.out_proj.weight', 'vpm.encoder.layers.1.self_attn.q_proj.weight', 'vpm.encoder.layers.1.self_attn.v_proj.weight', 'vpm.encoder.layers.10.mlp.fc1.weight', 'vpm.encoder.layers.10.mlp.fc2.weight', 'vpm.encoder.layers.10.self_attn.k_proj.weight', 'vpm.encoder.layers.10.self_attn.out_proj.weight', 'vpm.encoder.layers.10.self_attn.q_proj.weight', 'vpm.encoder.layers.10.self_attn.v_proj.weight', 'vpm.encoder.layers.11.mlp.fc1.weight', 'vpm.encoder.layers.11.mlp.fc2.weight', 'vpm.encoder.layers.11.self_attn.k_proj.weight', 'vpm.encoder.layers.11.self_attn.out_proj.weight', 'vpm.encoder.layers.11.self_attn.q_proj.weight', 'vpm.encoder.layers.11.self_attn.v_proj.weight', 'vpm.encoder.layers.12.mlp.fc1.weight', 'vpm.encoder.layers.12.mlp.fc2.weight', 'vpm.encoder.layers.12.self_attn.k_proj.weight', 'vpm.encoder.layers.12.self_attn.out_proj.weight', 'vpm.encoder.layers.12.self_attn.q_proj.weight', 'vpm.encoder.layers.12.self_attn.v_proj.weight', 'vpm.encoder.layers.13.mlp.fc1.weight', 'vpm.encoder.layers.13.mlp.fc2.weight', 'vpm.encoder.layers.13.self_attn.k_proj.weight', 'vpm.encoder.layers.13.self_attn.out_proj.weight', 'vpm.encoder.layers.13.self_attn.q_proj.weight', 'vpm.encoder.layers.13.self_attn.v_proj.weight', 'vpm.encoder.layers.14.mlp.fc1.weight', 'vpm.encoder.layers.14.mlp.fc2.weight', 'vpm.encoder.layers.14.self_attn.k_proj.weight', 'vpm.encoder.layers.14.self_attn.out_proj.weight', 'vpm.encoder.layers.14.self_attn.q_proj.weight', 'vpm.encoder.layers.14.self_attn.v_proj.weight', 'vpm.encoder.layers.15.mlp.fc1.weight', 'vpm.encoder.layers.15.mlp.fc2.weight', 'vpm.encoder.layers.15.self_attn.k_proj.weight', 'vpm.encoder.layers.15.self_attn.out_proj.weight', 'vpm.encoder.layers.15.self_attn.q_proj.weight', 'vpm.encoder.layers.15.self_attn.v_proj.weight', 'vpm.encoder.layers.16.mlp.fc1.weight', 'vpm.encoder.layers.16.mlp.fc2.weight', 'vpm.encoder.layers.16.self_attn.k_proj.weight', 'vpm.encoder.layers.16.self_attn.out_proj.weight', 'vpm.encoder.layers.16.self_attn.q_proj.weight', 'vpm.encoder.layers.16.self_attn.v_proj.weight', 'vpm.encoder.layers.17.mlp.fc1.weight', 'vpm.encoder.layers.17.mlp.fc2.weight', 'vpm.encoder.layers.17.self_attn.k_proj.weight', 'vpm.encoder.layers.17.self_attn.out_proj.weight', 'vpm.encoder.layers.17.self_attn.q_proj.weight', 'vpm.encoder.layers.17.self_attn.v_proj.weight', 'vpm.encoder.layers.18.mlp.fc1.weight', 'vpm.encoder.layers.18.mlp.fc2.weight', 'vpm.encoder.layers.18.self_attn.k_proj.weight', 'vpm.encoder.layers.18.self_attn.out_proj.weight', 'vpm.encoder.layers.18.self_attn.q_proj.weight', 'vpm.encoder.layers.18.self_attn.v_proj.weight', 'vpm.encoder.layers.19.mlp.fc1.weight', 'vpm.encoder.layers.19.mlp.fc2.weight', 'vpm.encoder.layers.19.self_attn.k_proj.weight', 'vpm.encoder.layers.19.self_attn.out_proj.weight', 'vpm.encoder.layers.19.self_attn.q_proj.weight', 'vpm.encoder.layers.19.self_attn.v_proj.weight', 'vpm.encoder.layers.2.mlp.fc1.weight', 'vpm.encoder.layers.2.mlp.fc2.weight', 'vpm.encoder.layers.2.self_attn.k_proj.weight', 'vpm.encoder.layers.2.self_attn.out_proj.weight', 'vpm.encoder.layers.2.self_attn.q_proj.weight', 'vpm.encoder.layers.2.self_attn.v_proj.weight', 'vpm.encoder.layers.20.mlp.fc1.weight', 'vpm.encoder.layers.20.mlp.fc2.weight', 'vpm.encoder.layers.20.self_attn.k_proj.weight', 'vpm.encoder.layers.20.self_attn.out_proj.weight', 'vpm.encoder.layers.20.self_attn.q_proj.weight', 'vpm.encoder.layers.20.self_attn.v_proj.weight', 'vpm.encoder.layers.21.mlp.fc1.weight', 'vpm.encoder.layers.21.mlp.fc2.weight', 'vpm.encoder.layers.21.self_attn.k_proj.weight', 'vpm.encoder.layers.21.self_attn.out_proj.weight', 'vpm.encoder.layers.21.self_attn.q_proj.weight', 'vpm.encoder.layers.21.self_attn.v_proj.weight', 'vpm.encoder.layers.22.mlp.fc1.weight', 'vpm.encoder.layers.22.mlp.fc2.weight', 'vpm.encoder.layers.22.self_attn.k_proj.weight', 'vpm.encoder.layers.22.self_attn.out_proj.weight', 'vpm.encoder.layers.22.self_attn.q_proj.weight', 'vpm.encoder.layers.22.self_attn.v_proj.weight', 'vpm.encoder.layers.23.mlp.fc1.weight', 'vpm.encoder.layers.23.mlp.fc2.weight', 'vpm.encoder.layers.23.self_attn.k_proj.weight', 'vpm.encoder.layers.23.self_attn.out_proj.weight', 'vpm.encoder.layers.23.self_attn.q_proj.weight', 'vpm.encoder.layers.23.self_attn.v_proj.weight', 'vpm.encoder.layers.24.mlp.fc1.weight', 'vpm.encoder.layers.24.mlp.fc2.weight', 'vpm.encoder.layers.24.self_attn.k_proj.weight', 'vpm.encoder.layers.24.self_attn.out_proj.weight', 'vpm.encoder.layers.24.self_attn.q_proj.weight', 'vpm.encoder.layers.24.self_attn.v_proj.weight', 'vpm.encoder.layers.25.mlp.fc1.weight', 'vpm.encoder.layers.25.mlp.fc2.weight', 'vpm.encoder.layers.25.self_attn.k_proj.weight', 'vpm.encoder.layers.25.self_attn.out_proj.weight', 'vpm.encoder.layers.25.self_attn.q_proj.weight', 'vpm.encoder.layers.25.self_attn.v_proj.weight', 'vpm.encoder.layers.26.mlp.fc1.weight', 'vpm.encoder.layers.26.mlp.fc2.weight', 'vpm.encoder.layers.26.self_attn.k_proj.weight', 'vpm.encoder.layers.26.self_attn.out_proj.weight', 'vpm.encoder.layers.26.self_attn.q_proj.weight', 'vpm.encoder.layers.26.self_attn.v_proj.weight', 'vpm.encoder.layers.3.mlp.fc1.weight', 'vpm.encoder.layers.3.mlp.fc2.weight', 'vpm.encoder.layers.3.self_attn.k_proj.weight', 'vpm.encoder.layers.3.self_attn.out_proj.weight', 'vpm.encoder.layers.3.self_attn.q_proj.weight', 'vpm.encoder.layers.3.self_attn.v_proj.weight', 'vpm.encoder.layers.4.mlp.fc1.weight', 'vpm.encoder.layers.4.mlp.fc2.weight', 'vpm.encoder.layers.4.self_attn.k_proj.weight', 'vpm.encoder.layers.4.self_attn.out_proj.weight', 'vpm.encoder.layers.4.self_attn.q_proj.weight', 'vpm.encoder.layers.4.self_attn.v_proj.weight', 'vpm.encoder.layers.5.mlp.fc1.weight', 'vpm.encoder.layers.5.mlp.fc2.weight', 'vpm.encoder.layers.5.self_attn.k_proj.weight', 'vpm.encoder.layers.5.self_attn.out_proj.weight', 'vpm.encoder.layers.5.self_attn.q_proj.weight', 'vpm.encoder.layers.5.self_attn.v_proj.weight', 'vpm.encoder.layers.6.mlp.fc1.weight', 'vpm.encoder.layers.6.mlp.fc2.weight', 'vpm.encoder.layers.6.self_attn.k_proj.weight', 'vpm.encoder.layers.6.self_attn.out_proj.weight', 'vpm.encoder.layers.6.self_attn.q_proj.weight', 'vpm.encoder.layers.6.self_attn.v_proj.weight', 'vpm.encoder.layers.7.mlp.fc1.weight', 'vpm.encoder.layers.7.mlp.fc2.weight', 'vpm.encoder.layers.7.self_attn.k_proj.weight', 'vpm.encoder.layers.7.self_attn.out_proj.weight', 'vpm.encoder.layers.7.self_attn.q_proj.weight', 'vpm.encoder.layers.7.self_attn.v_proj.weight', 'vpm.encoder.layers.8.mlp.fc1.weight', 'vpm.encoder.layers.8.mlp.fc2.weight', 'vpm.encoder.layers.8.self_attn.k_proj.weight', 'vpm.encoder.layers.8.self_attn.out_proj.weight', 'vpm.encoder.layers.8.self_attn.q_proj.weight', 'vpm.encoder.layers.8.self_attn.v_proj.weight', 'vpm.encoder.layers.9.mlp.fc1.weight', 'vpm.encoder.layers.9.mlp.fc2.weight', 'vpm.encoder.layers.9.self_attn.k_proj.weight', 'vpm.encoder.layers.9.self_attn.out_proj.weight', 'vpm.encoder.layers.9.self_attn.q_proj.weight', 'vpm.encoder.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO - The layer llm.lm_head is not quantized.\n",
      "INFO - The layer vpm.embeddings.patch_embedding is not quantized.\n",
      "INFO - The layer resampler.kv_proj is not quantized.\n",
      "INFO - The layer resampler.attn.out_proj is not quantized.\n"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalMLM, BaseQuantizeConfig\n",
    "from auto_gptq import AutoGPTQForVIT, BaseQuantizeConfig\n",
    "quantized_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256-llm-w4-pc\"\n",
    "# quantized_model_dir = \"/home/workspace/model/m inicpm-3o-sft-v1-gptq-1112\"\n",
    "model = AutoGPTQForCausalMLM.from_quantized(quantized_model_dir, device=\"cuda:0\", use_triton=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake quanting llm.model.layers.34.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.9.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.44.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.27.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.8.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.11.mlp.up_proj.qweight\n",
      "fake quanting vpm.encoder.layers.4.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.36.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.4.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.33.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.29.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.9.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.15.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.14.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.6.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.11.self_attn.o_proj.qweight\n",
      "fake quanting vpm.encoder.layers.0.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.23.self_attn.out_proj.qweight\n",
      "fake quanting vpm.encoder.layers.4.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.20.self_attn.out_proj.qweight\n",
      "fake quanting llm.model.layers.10.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.20.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.21.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.28.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.10.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.19.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.17.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.33.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.20.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.38.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.42.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.26.self_attn.o_proj.qweight\n",
      "fake quanting vpm.encoder.layers.12.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.18.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.43.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.23.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.26.self_attn.out_proj.qweight\n",
      "fake quanting llm.model.layers.49.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.0.self_attn.o_proj.qweight\n",
      "fake quanting vpm.encoder.layers.15.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.26.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.23.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.35.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.6.self_attn.out_proj.qweight\n",
      "fake quanting llm.model.layers.32.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.51.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.0.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.25.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.40.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.10.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.14.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.14.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.24.mlp.down_proj.qweight\n",
      "fake quanting vpm.encoder.layers.13.mlp.fc1.qweight\n",
      "fake quanting vpm.encoder.layers.24.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.31.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.0.self_attn.out_proj.qweight\n",
      "fake quanting llm.model.layers.36.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.20.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.44.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.2.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.38.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.30.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.14.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.4.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.16.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.21.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.22.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.21.mlp.fc2.qweight\n",
      "fake quanting vpm.encoder.layers.18.self_attn.out_proj.qweight\n",
      "fake quanting vpm.encoder.layers.15.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.33.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.19.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.33.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.40.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.30.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.26.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.33.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.24.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.49.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.50.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.4.mlp.down_proj.qweight\n",
      "fake quanting vpm.encoder.layers.1.self_attn.out_proj.qweight\n",
      "fake quanting llm.model.layers.43.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.3.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.7.self_attn.out_proj.qweight\n",
      "fake quanting vpm.encoder.layers.6.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.20.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.41.mlp.down_proj.qweight\n",
      "fake quanting vpm.encoder.layers.11.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.46.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.50.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.13.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.13.self_attn.out_proj.qweight\n",
      "fake quanting vpm.encoder.layers.5.self_attn.out_proj.qweight\n",
      "fake quanting llm.model.layers.38.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.18.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.0.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.48.mlp.down_proj.qweight\n",
      "fake quanting vpm.encoder.layers.25.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.8.mlp.fc2.qweight\n",
      "fake quanting vpm.encoder.layers.20.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.18.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.21.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.24.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.6.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.38.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.48.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.45.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.50.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.11.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.8.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.19.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.50.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.48.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.21.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.0.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.30.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.40.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.23.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.22.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.10.self_attn.out_proj.qweight\n",
      "fake quanting vpm.encoder.layers.6.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.0.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.14.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.45.mlp.up_proj.qweight\n",
      "fake quanting vpm.encoder.layers.25.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.17.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.39.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.19.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.16.self_attn.o_proj.qweight\n",
      "fake quanting vpm.encoder.layers.13.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.7.mlp.down_proj.qweight\n",
      "fake quanting vpm.encoder.layers.3.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.11.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.13.self_attn.o_proj.qweight\n",
      "fake quanting vpm.encoder.layers.2.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.30.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.17.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.18.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.4.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.30.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.12.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.1.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.4.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.3.mlp.fc1.qweight\n",
      "fake quanting vpm.encoder.layers.15.mlp.fc1.qweight\n",
      "fake quanting vpm.encoder.layers.8.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.45.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.3.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.1.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.19.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.1.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.39.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.15.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.16.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.46.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.6.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.42.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.20.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.13.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.15.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.36.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.25.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.13.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.18.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.5.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.39.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.11.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.26.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.7.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.17.self_attn.out_proj.qweight\n",
      "fake quanting vpm.encoder.layers.23.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.17.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.5.self_attn.o_proj.qweight\n",
      "fake quanting vpm.encoder.layers.25.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.3.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.20.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.15.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.38.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.14.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.29.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.1.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.1.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.31.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.45.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.19.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.16.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.5.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.7.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.19.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.7.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.46.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.9.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.2.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.35.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.39.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.14.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.6.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.27.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.25.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.13.mlp.up_proj.qweight\n",
      "fake quanting vpm.encoder.layers.5.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.0.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.30.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.7.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.41.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.47.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.24.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.24.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.3.self_attn.out_proj.qweight\n",
      "fake quanting llm.model.layers.34.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.32.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.3.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.35.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.34.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.15.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.28.mlp.up_proj.qweight\n",
      "fake quanting vpm.encoder.layers.19.mlp.fc1.qweight\n",
      "fake quanting vpm.encoder.layers.12.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.18.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.2.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.42.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.45.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.28.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.49.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.10.mlp.down_proj.qweight\n",
      "fake quanting vpm.encoder.layers.22.mlp.fc1.qweight\n",
      "fake quanting vpm.encoder.layers.24.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.40.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.31.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.51.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.10.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.49.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.25.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.43.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.47.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.16.mlp.fc1.qweight\n",
      "fake quanting vpm.encoder.layers.24.self_attn.out_proj.qweight\n",
      "fake quanting llm.model.layers.16.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.32.mlp.down_proj.qweight\n",
      "fake quanting vpm.encoder.layers.15.self_attn.out_proj.qweight\n",
      "fake quanting vpm.encoder.layers.13.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.37.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.37.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.6.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.40.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.2.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.16.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.37.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.22.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.50.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.26.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.26.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.22.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.47.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.8.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.10.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.20.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.9.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.21.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.33.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.36.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.3.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.4.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.44.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.22.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.14.self_attn.out_proj.qweight\n",
      "fake quanting vpm.encoder.layers.11.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.26.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.3.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.11.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.46.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.19.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.0.mlp.fc1.qweight\n",
      "fake quanting vpm.encoder.layers.18.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.39.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.11.self_attn.out_proj.qweight\n",
      "fake quanting vpm.encoder.layers.4.self_attn.out_proj.qweight\n",
      "fake quanting llm.model.layers.8.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.44.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.34.self_attn.o_proj.qweight\n",
      "fake quanting vpm.encoder.layers.4.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.46.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.26.mlp.down_proj.qweight\n",
      "fake quanting vpm.encoder.layers.12.self_attn.out_proj.qweight\n",
      "fake quanting vpm.encoder.layers.23.mlp.fc1.qweight\n",
      "fake quanting vpm.encoder.layers.14.mlp.fc2.qweight\n",
      "fake quanting vpm.encoder.layers.23.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.3.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.18.mlp.fc2.qweight\n",
      "fake quanting vpm.encoder.layers.10.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.25.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.15.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.51.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.21.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.50.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.34.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.7.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.37.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.18.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.50.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.17.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.24.self_attn.o_proj.qweight\n",
      "fake quanting vpm.encoder.layers.9.mlp.fc1.qweight\n",
      "fake quanting vpm.encoder.layers.1.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.51.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.7.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.34.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.6.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.16.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.32.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.6.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.45.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.15.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.6.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.36.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.1.mlp.down_proj.qweight\n",
      "fake quanting vpm.encoder.layers.21.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.26.mlp.up_proj.qweight\n",
      "fake quanting vpm.encoder.layers.16.self_attn.out_proj.qweight\n",
      "fake quanting vpm.encoder.layers.22.self_attn.out_proj.qweight\n",
      "fake quanting vpm.encoder.layers.12.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.17.mlp.up_proj.qweight\n",
      "fake quanting vpm.encoder.layers.25.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.3.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.26.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.25.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.12.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.40.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.42.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.43.self_attn.o_proj.qweight\n",
      "fake quanting vpm.encoder.layers.4.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.41.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.22.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.47.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.5.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.7.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.27.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.27.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.41.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.32.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.9.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.42.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.23.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.12.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.41.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.35.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.28.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.12.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.35.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.31.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.46.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.11.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.23.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.24.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.31.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.0.mlp.down_proj.qweight\n",
      "fake quanting vpm.encoder.layers.9.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.39.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.30.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.23.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.10.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.8.mlp.up_proj.qweight\n",
      "fake quanting vpm.encoder.layers.9.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.22.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.8.self_attn.o_proj.qweight\n",
      "fake quanting vpm.encoder.layers.7.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.10.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.15.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.18.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.4.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.38.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.1.mlp.up_proj.qweight\n",
      "fake quanting vpm.encoder.layers.17.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.8.self_attn.out_proj.qweight\n",
      "fake quanting llm.model.layers.36.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.18.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.41.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.2.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.20.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.13.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.48.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.22.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.5.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.1.self_attn.o_proj.qweight\n",
      "fake quanting vpm.encoder.layers.20.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.1.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.42.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.32.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.24.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.18.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.35.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.5.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.16.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.17.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.8.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.5.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.2.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.7.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.47.mlp.up_proj.qweight\n",
      "fake quanting vpm.encoder.layers.19.self_attn.out_proj.qweight\n",
      "fake quanting llm.model.layers.49.mlp.up_proj.qweight\n",
      "fake quanting vpm.encoder.layers.25.self_attn.out_proj.qweight\n",
      "fake quanting llm.model.layers.16.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.27.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.5.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.7.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.16.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.22.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.8.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.22.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.29.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.17.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.39.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.5.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.20.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.28.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.23.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.10.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.23.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.3.self_attn.o_proj.qweight\n",
      "fake quanting vpm.encoder.layers.21.self_attn.out_proj.qweight\n",
      "fake quanting llm.model.layers.38.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.0.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.42.mlp.down_proj.qweight\n",
      "fake quanting vpm.encoder.layers.0.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.14.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.9.self_attn.out_proj.qweight\n",
      "fake quanting llm.model.layers.33.self_attn.v_proj.qweight\n",
      "fake quanting vpm.encoder.layers.21.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.1.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.49.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.0.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.45.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.12.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.29.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.2.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.36.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.9.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.29.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.28.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.12.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.34.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.37.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.40.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.5.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.24.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.7.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.17.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.19.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.16.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.1.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.9.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.22.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.25.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.12.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.51.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.13.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.2.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.48.mlp.up_proj.qweight\n",
      "fake quanting vpm.encoder.layers.14.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.46.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.43.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.43.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.23.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.17.self_attn.o_proj.qweight\n",
      "fake quanting vpm.encoder.layers.2.self_attn.out_proj.qweight\n",
      "fake quanting vpm.encoder.layers.20.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.5.mlp.fc2.qweight\n",
      "fake quanting vpm.encoder.layers.2.self_attn.q_proj.qweight\n",
      "fake quanting vpm.encoder.layers.13.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.21.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.31.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.14.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.31.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.10.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.11.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.8.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.35.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.6.self_attn.o_proj.qweight\n",
      "fake quanting vpm.encoder.layers.21.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.9.mlp.up_proj.qweight\n",
      "fake quanting vpm.encoder.layers.11.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.51.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.27.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.19.mlp.up_proj.qweight\n",
      "fake quanting vpm.encoder.layers.4.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.41.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.49.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.32.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.44.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.9.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.10.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.37.mlp.down_proj.qweight\n",
      "fake quanting llm.model.layers.43.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.29.mlp.up_proj.qweight\n",
      "fake quanting vpm.encoder.layers.11.mlp.fc1.qweight\n",
      "fake quanting llm.model.layers.8.self_attn.k_proj.qweight\n",
      "fake quanting vpm.encoder.layers.26.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.17.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.44.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.29.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.12.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.2.mlp.down_proj.qweight\n",
      "fake quanting vpm.encoder.layers.6.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.37.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.19.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.3.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.48.self_attn.q_proj.qweight\n",
      "fake quanting llm.model.layers.21.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.25.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.28.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.26.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.12.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.44.self_attn.k_proj.qweight\n",
      "fake quanting llm.model.layers.48.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.14.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.24.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.51.mlp.up_proj.qweight\n",
      "fake quanting llm.model.layers.47.self_attn.o_proj.qweight\n",
      "fake quanting llm.model.layers.15.mlp.gate_proj.qweight\n",
      "fake quanting llm.model.layers.13.self_attn.v_proj.qweight\n",
      "fake quanting llm.model.layers.47.mlp.gate_proj.qweight\n",
      "fake quanting vpm.encoder.layers.2.mlp.fc2.qweight\n",
      "fake quanting llm.model.layers.27.mlp.down_proj.qweight\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from safetensors.torch import load\n",
    "import shutil\n",
    "import torch\n",
    "# for bits in [2,4,8], per-channel quantization\n",
    "def get_fake_weight(bits, qzeros, scales, qweight, group_size):\n",
    "    wf = torch.tensor(list(range(0, 32, bits)), dtype=torch.int32).unsqueeze(0)\n",
    "    zeros = torch.bitwise_right_shift(\n",
    "        torch.unsqueeze(qzeros, 2).expand(-1, -1, 32 // bits),\n",
    "        wf.unsqueeze(0),\n",
    "    ).to(torch.int16 if bits == 8 else torch.int8)\n",
    "    zeros = zeros + 1\n",
    "    zeros = torch.bitwise_and(\n",
    "        zeros, (2**bits) - 1\n",
    "    )  # NOTE: It appears that casting here after the `zeros = zeros + 1` is important.\n",
    "    zeros = zeros.reshape(-1, 1, zeros.shape[1] * zeros.shape[2])\n",
    "    scales = scales\n",
    "    scales = scales.reshape(-1, 1, scales.shape[-1])\n",
    "    weight = torch.bitwise_right_shift(\n",
    "        torch.unsqueeze(qweight, 1).expand(-1, 32 // bits, -1),\n",
    "        wf.unsqueeze(-1),\n",
    "    ).to(torch.int16 if bits == 8 else torch.int8)\n",
    "    weight = torch.bitwise_and(weight, (2**bits) - 1)\n",
    "    weight = weight.reshape(-1, weight.shape[2])\n",
    "    weight = scales * (weight - zeros)\n",
    "    weight = weight.squeeze(0).T\n",
    "    return weight\n",
    "\n",
    "# real_path = '/data/checkpoints/gptq_models/MiniCPM-1B-sft-llama-format-gptq-1028-v2dataset-self-generated-wlmhead-perchannel-desc-true-v3/model.safetensors'\n",
    "# real_path = '/data/zyq/8295/checkpoints/minicpm-3o-sft-v1-gptq-1107/model.safetensors'\n",
    "# real_path = '/data/zyq/8295/checkpoints/minicpm-3o-sft-v1-gptq-1112/model.safetensors'\n",
    "real_path = '/home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256-llm-w4-pc/model.safetensors'\n",
    "# real_path = '/data/checkpoints/gptq_models/MiniCPM-1B-sft-llama-format-gptq-1014-v2dataset-wlmhead-perchannel-desc-true/model.safetensors' # autogptq的int4模型文件路径\n",
    "# real_path = '/data/checkpoints/gptq_models/MiniCPM-1B-sft-llama-format-rotate-smooth-gptq-0925-fix-dataset-wolmhead-perchannel/model.safetensors'\n",
    "# fake_path = '/data/checkpoints/gptq_models/fake_quant/from_autogptq_real_quant/MiniCPM-1B-sft-llama-format' # 存fp16伪量化模型路径\n",
    "# fake_path = '/data/checkpoints/gptq_models/fake_quant/from_autogptq_real_quant/MiniCPM-1B-sft-llama-format-rotate-smooth'\n",
    "fake_path = '/home/workspace/model/MiniCPM-3o-1B-sft-v1-vit-w8-pc-c256-llm-w4-pc-fp'\n",
    "# source_dir = '/mnt/data/user/tc_agi/zhaoyq/minicpm-1b-sft-llama-format' # 另一个fp16模型文件，用来拿其他配置文件，tokenizer等\n",
    "# source_dir = '/data/checkpoints/minicpm-1b-sft-llama-format-rotate-smooth'\n",
    "source_dir = '/data/zyq/8295/checkpoints/fake/minicpm-3o-sft-v1-gptq-1112/'\n",
    "\n",
    "os.makedirs(fake_path, exist_ok=True)\n",
    "with open(real_path, \"rb\") as f:\n",
    "    file = f.read()\n",
    "loaded_data = load(file)\n",
    "result_dict = {}\n",
    "for key, value in loaded_data.items():\n",
    "    # print(\"key: \", key)\n",
    "    # print(value.shape)\n",
    "    common_part = '.'.join(key.split('.')[:-1])  \n",
    "    last_part = key.split('.')[-1]\n",
    "    bits=8 if \"vpm\" in key else 4\n",
    "    if 'qweight' in last_part:\n",
    "        print(f\"fake quanting {key}\")\n",
    "        qweight = loaded_data.get(f\"{common_part}.qweight\")\n",
    "        qzeros = loaded_data.get(f\"{common_part}.qzeros\")\n",
    "        scales = loaded_data.get(f\"{common_part}.scales\")\n",
    "        weight = get_fake_weight(bits, qzeros, scales, qweight, -1)\n",
    "        result_dict[common_part+\".weight\"] = weight\n",
    "    elif \"qzeros\" in last_part or \"scales\" in last_part or \"g_idx\" in last_part:\n",
    "        continue\n",
    "    else:\n",
    "        result_dict[common_part+f\".{last_part}\"] =value\n",
    "torch.save(result_dict, os.path.join(fake_path, \"pytorch_model.bin\"))\n",
    "os.makedirs(fake_path, exist_ok=True)\n",
    "# for root, dirs, files in os.walk(source_dir):\n",
    "#     for file in files:\n",
    "#         if file == 'pytorch_model.bin':\n",
    "#             continue \n",
    "#         source_file = os.path.join(root, file)\n",
    "#         relative_path = os.path.relpath(source_file, source_dir)\n",
    "#         destination_file = os.path.join(fake_path, relative_path)\n",
    "#         os.makedirs(os.path.dirname(destination_file), exist_ok=True)\n",
    "#         shutil.copy2(source_file, destination_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoProcessor\n",
    "pretrained_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1\"\n",
    "model.model.processor = AutoProcessor.from_pretrained(pretrained_model_dir, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MiniCPMVTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这幅图像描绘了一架商用客机，可能是一架喷气式飞机，它正处于起飞或降落过程中。由于缺乏明显的运动模糊，这似乎是一张捕捉到飞机在空中飞行的照片。\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image = Image.open('/home/workspace/code/llm-awq/awq/airplane.jpeg').convert('RGB')\n",
    "\n",
    "# First round chat \n",
    "question = \"这是什么东西\"\n",
    "msgs = [{'role': 'user', 'content': [image, question]}]\n",
    "\n",
    "# res = model.chat(\n",
    "#         image=None,\n",
    "#         msgs=msgs,\n",
    "#         tokenizer=tokenizer,\n",
    "#         # max_new_tokens=max_new_tokens,\n",
    "#         sampling=False,\n",
    "#         num_beams=1,\n",
    "#         repetition_penalty=1,\n",
    "# )\n",
    "res = model.model.chat(\n",
    "    image=None,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You're using a MiniCPMVTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoProcessor\n",
    "pretrained_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1\"\n",
    "traindataset = get_ScienceQA(16, 0, 1024, AutoProcessor.from_pretrained(pretrained_model_dir, trust_remote_code=True),0)\n",
    "len(traindataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 14112])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindataset[0][\"pixel_values\"][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in traindataset:\n",
    "    example[\"input_ids\"] = example[\"input_ids\"].cuda()\n",
    "    example[\"attention_mask\"] = example[\"attention_mask\"].cuda()\n",
    "    example[\"pixel_values\"][0][0] = example[\"pixel_values\"][0][0].cuda()\n",
    "    model.model(example)\n",
    "    # model(example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
