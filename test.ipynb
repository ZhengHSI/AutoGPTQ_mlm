{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 纯文本模型量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lmquant/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [02:09<00:00, 32.34s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "pretrained_model_dir = \"/home/workspace/model/meta-llama-3-8b-instruct\"\n",
    "quantized_model_dir = \"/home/workspace/model/meta-llama-3-8b-instruct-w4-g128\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\n",
    "examples = [\n",
    "    tokenizer(\n",
    "        \"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,  # quantize model to 4-bit\n",
    "    group_size=128,  # it is recommended to set the value to 128\n",
    "    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    ")\n",
    "\n",
    "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/32...\n",
      "2024-10-30 08:14:41 INFO [auto_gptq.quantization.gptq] duration: 10.037084102630615\n",
      "2024-10-30 08:14:41 INFO [auto_gptq.quantization.gptq] avg loss: 2.1992526054382324\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/32...\n",
      "2024-10-30 08:14:48 INFO [auto_gptq.quantization.gptq] duration: 7.339477300643921\n",
      "2024-10-30 08:14:48 INFO [auto_gptq.quantization.gptq] avg loss: 0.03380981832742691\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/32...\n",
      "2024-10-30 08:14:58 INFO [auto_gptq.quantization.gptq] duration: 9.718609094619751\n",
      "2024-10-30 08:14:58 INFO [auto_gptq.quantization.gptq] avg loss: 3.3699684143066406\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/32...\n",
      "2024-10-30 08:15:08 INFO [auto_gptq.quantization.gptq] duration: 10.15040373802185\n",
      "2024-10-30 08:15:08 INFO [auto_gptq.quantization.gptq] avg loss: 0.0003416052204556763\n",
      "INFO - Quantizing mlp.up_proj in layer 1/32...\n",
      "2024-10-30 08:15:19 INFO [auto_gptq.quantization.gptq] duration: 10.828808546066284\n",
      "2024-10-30 08:15:19 INFO [auto_gptq.quantization.gptq] avg loss: 0.3819321095943451\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/32...\n",
      "2024-10-30 08:15:28 INFO [auto_gptq.quantization.gptq] duration: 9.118736743927002\n",
      "2024-10-30 08:15:28 INFO [auto_gptq.quantization.gptq] avg loss: 0.4896368682384491\n",
      "INFO - Quantizing mlp.down_proj in layer 1/32...\n",
      "2024-10-30 08:16:02 INFO [auto_gptq.quantization.gptq] duration: 34.22894525527954\n",
      "2024-10-30 08:16:02 INFO [auto_gptq.quantization.gptq] avg loss: 0.004767216742038727\n",
      "INFO - Start quantizing layer 2/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/32...\n",
      "2024-10-30 08:16:11 INFO [auto_gptq.quantization.gptq] duration: 8.335230112075806\n",
      "2024-10-30 08:16:11 INFO [auto_gptq.quantization.gptq] avg loss: 1.1905415058135986\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/32...\n",
      "2024-10-30 08:16:22 INFO [auto_gptq.quantization.gptq] duration: 10.828464031219482\n",
      "2024-10-30 08:16:22 INFO [auto_gptq.quantization.gptq] avg loss: 0.06976363062858582\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/32...\n",
      "2024-10-30 08:16:25 INFO [auto_gptq.quantization.gptq] duration: 3.3788704872131348\n",
      "2024-10-30 08:16:25 INFO [auto_gptq.quantization.gptq] avg loss: 2.039285182952881\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/32...\n",
      "2024-10-30 08:16:31 INFO [auto_gptq.quantization.gptq] duration: 5.865533113479614\n",
      "2024-10-30 08:16:31 INFO [auto_gptq.quantization.gptq] avg loss: 0.0007455475861206651\n",
      "INFO - Quantizing mlp.up_proj in layer 2/32...\n",
      "2024-10-30 08:16:41 INFO [auto_gptq.quantization.gptq] duration: 9.759242057800293\n",
      "2024-10-30 08:16:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.7103757858276367\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/32...\n",
      "2024-10-30 08:16:50 INFO [auto_gptq.quantization.gptq] duration: 9.233519554138184\n",
      "2024-10-30 08:16:50 INFO [auto_gptq.quantization.gptq] avg loss: 0.8590945601463318\n",
      "INFO - Quantizing mlp.down_proj in layer 2/32...\n",
      "2024-10-30 08:17:22 INFO [auto_gptq.quantization.gptq] duration: 31.486181497573853\n",
      "2024-10-30 08:17:22 INFO [auto_gptq.quantization.gptq] avg loss: 16.221515655517578\n",
      "INFO - Start quantizing layer 3/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/32...\n",
      "2024-10-30 08:17:31 INFO [auto_gptq.quantization.gptq] duration: 8.975847244262695\n",
      "2024-10-30 08:17:31 INFO [auto_gptq.quantization.gptq] avg loss: 3.683974027633667\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/32...\n",
      "2024-10-30 08:17:38 INFO [auto_gptq.quantization.gptq] duration: 7.537810802459717\n",
      "2024-10-30 08:17:38 INFO [auto_gptq.quantization.gptq] avg loss: 0.17160382866859436\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/32...\n",
      "2024-10-30 08:17:47 INFO [auto_gptq.quantization.gptq] duration: 8.988044738769531\n",
      "2024-10-30 08:17:47 INFO [auto_gptq.quantization.gptq] avg loss: 6.08571720123291\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/32...\n",
      "2024-10-30 08:17:56 INFO [auto_gptq.quantization.gptq] duration: 8.53365683555603\n",
      "2024-10-30 08:17:56 INFO [auto_gptq.quantization.gptq] avg loss: 0.0006117637967690825\n",
      "INFO - Quantizing mlp.up_proj in layer 3/32...\n",
      "2024-10-30 08:18:03 INFO [auto_gptq.quantization.gptq] duration: 7.475140571594238\n",
      "2024-10-30 08:18:03 INFO [auto_gptq.quantization.gptq] avg loss: 1.6254947185516357\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/32...\n",
      "2024-10-30 08:18:12 INFO [auto_gptq.quantization.gptq] duration: 8.30447268486023\n",
      "2024-10-30 08:18:12 INFO [auto_gptq.quantization.gptq] avg loss: 2.1320695877075195\n",
      "INFO - Quantizing mlp.down_proj in layer 3/32...\n",
      "2024-10-30 08:18:40 INFO [auto_gptq.quantization.gptq] duration: 28.691283464431763\n",
      "2024-10-30 08:18:40 INFO [auto_gptq.quantization.gptq] avg loss: 0.00424563605338335\n",
      "INFO - Start quantizing layer 4/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/32...\n",
      "2024-10-30 08:18:49 INFO [auto_gptq.quantization.gptq] duration: 8.147428274154663\n",
      "2024-10-30 08:18:49 INFO [auto_gptq.quantization.gptq] avg loss: 1.921912431716919\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/32...\n",
      "2024-10-30 08:18:57 INFO [auto_gptq.quantization.gptq] duration: 8.485686302185059\n",
      "2024-10-30 08:18:57 INFO [auto_gptq.quantization.gptq] avg loss: 0.15430966019630432\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/32...\n",
      "2024-10-30 08:19:05 INFO [auto_gptq.quantization.gptq] duration: 7.6191229820251465\n",
      "2024-10-30 08:19:05 INFO [auto_gptq.quantization.gptq] avg loss: 3.3717758655548096\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/32...\n",
      "2024-10-30 08:19:13 INFO [auto_gptq.quantization.gptq] duration: 8.217707633972168\n",
      "2024-10-30 08:19:13 INFO [auto_gptq.quantization.gptq] avg loss: 0.0018652137368917465\n",
      "INFO - Quantizing mlp.up_proj in layer 4/32...\n",
      "2024-10-30 08:19:22 INFO [auto_gptq.quantization.gptq] duration: 8.477033853530884\n",
      "2024-10-30 08:19:22 INFO [auto_gptq.quantization.gptq] avg loss: 2.122600793838501\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/32...\n",
      "2024-10-30 08:19:30 INFO [auto_gptq.quantization.gptq] duration: 8.34442400932312\n",
      "2024-10-30 08:19:30 INFO [auto_gptq.quantization.gptq] avg loss: 3.2897701263427734\n",
      "INFO - Quantizing mlp.down_proj in layer 4/32...\n",
      "2024-10-30 08:20:00 INFO [auto_gptq.quantization.gptq] duration: 29.632654428482056\n",
      "2024-10-30 08:20:00 INFO [auto_gptq.quantization.gptq] avg loss: 0.007349357940256596\n",
      "INFO - Start quantizing layer 5/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/32...\n",
      "2024-10-30 08:20:08 INFO [auto_gptq.quantization.gptq] duration: 8.474597692489624\n",
      "2024-10-30 08:20:08 INFO [auto_gptq.quantization.gptq] avg loss: 1.9397143125534058\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/32...\n",
      "2024-10-30 08:20:15 INFO [auto_gptq.quantization.gptq] duration: 7.060450553894043\n",
      "2024-10-30 08:20:15 INFO [auto_gptq.quantization.gptq] avg loss: 0.16960358619689941\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/32...\n",
      "2024-10-30 08:20:24 INFO [auto_gptq.quantization.gptq] duration: 8.526679992675781\n",
      "2024-10-30 08:20:24 INFO [auto_gptq.quantization.gptq] avg loss: 3.2601706981658936\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/32...\n",
      "2024-10-30 08:20:32 INFO [auto_gptq.quantization.gptq] duration: 8.336119174957275\n",
      "2024-10-30 08:20:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.003427261020988226\n",
      "INFO - Quantizing mlp.up_proj in layer 5/32...\n",
      "2024-10-30 08:20:41 INFO [auto_gptq.quantization.gptq] duration: 8.90831971168518\n",
      "2024-10-30 08:20:41 INFO [auto_gptq.quantization.gptq] avg loss: 2.5386962890625\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/32...\n",
      "2024-10-30 08:20:50 INFO [auto_gptq.quantization.gptq] duration: 8.380399703979492\n",
      "2024-10-30 08:20:50 INFO [auto_gptq.quantization.gptq] avg loss: 4.482138633728027\n",
      "INFO - Quantizing mlp.down_proj in layer 5/32...\n",
      "2024-10-30 08:21:19 INFO [auto_gptq.quantization.gptq] duration: 29.324504375457764\n",
      "2024-10-30 08:21:19 INFO [auto_gptq.quantization.gptq] avg loss: 0.012558701448142529\n",
      "INFO - Start quantizing layer 6/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/32...\n",
      "2024-10-30 08:21:27 INFO [auto_gptq.quantization.gptq] duration: 7.340981721878052\n",
      "2024-10-30 08:21:27 INFO [auto_gptq.quantization.gptq] avg loss: 2.871307611465454\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/32...\n",
      "2024-10-30 08:21:35 INFO [auto_gptq.quantization.gptq] duration: 8.701762437820435\n",
      "2024-10-30 08:21:35 INFO [auto_gptq.quantization.gptq] avg loss: 0.16306540369987488\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/32...\n",
      "2024-10-30 08:21:43 INFO [auto_gptq.quantization.gptq] duration: 8.091735363006592\n",
      "2024-10-30 08:21:43 INFO [auto_gptq.quantization.gptq] avg loss: 4.574190616607666\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/32...\n",
      "2024-10-30 08:21:51 INFO [auto_gptq.quantization.gptq] duration: 7.8829026222229\n",
      "2024-10-30 08:21:51 INFO [auto_gptq.quantization.gptq] avg loss: 0.004759425297379494\n",
      "INFO - Quantizing mlp.up_proj in layer 6/32...\n",
      "2024-10-30 08:22:01 INFO [auto_gptq.quantization.gptq] duration: 9.728351354598999\n",
      "2024-10-30 08:22:01 INFO [auto_gptq.quantization.gptq] avg loss: 2.935046434402466\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/32...\n",
      "2024-10-30 08:22:10 INFO [auto_gptq.quantization.gptq] duration: 9.049590110778809\n",
      "2024-10-30 08:22:10 INFO [auto_gptq.quantization.gptq] avg loss: 5.208375453948975\n",
      "INFO - Quantizing mlp.down_proj in layer 6/32...\n",
      "2024-10-30 08:22:38 INFO [auto_gptq.quantization.gptq] duration: 27.521717071533203\n",
      "2024-10-30 08:22:38 INFO [auto_gptq.quantization.gptq] avg loss: 0.01708236336708069\n",
      "INFO - Start quantizing layer 7/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/32...\n",
      "2024-10-30 08:22:47 INFO [auto_gptq.quantization.gptq] duration: 9.297240018844604\n",
      "2024-10-30 08:22:47 INFO [auto_gptq.quantization.gptq] avg loss: 2.17334246635437\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/32...\n",
      "2024-10-30 08:22:56 INFO [auto_gptq.quantization.gptq] duration: 8.46350622177124\n",
      "2024-10-30 08:22:56 INFO [auto_gptq.quantization.gptq] avg loss: 0.16762012243270874\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/32...\n",
      "2024-10-30 08:22:58 INFO [auto_gptq.quantization.gptq] duration: 2.3084716796875\n",
      "2024-10-30 08:22:58 INFO [auto_gptq.quantization.gptq] avg loss: 3.716184616088867\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/32...\n",
      "2024-10-30 08:22:59 INFO [auto_gptq.quantization.gptq] duration: 0.7515566349029541\n",
      "2024-10-30 08:22:59 INFO [auto_gptq.quantization.gptq] avg loss: 0.00757574662566185\n",
      "INFO - Quantizing mlp.up_proj in layer 7/32...\n",
      "2024-10-30 08:23:00 INFO [auto_gptq.quantization.gptq] duration: 0.7646174430847168\n",
      "2024-10-30 08:23:00 INFO [auto_gptq.quantization.gptq] avg loss: 3.3043696880340576\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/32...\n",
      "2024-10-30 08:23:00 INFO [auto_gptq.quantization.gptq] duration: 0.7752165794372559\n",
      "2024-10-30 08:23:00 INFO [auto_gptq.quantization.gptq] avg loss: 5.949164390563965\n",
      "INFO - Quantizing mlp.down_proj in layer 7/32...\n",
      "2024-10-30 08:23:03 INFO [auto_gptq.quantization.gptq] duration: 3.0168724060058594\n",
      "2024-10-30 08:23:03 INFO [auto_gptq.quantization.gptq] avg loss: 0.019619928672909737\n",
      "INFO - Start quantizing layer 8/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/32...\n",
      "2024-10-30 08:23:05 INFO [auto_gptq.quantization.gptq] duration: 1.0251798629760742\n",
      "2024-10-30 08:23:05 INFO [auto_gptq.quantization.gptq] avg loss: 2.1070549488067627\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/32...\n",
      "2024-10-30 08:23:06 INFO [auto_gptq.quantization.gptq] duration: 1.0046427249908447\n",
      "2024-10-30 08:23:06 INFO [auto_gptq.quantization.gptq] avg loss: 0.1636628806591034\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/32...\n",
      "2024-10-30 08:23:07 INFO [auto_gptq.quantization.gptq] duration: 1.036513328552246\n",
      "2024-10-30 08:23:07 INFO [auto_gptq.quantization.gptq] avg loss: 3.366272449493408\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/32...\n",
      "2024-10-30 08:23:08 INFO [auto_gptq.quantization.gptq] duration: 1.0406818389892578\n",
      "2024-10-30 08:23:08 INFO [auto_gptq.quantization.gptq] avg loss: 0.012991133145987988\n",
      "INFO - Quantizing mlp.up_proj in layer 8/32...\n",
      "2024-10-30 08:23:09 INFO [auto_gptq.quantization.gptq] duration: 0.8265926837921143\n",
      "2024-10-30 08:23:09 INFO [auto_gptq.quantization.gptq] avg loss: 3.3469395637512207\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/32...\n",
      "2024-10-30 08:23:09 INFO [auto_gptq.quantization.gptq] duration: 0.6927039623260498\n",
      "2024-10-30 08:23:09 INFO [auto_gptq.quantization.gptq] avg loss: 5.6031951904296875\n",
      "INFO - Quantizing mlp.down_proj in layer 8/32...\n",
      "2024-10-30 08:23:13 INFO [auto_gptq.quantization.gptq] duration: 3.4401893615722656\n",
      "2024-10-30 08:23:13 INFO [auto_gptq.quantization.gptq] avg loss: 0.022065721452236176\n",
      "INFO - Start quantizing layer 9/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/32...\n",
      "2024-10-30 08:23:14 INFO [auto_gptq.quantization.gptq] duration: 1.00180983543396\n",
      "2024-10-30 08:23:14 INFO [auto_gptq.quantization.gptq] avg loss: 2.7274293899536133\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/32...\n",
      "2024-10-30 08:23:15 INFO [auto_gptq.quantization.gptq] duration: 0.6552410125732422\n",
      "2024-10-30 08:23:15 INFO [auto_gptq.quantization.gptq] avg loss: 0.21766620874404907\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/32...\n",
      "2024-10-30 08:23:15 INFO [auto_gptq.quantization.gptq] duration: 0.6730568408966064\n",
      "2024-10-30 08:23:15 INFO [auto_gptq.quantization.gptq] avg loss: 4.318254470825195\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/32...\n",
      "2024-10-30 08:23:16 INFO [auto_gptq.quantization.gptq] duration: 0.669461727142334\n",
      "2024-10-30 08:23:16 INFO [auto_gptq.quantization.gptq] avg loss: 0.01973799802362919\n",
      "INFO - Quantizing mlp.up_proj in layer 9/32...\n",
      "2024-10-30 08:23:17 INFO [auto_gptq.quantization.gptq] duration: 0.6900115013122559\n",
      "2024-10-30 08:23:17 INFO [auto_gptq.quantization.gptq] avg loss: 3.496938467025757\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/32...\n",
      "2024-10-30 08:23:17 INFO [auto_gptq.quantization.gptq] duration: 0.6921286582946777\n",
      "2024-10-30 08:23:17 INFO [auto_gptq.quantization.gptq] avg loss: 5.990079402923584\n",
      "INFO - Quantizing mlp.down_proj in layer 9/32...\n",
      "2024-10-30 08:23:20 INFO [auto_gptq.quantization.gptq] duration: 2.8321006298065186\n",
      "2024-10-30 08:23:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.02571762353181839\n",
      "INFO - Start quantizing layer 10/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/32...\n",
      "2024-10-30 08:23:21 INFO [auto_gptq.quantization.gptq] duration: 0.6327075958251953\n",
      "2024-10-30 08:23:21 INFO [auto_gptq.quantization.gptq] avg loss: 2.4174017906188965\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/32...\n",
      "2024-10-30 08:23:22 INFO [auto_gptq.quantization.gptq] duration: 0.6408584117889404\n",
      "2024-10-30 08:23:22 INFO [auto_gptq.quantization.gptq] avg loss: 0.27908700704574585\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/32...\n",
      "2024-10-30 08:23:22 INFO [auto_gptq.quantization.gptq] duration: 0.6703073978424072\n",
      "2024-10-30 08:23:22 INFO [auto_gptq.quantization.gptq] avg loss: 3.8653459548950195\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/32...\n",
      "2024-10-30 08:23:23 INFO [auto_gptq.quantization.gptq] duration: 0.6694848537445068\n",
      "2024-10-30 08:23:23 INFO [auto_gptq.quantization.gptq] avg loss: 0.01904677227139473\n",
      "INFO - Quantizing mlp.up_proj in layer 10/32...\n",
      "2024-10-30 08:23:24 INFO [auto_gptq.quantization.gptq] duration: 0.689176082611084\n",
      "2024-10-30 08:23:24 INFO [auto_gptq.quantization.gptq] avg loss: 3.709043025970459\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/32...\n",
      "2024-10-30 08:23:24 INFO [auto_gptq.quantization.gptq] duration: 0.6884393692016602\n",
      "2024-10-30 08:23:24 INFO [auto_gptq.quantization.gptq] avg loss: 6.343160152435303\n",
      "INFO - Quantizing mlp.down_proj in layer 10/32...\n",
      "2024-10-30 08:23:27 INFO [auto_gptq.quantization.gptq] duration: 2.8250367641448975\n",
      "2024-10-30 08:23:27 INFO [auto_gptq.quantization.gptq] avg loss: 0.02898033708333969\n",
      "INFO - Start quantizing layer 11/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/32...\n",
      "2024-10-30 08:23:29 INFO [auto_gptq.quantization.gptq] duration: 0.9903228282928467\n",
      "2024-10-30 08:23:29 INFO [auto_gptq.quantization.gptq] avg loss: 3.3559765815734863\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/32...\n",
      "2024-10-30 08:23:30 INFO [auto_gptq.quantization.gptq] duration: 1.0269114971160889\n",
      "2024-10-30 08:23:30 INFO [auto_gptq.quantization.gptq] avg loss: 0.24244371056556702\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/32...\n",
      "2024-10-30 08:23:31 INFO [auto_gptq.quantization.gptq] duration: 0.9965977668762207\n",
      "2024-10-30 08:23:31 INFO [auto_gptq.quantization.gptq] avg loss: 5.141538619995117\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/32...\n",
      "2024-10-30 08:23:31 INFO [auto_gptq.quantization.gptq] duration: 0.769179105758667\n",
      "2024-10-30 08:23:31 INFO [auto_gptq.quantization.gptq] avg loss: 0.019364133477211\n",
      "INFO - Quantizing mlp.up_proj in layer 11/32...\n",
      "2024-10-30 08:23:32 INFO [auto_gptq.quantization.gptq] duration: 0.6824836730957031\n",
      "2024-10-30 08:23:32 INFO [auto_gptq.quantization.gptq] avg loss: 3.2441835403442383\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/32...\n",
      "2024-10-30 08:23:33 INFO [auto_gptq.quantization.gptq] duration: 0.6846058368682861\n",
      "2024-10-30 08:23:33 INFO [auto_gptq.quantization.gptq] avg loss: 5.129234313964844\n",
      "INFO - Quantizing mlp.down_proj in layer 11/32...\n",
      "2024-10-30 08:23:36 INFO [auto_gptq.quantization.gptq] duration: 2.824298143386841\n",
      "2024-10-30 08:23:36 INFO [auto_gptq.quantization.gptq] avg loss: 0.22699344158172607\n",
      "INFO - Start quantizing layer 12/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/32...\n",
      "2024-10-30 08:23:36 INFO [auto_gptq.quantization.gptq] duration: 0.6340494155883789\n",
      "2024-10-30 08:23:36 INFO [auto_gptq.quantization.gptq] avg loss: 2.790926694869995\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/32...\n",
      "2024-10-30 08:23:37 INFO [auto_gptq.quantization.gptq] duration: 0.6346836090087891\n",
      "2024-10-30 08:23:37 INFO [auto_gptq.quantization.gptq] avg loss: 0.2277548462152481\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/32...\n",
      "2024-10-30 08:23:38 INFO [auto_gptq.quantization.gptq] duration: 0.6625807285308838\n",
      "2024-10-30 08:23:38 INFO [auto_gptq.quantization.gptq] avg loss: 4.091871738433838\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/32...\n",
      "2024-10-30 08:23:38 INFO [auto_gptq.quantization.gptq] duration: 0.6640803813934326\n",
      "2024-10-30 08:23:38 INFO [auto_gptq.quantization.gptq] avg loss: 0.023841027170419693\n",
      "INFO - Quantizing mlp.up_proj in layer 12/32...\n",
      "2024-10-30 08:23:39 INFO [auto_gptq.quantization.gptq] duration: 0.6851658821105957\n",
      "2024-10-30 08:23:39 INFO [auto_gptq.quantization.gptq] avg loss: 3.7377548217773438\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/32...\n",
      "2024-10-30 08:23:40 INFO [auto_gptq.quantization.gptq] duration: 0.6864769458770752\n",
      "2024-10-30 08:23:40 INFO [auto_gptq.quantization.gptq] avg loss: 5.753993511199951\n",
      "INFO - Quantizing mlp.down_proj in layer 12/32...\n",
      "2024-10-30 08:23:43 INFO [auto_gptq.quantization.gptq] duration: 2.8230326175689697\n",
      "2024-10-30 08:23:43 INFO [auto_gptq.quantization.gptq] avg loss: 0.02707657963037491\n",
      "INFO - Start quantizing layer 13/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/32...\n",
      "2024-10-30 08:23:44 INFO [auto_gptq.quantization.gptq] duration: 0.6381351947784424\n",
      "2024-10-30 08:23:44 INFO [auto_gptq.quantization.gptq] avg loss: 2.15982985496521\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/32...\n",
      "2024-10-30 08:23:44 INFO [auto_gptq.quantization.gptq] duration: 0.9032399654388428\n",
      "2024-10-30 08:23:44 INFO [auto_gptq.quantization.gptq] avg loss: 0.2686922550201416\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/32...\n",
      "2024-10-30 08:23:45 INFO [auto_gptq.quantization.gptq] duration: 1.0056190490722656\n",
      "2024-10-30 08:23:45 INFO [auto_gptq.quantization.gptq] avg loss: 3.6072685718536377\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/32...\n",
      "2024-10-30 08:23:46 INFO [auto_gptq.quantization.gptq] duration: 0.6904573440551758\n",
      "2024-10-30 08:23:46 INFO [auto_gptq.quantization.gptq] avg loss: 0.02680094540119171\n",
      "INFO - Quantizing mlp.up_proj in layer 13/32...\n",
      "2024-10-30 08:23:47 INFO [auto_gptq.quantization.gptq] duration: 0.6867432594299316\n",
      "2024-10-30 08:23:47 INFO [auto_gptq.quantization.gptq] avg loss: 4.1996073722839355\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/32...\n",
      "2024-10-30 08:23:48 INFO [auto_gptq.quantization.gptq] duration: 0.6889848709106445\n",
      "2024-10-30 08:23:48 INFO [auto_gptq.quantization.gptq] avg loss: 6.219916343688965\n",
      "INFO - Quantizing mlp.down_proj in layer 13/32...\n",
      "2024-10-30 08:23:50 INFO [auto_gptq.quantization.gptq] duration: 2.817117929458618\n",
      "2024-10-30 08:23:50 INFO [auto_gptq.quantization.gptq] avg loss: 0.031323887407779694\n",
      "INFO - Start quantizing layer 14/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/32...\n",
      "2024-10-30 08:23:51 INFO [auto_gptq.quantization.gptq] duration: 0.6316406726837158\n",
      "2024-10-30 08:23:51 INFO [auto_gptq.quantization.gptq] avg loss: 3.045475721359253\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/32...\n",
      "2024-10-30 08:23:52 INFO [auto_gptq.quantization.gptq] duration: 0.6345148086547852\n",
      "2024-10-30 08:23:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.2771906852722168\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/32...\n",
      "2024-10-30 08:23:53 INFO [auto_gptq.quantization.gptq] duration: 0.6674294471740723\n",
      "2024-10-30 08:23:53 INFO [auto_gptq.quantization.gptq] avg loss: 4.456707954406738\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/32...\n",
      "2024-10-30 08:23:53 INFO [auto_gptq.quantization.gptq] duration: 0.6695916652679443\n",
      "2024-10-30 08:23:53 INFO [auto_gptq.quantization.gptq] avg loss: 0.02957991324365139\n",
      "INFO - Quantizing mlp.up_proj in layer 14/32...\n",
      "2024-10-30 08:23:54 INFO [auto_gptq.quantization.gptq] duration: 0.6873900890350342\n",
      "2024-10-30 08:23:54 INFO [auto_gptq.quantization.gptq] avg loss: 4.3941264152526855\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/32...\n",
      "2024-10-30 08:23:55 INFO [auto_gptq.quantization.gptq] duration: 1.0173759460449219\n",
      "2024-10-30 08:23:55 INFO [auto_gptq.quantization.gptq] avg loss: 6.605837345123291\n",
      "INFO - Quantizing mlp.down_proj in layer 14/32...\n",
      "2024-10-30 08:23:58 INFO [auto_gptq.quantization.gptq] duration: 2.8982298374176025\n",
      "2024-10-30 08:23:58 INFO [auto_gptq.quantization.gptq] avg loss: 0.03599254786968231\n",
      "INFO - Start quantizing layer 15/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/32...\n",
      "2024-10-30 08:23:59 INFO [auto_gptq.quantization.gptq] duration: 0.6431002616882324\n",
      "2024-10-30 08:23:59 INFO [auto_gptq.quantization.gptq] avg loss: 3.7889716625213623\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/32...\n",
      "2024-10-30 08:23:59 INFO [auto_gptq.quantization.gptq] duration: 0.6428864002227783\n",
      "2024-10-30 08:23:59 INFO [auto_gptq.quantization.gptq] avg loss: 0.3350851833820343\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/32...\n",
      "2024-10-30 08:24:00 INFO [auto_gptq.quantization.gptq] duration: 0.6642353534698486\n",
      "2024-10-30 08:24:00 INFO [auto_gptq.quantization.gptq] avg loss: 5.321682929992676\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/32...\n",
      "2024-10-30 08:24:01 INFO [auto_gptq.quantization.gptq] duration: 0.6700839996337891\n",
      "2024-10-30 08:24:01 INFO [auto_gptq.quantization.gptq] avg loss: 0.030687108635902405\n",
      "INFO - Quantizing mlp.up_proj in layer 15/32...\n",
      "2024-10-30 08:24:01 INFO [auto_gptq.quantization.gptq] duration: 0.6833271980285645\n",
      "2024-10-30 08:24:01 INFO [auto_gptq.quantization.gptq] avg loss: 5.0542497634887695\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/32...\n",
      "2024-10-30 08:24:02 INFO [auto_gptq.quantization.gptq] duration: 0.68373703956604\n",
      "2024-10-30 08:24:02 INFO [auto_gptq.quantization.gptq] avg loss: 8.230281829833984\n",
      "INFO - Quantizing mlp.down_proj in layer 15/32...\n",
      "2024-10-30 08:24:05 INFO [auto_gptq.quantization.gptq] duration: 2.8208367824554443\n",
      "2024-10-30 08:24:05 INFO [auto_gptq.quantization.gptq] avg loss: 0.04462345689535141\n",
      "INFO - Start quantizing layer 16/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/32...\n",
      "2024-10-30 08:24:06 INFO [auto_gptq.quantization.gptq] duration: 0.638861894607544\n",
      "2024-10-30 08:24:06 INFO [auto_gptq.quantization.gptq] avg loss: 3.027815818786621\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/32...\n",
      "2024-10-30 08:24:06 INFO [auto_gptq.quantization.gptq] duration: 0.6427297592163086\n",
      "2024-10-30 08:24:06 INFO [auto_gptq.quantization.gptq] avg loss: 0.3707875609397888\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/32...\n",
      "2024-10-30 08:24:07 INFO [auto_gptq.quantization.gptq] duration: 0.6638426780700684\n",
      "2024-10-30 08:24:07 INFO [auto_gptq.quantization.gptq] avg loss: 5.551547050476074\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/32...\n",
      "2024-10-30 08:24:08 INFO [auto_gptq.quantization.gptq] duration: 0.6663141250610352\n",
      "2024-10-30 08:24:08 INFO [auto_gptq.quantization.gptq] avg loss: 0.02362232469022274\n",
      "INFO - Quantizing mlp.up_proj in layer 16/32...\n",
      "2024-10-30 08:24:08 INFO [auto_gptq.quantization.gptq] duration: 0.683795690536499\n",
      "2024-10-30 08:24:08 INFO [auto_gptq.quantization.gptq] avg loss: 4.809642314910889\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/32...\n",
      "2024-10-30 08:24:09 INFO [auto_gptq.quantization.gptq] duration: 0.6837301254272461\n",
      "2024-10-30 08:24:09 INFO [auto_gptq.quantization.gptq] avg loss: 8.196983337402344\n",
      "INFO - Quantizing mlp.down_proj in layer 16/32...\n",
      "2024-10-30 08:24:12 INFO [auto_gptq.quantization.gptq] duration: 2.8177568912506104\n",
      "2024-10-30 08:24:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.05468032509088516\n",
      "INFO - Start quantizing layer 17/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/32...\n",
      "2024-10-30 08:24:13 INFO [auto_gptq.quantization.gptq] duration: 0.6448872089385986\n",
      "2024-10-30 08:24:13 INFO [auto_gptq.quantization.gptq] avg loss: 3.1279354095458984\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/32...\n",
      "2024-10-30 08:24:13 INFO [auto_gptq.quantization.gptq] duration: 0.6390290260314941\n",
      "2024-10-30 08:24:13 INFO [auto_gptq.quantization.gptq] avg loss: 0.3542017936706543\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/32...\n",
      "2024-10-30 08:24:14 INFO [auto_gptq.quantization.gptq] duration: 0.6629433631896973\n",
      "2024-10-30 08:24:14 INFO [auto_gptq.quantization.gptq] avg loss: 5.378959655761719\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/32...\n",
      "2024-10-30 08:24:15 INFO [auto_gptq.quantization.gptq] duration: 0.6657412052154541\n",
      "2024-10-30 08:24:15 INFO [auto_gptq.quantization.gptq] avg loss: 0.02935163490474224\n",
      "INFO - Quantizing mlp.up_proj in layer 17/32...\n",
      "2024-10-30 08:24:16 INFO [auto_gptq.quantization.gptq] duration: 0.6856932640075684\n",
      "2024-10-30 08:24:16 INFO [auto_gptq.quantization.gptq] avg loss: 4.608745098114014\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/32...\n",
      "2024-10-30 08:24:16 INFO [auto_gptq.quantization.gptq] duration: 0.6827833652496338\n",
      "2024-10-30 08:24:16 INFO [auto_gptq.quantization.gptq] avg loss: 8.244406700134277\n",
      "INFO - Quantizing mlp.down_proj in layer 17/32...\n",
      "2024-10-30 08:24:19 INFO [auto_gptq.quantization.gptq] duration: 2.821237087249756\n",
      "2024-10-30 08:24:19 INFO [auto_gptq.quantization.gptq] avg loss: 0.06366308033466339\n",
      "INFO - Start quantizing layer 18/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/32...\n",
      "2024-10-30 08:24:20 INFO [auto_gptq.quantization.gptq] duration: 0.6371912956237793\n",
      "2024-10-30 08:24:20 INFO [auto_gptq.quantization.gptq] avg loss: 3.0500950813293457\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/32...\n",
      "2024-10-30 08:24:21 INFO [auto_gptq.quantization.gptq] duration: 0.6385703086853027\n",
      "2024-10-30 08:24:21 INFO [auto_gptq.quantization.gptq] avg loss: 0.3493248224258423\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/32...\n",
      "2024-10-30 08:24:21 INFO [auto_gptq.quantization.gptq] duration: 0.6648209095001221\n",
      "2024-10-30 08:24:21 INFO [auto_gptq.quantization.gptq] avg loss: 5.062129497528076\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/32...\n",
      "2024-10-30 08:24:22 INFO [auto_gptq.quantization.gptq] duration: 1.0235822200775146\n",
      "2024-10-30 08:24:22 INFO [auto_gptq.quantization.gptq] avg loss: 0.01996644213795662\n",
      "INFO - Quantizing mlp.up_proj in layer 18/32...\n",
      "2024-10-30 08:24:23 INFO [auto_gptq.quantization.gptq] duration: 1.0249526500701904\n",
      "2024-10-30 08:24:23 INFO [auto_gptq.quantization.gptq] avg loss: 4.525355339050293\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/32...\n",
      "2024-10-30 08:24:24 INFO [auto_gptq.quantization.gptq] duration: 0.9934630393981934\n",
      "2024-10-30 08:24:24 INFO [auto_gptq.quantization.gptq] avg loss: 8.311567306518555\n",
      "INFO - Quantizing mlp.down_proj in layer 18/32...\n",
      "2024-10-30 08:24:27 INFO [auto_gptq.quantization.gptq] duration: 3.0210278034210205\n",
      "2024-10-30 08:24:27 INFO [auto_gptq.quantization.gptq] avg loss: 0.08415672183036804\n",
      "INFO - Start quantizing layer 19/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/32...\n",
      "2024-10-30 08:24:28 INFO [auto_gptq.quantization.gptq] duration: 0.637204647064209\n",
      "2024-10-30 08:24:28 INFO [auto_gptq.quantization.gptq] avg loss: 3.4031903743743896\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/32...\n",
      "2024-10-30 08:24:29 INFO [auto_gptq.quantization.gptq] duration: 0.630835771560669\n",
      "2024-10-30 08:24:29 INFO [auto_gptq.quantization.gptq] avg loss: 0.3560396134853363\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/32...\n",
      "2024-10-30 08:24:29 INFO [auto_gptq.quantization.gptq] duration: 0.6643140316009521\n",
      "2024-10-30 08:24:29 INFO [auto_gptq.quantization.gptq] avg loss: 5.068970680236816\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/32...\n",
      "2024-10-30 08:24:30 INFO [auto_gptq.quantization.gptq] duration: 0.6656832695007324\n",
      "2024-10-30 08:24:30 INFO [auto_gptq.quantization.gptq] avg loss: 0.01162052620202303\n",
      "INFO - Quantizing mlp.up_proj in layer 19/32...\n",
      "2024-10-30 08:24:31 INFO [auto_gptq.quantization.gptq] duration: 0.6825823783874512\n",
      "2024-10-30 08:24:31 INFO [auto_gptq.quantization.gptq] avg loss: 4.310252666473389\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/32...\n",
      "2024-10-30 08:24:32 INFO [auto_gptq.quantization.gptq] duration: 0.6825344562530518\n",
      "2024-10-30 08:24:32 INFO [auto_gptq.quantization.gptq] avg loss: 7.924889087677002\n",
      "INFO - Quantizing mlp.down_proj in layer 19/32...\n",
      "2024-10-30 08:24:34 INFO [auto_gptq.quantization.gptq] duration: 2.819945812225342\n",
      "2024-10-30 08:24:34 INFO [auto_gptq.quantization.gptq] avg loss: 0.06767208874225616\n",
      "INFO - Start quantizing layer 20/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/32...\n",
      "2024-10-30 08:24:35 INFO [auto_gptq.quantization.gptq] duration: 0.6377036571502686\n",
      "2024-10-30 08:24:35 INFO [auto_gptq.quantization.gptq] avg loss: 3.2177462577819824\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/32...\n",
      "2024-10-30 08:24:36 INFO [auto_gptq.quantization.gptq] duration: 0.6337978839874268\n",
      "2024-10-30 08:24:36 INFO [auto_gptq.quantization.gptq] avg loss: 0.3803046643733978\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/32...\n",
      "2024-10-30 08:24:37 INFO [auto_gptq.quantization.gptq] duration: 0.6685996055603027\n",
      "2024-10-30 08:24:37 INFO [auto_gptq.quantization.gptq] avg loss: 5.318844795227051\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/32...\n",
      "2024-10-30 08:24:37 INFO [auto_gptq.quantization.gptq] duration: 0.6665921211242676\n",
      "2024-10-30 08:24:37 INFO [auto_gptq.quantization.gptq] avg loss: 0.010474678128957748\n",
      "INFO - Quantizing mlp.up_proj in layer 20/32...\n",
      "2024-10-30 08:24:38 INFO [auto_gptq.quantization.gptq] duration: 0.6838321685791016\n",
      "2024-10-30 08:24:38 INFO [auto_gptq.quantization.gptq] avg loss: 4.528053283691406\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/32...\n",
      "2024-10-30 08:24:39 INFO [auto_gptq.quantization.gptq] duration: 0.6849365234375\n",
      "2024-10-30 08:24:39 INFO [auto_gptq.quantization.gptq] avg loss: 8.470098495483398\n",
      "INFO - Quantizing mlp.down_proj in layer 20/32...\n",
      "2024-10-30 08:24:41 INFO [auto_gptq.quantization.gptq] duration: 2.821955442428589\n",
      "2024-10-30 08:24:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.06649299710988998\n",
      "INFO - Start quantizing layer 21/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/32...\n",
      "2024-10-30 08:24:42 INFO [auto_gptq.quantization.gptq] duration: 0.6388185024261475\n",
      "2024-10-30 08:24:42 INFO [auto_gptq.quantization.gptq] avg loss: 3.4987025260925293\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/32...\n",
      "2024-10-30 08:24:43 INFO [auto_gptq.quantization.gptq] duration: 0.9887664318084717\n",
      "2024-10-30 08:24:43 INFO [auto_gptq.quantization.gptq] avg loss: 0.4227803647518158\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/32...\n",
      "2024-10-30 08:24:44 INFO [auto_gptq.quantization.gptq] duration: 0.9909965991973877\n",
      "2024-10-30 08:24:44 INFO [auto_gptq.quantization.gptq] avg loss: 5.475636959075928\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/32...\n",
      "2024-10-30 08:24:45 INFO [auto_gptq.quantization.gptq] duration: 0.9630591869354248\n",
      "2024-10-30 08:24:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.008370958268642426\n",
      "INFO - Quantizing mlp.up_proj in layer 21/32...\n",
      "2024-10-30 08:24:46 INFO [auto_gptq.quantization.gptq] duration: 0.6820778846740723\n",
      "2024-10-30 08:24:46 INFO [auto_gptq.quantization.gptq] avg loss: 4.668856143951416\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/32...\n",
      "2024-10-30 08:24:47 INFO [auto_gptq.quantization.gptq] duration: 1.0569522380828857\n",
      "2024-10-30 08:24:47 INFO [auto_gptq.quantization.gptq] avg loss: 8.70762825012207\n",
      "INFO - Quantizing mlp.down_proj in layer 21/32...\n",
      "2024-10-30 08:24:50 INFO [auto_gptq.quantization.gptq] duration: 3.290433168411255\n",
      "2024-10-30 08:24:50 INFO [auto_gptq.quantization.gptq] avg loss: 0.08092903345823288\n",
      "INFO - Start quantizing layer 22/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/32...\n",
      "2024-10-30 08:24:51 INFO [auto_gptq.quantization.gptq] duration: 0.6803467273712158\n",
      "2024-10-30 08:24:51 INFO [auto_gptq.quantization.gptq] avg loss: 3.2141809463500977\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/32...\n",
      "2024-10-30 08:24:52 INFO [auto_gptq.quantization.gptq] duration: 1.0341272354125977\n",
      "2024-10-30 08:24:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.4506223797798157\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/32...\n",
      "2024-10-30 08:24:53 INFO [auto_gptq.quantization.gptq] duration: 0.997504711151123\n",
      "2024-10-30 08:24:53 INFO [auto_gptq.quantization.gptq] avg loss: 5.119865417480469\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/32...\n",
      "2024-10-30 08:24:54 INFO [auto_gptq.quantization.gptq] duration: 0.852484941482544\n",
      "2024-10-30 08:24:54 INFO [auto_gptq.quantization.gptq] avg loss: 0.015781033784151077\n",
      "INFO - Quantizing mlp.up_proj in layer 22/32...\n",
      "2024-10-30 08:24:55 INFO [auto_gptq.quantization.gptq] duration: 0.6865391731262207\n",
      "2024-10-30 08:24:55 INFO [auto_gptq.quantization.gptq] avg loss: 4.993688583374023\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/32...\n",
      "2024-10-30 08:24:56 INFO [auto_gptq.quantization.gptq] duration: 1.0961449146270752\n",
      "2024-10-30 08:24:56 INFO [auto_gptq.quantization.gptq] avg loss: 9.444746017456055\n",
      "INFO - Quantizing mlp.down_proj in layer 22/32...\n",
      "2024-10-30 08:25:00 INFO [auto_gptq.quantization.gptq] duration: 3.749769687652588\n",
      "2024-10-30 08:25:00 INFO [auto_gptq.quantization.gptq] avg loss: 0.08386433124542236\n",
      "INFO - Start quantizing layer 23/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/32...\n",
      "2024-10-30 08:25:01 INFO [auto_gptq.quantization.gptq] duration: 1.0289595127105713\n",
      "2024-10-30 08:25:01 INFO [auto_gptq.quantization.gptq] avg loss: 3.515753746032715\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/32...\n",
      "2024-10-30 08:25:02 INFO [auto_gptq.quantization.gptq] duration: 0.8241438865661621\n",
      "2024-10-30 08:25:02 INFO [auto_gptq.quantization.gptq] avg loss: 0.5364612340927124\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/32...\n",
      "2024-10-30 08:25:03 INFO [auto_gptq.quantization.gptq] duration: 0.6926472187042236\n",
      "2024-10-30 08:25:03 INFO [auto_gptq.quantization.gptq] avg loss: 5.255095481872559\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/32...\n",
      "2024-10-30 08:25:03 INFO [auto_gptq.quantization.gptq] duration: 0.6773984432220459\n",
      "2024-10-30 08:25:03 INFO [auto_gptq.quantization.gptq] avg loss: 0.011732600629329681\n",
      "INFO - Quantizing mlp.up_proj in layer 23/32...\n",
      "2024-10-30 08:25:04 INFO [auto_gptq.quantization.gptq] duration: 0.7409934997558594\n",
      "2024-10-30 08:25:04 INFO [auto_gptq.quantization.gptq] avg loss: 5.264748573303223\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/32...\n",
      "2024-10-30 08:25:05 INFO [auto_gptq.quantization.gptq] duration: 0.7238552570343018\n",
      "2024-10-30 08:25:05 INFO [auto_gptq.quantization.gptq] avg loss: 9.779818534851074\n",
      "INFO - Quantizing mlp.down_proj in layer 23/32...\n",
      "2024-10-30 08:25:09 INFO [auto_gptq.quantization.gptq] duration: 3.9880189895629883\n",
      "2024-10-30 08:25:09 INFO [auto_gptq.quantization.gptq] avg loss: 0.08359816670417786\n",
      "INFO - Start quantizing layer 24/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/32...\n",
      "2024-10-30 08:25:10 INFO [auto_gptq.quantization.gptq] duration: 0.6403539180755615\n",
      "2024-10-30 08:25:10 INFO [auto_gptq.quantization.gptq] avg loss: 3.4418718814849854\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/32...\n",
      "2024-10-30 08:25:10 INFO [auto_gptq.quantization.gptq] duration: 0.6408452987670898\n",
      "2024-10-30 08:25:10 INFO [auto_gptq.quantization.gptq] avg loss: 0.5572623014450073\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/32...\n",
      "2024-10-30 08:25:11 INFO [auto_gptq.quantization.gptq] duration: 0.6641359329223633\n",
      "2024-10-30 08:25:11 INFO [auto_gptq.quantization.gptq] avg loss: 5.203144073486328\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/32...\n",
      "2024-10-30 08:25:12 INFO [auto_gptq.quantization.gptq] duration: 0.6637964248657227\n",
      "2024-10-30 08:25:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.009468404576182365\n",
      "INFO - Quantizing mlp.up_proj in layer 24/32...\n",
      "2024-10-30 08:25:12 INFO [auto_gptq.quantization.gptq] duration: 0.6857712268829346\n",
      "2024-10-30 08:25:12 INFO [auto_gptq.quantization.gptq] avg loss: 5.434329509735107\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/32...\n",
      "2024-10-30 08:25:13 INFO [auto_gptq.quantization.gptq] duration: 0.6873073577880859\n",
      "2024-10-30 08:25:13 INFO [auto_gptq.quantization.gptq] avg loss: 9.982233047485352\n",
      "INFO - Quantizing mlp.down_proj in layer 24/32...\n",
      "2024-10-30 08:25:16 INFO [auto_gptq.quantization.gptq] duration: 2.819167137145996\n",
      "2024-10-30 08:25:16 INFO [auto_gptq.quantization.gptq] avg loss: 0.10776354372501373\n",
      "INFO - Start quantizing layer 25/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/32...\n",
      "2024-10-30 08:25:17 INFO [auto_gptq.quantization.gptq] duration: 0.643519401550293\n",
      "2024-10-30 08:25:17 INFO [auto_gptq.quantization.gptq] avg loss: 3.3116140365600586\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/32...\n",
      "2024-10-30 08:25:17 INFO [auto_gptq.quantization.gptq] duration: 0.6523044109344482\n",
      "2024-10-30 08:25:17 INFO [auto_gptq.quantization.gptq] avg loss: 0.7263853549957275\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/32...\n",
      "2024-10-30 08:25:18 INFO [auto_gptq.quantization.gptq] duration: 0.6640760898590088\n",
      "2024-10-30 08:25:18 INFO [auto_gptq.quantization.gptq] avg loss: 5.201694011688232\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/32...\n",
      "2024-10-30 08:25:19 INFO [auto_gptq.quantization.gptq] duration: 0.6663703918457031\n",
      "2024-10-30 08:25:19 INFO [auto_gptq.quantization.gptq] avg loss: 0.013845521956682205\n",
      "INFO - Quantizing mlp.up_proj in layer 25/32...\n",
      "2024-10-30 08:25:20 INFO [auto_gptq.quantization.gptq] duration: 1.0460145473480225\n",
      "2024-10-30 08:25:20 INFO [auto_gptq.quantization.gptq] avg loss: 5.803417205810547\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/32...\n",
      "2024-10-30 08:25:20 INFO [auto_gptq.quantization.gptq] duration: 0.6851811408996582\n",
      "2024-10-30 08:25:20 INFO [auto_gptq.quantization.gptq] avg loss: 10.740999221801758\n",
      "INFO - Quantizing mlp.down_proj in layer 25/32...\n",
      "2024-10-30 08:25:23 INFO [auto_gptq.quantization.gptq] duration: 2.8198721408843994\n",
      "2024-10-30 08:25:23 INFO [auto_gptq.quantization.gptq] avg loss: 0.09952988475561142\n",
      "INFO - Start quantizing layer 26/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/32...\n",
      "2024-10-30 08:25:24 INFO [auto_gptq.quantization.gptq] duration: 0.640430212020874\n",
      "2024-10-30 08:25:24 INFO [auto_gptq.quantization.gptq] avg loss: 3.178253173828125\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/32...\n",
      "2024-10-30 08:25:25 INFO [auto_gptq.quantization.gptq] duration: 0.6389398574829102\n",
      "2024-10-30 08:25:25 INFO [auto_gptq.quantization.gptq] avg loss: 0.7512090802192688\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/32...\n",
      "2024-10-30 08:25:25 INFO [auto_gptq.quantization.gptq] duration: 0.6634540557861328\n",
      "2024-10-30 08:25:25 INFO [auto_gptq.quantization.gptq] avg loss: 5.1941680908203125\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/32...\n",
      "2024-10-30 08:25:26 INFO [auto_gptq.quantization.gptq] duration: 0.6646127700805664\n",
      "2024-10-30 08:25:26 INFO [auto_gptq.quantization.gptq] avg loss: 0.018195414915680885\n",
      "INFO - Quantizing mlp.up_proj in layer 26/32...\n",
      "2024-10-30 08:25:27 INFO [auto_gptq.quantization.gptq] duration: 0.6831169128417969\n",
      "2024-10-30 08:25:27 INFO [auto_gptq.quantization.gptq] avg loss: 6.287962913513184\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/32...\n",
      "2024-10-30 08:25:28 INFO [auto_gptq.quantization.gptq] duration: 0.6848940849304199\n",
      "2024-10-30 08:25:28 INFO [auto_gptq.quantization.gptq] avg loss: 11.5087890625\n",
      "INFO - Quantizing mlp.down_proj in layer 26/32...\n",
      "2024-10-30 08:25:30 INFO [auto_gptq.quantization.gptq] duration: 2.81850266456604\n",
      "2024-10-30 08:25:30 INFO [auto_gptq.quantization.gptq] avg loss: 0.10482949763536453\n",
      "INFO - Start quantizing layer 27/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/32...\n",
      "2024-10-30 08:25:31 INFO [auto_gptq.quantization.gptq] duration: 0.6368556022644043\n",
      "2024-10-30 08:25:31 INFO [auto_gptq.quantization.gptq] avg loss: 3.36843204498291\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/32...\n",
      "2024-10-30 08:25:32 INFO [auto_gptq.quantization.gptq] duration: 0.6367359161376953\n",
      "2024-10-30 08:25:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.7358830571174622\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/32...\n",
      "2024-10-30 08:25:33 INFO [auto_gptq.quantization.gptq] duration: 0.667107105255127\n",
      "2024-10-30 08:25:33 INFO [auto_gptq.quantization.gptq] avg loss: 4.9942779541015625\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/32...\n",
      "2024-10-30 08:25:33 INFO [auto_gptq.quantization.gptq] duration: 0.6665992736816406\n",
      "2024-10-30 08:25:33 INFO [auto_gptq.quantization.gptq] avg loss: 0.02009281888604164\n",
      "INFO - Quantizing mlp.up_proj in layer 27/32...\n",
      "2024-10-30 08:25:34 INFO [auto_gptq.quantization.gptq] duration: 0.6845989227294922\n",
      "2024-10-30 08:25:34 INFO [auto_gptq.quantization.gptq] avg loss: 6.995084285736084\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/32...\n",
      "2024-10-30 08:25:35 INFO [auto_gptq.quantization.gptq] duration: 1.051100492477417\n",
      "2024-10-30 08:25:35 INFO [auto_gptq.quantization.gptq] avg loss: 12.929121971130371\n",
      "INFO - Quantizing mlp.down_proj in layer 27/32...\n",
      "2024-10-30 08:25:38 INFO [auto_gptq.quantization.gptq] duration: 3.2517030239105225\n",
      "2024-10-30 08:25:38 INFO [auto_gptq.quantization.gptq] avg loss: 0.12300819158554077\n",
      "INFO - Start quantizing layer 28/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/32...\n",
      "2024-10-30 08:25:39 INFO [auto_gptq.quantization.gptq] duration: 0.6360998153686523\n",
      "2024-10-30 08:25:39 INFO [auto_gptq.quantization.gptq] avg loss: 3.7245657444000244\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/32...\n",
      "2024-10-30 08:25:40 INFO [auto_gptq.quantization.gptq] duration: 0.6393418312072754\n",
      "2024-10-30 08:25:40 INFO [auto_gptq.quantization.gptq] avg loss: 1.0377029180526733\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/32...\n",
      "2024-10-30 08:25:40 INFO [auto_gptq.quantization.gptq] duration: 0.6702265739440918\n",
      "2024-10-30 08:25:40 INFO [auto_gptq.quantization.gptq] avg loss: 5.300141334533691\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/32...\n",
      "2024-10-30 08:25:41 INFO [auto_gptq.quantization.gptq] duration: 0.6703271865844727\n",
      "2024-10-30 08:25:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.031553056091070175\n",
      "INFO - Quantizing mlp.up_proj in layer 28/32...\n",
      "2024-10-30 08:25:42 INFO [auto_gptq.quantization.gptq] duration: 0.6887867450714111\n",
      "2024-10-30 08:25:42 INFO [auto_gptq.quantization.gptq] avg loss: 7.660228729248047\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/32...\n",
      "2024-10-30 08:25:42 INFO [auto_gptq.quantization.gptq] duration: 0.6890602111816406\n",
      "2024-10-30 08:25:42 INFO [auto_gptq.quantization.gptq] avg loss: 13.831243515014648\n",
      "INFO - Quantizing mlp.down_proj in layer 28/32...\n",
      "2024-10-30 08:25:45 INFO [auto_gptq.quantization.gptq] duration: 2.8346569538116455\n",
      "2024-10-30 08:25:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.17476379871368408\n",
      "INFO - Start quantizing layer 29/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/32...\n",
      "2024-10-30 08:25:46 INFO [auto_gptq.quantization.gptq] duration: 0.6378612518310547\n",
      "2024-10-30 08:25:46 INFO [auto_gptq.quantization.gptq] avg loss: 3.0047378540039062\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/32...\n",
      "2024-10-30 08:25:47 INFO [auto_gptq.quantization.gptq] duration: 0.6373991966247559\n",
      "2024-10-30 08:25:47 INFO [auto_gptq.quantization.gptq] avg loss: 0.9363851547241211\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/32...\n",
      "2024-10-30 08:25:47 INFO [auto_gptq.quantization.gptq] duration: 0.6717967987060547\n",
      "2024-10-30 08:25:47 INFO [auto_gptq.quantization.gptq] avg loss: 4.8604230880737305\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/32...\n",
      "2024-10-30 08:25:48 INFO [auto_gptq.quantization.gptq] duration: 0.6695327758789062\n",
      "2024-10-30 08:25:48 INFO [auto_gptq.quantization.gptq] avg loss: 0.049208927899599075\n",
      "INFO - Quantizing mlp.up_proj in layer 29/32...\n",
      "2024-10-30 08:25:49 INFO [auto_gptq.quantization.gptq] duration: 0.6879699230194092\n",
      "2024-10-30 08:25:49 INFO [auto_gptq.quantization.gptq] avg loss: 8.440662384033203\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/32...\n",
      "2024-10-30 08:25:50 INFO [auto_gptq.quantization.gptq] duration: 0.6942665576934814\n",
      "2024-10-30 08:25:50 INFO [auto_gptq.quantization.gptq] avg loss: 14.636728286743164\n",
      "INFO - Quantizing mlp.down_proj in layer 29/32...\n",
      "2024-10-30 08:25:52 INFO [auto_gptq.quantization.gptq] duration: 2.835653305053711\n",
      "2024-10-30 08:25:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.2249867022037506\n",
      "INFO - Start quantizing layer 30/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/32...\n",
      "2024-10-30 08:25:53 INFO [auto_gptq.quantization.gptq] duration: 0.6398940086364746\n",
      "2024-10-30 08:25:53 INFO [auto_gptq.quantization.gptq] avg loss: 3.177680730819702\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/32...\n",
      "2024-10-30 08:25:54 INFO [auto_gptq.quantization.gptq] duration: 0.6413218975067139\n",
      "2024-10-30 08:25:54 INFO [auto_gptq.quantization.gptq] avg loss: 1.1065967082977295\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/32...\n",
      "2024-10-30 08:25:55 INFO [auto_gptq.quantization.gptq] duration: 0.6757369041442871\n",
      "2024-10-30 08:25:55 INFO [auto_gptq.quantization.gptq] avg loss: 5.337040901184082\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/32...\n",
      "2024-10-30 08:25:55 INFO [auto_gptq.quantization.gptq] duration: 0.6714370250701904\n",
      "2024-10-30 08:25:55 INFO [auto_gptq.quantization.gptq] avg loss: 0.060473158955574036\n",
      "INFO - Quantizing mlp.up_proj in layer 30/32...\n",
      "2024-10-30 08:25:56 INFO [auto_gptq.quantization.gptq] duration: 0.6895837783813477\n",
      "2024-10-30 08:25:56 INFO [auto_gptq.quantization.gptq] avg loss: 9.741785049438477\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/32...\n",
      "2024-10-30 08:25:57 INFO [auto_gptq.quantization.gptq] duration: 0.691828727722168\n",
      "2024-10-30 08:25:57 INFO [auto_gptq.quantization.gptq] avg loss: 15.94802188873291\n",
      "INFO - Quantizing mlp.down_proj in layer 30/32...\n",
      "2024-10-30 08:25:59 INFO [auto_gptq.quantization.gptq] duration: 2.8308465480804443\n",
      "2024-10-30 08:25:59 INFO [auto_gptq.quantization.gptq] avg loss: 0.3754199743270874\n",
      "INFO - Start quantizing layer 31/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/32...\n",
      "2024-10-30 08:26:00 INFO [auto_gptq.quantization.gptq] duration: 0.6346733570098877\n",
      "2024-10-30 08:26:00 INFO [auto_gptq.quantization.gptq] avg loss: 3.111067771911621\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/32...\n",
      "2024-10-30 08:26:01 INFO [auto_gptq.quantization.gptq] duration: 0.6351008415222168\n",
      "2024-10-30 08:26:01 INFO [auto_gptq.quantization.gptq] avg loss: 1.5940788984298706\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/32...\n",
      "2024-10-30 08:26:02 INFO [auto_gptq.quantization.gptq] duration: 0.6678719520568848\n",
      "2024-10-30 08:26:02 INFO [auto_gptq.quantization.gptq] avg loss: 4.6921210289001465\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/32...\n",
      "2024-10-30 08:26:02 INFO [auto_gptq.quantization.gptq] duration: 0.6708347797393799\n",
      "2024-10-30 08:26:02 INFO [auto_gptq.quantization.gptq] avg loss: 0.08755072206258774\n",
      "INFO - Quantizing mlp.up_proj in layer 31/32...\n",
      "2024-10-30 08:26:03 INFO [auto_gptq.quantization.gptq] duration: 0.6883659362792969\n",
      "2024-10-30 08:26:03 INFO [auto_gptq.quantization.gptq] avg loss: 11.385140419006348\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/32...\n",
      "2024-10-30 08:26:04 INFO [auto_gptq.quantization.gptq] duration: 0.6866903305053711\n",
      "2024-10-30 08:26:04 INFO [auto_gptq.quantization.gptq] avg loss: 18.65461540222168\n",
      "INFO - Quantizing mlp.down_proj in layer 31/32...\n",
      "2024-10-30 08:26:07 INFO [auto_gptq.quantization.gptq] duration: 2.8347184658050537\n",
      "2024-10-30 08:26:07 INFO [auto_gptq.quantization.gptq] avg loss: 0.9421476125717163\n",
      "INFO - Start quantizing layer 32/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/32...\n",
      "2024-10-30 08:26:07 INFO [auto_gptq.quantization.gptq] duration: 0.6402497291564941\n",
      "2024-10-30 08:26:07 INFO [auto_gptq.quantization.gptq] avg loss: 2.465703010559082\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/32...\n",
      "2024-10-30 08:26:08 INFO [auto_gptq.quantization.gptq] duration: 0.6428651809692383\n",
      "2024-10-30 08:26:08 INFO [auto_gptq.quantization.gptq] avg loss: 1.0032373666763306\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/32...\n",
      "2024-10-30 08:26:09 INFO [auto_gptq.quantization.gptq] duration: 0.6695475578308105\n",
      "2024-10-30 08:26:09 INFO [auto_gptq.quantization.gptq] avg loss: 4.3938446044921875\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/32...\n",
      "2024-10-30 08:26:09 INFO [auto_gptq.quantization.gptq] duration: 0.6682565212249756\n",
      "2024-10-30 08:26:09 INFO [auto_gptq.quantization.gptq] avg loss: 0.40166765451431274\n",
      "INFO - Quantizing mlp.up_proj in layer 32/32...\n",
      "2024-10-30 08:26:10 INFO [auto_gptq.quantization.gptq] duration: 0.6884582042694092\n",
      "2024-10-30 08:26:10 INFO [auto_gptq.quantization.gptq] avg loss: 14.70301628112793\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/32...\n",
      "2024-10-30 08:26:11 INFO [auto_gptq.quantization.gptq] duration: 0.6912364959716797\n",
      "2024-10-30 08:26:11 INFO [auto_gptq.quantization.gptq] avg loss: 20.083524703979492\n",
      "INFO - Quantizing mlp.down_proj in layer 32/32...\n",
      "2024-10-30 08:26:14 INFO [auto_gptq.quantization.gptq] duration: 2.837254047393799\n",
      "2024-10-30 08:26:14 INFO [auto_gptq.quantization.gptq] avg loss: 10.1775484085083\n",
      "INFO - Packing model...\n",
      "2024-10-30 08:26:14 INFO [auto_gptq.modeling._utils] Packing model...\n",
      "Packing model.layers.31.mlp.down_proj...: 100%|██████████| 224/224 [03:43<00:00,  1.00it/s]   \n",
      "INFO - Model packed.\n",
      "2024-10-30 08:30:01 INFO [auto_gptq.modeling._utils] Model packed.\n",
      "INFO - You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.\n",
      "INFO - The layer lm_head is not quantized.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "/opt/conda/envs/lmquant/lib/python3.11/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "The model 'LlamaGPTQForCausalLM' is not supported for . Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>auto_gptq is a simple Python script that uses the AutoGPT model to generate text\n",
      "auto-gptq is a type of gptq that is generated internally by the system. It\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
    "model.quantize(examples)\n",
    "\n",
    "# save quantized model\n",
    "model.save_quantized(quantized_model_dir)\n",
    "\n",
    "# save quantized model using safetensors\n",
    "model.save_quantized(quantized_model_dir, use_safetensors=True)\n",
    "\n",
    "# push quantized model to Hugging Face Hub.\n",
    "# to use use_auth_token=True, Login first via huggingface-cli login.\n",
    "# or pass explcit token with: use_auth_token=\"hf_xxxxxxx\"\n",
    "# (uncomment the following three lines to enable this feature)\n",
    "# repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
    "# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
    "# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)\n",
    "\n",
    "# alternatively you can save and push at the same time\n",
    "# (uncomment the following three lines to enable this feature)\n",
    "# repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
    "# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
    "# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.\n",
      "INFO - The layer lm_head is not quantized.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The model 'LlamaGPTQForCausalLM' is not supported for . Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>auto_gptq is a simple Python script that uses the AutoGPT model to generate text\n",
      "auto-gptq is a type of gptq that is generated internally by the system. It\n"
     ]
    }
   ],
   "source": [
    "# load quantized model to the first GPU\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device=\"cuda:0\")\n",
    "\n",
    "# download quantized model from Hugging Face Hub and load to the first GPU\n",
    "# model = AutoGPTQForCausalLM.from_quantized(repo_id, device=\"cuda:0\", use_safetensors=True, use_triton=False)\n",
    "\n",
    "# inference with model.generate\n",
    "print(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\", return_tensors=\"pt\").to(model.device))[0]))\n",
    "\n",
    "# or you can also use pipeline\n",
    "pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "print(pipeline(\"auto-gptq is\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多模态模型量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def get_wikitext2(nsamples, seed, seqlen, tokenizer):\n",
    "    # set seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "\n",
    "    # load dataset and preprocess\n",
    "    # traindata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    # testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    traindata = load_dataset(\"/home/workspace/code/git/FlatQuant_mlm/datasets/wikitext\", split=\"train\")\n",
    "    testdata = load_dataset(\"/home/workspace/code/git/FlatQuant_mlm/datasets/wikitext\", split=\"test\")\n",
    "    trainenc = tokenizer(\"\\n\\n\".join(traindata[\"text\"]), return_tensors=\"pt\")\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    traindataset = []\n",
    "    for _ in range(nsamples):\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        attention_mask = torch.ones_like(inp)\n",
    "        traindataset.append({\"input_ids\": inp, \"attention_mask\": attention_mask})\n",
    "    return traindataset, testenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-11-06 03:36:13 INFO [transformers_modules.MiniCPM-Llama3-V-2_5.configuration_minicpm] vision_config is None, using default vision config\n",
      "2024-11-06 03:36:13 INFO [transformers_modules.MiniCPM-Llama3-V-2_5.configuration_minicpm] vision_config is None, using default vision config\n",
      "2024-11-06 03:36:13 INFO [transformers_modules.MiniCPM-Llama3-V-2_5.configuration_minicpm] vision_config is None, using default vision config\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:33<00:00,  4.85s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "from auto_gptq import AutoGPTQForCausalMLM, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "pretrained_model_dir = \"/home/workspace/model/MiniCPM-Llama3-V-2_5\"\n",
    "quantized_model_dir = \"/home/workspace/model/MiniCPM-Llama3-V-2_5-w4-g128\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True, trust_remote_code=True)\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,  # quantize model to 4-bit\n",
    "    group_size=128,  # it is recommended to set the value to 128\n",
    "    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    ")\n",
    "traindataset, testenc = get_wikitext2(128, 0, model.seqlen, tokenizer)\n",
    "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "model = AutoGPTQForCausalMLM.from_pretrained(pretrained_model_dir, quantize_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "from logging import getLogger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from .quantizer import Quantizer\n",
    "logger = getLogger(__name__)\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "class GPTQ:\n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer\n",
    "        self.dev = self.layer.weight.device\n",
    "        W = layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.pytorch_utils.Conv1D):\n",
    "            W = W.t()\n",
    "        self.rows = W.shape[0]\n",
    "        self.columns = W.shape[1]\n",
    "        self.H = torch.zeros((self.columns, self.columns), device=self.dev)\n",
    "        self.nsamples = 0\n",
    "        self.quantizer = Quantizer()\n",
    "    def add_batch(self, inp, out):\n",
    "        if os.environ.get(\"DEBUG\"):\n",
    "            self.inp1 = inp\n",
    "            self.out1 = out\n",
    "        if len(inp.shape) == 2:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        tmp = inp.shape[0]\n",
    "        if isinstance(self.layer, nn.Linear) or isinstance(self.layer, transformers.Conv1D):\n",
    "            if len(inp.shape) == 3:\n",
    "                inp = inp.reshape((-1, inp.shape[-1]))\n",
    "            inp = inp.t()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            unfold = nn.Unfold(\n",
    "                self.layer.kernel_size,\n",
    "                dilation=self.layer.dilation,\n",
    "                padding=self.layer.padding,\n",
    "                stride=self.layer.stride,\n",
    "            )\n",
    "            inp = unfold(inp)\n",
    "            inp = inp.permute([1, 0, 2])\n",
    "            inp = inp.flatten(1)\n",
    "        self.H *= self.nsamples / (self.nsamples + tmp)\n",
    "        self.nsamples += tmp\n",
    "        # inp = inp.float()\n",
    "        inp = math.sqrt(2 / self.nsamples) * inp.float()\n",
    "        # print(\"inp.shape: \", inp.shape)\n",
    "        # print(\"H.shape: \", self.H.shape)\n",
    "        # self.H += 2 / self.nsamples * inp.matmul(inp.t())\n",
    "        self.H += inp.matmul(inp.t())\n",
    "    def fasterquant(\n",
    "        self,\n",
    "        blocksize=128,\n",
    "        percdamp=0.01,\n",
    "        group_size=-1,\n",
    "        actorder=False,\n",
    "        static_groups=False,\n",
    "    ):\n",
    "        W = self.layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        W = W.float()\n",
    "        tick = time.time()\n",
    "        if not self.quantizer.ready():\n",
    "            self.quantizer.find_params(W, weight=True)\n",
    "        H = self.H\n",
    "        del self.H\n",
    "        dead = torch.diag(H) == 0\n",
    "        H[dead, dead] = 1\n",
    "        W[:, dead] = 0\n",
    "        g_idx = []\n",
    "        scale = []\n",
    "        zero = []\n",
    "        now_idx = 1\n",
    "        if static_groups:\n",
    "            import copy\n",
    "            groups = []\n",
    "            for i in range(0, self.columns, group_size):\n",
    "                quantizer = copy.deepcopy(self.quantizer)\n",
    "                quantizer.find_params(W[:, i : (i + group_size)], weight=True)\n",
    "                scale.append(quantizer.scale)\n",
    "                zero.append(quantizer.zero)\n",
    "                groups.append(quantizer)\n",
    "        if actorder:\n",
    "            perm = torch.argsort(torch.diag(H), descending=True)\n",
    "            W = W[:, perm]\n",
    "            H = H[perm][:, perm]\n",
    "            invperm = torch.argsort(perm)\n",
    "        Losses = torch.zeros_like(W)\n",
    "        Q = torch.zeros_like(W)\n",
    "        damp = percdamp * torch.mean(torch.diag(H))\n",
    "        print(\"1st damp: \", damp)\n",
    "        # NOTE: To Make Sure positive-definite\n",
    "        eigenvalues = torch.linalg.eigvalsh(H)\n",
    "        min_eigenvalue = torch.min(eigenvalues)\n",
    "        if min_eigenvalue < 0:\n",
    "            damp = max(damp, -min_eigenvalue + 0.01 * torch.mean(torch.diag(H)))  # 确保至少覆盖最小特征值的负面效应\n",
    "            print(\"2nd damp: \", damp)\n",
    "        diag = torch.arange(self.columns, device=self.dev)\n",
    "        H[diag, diag] += damp\n",
    "        H = torch.linalg.cholesky(H)\n",
    "        H = torch.cholesky_inverse(H)\n",
    "        H = torch.linalg.cholesky(H, upper=True)\n",
    "        Hinv = H\n",
    "        for i1 in range(0, self.columns, blocksize):\n",
    "            i2 = min(i1 + blocksize, self.columns)\n",
    "            count = i2 - i1\n",
    "            W1 = W[:, i1:i2].clone()\n",
    "            Q1 = torch.zeros_like(W1)\n",
    "            Err1 = torch.zeros_like(W1)\n",
    "            Losses1 = torch.zeros_like(W1)\n",
    "            Hinv1 = Hinv[i1:i2, i1:i2]\n",
    "            for i in range(count):\n",
    "                w = W1[:, i]\n",
    "                d = Hinv1[i, i]\n",
    "                if group_size != -1:\n",
    "                    if not static_groups:\n",
    "                        if (i1 + i) % group_size == 0:\n",
    "                            self.quantizer.find_params(W[:, (i1 + i) : (i1 + i + group_size)], weight=True)\n",
    "                        if ((i1 + i) // group_size) - now_idx == -1:\n",
    "                            scale.append(self.quantizer.scale)\n",
    "                            zero.append(self.quantizer.zero)\n",
    "                            now_idx += 1\n",
    "                    else:\n",
    "                        idx = i1 + i\n",
    "                        if actorder:\n",
    "                            idx = perm[idx]\n",
    "                        self.quantizer = groups[idx // group_size]\n",
    "                q = self.quantizer.quantize(w.unsqueeze(1)).flatten()\n",
    "                Q1[:, i] = q\n",
    "                Losses1[:, i] = (w - q) ** 2 / d**2\n",
    "                err1 = (w - q) / d\n",
    "                W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n",
    "                Err1[:, i] = err1\n",
    "            Q[:, i1:i2] = Q1\n",
    "            Losses[:, i1:i2] = Losses1 / 2\n",
    "            W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])\n",
    "            if os.environ.get(\"DEBUG\"):\n",
    "                self.layer.weight.data[:, :i2] = Q[:, :i2]\n",
    "                self.layer.weight.data[:, i2:] = W[:, i2:]\n",
    "                logger.debug(torch.sum((self.layer(self.inp1) - self.out1) ** 2))\n",
    "                logger.debug(torch.sum(Losses))\n",
    "        torch.cuda.synchronize()\n",
    "        logger.info(f\"duration: {(time.time() - tick)}\")\n",
    "        logger.info(f\"avg loss: {torch.sum(Losses).item() / self.nsamples}\")\n",
    "        group_size = group_size if group_size != -1 else self.columns\n",
    "        if static_groups and actorder:\n",
    "            g_idx = [perm[i] // group_size for i in range(self.columns)]\n",
    "        else:\n",
    "            g_idx = [i // group_size for i in range(self.columns)]\n",
    "        g_idx = torch.tensor(g_idx, dtype=torch.int32, device=Q.device)\n",
    "        if actorder:\n",
    "            Q = Q[:, invperm]\n",
    "            g_idx = g_idx[invperm]\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            Q = Q.t()\n",
    "        self.layer.weight.data = Q.reshape(self.layer.weight.shape).type_as(self.layer.weight.data)\n",
    "        if os.environ.get(\"DEBUG\"):\n",
    "            logger.debug(torch.sum((self.layer(self.inp1) - self.out1) ** 2))\n",
    "        if scale == []:\n",
    "            scale.append(self.quantizer.scale)\n",
    "            zero.append(self.quantizer.zero)\n",
    "        scale = torch.cat(scale, dim=1)\n",
    "        zero = torch.cat(zero, dim=1)\n",
    "        return scale, zero, g_idx\n",
    "    def free(self):\n",
    "        if os.environ.get(\"DEBUG\"):\n",
    "            self.inp1 = None\n",
    "            self.out1 = None\n",
    "        self.H = None\n",
    "        self.Losses = None\n",
    "        self.Trace = None\n",
    "        torch.cuda.empty_cache()\n",
    "__all__ = [\"GPTQ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/32...\n",
      "2024-11-06 03:37:45 INFO [auto_gptq.quantization.gptq] duration: 0.8841938972473145\n",
      "2024-11-06 03:37:45 INFO [auto_gptq.quantization.gptq] avg loss: 41.80743408203125\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/32...\n",
      "2024-11-06 03:37:46 INFO [auto_gptq.quantization.gptq] duration: 0.6244401931762695\n",
      "2024-11-06 03:37:46 INFO [auto_gptq.quantization.gptq] avg loss: 1.3841116428375244\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/32...\n",
      "2024-11-06 03:37:47 INFO [auto_gptq.quantization.gptq] duration: 0.6435339450836182\n",
      "2024-11-06 03:37:47 INFO [auto_gptq.quantization.gptq] avg loss: 71.19627380371094\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/32...\n",
      "2024-11-06 03:38:20 INFO [auto_gptq.quantization.gptq] duration: 0.8739309310913086\n",
      "2024-11-06 03:38:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.025352805852890015\n",
      "INFO - Quantizing mlp.up_proj in layer 1/32...\n",
      "2024-11-06 03:38:54 INFO [auto_gptq.quantization.gptq] duration: 0.9373400211334229\n",
      "2024-11-06 03:38:54 INFO [auto_gptq.quantization.gptq] avg loss: 27.4874210357666\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/32...\n",
      "2024-11-06 03:38:55 INFO [auto_gptq.quantization.gptq] duration: 0.6897335052490234\n",
      "2024-11-06 03:38:55 INFO [auto_gptq.quantization.gptq] avg loss: 34.76812744140625\n",
      "INFO - Quantizing mlp.down_proj in layer 1/32...\n",
      "2024-11-06 03:39:50 INFO [auto_gptq.quantization.gptq] duration: 3.1375675201416016\n",
      "2024-11-06 03:39:50 INFO [auto_gptq.quantization.gptq] avg loss: 0.09883140027523041\n",
      "INFO - Start quantizing layer 2/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/32...\n",
      "2024-11-06 03:40:53 INFO [auto_gptq.quantization.gptq] duration: 0.8949518203735352\n",
      "2024-11-06 03:40:53 INFO [auto_gptq.quantization.gptq] avg loss: 35.014137268066406\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/32...\n",
      "2024-11-06 03:40:54 INFO [auto_gptq.quantization.gptq] duration: 0.6292374134063721\n",
      "2024-11-06 03:40:54 INFO [auto_gptq.quantization.gptq] avg loss: 2.104182243347168\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/32...\n",
      "2024-11-06 03:40:55 INFO [auto_gptq.quantization.gptq] duration: 0.6612472534179688\n",
      "2024-11-06 03:40:55 INFO [auto_gptq.quantization.gptq] avg loss: 59.347686767578125\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/32...\n",
      "2024-11-06 03:41:25 INFO [auto_gptq.quantization.gptq] duration: 0.8778719902038574\n",
      "2024-11-06 03:41:25 INFO [auto_gptq.quantization.gptq] avg loss: 0.1856120079755783\n",
      "INFO - Quantizing mlp.up_proj in layer 2/32...\n",
      "2024-11-06 03:41:57 INFO [auto_gptq.quantization.gptq] duration: 0.9359395503997803\n",
      "2024-11-06 03:41:57 INFO [auto_gptq.quantization.gptq] avg loss: 42.35276412963867\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/32...\n",
      "2024-11-06 03:41:58 INFO [auto_gptq.quantization.gptq] duration: 0.7181816101074219\n",
      "2024-11-06 03:41:58 INFO [auto_gptq.quantization.gptq] avg loss: 51.18098831176758\n",
      "INFO - Quantizing mlp.down_proj in layer 2/32...\n",
      "2024-11-06 03:42:52 INFO [auto_gptq.quantization.gptq] duration: 3.850154161453247\n",
      "2024-11-06 03:42:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.9188196659088135\n",
      "INFO - Start quantizing layer 3/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/32...\n",
      "2024-11-06 03:43:54 INFO [auto_gptq.quantization.gptq] duration: 0.8904681205749512\n",
      "2024-11-06 03:43:54 INFO [auto_gptq.quantization.gptq] avg loss: 217.08541870117188\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/32...\n",
      "2024-11-06 03:43:54 INFO [auto_gptq.quantization.gptq] duration: 0.626941442489624\n",
      "2024-11-06 03:43:54 INFO [auto_gptq.quantization.gptq] avg loss: 9.601158142089844\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/32...\n",
      "2024-11-06 03:43:55 INFO [auto_gptq.quantization.gptq] duration: 0.6452288627624512\n",
      "2024-11-06 03:43:55 INFO [auto_gptq.quantization.gptq] avg loss: 335.13446044921875\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/32...\n",
      "2024-11-06 03:44:25 INFO [auto_gptq.quantization.gptq] duration: 0.8713991641998291\n",
      "2024-11-06 03:44:25 INFO [auto_gptq.quantization.gptq] avg loss: 0.06919470429420471\n",
      "INFO - Quantizing mlp.up_proj in layer 3/32...\n",
      "2024-11-06 03:44:57 INFO [auto_gptq.quantization.gptq] duration: 0.9230904579162598\n",
      "2024-11-06 03:44:57 INFO [auto_gptq.quantization.gptq] avg loss: 90.20467376708984\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/32...\n",
      "2024-11-06 03:44:58 INFO [auto_gptq.quantization.gptq] duration: 0.6799185276031494\n",
      "2024-11-06 03:44:58 INFO [auto_gptq.quantization.gptq] avg loss: 117.17925262451172\n",
      "INFO - Quantizing mlp.down_proj in layer 3/32...\n",
      "2024-11-06 03:45:51 INFO [auto_gptq.quantization.gptq] duration: 3.127979278564453\n",
      "2024-11-06 03:45:51 INFO [auto_gptq.quantization.gptq] avg loss: 0.4865586459636688\n",
      "INFO - Start quantizing layer 4/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/32...\n",
      "2024-11-06 03:46:53 INFO [auto_gptq.quantization.gptq] duration: 0.8827965259552002\n",
      "2024-11-06 03:46:53 INFO [auto_gptq.quantization.gptq] avg loss: 164.76022338867188\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/32...\n",
      "2024-11-06 03:46:54 INFO [auto_gptq.quantization.gptq] duration: 0.6199109554290771\n",
      "2024-11-06 03:46:54 INFO [auto_gptq.quantization.gptq] avg loss: 13.14059066772461\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/32...\n",
      "2024-11-06 03:46:55 INFO [auto_gptq.quantization.gptq] duration: 0.6548497676849365\n",
      "2024-11-06 03:46:55 INFO [auto_gptq.quantization.gptq] avg loss: 285.0863342285156\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/32...\n",
      "2024-11-06 03:47:25 INFO [auto_gptq.quantization.gptq] duration: 0.8735063076019287\n",
      "2024-11-06 03:47:25 INFO [auto_gptq.quantization.gptq] avg loss: 0.1477361023426056\n",
      "INFO - Quantizing mlp.up_proj in layer 4/32...\n",
      "2024-11-06 03:47:57 INFO [auto_gptq.quantization.gptq] duration: 0.9244320392608643\n",
      "2024-11-06 03:47:57 INFO [auto_gptq.quantization.gptq] avg loss: 122.41766357421875\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/32...\n",
      "2024-11-06 03:47:57 INFO [auto_gptq.quantization.gptq] duration: 0.676572322845459\n",
      "2024-11-06 03:47:57 INFO [auto_gptq.quantization.gptq] avg loss: 185.96792602539062\n",
      "INFO - Quantizing mlp.down_proj in layer 4/32...\n",
      "2024-11-06 03:48:50 INFO [auto_gptq.quantization.gptq] duration: 3.115055799484253\n",
      "2024-11-06 03:48:50 INFO [auto_gptq.quantization.gptq] avg loss: 0.8117116689682007\n",
      "INFO - Start quantizing layer 5/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/32...\n",
      "2024-11-06 03:49:53 INFO [auto_gptq.quantization.gptq] duration: 0.8925614356994629\n",
      "2024-11-06 03:49:53 INFO [auto_gptq.quantization.gptq] avg loss: 156.83575439453125\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/32...\n",
      "2024-11-06 03:49:53 INFO [auto_gptq.quantization.gptq] duration: 0.6306180953979492\n",
      "2024-11-06 03:49:53 INFO [auto_gptq.quantization.gptq] avg loss: 13.720349311828613\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/32...\n",
      "2024-11-06 03:49:54 INFO [auto_gptq.quantization.gptq] duration: 0.6618943214416504\n",
      "2024-11-06 03:49:54 INFO [auto_gptq.quantization.gptq] avg loss: 260.9587097167969\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/32...\n",
      "2024-11-06 03:50:24 INFO [auto_gptq.quantization.gptq] duration: 0.881458044052124\n",
      "2024-11-06 03:50:24 INFO [auto_gptq.quantization.gptq] avg loss: 0.28946250677108765\n",
      "INFO - Quantizing mlp.up_proj in layer 5/32...\n",
      "2024-11-06 03:50:56 INFO [auto_gptq.quantization.gptq] duration: 0.9356465339660645\n",
      "2024-11-06 03:50:56 INFO [auto_gptq.quantization.gptq] avg loss: 152.0654296875\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/32...\n",
      "2024-11-06 03:50:57 INFO [auto_gptq.quantization.gptq] duration: 1.0343708992004395\n",
      "2024-11-06 03:50:57 INFO [auto_gptq.quantization.gptq] avg loss: 265.95684814453125\n",
      "INFO - Quantizing mlp.down_proj in layer 5/32...\n",
      "2024-11-06 03:51:50 INFO [auto_gptq.quantization.gptq] duration: 3.1140060424804688\n",
      "2024-11-06 03:51:50 INFO [auto_gptq.quantization.gptq] avg loss: 1.4272148609161377\n",
      "INFO - Start quantizing layer 6/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/32...\n",
      "2024-11-06 03:52:53 INFO [auto_gptq.quantization.gptq] duration: 0.88395094871521\n",
      "2024-11-06 03:52:53 INFO [auto_gptq.quantization.gptq] avg loss: 247.23243713378906\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/32...\n",
      "2024-11-06 03:52:53 INFO [auto_gptq.quantization.gptq] duration: 0.6201510429382324\n",
      "2024-11-06 03:52:53 INFO [auto_gptq.quantization.gptq] avg loss: 13.76872444152832\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/32...\n",
      "2024-11-06 03:52:54 INFO [auto_gptq.quantization.gptq] duration: 0.6531863212585449\n",
      "2024-11-06 03:52:54 INFO [auto_gptq.quantization.gptq] avg loss: 381.3305969238281\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/32...\n",
      "2024-11-06 03:53:24 INFO [auto_gptq.quantization.gptq] duration: 0.8784067630767822\n",
      "2024-11-06 03:53:24 INFO [auto_gptq.quantization.gptq] avg loss: 0.4734765291213989\n",
      "INFO - Quantizing mlp.up_proj in layer 6/32...\n",
      "2024-11-06 03:53:56 INFO [auto_gptq.quantization.gptq] duration: 0.9313552379608154\n",
      "2024-11-06 03:53:56 INFO [auto_gptq.quantization.gptq] avg loss: 179.18429565429688\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/32...\n",
      "2024-11-06 03:53:57 INFO [auto_gptq.quantization.gptq] duration: 0.6818623542785645\n",
      "2024-11-06 03:53:57 INFO [auto_gptq.quantization.gptq] avg loss: 311.756103515625\n",
      "INFO - Quantizing mlp.down_proj in layer 6/32...\n",
      "2024-11-06 03:54:51 INFO [auto_gptq.quantization.gptq] duration: 3.8620121479034424\n",
      "2024-11-06 03:54:51 INFO [auto_gptq.quantization.gptq] avg loss: 2.0824036598205566\n",
      "INFO - Start quantizing layer 7/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/32...\n",
      "2024-11-06 03:55:53 INFO [auto_gptq.quantization.gptq] duration: 0.8857803344726562\n",
      "2024-11-06 03:55:53 INFO [auto_gptq.quantization.gptq] avg loss: 189.25401306152344\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/32...\n",
      "2024-11-06 03:55:53 INFO [auto_gptq.quantization.gptq] duration: 0.6202521324157715\n",
      "2024-11-06 03:55:53 INFO [auto_gptq.quantization.gptq] avg loss: 14.417367935180664\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/32...\n",
      "2024-11-06 03:55:54 INFO [auto_gptq.quantization.gptq] duration: 0.6403653621673584\n",
      "2024-11-06 03:55:54 INFO [auto_gptq.quantization.gptq] avg loss: 324.7474365234375\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/32...\n",
      "2024-11-06 03:56:24 INFO [auto_gptq.quantization.gptq] duration: 0.875725507736206\n",
      "2024-11-06 03:56:24 INFO [auto_gptq.quantization.gptq] avg loss: 0.8366045951843262\n",
      "INFO - Quantizing mlp.up_proj in layer 7/32...\n",
      "2024-11-06 03:56:56 INFO [auto_gptq.quantization.gptq] duration: 0.9277386665344238\n",
      "2024-11-06 03:56:56 INFO [auto_gptq.quantization.gptq] avg loss: 194.320068359375\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/32...\n",
      "2024-11-06 03:56:57 INFO [auto_gptq.quantization.gptq] duration: 0.677220344543457\n",
      "2024-11-06 03:56:57 INFO [auto_gptq.quantization.gptq] avg loss: 344.3106384277344\n",
      "INFO - Quantizing mlp.down_proj in layer 7/32...\n",
      "2024-11-06 03:57:50 INFO [auto_gptq.quantization.gptq] duration: 3.120558738708496\n",
      "2024-11-06 03:57:50 INFO [auto_gptq.quantization.gptq] avg loss: 2.542788028717041\n",
      "INFO - Start quantizing layer 8/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/32...\n",
      "2024-11-06 03:58:52 INFO [auto_gptq.quantization.gptq] duration: 0.8843283653259277\n",
      "2024-11-06 03:58:52 INFO [auto_gptq.quantization.gptq] avg loss: 185.44412231445312\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/32...\n",
      "2024-11-06 03:58:53 INFO [auto_gptq.quantization.gptq] duration: 0.6223065853118896\n",
      "2024-11-06 03:58:53 INFO [auto_gptq.quantization.gptq] avg loss: 14.249467849731445\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/32...\n",
      "2024-11-06 03:58:53 INFO [auto_gptq.quantization.gptq] duration: 0.6540648937225342\n",
      "2024-11-06 03:58:53 INFO [auto_gptq.quantization.gptq] avg loss: 296.3538818359375\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/32...\n",
      "2024-11-06 03:59:24 INFO [auto_gptq.quantization.gptq] duration: 0.8800845146179199\n",
      "2024-11-06 03:59:24 INFO [auto_gptq.quantization.gptq] avg loss: 1.2101187705993652\n",
      "INFO - Quantizing mlp.up_proj in layer 8/32...\n",
      "2024-11-06 03:59:56 INFO [auto_gptq.quantization.gptq] duration: 0.9250209331512451\n",
      "2024-11-06 03:59:56 INFO [auto_gptq.quantization.gptq] avg loss: 208.832763671875\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/32...\n",
      "2024-11-06 03:59:56 INFO [auto_gptq.quantization.gptq] duration: 0.6747956275939941\n",
      "2024-11-06 03:59:56 INFO [auto_gptq.quantization.gptq] avg loss: 342.98260498046875\n",
      "INFO - Quantizing mlp.down_proj in layer 8/32...\n",
      "2024-11-06 04:00:49 INFO [auto_gptq.quantization.gptq] duration: 3.13812518119812\n",
      "2024-11-06 04:00:49 INFO [auto_gptq.quantization.gptq] avg loss: 2.9702789783477783\n",
      "INFO - Start quantizing layer 9/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/32...\n",
      "2024-11-06 04:01:52 INFO [auto_gptq.quantization.gptq] duration: 0.8861453533172607\n",
      "2024-11-06 04:01:52 INFO [auto_gptq.quantization.gptq] avg loss: 243.98094177246094\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/32...\n",
      "2024-11-06 04:01:52 INFO [auto_gptq.quantization.gptq] duration: 0.622246503829956\n",
      "2024-11-06 04:01:52 INFO [auto_gptq.quantization.gptq] avg loss: 18.828811645507812\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/32...\n",
      "2024-11-06 04:01:53 INFO [auto_gptq.quantization.gptq] duration: 0.6529600620269775\n",
      "2024-11-06 04:01:53 INFO [auto_gptq.quantization.gptq] avg loss: 375.31890869140625\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/32...\n",
      "2024-11-06 04:02:23 INFO [auto_gptq.quantization.gptq] duration: 0.8742074966430664\n",
      "2024-11-06 04:02:23 INFO [auto_gptq.quantization.gptq] avg loss: 1.6671380996704102\n",
      "INFO - Quantizing mlp.up_proj in layer 9/32...\n",
      "2024-11-06 04:02:55 INFO [auto_gptq.quantization.gptq] duration: 0.9287393093109131\n",
      "2024-11-06 04:02:55 INFO [auto_gptq.quantization.gptq] avg loss: 216.60044860839844\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/32...\n",
      "2024-11-06 04:02:56 INFO [auto_gptq.quantization.gptq] duration: 0.6773121356964111\n",
      "2024-11-06 04:02:56 INFO [auto_gptq.quantization.gptq] avg loss: 363.3167724609375\n",
      "INFO - Quantizing mlp.down_proj in layer 9/32...\n",
      "2024-11-06 04:03:49 INFO [auto_gptq.quantization.gptq] duration: 3.1127755641937256\n",
      "2024-11-06 04:03:49 INFO [auto_gptq.quantization.gptq] avg loss: 3.158564329147339\n",
      "INFO - Start quantizing layer 10/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/32...\n",
      "2024-11-06 04:04:51 INFO [auto_gptq.quantization.gptq] duration: 0.8831899166107178\n",
      "2024-11-06 04:04:51 INFO [auto_gptq.quantization.gptq] avg loss: 226.21499633789062\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/32...\n",
      "2024-11-06 04:04:52 INFO [auto_gptq.quantization.gptq] duration: 0.6188113689422607\n",
      "2024-11-06 04:04:52 INFO [auto_gptq.quantization.gptq] avg loss: 25.311656951904297\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/32...\n",
      "2024-11-06 04:04:52 INFO [auto_gptq.quantization.gptq] duration: 0.6529288291931152\n",
      "2024-11-06 04:04:52 INFO [auto_gptq.quantization.gptq] avg loss: 355.7836608886719\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/32...\n",
      "2024-11-06 04:05:23 INFO [auto_gptq.quantization.gptq] duration: 0.8730611801147461\n",
      "2024-11-06 04:05:23 INFO [auto_gptq.quantization.gptq] avg loss: 1.6616812944412231\n",
      "INFO - Quantizing mlp.up_proj in layer 10/32...\n",
      "2024-11-06 04:05:55 INFO [auto_gptq.quantization.gptq] duration: 0.9260623455047607\n",
      "2024-11-06 04:05:55 INFO [auto_gptq.quantization.gptq] avg loss: 228.55050659179688\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/32...\n",
      "2024-11-06 04:05:56 INFO [auto_gptq.quantization.gptq] duration: 0.908531665802002\n",
      "2024-11-06 04:05:56 INFO [auto_gptq.quantization.gptq] avg loss: 386.546142578125\n",
      "INFO - Quantizing mlp.down_proj in layer 10/32...\n",
      "2024-11-06 04:06:49 INFO [auto_gptq.quantization.gptq] duration: 3.11899733543396\n",
      "2024-11-06 04:06:49 INFO [auto_gptq.quantization.gptq] avg loss: 3.473050117492676\n",
      "INFO - Start quantizing layer 11/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/32...\n",
      "2024-11-06 04:07:51 INFO [auto_gptq.quantization.gptq] duration: 0.8858194351196289\n",
      "2024-11-06 04:07:51 INFO [auto_gptq.quantization.gptq] avg loss: 294.4061584472656\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/32...\n",
      "2024-11-06 04:07:51 INFO [auto_gptq.quantization.gptq] duration: 0.6199827194213867\n",
      "2024-11-06 04:07:51 INFO [auto_gptq.quantization.gptq] avg loss: 20.68038558959961\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/32...\n",
      "2024-11-06 04:07:52 INFO [auto_gptq.quantization.gptq] duration: 0.6423125267028809\n",
      "2024-11-06 04:07:52 INFO [auto_gptq.quantization.gptq] avg loss: 442.34051513671875\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/32...\n",
      "2024-11-06 04:08:22 INFO [auto_gptq.quantization.gptq] duration: 0.8743517398834229\n",
      "2024-11-06 04:08:22 INFO [auto_gptq.quantization.gptq] avg loss: 1.9261951446533203\n",
      "INFO - Quantizing mlp.up_proj in layer 11/32...\n",
      "2024-11-06 04:08:54 INFO [auto_gptq.quantization.gptq] duration: 0.9278304576873779\n",
      "2024-11-06 04:08:54 INFO [auto_gptq.quantization.gptq] avg loss: 231.3968505859375\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/32...\n",
      "2024-11-06 04:08:55 INFO [auto_gptq.quantization.gptq] duration: 0.6766159534454346\n",
      "2024-11-06 04:08:55 INFO [auto_gptq.quantization.gptq] avg loss: 364.15826416015625\n",
      "INFO - Quantizing mlp.down_proj in layer 11/32...\n",
      "2024-11-06 04:09:48 INFO [auto_gptq.quantization.gptq] duration: 3.1120123863220215\n",
      "2024-11-06 04:09:48 INFO [auto_gptq.quantization.gptq] avg loss: 3.6732468605041504\n",
      "INFO - Start quantizing layer 12/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/32...\n",
      "2024-11-06 04:10:50 INFO [auto_gptq.quantization.gptq] duration: 0.8833777904510498\n",
      "2024-11-06 04:10:50 INFO [auto_gptq.quantization.gptq] avg loss: 250.51181030273438\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/32...\n",
      "2024-11-06 04:10:51 INFO [auto_gptq.quantization.gptq] duration: 0.617692232131958\n",
      "2024-11-06 04:10:51 INFO [auto_gptq.quantization.gptq] avg loss: 19.88942527770996\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/32...\n",
      "2024-11-06 04:10:52 INFO [auto_gptq.quantization.gptq] duration: 0.6544013023376465\n",
      "2024-11-06 04:10:52 INFO [auto_gptq.quantization.gptq] avg loss: 364.98736572265625\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/32...\n",
      "2024-11-06 04:11:22 INFO [auto_gptq.quantization.gptq] duration: 0.8744127750396729\n",
      "2024-11-06 04:11:22 INFO [auto_gptq.quantization.gptq] avg loss: 1.9355721473693848\n",
      "INFO - Quantizing mlp.up_proj in layer 12/32...\n",
      "2024-11-06 04:11:54 INFO [auto_gptq.quantization.gptq] duration: 1.2997286319732666\n",
      "2024-11-06 04:11:54 INFO [auto_gptq.quantization.gptq] avg loss: 246.39659118652344\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/32...\n",
      "2024-11-06 04:11:55 INFO [auto_gptq.quantization.gptq] duration: 0.9983603954315186\n",
      "2024-11-06 04:11:55 INFO [auto_gptq.quantization.gptq] avg loss: 376.3271484375\n",
      "INFO - Quantizing mlp.down_proj in layer 12/32...\n",
      "2024-11-06 04:12:48 INFO [auto_gptq.quantization.gptq] duration: 3.112536907196045\n",
      "2024-11-06 04:12:48 INFO [auto_gptq.quantization.gptq] avg loss: 4.052737236022949\n",
      "INFO - Start quantizing layer 13/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/32...\n",
      "2024-11-06 04:13:51 INFO [auto_gptq.quantization.gptq] duration: 0.8852725028991699\n",
      "2024-11-06 04:13:51 INFO [auto_gptq.quantization.gptq] avg loss: 188.81475830078125\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/32...\n",
      "2024-11-06 04:13:51 INFO [auto_gptq.quantization.gptq] duration: 0.6165266036987305\n",
      "2024-11-06 04:13:51 INFO [auto_gptq.quantization.gptq] avg loss: 23.14733123779297\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/32...\n",
      "2024-11-06 04:13:52 INFO [auto_gptq.quantization.gptq] duration: 0.6387584209442139\n",
      "2024-11-06 04:13:52 INFO [auto_gptq.quantization.gptq] avg loss: 315.41796875\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/32...\n",
      "2024-11-06 04:14:22 INFO [auto_gptq.quantization.gptq] duration: 0.874302864074707\n",
      "2024-11-06 04:14:22 INFO [auto_gptq.quantization.gptq] avg loss: 2.633234977722168\n",
      "INFO - Quantizing mlp.up_proj in layer 13/32...\n",
      "2024-11-06 04:14:54 INFO [auto_gptq.quantization.gptq] duration: 0.921673059463501\n",
      "2024-11-06 04:14:54 INFO [auto_gptq.quantization.gptq] avg loss: 258.1624755859375\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/32...\n",
      "2024-11-06 04:14:55 INFO [auto_gptq.quantization.gptq] duration: 0.6772284507751465\n",
      "2024-11-06 04:14:55 INFO [auto_gptq.quantization.gptq] avg loss: 375.35260009765625\n",
      "INFO - Quantizing mlp.down_proj in layer 13/32...\n",
      "2024-11-06 04:15:48 INFO [auto_gptq.quantization.gptq] duration: 3.119781970977783\n",
      "2024-11-06 04:15:48 INFO [auto_gptq.quantization.gptq] avg loss: 4.5864667892456055\n",
      "INFO - Start quantizing layer 14/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/32...\n",
      "2024-11-06 04:16:50 INFO [auto_gptq.quantization.gptq] duration: 0.8873553276062012\n",
      "2024-11-06 04:16:50 INFO [auto_gptq.quantization.gptq] avg loss: 298.60772705078125\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/32...\n",
      "2024-11-06 04:16:51 INFO [auto_gptq.quantization.gptq] duration: 0.6204650402069092\n",
      "2024-11-06 04:16:51 INFO [auto_gptq.quantization.gptq] avg loss: 25.771652221679688\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/32...\n",
      "2024-11-06 04:16:51 INFO [auto_gptq.quantization.gptq] duration: 0.6552302837371826\n",
      "2024-11-06 04:16:51 INFO [auto_gptq.quantization.gptq] avg loss: 419.00250244140625\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/32...\n",
      "2024-11-06 04:17:21 INFO [auto_gptq.quantization.gptq] duration: 0.8767244815826416\n",
      "2024-11-06 04:17:21 INFO [auto_gptq.quantization.gptq] avg loss: 2.7584381103515625\n",
      "INFO - Quantizing mlp.up_proj in layer 14/32...\n",
      "2024-11-06 04:17:53 INFO [auto_gptq.quantization.gptq] duration: 0.9249057769775391\n",
      "2024-11-06 04:17:53 INFO [auto_gptq.quantization.gptq] avg loss: 274.3155517578125\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/32...\n",
      "2024-11-06 04:17:54 INFO [auto_gptq.quantization.gptq] duration: 0.6753747463226318\n",
      "2024-11-06 04:17:54 INFO [auto_gptq.quantization.gptq] avg loss: 404.3730773925781\n",
      "INFO - Quantizing mlp.down_proj in layer 14/32...\n",
      "2024-11-06 04:18:47 INFO [auto_gptq.quantization.gptq] duration: 3.1269164085388184\n",
      "2024-11-06 04:18:47 INFO [auto_gptq.quantization.gptq] avg loss: 5.0370941162109375\n",
      "INFO - Start quantizing layer 15/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/32...\n",
      "2024-11-06 04:19:50 INFO [auto_gptq.quantization.gptq] duration: 0.8855986595153809\n",
      "2024-11-06 04:19:50 INFO [auto_gptq.quantization.gptq] avg loss: 334.58447265625\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/32...\n",
      "2024-11-06 04:19:50 INFO [auto_gptq.quantization.gptq] duration: 0.6190075874328613\n",
      "2024-11-06 04:19:50 INFO [auto_gptq.quantization.gptq] avg loss: 27.91973114013672\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/32...\n",
      "2024-11-06 04:19:51 INFO [auto_gptq.quantization.gptq] duration: 0.652667760848999\n",
      "2024-11-06 04:19:51 INFO [auto_gptq.quantization.gptq] avg loss: 445.89056396484375\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/32...\n",
      "2024-11-06 04:20:21 INFO [auto_gptq.quantization.gptq] duration: 0.8773844242095947\n",
      "2024-11-06 04:20:21 INFO [auto_gptq.quantization.gptq] avg loss: 3.362886667251587\n",
      "INFO - Quantizing mlp.up_proj in layer 15/32...\n",
      "2024-11-06 04:20:53 INFO [auto_gptq.quantization.gptq] duration: 0.9261298179626465\n",
      "2024-11-06 04:20:53 INFO [auto_gptq.quantization.gptq] avg loss: 303.91656494140625\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/32...\n",
      "2024-11-06 04:20:54 INFO [auto_gptq.quantization.gptq] duration: 0.6729717254638672\n",
      "2024-11-06 04:20:54 INFO [auto_gptq.quantization.gptq] avg loss: 482.343994140625\n",
      "INFO - Quantizing mlp.down_proj in layer 15/32...\n",
      "2024-11-06 04:21:47 INFO [auto_gptq.quantization.gptq] duration: 3.115557909011841\n",
      "2024-11-06 04:21:47 INFO [auto_gptq.quantization.gptq] avg loss: 6.458828926086426\n",
      "INFO - Start quantizing layer 16/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/32...\n",
      "2024-11-06 04:22:49 INFO [auto_gptq.quantization.gptq] duration: 0.8856348991394043\n",
      "2024-11-06 04:22:49 INFO [auto_gptq.quantization.gptq] avg loss: 291.2372741699219\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/32...\n",
      "2024-11-06 04:22:50 INFO [auto_gptq.quantization.gptq] duration: 0.6221270561218262\n",
      "2024-11-06 04:22:50 INFO [auto_gptq.quantization.gptq] avg loss: 33.72458267211914\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/32...\n",
      "2024-11-06 04:22:50 INFO [auto_gptq.quantization.gptq] duration: 0.6554114818572998\n",
      "2024-11-06 04:22:50 INFO [auto_gptq.quantization.gptq] avg loss: 514.0741577148438\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/32...\n",
      "2024-11-06 04:23:20 INFO [auto_gptq.quantization.gptq] duration: 0.8770954608917236\n",
      "2024-11-06 04:23:20 INFO [auto_gptq.quantization.gptq] avg loss: 3.1065335273742676\n",
      "INFO - Quantizing mlp.up_proj in layer 16/32...\n",
      "2024-11-06 04:23:53 INFO [auto_gptq.quantization.gptq] duration: 0.9288697242736816\n",
      "2024-11-06 04:23:53 INFO [auto_gptq.quantization.gptq] avg loss: 320.1373291015625\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/32...\n",
      "2024-11-06 04:23:53 INFO [auto_gptq.quantization.gptq] duration: 0.67500901222229\n",
      "2024-11-06 04:23:53 INFO [auto_gptq.quantization.gptq] avg loss: 537.627685546875\n",
      "INFO - Quantizing mlp.down_proj in layer 16/32...\n",
      "2024-11-06 04:24:46 INFO [auto_gptq.quantization.gptq] duration: 3.1301465034484863\n",
      "2024-11-06 04:24:46 INFO [auto_gptq.quantization.gptq] avg loss: 7.751988887786865\n",
      "INFO - Start quantizing layer 17/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/32...\n",
      "2024-11-06 04:25:49 INFO [auto_gptq.quantization.gptq] duration: 0.912315845489502\n",
      "2024-11-06 04:25:49 INFO [auto_gptq.quantization.gptq] avg loss: 304.3836669921875\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/32...\n",
      "2024-11-06 04:25:49 INFO [auto_gptq.quantization.gptq] duration: 0.6303572654724121\n",
      "2024-11-06 04:25:49 INFO [auto_gptq.quantization.gptq] avg loss: 31.485580444335938\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/32...\n",
      "2024-11-06 04:25:50 INFO [auto_gptq.quantization.gptq] duration: 0.6459059715270996\n",
      "2024-11-06 04:25:50 INFO [auto_gptq.quantization.gptq] avg loss: 488.8138427734375\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/32...\n",
      "2024-11-06 04:26:20 INFO [auto_gptq.quantization.gptq] duration: 0.8783209323883057\n",
      "2024-11-06 04:26:20 INFO [auto_gptq.quantization.gptq] avg loss: 2.8860976696014404\n",
      "INFO - Quantizing mlp.up_proj in layer 17/32...\n",
      "2024-11-06 04:26:52 INFO [auto_gptq.quantization.gptq] duration: 0.931588888168335\n",
      "2024-11-06 04:26:52 INFO [auto_gptq.quantization.gptq] avg loss: 336.9053649902344\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/32...\n",
      "2024-11-06 04:26:53 INFO [auto_gptq.quantization.gptq] duration: 0.687218427658081\n",
      "2024-11-06 04:26:53 INFO [auto_gptq.quantization.gptq] avg loss: 595.1361694335938\n",
      "INFO - Quantizing mlp.down_proj in layer 17/32...\n",
      "2024-11-06 04:27:46 INFO [auto_gptq.quantization.gptq] duration: 3.124692678451538\n",
      "2024-11-06 04:27:46 INFO [auto_gptq.quantization.gptq] avg loss: 8.380467414855957\n",
      "INFO - Start quantizing layer 18/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/32...\n",
      "2024-11-06 04:28:48 INFO [auto_gptq.quantization.gptq] duration: 0.886481523513794\n",
      "2024-11-06 04:28:48 INFO [auto_gptq.quantization.gptq] avg loss: 329.1937255859375\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/32...\n",
      "2024-11-06 04:28:49 INFO [auto_gptq.quantization.gptq] duration: 0.6200182437896729\n",
      "2024-11-06 04:28:49 INFO [auto_gptq.quantization.gptq] avg loss: 34.41559600830078\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/32...\n",
      "2024-11-06 04:28:49 INFO [auto_gptq.quantization.gptq] duration: 0.6534552574157715\n",
      "2024-11-06 04:28:49 INFO [auto_gptq.quantization.gptq] avg loss: 507.52972412109375\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/32...\n",
      "2024-11-06 04:29:19 INFO [auto_gptq.quantization.gptq] duration: 0.8748157024383545\n",
      "2024-11-06 04:29:19 INFO [auto_gptq.quantization.gptq] avg loss: 2.3883204460144043\n",
      "INFO - Quantizing mlp.up_proj in layer 18/32...\n",
      "2024-11-06 04:29:52 INFO [auto_gptq.quantization.gptq] duration: 0.9335341453552246\n",
      "2024-11-06 04:29:52 INFO [auto_gptq.quantization.gptq] avg loss: 351.29400634765625\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/32...\n",
      "2024-11-06 04:29:52 INFO [auto_gptq.quantization.gptq] duration: 0.6816675662994385\n",
      "2024-11-06 04:29:52 INFO [auto_gptq.quantization.gptq] avg loss: 638.7813720703125\n",
      "INFO - Quantizing mlp.down_proj in layer 18/32...\n",
      "2024-11-06 04:30:45 INFO [auto_gptq.quantization.gptq] duration: 3.124307155609131\n",
      "2024-11-06 04:30:45 INFO [auto_gptq.quantization.gptq] avg loss: 9.917938232421875\n",
      "INFO - Start quantizing layer 19/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/32...\n",
      "2024-11-06 04:31:48 INFO [auto_gptq.quantization.gptq] duration: 0.8854269981384277\n",
      "2024-11-06 04:31:48 INFO [auto_gptq.quantization.gptq] avg loss: 362.5993957519531\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/32...\n",
      "2024-11-06 04:31:48 INFO [auto_gptq.quantization.gptq] duration: 0.6197605133056641\n",
      "2024-11-06 04:31:48 INFO [auto_gptq.quantization.gptq] avg loss: 34.763648986816406\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/32...\n",
      "2024-11-06 04:31:49 INFO [auto_gptq.quantization.gptq] duration: 0.6681292057037354\n",
      "2024-11-06 04:31:49 INFO [auto_gptq.quantization.gptq] avg loss: 511.8601989746094\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/32...\n",
      "2024-11-06 04:32:19 INFO [auto_gptq.quantization.gptq] duration: 0.8732986450195312\n",
      "2024-11-06 04:32:19 INFO [auto_gptq.quantization.gptq] avg loss: 1.7039899826049805\n",
      "INFO - Quantizing mlp.up_proj in layer 19/32...\n",
      "2024-11-06 04:32:51 INFO [auto_gptq.quantization.gptq] duration: 0.9251542091369629\n",
      "2024-11-06 04:32:51 INFO [auto_gptq.quantization.gptq] avg loss: 368.2109375\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/32...\n",
      "2024-11-06 04:32:52 INFO [auto_gptq.quantization.gptq] duration: 0.673774003982544\n",
      "2024-11-06 04:32:52 INFO [auto_gptq.quantization.gptq] avg loss: 673.2882080078125\n",
      "INFO - Quantizing mlp.down_proj in layer 19/32...\n",
      "2024-11-06 04:33:45 INFO [auto_gptq.quantization.gptq] duration: 3.109367847442627\n",
      "2024-11-06 04:33:45 INFO [auto_gptq.quantization.gptq] avg loss: 9.90238094329834\n",
      "INFO - Start quantizing layer 20/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/32...\n",
      "2024-11-06 04:34:47 INFO [auto_gptq.quantization.gptq] duration: 0.8882613182067871\n",
      "2024-11-06 04:34:47 INFO [auto_gptq.quantization.gptq] avg loss: 330.3059997558594\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/32...\n",
      "2024-11-06 04:34:48 INFO [auto_gptq.quantization.gptq] duration: 0.6213798522949219\n",
      "2024-11-06 04:34:48 INFO [auto_gptq.quantization.gptq] avg loss: 36.47534942626953\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/32...\n",
      "2024-11-06 04:34:48 INFO [auto_gptq.quantization.gptq] duration: 0.6547250747680664\n",
      "2024-11-06 04:34:48 INFO [auto_gptq.quantization.gptq] avg loss: 521.5543823242188\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/32...\n",
      "2024-11-06 04:35:18 INFO [auto_gptq.quantization.gptq] duration: 0.8731184005737305\n",
      "2024-11-06 04:35:18 INFO [auto_gptq.quantization.gptq] avg loss: 1.4498741626739502\n",
      "INFO - Quantizing mlp.up_proj in layer 20/32...\n",
      "2024-11-06 04:35:51 INFO [auto_gptq.quantization.gptq] duration: 0.9273695945739746\n",
      "2024-11-06 04:35:51 INFO [auto_gptq.quantization.gptq] avg loss: 385.9858093261719\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/32...\n",
      "2024-11-06 04:35:51 INFO [auto_gptq.quantization.gptq] duration: 0.6746447086334229\n",
      "2024-11-06 04:35:51 INFO [auto_gptq.quantization.gptq] avg loss: 720.6495971679688\n",
      "INFO - Quantizing mlp.down_proj in layer 20/32...\n",
      "2024-11-06 04:36:44 INFO [auto_gptq.quantization.gptq] duration: 3.120296001434326\n",
      "2024-11-06 04:36:44 INFO [auto_gptq.quantization.gptq] avg loss: 10.731500625610352\n",
      "INFO - Start quantizing layer 21/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/32...\n",
      "2024-11-06 04:37:47 INFO [auto_gptq.quantization.gptq] duration: 0.8853309154510498\n",
      "2024-11-06 04:37:47 INFO [auto_gptq.quantization.gptq] avg loss: 375.40557861328125\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/32...\n",
      "2024-11-06 04:37:47 INFO [auto_gptq.quantization.gptq] duration: 0.6171081066131592\n",
      "2024-11-06 04:37:47 INFO [auto_gptq.quantization.gptq] avg loss: 42.064117431640625\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/32...\n",
      "2024-11-06 04:37:48 INFO [auto_gptq.quantization.gptq] duration: 0.639779806137085\n",
      "2024-11-06 04:37:48 INFO [auto_gptq.quantization.gptq] avg loss: 548.99658203125\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/32...\n",
      "2024-11-06 04:38:18 INFO [auto_gptq.quantization.gptq] duration: 0.8737349510192871\n",
      "2024-11-06 04:38:18 INFO [auto_gptq.quantization.gptq] avg loss: 1.5379877090454102\n",
      "INFO - Quantizing mlp.up_proj in layer 21/32...\n",
      "2024-11-06 04:38:50 INFO [auto_gptq.quantization.gptq] duration: 0.9236836433410645\n",
      "2024-11-06 04:38:50 INFO [auto_gptq.quantization.gptq] avg loss: 416.31280517578125\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/32...\n",
      "2024-11-06 04:38:51 INFO [auto_gptq.quantization.gptq] duration: 0.6779956817626953\n",
      "2024-11-06 04:38:51 INFO [auto_gptq.quantization.gptq] avg loss: 769.452880859375\n",
      "INFO - Quantizing mlp.down_proj in layer 21/32...\n",
      "2024-11-06 04:39:44 INFO [auto_gptq.quantization.gptq] duration: 3.1077754497528076\n",
      "2024-11-06 04:39:44 INFO [auto_gptq.quantization.gptq] avg loss: 11.57577896118164\n",
      "INFO - Start quantizing layer 22/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/32...\n",
      "2024-11-06 04:40:46 INFO [auto_gptq.quantization.gptq] duration: 0.8844385147094727\n",
      "2024-11-06 04:40:46 INFO [auto_gptq.quantization.gptq] avg loss: 350.2862548828125\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/32...\n",
      "2024-11-06 04:40:47 INFO [auto_gptq.quantization.gptq] duration: 0.6208152770996094\n",
      "2024-11-06 04:40:47 INFO [auto_gptq.quantization.gptq] avg loss: 44.69572067260742\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/32...\n",
      "2024-11-06 04:40:47 INFO [auto_gptq.quantization.gptq] duration: 0.6527080535888672\n",
      "2024-11-06 04:40:47 INFO [auto_gptq.quantization.gptq] avg loss: 525.7906494140625\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/32...\n",
      "2024-11-06 04:41:17 INFO [auto_gptq.quantization.gptq] duration: 0.8744211196899414\n",
      "2024-11-06 04:41:17 INFO [auto_gptq.quantization.gptq] avg loss: 2.4319984912872314\n",
      "INFO - Quantizing mlp.up_proj in layer 22/32...\n",
      "2024-11-06 04:41:49 INFO [auto_gptq.quantization.gptq] duration: 0.9282524585723877\n",
      "2024-11-06 04:41:49 INFO [auto_gptq.quantization.gptq] avg loss: 445.2806701660156\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/32...\n",
      "2024-11-06 04:41:50 INFO [auto_gptq.quantization.gptq] duration: 0.6738531589508057\n",
      "2024-11-06 04:41:50 INFO [auto_gptq.quantization.gptq] avg loss: 834.3740844726562\n",
      "INFO - Quantizing mlp.down_proj in layer 22/32...\n",
      "2024-11-06 04:42:43 INFO [auto_gptq.quantization.gptq] duration: 3.123236656188965\n",
      "2024-11-06 04:42:43 INFO [auto_gptq.quantization.gptq] avg loss: 13.963398933410645\n",
      "INFO - Start quantizing layer 23/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/32...\n",
      "2024-11-06 04:43:45 INFO [auto_gptq.quantization.gptq] duration: 0.8831770420074463\n",
      "2024-11-06 04:43:45 INFO [auto_gptq.quantization.gptq] avg loss: 382.9090270996094\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/32...\n",
      "2024-11-06 04:43:46 INFO [auto_gptq.quantization.gptq] duration: 0.6240484714508057\n",
      "2024-11-06 04:43:46 INFO [auto_gptq.quantization.gptq] avg loss: 52.359535217285156\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/32...\n",
      "2024-11-06 04:43:47 INFO [auto_gptq.quantization.gptq] duration: 0.653076171875\n",
      "2024-11-06 04:43:47 INFO [auto_gptq.quantization.gptq] avg loss: 538.3993530273438\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/32...\n",
      "2024-11-06 04:44:17 INFO [auto_gptq.quantization.gptq] duration: 0.8793246746063232\n",
      "2024-11-06 04:44:17 INFO [auto_gptq.quantization.gptq] avg loss: 2.2333288192749023\n",
      "INFO - Quantizing mlp.up_proj in layer 23/32...\n",
      "2024-11-06 04:44:49 INFO [auto_gptq.quantization.gptq] duration: 0.927009105682373\n",
      "2024-11-06 04:44:49 INFO [auto_gptq.quantization.gptq] avg loss: 464.9842529296875\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/32...\n",
      "2024-11-06 04:44:50 INFO [auto_gptq.quantization.gptq] duration: 1.0354571342468262\n",
      "2024-11-06 04:44:50 INFO [auto_gptq.quantization.gptq] avg loss: 860.2703857421875\n",
      "INFO - Quantizing mlp.down_proj in layer 23/32...\n",
      "2024-11-06 04:45:43 INFO [auto_gptq.quantization.gptq] duration: 3.1320414543151855\n",
      "2024-11-06 04:45:43 INFO [auto_gptq.quantization.gptq] avg loss: 14.423909187316895\n",
      "INFO - Start quantizing layer 24/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/32...\n",
      "2024-11-06 04:46:45 INFO [auto_gptq.quantization.gptq] duration: 0.8886387348175049\n",
      "2024-11-06 04:46:45 INFO [auto_gptq.quantization.gptq] avg loss: 383.82379150390625\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/32...\n",
      "2024-11-06 04:46:46 INFO [auto_gptq.quantization.gptq] duration: 0.6241321563720703\n",
      "2024-11-06 04:46:46 INFO [auto_gptq.quantization.gptq] avg loss: 56.29400634765625\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/32...\n",
      "2024-11-06 04:46:46 INFO [auto_gptq.quantization.gptq] duration: 0.6584267616271973\n",
      "2024-11-06 04:46:47 INFO [auto_gptq.quantization.gptq] avg loss: 556.9692993164062\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/32...\n",
      "2024-11-06 04:47:17 INFO [auto_gptq.quantization.gptq] duration: 0.8772032260894775\n",
      "2024-11-06 04:47:17 INFO [auto_gptq.quantization.gptq] avg loss: 2.0297906398773193\n",
      "INFO - Quantizing mlp.up_proj in layer 24/32...\n",
      "2024-11-06 04:47:49 INFO [auto_gptq.quantization.gptq] duration: 0.9292457103729248\n",
      "2024-11-06 04:47:49 INFO [auto_gptq.quantization.gptq] avg loss: 494.72674560546875\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/32...\n",
      "2024-11-06 04:47:49 INFO [auto_gptq.quantization.gptq] duration: 0.6776440143585205\n",
      "2024-11-06 04:47:49 INFO [auto_gptq.quantization.gptq] avg loss: 909.8065185546875\n",
      "INFO - Quantizing mlp.down_proj in layer 24/32...\n",
      "2024-11-06 04:48:42 INFO [auto_gptq.quantization.gptq] duration: 3.128838062286377\n",
      "2024-11-06 04:48:42 INFO [auto_gptq.quantization.gptq] avg loss: 15.375802040100098\n",
      "INFO - Start quantizing layer 25/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/32...\n",
      "2024-11-06 04:49:45 INFO [auto_gptq.quantization.gptq] duration: 1.0511589050292969\n",
      "2024-11-06 04:49:45 INFO [auto_gptq.quantization.gptq] avg loss: 371.7705078125\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/32...\n",
      "2024-11-06 04:49:45 INFO [auto_gptq.quantization.gptq] duration: 0.616584300994873\n",
      "2024-11-06 04:49:45 INFO [auto_gptq.quantization.gptq] avg loss: 72.55015563964844\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/32...\n",
      "2024-11-06 04:49:46 INFO [auto_gptq.quantization.gptq] duration: 1.029059886932373\n",
      "2024-11-06 04:49:46 INFO [auto_gptq.quantization.gptq] avg loss: 553.376953125\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/32...\n",
      "2024-11-06 04:50:17 INFO [auto_gptq.quantization.gptq] duration: 0.8765766620635986\n",
      "2024-11-06 04:50:17 INFO [auto_gptq.quantization.gptq] avg loss: 2.682605743408203\n",
      "INFO - Quantizing mlp.up_proj in layer 25/32...\n",
      "2024-11-06 04:50:49 INFO [auto_gptq.quantization.gptq] duration: 0.9282796382904053\n",
      "2024-11-06 04:50:49 INFO [auto_gptq.quantization.gptq] avg loss: 528.88037109375\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/32...\n",
      "2024-11-06 04:50:49 INFO [auto_gptq.quantization.gptq] duration: 0.6778197288513184\n",
      "2024-11-06 04:50:49 INFO [auto_gptq.quantization.gptq] avg loss: 972.2393798828125\n",
      "INFO - Quantizing mlp.down_proj in layer 25/32...\n",
      "2024-11-06 04:51:42 INFO [auto_gptq.quantization.gptq] duration: 3.1241328716278076\n",
      "2024-11-06 04:51:42 INFO [auto_gptq.quantization.gptq] avg loss: 16.49424934387207\n",
      "INFO - Start quantizing layer 26/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/32...\n",
      "2024-11-06 04:52:45 INFO [auto_gptq.quantization.gptq] duration: 0.8893704414367676\n",
      "2024-11-06 04:52:45 INFO [auto_gptq.quantization.gptq] avg loss: 355.6899719238281\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/32...\n",
      "2024-11-06 04:52:45 INFO [auto_gptq.quantization.gptq] duration: 0.6233737468719482\n",
      "2024-11-06 04:52:45 INFO [auto_gptq.quantization.gptq] avg loss: 75.22178649902344\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/32...\n",
      "2024-11-06 04:52:46 INFO [auto_gptq.quantization.gptq] duration: 0.6562559604644775\n",
      "2024-11-06 04:52:46 INFO [auto_gptq.quantization.gptq] avg loss: 555.29345703125\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/32...\n",
      "2024-11-06 04:53:16 INFO [auto_gptq.quantization.gptq] duration: 0.873664379119873\n",
      "2024-11-06 04:53:16 INFO [auto_gptq.quantization.gptq] avg loss: 2.995260238647461\n",
      "INFO - Quantizing mlp.up_proj in layer 26/32...\n",
      "2024-11-06 04:53:48 INFO [auto_gptq.quantization.gptq] duration: 0.9261627197265625\n",
      "2024-11-06 04:53:48 INFO [auto_gptq.quantization.gptq] avg loss: 572.870361328125\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/32...\n",
      "2024-11-06 04:53:49 INFO [auto_gptq.quantization.gptq] duration: 0.6946048736572266\n",
      "2024-11-06 04:53:49 INFO [auto_gptq.quantization.gptq] avg loss: 1049.382080078125\n",
      "INFO - Quantizing mlp.down_proj in layer 26/32...\n",
      "2024-11-06 04:54:42 INFO [auto_gptq.quantization.gptq] duration: 3.112515687942505\n",
      "2024-11-06 04:54:42 INFO [auto_gptq.quantization.gptq] avg loss: 18.490041732788086\n",
      "INFO - Start quantizing layer 27/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/32...\n",
      "2024-11-06 04:55:44 INFO [auto_gptq.quantization.gptq] duration: 0.8837041854858398\n",
      "2024-11-06 04:55:44 INFO [auto_gptq.quantization.gptq] avg loss: 382.33721923828125\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/32...\n",
      "2024-11-06 04:55:45 INFO [auto_gptq.quantization.gptq] duration: 0.6218018531799316\n",
      "2024-11-06 04:55:45 INFO [auto_gptq.quantization.gptq] avg loss: 74.65904998779297\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/32...\n",
      "2024-11-06 04:55:45 INFO [auto_gptq.quantization.gptq] duration: 0.6544709205627441\n",
      "2024-11-06 04:55:45 INFO [auto_gptq.quantization.gptq] avg loss: 539.3048706054688\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/32...\n",
      "2024-11-06 04:56:16 INFO [auto_gptq.quantization.gptq] duration: 0.8773293495178223\n",
      "2024-11-06 04:56:16 INFO [auto_gptq.quantization.gptq] avg loss: 4.149086952209473\n",
      "INFO - Quantizing mlp.up_proj in layer 27/32...\n",
      "2024-11-06 04:56:48 INFO [auto_gptq.quantization.gptq] duration: 0.9253723621368408\n",
      "2024-11-06 04:56:48 INFO [auto_gptq.quantization.gptq] avg loss: 621.6737060546875\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/32...\n",
      "2024-11-06 04:56:48 INFO [auto_gptq.quantization.gptq] duration: 0.673302173614502\n",
      "2024-11-06 04:56:48 INFO [auto_gptq.quantization.gptq] avg loss: 1143.1080322265625\n",
      "INFO - Quantizing mlp.down_proj in layer 27/32...\n",
      "2024-11-06 04:57:41 INFO [auto_gptq.quantization.gptq] duration: 3.116442918777466\n",
      "2024-11-06 04:57:41 INFO [auto_gptq.quantization.gptq] avg loss: 21.292688369750977\n",
      "INFO - Start quantizing layer 28/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/32...\n",
      "2024-11-06 04:58:44 INFO [auto_gptq.quantization.gptq] duration: 0.8849022388458252\n",
      "2024-11-06 04:58:44 INFO [auto_gptq.quantization.gptq] avg loss: 419.7774963378906\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/32...\n",
      "2024-11-06 04:58:44 INFO [auto_gptq.quantization.gptq] duration: 0.6193876266479492\n",
      "2024-11-06 04:58:44 INFO [auto_gptq.quantization.gptq] avg loss: 106.3179931640625\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/32...\n",
      "2024-11-06 04:58:45 INFO [auto_gptq.quantization.gptq] duration: 0.6531901359558105\n",
      "2024-11-06 04:58:45 INFO [auto_gptq.quantization.gptq] avg loss: 570.5443115234375\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/32...\n",
      "2024-11-06 04:59:15 INFO [auto_gptq.quantization.gptq] duration: 0.8740484714508057\n",
      "2024-11-06 04:59:15 INFO [auto_gptq.quantization.gptq] avg loss: 5.926787376403809\n",
      "INFO - Quantizing mlp.up_proj in layer 28/32...\n",
      "2024-11-06 04:59:47 INFO [auto_gptq.quantization.gptq] duration: 0.9262294769287109\n",
      "2024-11-06 04:59:47 INFO [auto_gptq.quantization.gptq] avg loss: 682.98779296875\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/32...\n",
      "2024-11-06 04:59:48 INFO [auto_gptq.quantization.gptq] duration: 0.676354169845581\n",
      "2024-11-06 04:59:48 INFO [auto_gptq.quantization.gptq] avg loss: 1240.0115966796875\n",
      "INFO - Quantizing mlp.down_proj in layer 28/32...\n",
      "2024-11-06 05:00:41 INFO [auto_gptq.quantization.gptq] duration: 3.1173532009124756\n",
      "2024-11-06 05:00:41 INFO [auto_gptq.quantization.gptq] avg loss: 25.770559310913086\n",
      "INFO - Start quantizing layer 29/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/32...\n",
      "2024-11-06 05:01:43 INFO [auto_gptq.quantization.gptq] duration: 0.8874199390411377\n",
      "2024-11-06 05:01:43 INFO [auto_gptq.quantization.gptq] avg loss: 330.9215087890625\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/32...\n",
      "2024-11-06 05:01:44 INFO [auto_gptq.quantization.gptq] duration: 0.6210439205169678\n",
      "2024-11-06 05:01:44 INFO [auto_gptq.quantization.gptq] avg loss: 94.48287963867188\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/32...\n",
      "2024-11-06 05:01:44 INFO [auto_gptq.quantization.gptq] duration: 0.6418430805206299\n",
      "2024-11-06 05:01:44 INFO [auto_gptq.quantization.gptq] avg loss: 514.7077026367188\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/32...\n",
      "2024-11-06 05:02:14 INFO [auto_gptq.quantization.gptq] duration: 0.8782639503479004\n",
      "2024-11-06 05:02:14 INFO [auto_gptq.quantization.gptq] avg loss: 7.232316970825195\n",
      "INFO - Quantizing mlp.up_proj in layer 29/32...\n",
      "2024-11-06 05:02:47 INFO [auto_gptq.quantization.gptq] duration: 0.922982931137085\n",
      "2024-11-06 05:02:47 INFO [auto_gptq.quantization.gptq] avg loss: 767.3657836914062\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/32...\n",
      "2024-11-06 05:02:47 INFO [auto_gptq.quantization.gptq] duration: 0.6760315895080566\n",
      "2024-11-06 05:02:47 INFO [auto_gptq.quantization.gptq] avg loss: 1328.826171875\n",
      "INFO - Quantizing mlp.down_proj in layer 29/32...\n",
      "2024-11-06 05:03:40 INFO [auto_gptq.quantization.gptq] duration: 3.115863084793091\n",
      "2024-11-06 05:03:40 INFO [auto_gptq.quantization.gptq] avg loss: 33.44029998779297\n",
      "INFO - Start quantizing layer 30/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/32...\n",
      "2024-11-06 05:04:43 INFO [auto_gptq.quantization.gptq] duration: 0.892392635345459\n",
      "2024-11-06 05:04:43 INFO [auto_gptq.quantization.gptq] avg loss: 356.5555725097656\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/32...\n",
      "2024-11-06 05:04:43 INFO [auto_gptq.quantization.gptq] duration: 0.6356966495513916\n",
      "2024-11-06 05:04:43 INFO [auto_gptq.quantization.gptq] avg loss: 114.91954803466797\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/32...\n",
      "2024-11-06 05:04:44 INFO [auto_gptq.quantization.gptq] duration: 0.6630890369415283\n",
      "2024-11-06 05:04:44 INFO [auto_gptq.quantization.gptq] avg loss: 569.2784423828125\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/32...\n",
      "2024-11-06 05:05:14 INFO [auto_gptq.quantization.gptq] duration: 0.882286548614502\n",
      "2024-11-06 05:05:14 INFO [auto_gptq.quantization.gptq] avg loss: 8.773412704467773\n",
      "INFO - Quantizing mlp.up_proj in layer 30/32...\n",
      "2024-11-06 05:05:46 INFO [auto_gptq.quantization.gptq] duration: 0.933013916015625\n",
      "2024-11-06 05:05:46 INFO [auto_gptq.quantization.gptq] avg loss: 807.730224609375\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/32...\n",
      "2024-11-06 05:05:47 INFO [auto_gptq.quantization.gptq] duration: 0.6812100410461426\n",
      "2024-11-06 05:05:47 INFO [auto_gptq.quantization.gptq] avg loss: 1325.53369140625\n",
      "INFO - Quantizing mlp.down_proj in layer 30/32...\n",
      "2024-11-06 05:06:40 INFO [auto_gptq.quantization.gptq] duration: 3.1287214756011963\n",
      "2024-11-06 05:06:40 INFO [auto_gptq.quantization.gptq] avg loss: 45.205299377441406\n",
      "INFO - Start quantizing layer 31/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/32...\n",
      "2024-11-06 05:07:42 INFO [auto_gptq.quantization.gptq] duration: 0.8852822780609131\n",
      "2024-11-06 05:07:42 INFO [auto_gptq.quantization.gptq] avg loss: 357.8303527832031\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/32...\n",
      "2024-11-06 05:07:43 INFO [auto_gptq.quantization.gptq] duration: 0.6224911212921143\n",
      "2024-11-06 05:07:43 INFO [auto_gptq.quantization.gptq] avg loss: 156.53375244140625\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/32...\n",
      "2024-11-06 05:07:43 INFO [auto_gptq.quantization.gptq] duration: 0.653369665145874\n",
      "2024-11-06 05:07:43 INFO [auto_gptq.quantization.gptq] avg loss: 491.04302978515625\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/32...\n",
      "2024-11-06 05:08:14 INFO [auto_gptq.quantization.gptq] duration: 0.8745908737182617\n",
      "2024-11-06 05:08:14 INFO [auto_gptq.quantization.gptq] avg loss: 14.093976020812988\n",
      "INFO - Quantizing mlp.up_proj in layer 31/32...\n",
      "2024-11-06 05:08:46 INFO [auto_gptq.quantization.gptq] duration: 0.9260601997375488\n",
      "2024-11-06 05:08:46 INFO [auto_gptq.quantization.gptq] avg loss: 854.6463623046875\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/32...\n",
      "2024-11-06 05:08:46 INFO [auto_gptq.quantization.gptq] duration: 0.7371578216552734\n",
      "2024-11-06 05:08:46 INFO [auto_gptq.quantization.gptq] avg loss: 1415.4761962890625\n",
      "INFO - Quantizing mlp.down_proj in layer 31/32...\n",
      "2024-11-06 05:09:39 INFO [auto_gptq.quantization.gptq] duration: 3.1280949115753174\n",
      "2024-11-06 05:09:39 INFO [auto_gptq.quantization.gptq] avg loss: 82.30877685546875\n",
      "INFO - Start quantizing layer 32/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/32...\n",
      "2024-11-06 05:10:42 INFO [auto_gptq.quantization.gptq] duration: 0.8956267833709717\n",
      "2024-11-06 05:10:42 INFO [auto_gptq.quantization.gptq] avg loss: 235.07017517089844\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/32...\n",
      "2024-11-06 05:10:42 INFO [auto_gptq.quantization.gptq] duration: 0.6301207542419434\n",
      "2024-11-06 05:10:42 INFO [auto_gptq.quantization.gptq] avg loss: 87.2273178100586\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/32...\n",
      "2024-11-06 05:10:43 INFO [auto_gptq.quantization.gptq] duration: 0.856102705001831\n",
      "2024-11-06 05:10:43 INFO [auto_gptq.quantization.gptq] avg loss: 399.232177734375\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/32...\n",
      "2024-11-06 05:11:13 INFO [auto_gptq.quantization.gptq] duration: 0.8785762786865234\n",
      "2024-11-06 05:11:13 INFO [auto_gptq.quantization.gptq] avg loss: 22.938121795654297\n",
      "INFO - Quantizing mlp.up_proj in layer 32/32...\n",
      "2024-11-06 05:11:45 INFO [auto_gptq.quantization.gptq] duration: 0.9268498420715332\n",
      "2024-11-06 05:11:45 INFO [auto_gptq.quantization.gptq] avg loss: 816.6583862304688\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/32...\n",
      "2024-11-06 05:11:46 INFO [auto_gptq.quantization.gptq] duration: 0.674781322479248\n",
      "2024-11-06 05:11:46 INFO [auto_gptq.quantization.gptq] avg loss: 1244.391357421875\n",
      "INFO - Quantizing mlp.down_proj in layer 32/32...\n",
      "2024-11-06 05:12:39 INFO [auto_gptq.quantization.gptq] duration: 3.111452102661133\n",
      "2024-11-06 05:12:39 INFO [auto_gptq.quantization.gptq] avg loss: 348.9609375\n",
      "INFO - Packing model...\n",
      "2024-11-06 05:13:07 INFO [auto_gptq.modeling._utils] Packing model...\n",
      "Packing llm.model.layers.31.mlp.down_proj...: 100%|██████████| 224/224 [04:16<00:00,  1.15s/it]   \n",
      "INFO - Model packed.\n",
      "2024-11-06 05:17:28 INFO [auto_gptq.modeling._utils] Model packed.\n"
     ]
    }
   ],
   "source": [
    "model.quantize(traindataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save quantized model\n",
    "model.save_quantized(quantized_model_dir)\n",
    "\n",
    "# save quantized model using safetensors\n",
    "model.save_quantized(quantized_model_dir, use_safetensors=True)\n",
    "\n",
    "# push quantized model to Hugging Face Hub.\n",
    "# to use use_auth_token=True, Login first via huggingface-cli login.\n",
    "# or pass explcit token with: use_auth_token=\"hf_xxxxxxx\"\n",
    "# (uncomment the following three lines to enable this feature)\n",
    "# repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
    "# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
    "# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)\n",
    "\n",
    "# alternatively you can save and push at the same time\n",
    "# (uncomment the following three lines to enable this feature)\n",
    "# repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
    "# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
    "# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully selected and copied 64 images to /home/workspace/dataset/imagenet/calibration.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# 定义源数据集路径和目标保存路径\n",
    "imagenet_train_dir = '/home/workspace/dataset/imagenet/train'  # 替换为你本地 ImageNet 训练集的路径\n",
    "output_dir = '/home/workspace/dataset/imagenet/calibration'  # 替换为你要保存图片的路径\n",
    "num_images_to_select = 64  # 要随机选取的图片数量\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 获取训练集中所有类别文件夹\n",
    "all_images = []\n",
    "for root, _, files in os.walk(imagenet_train_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            all_images.append(os.path.join(root, file))\n",
    "\n",
    "# 随机选取64张图片\n",
    "selected_images = random.sample(all_images, num_images_to_select)\n",
    "\n",
    "# 将选取的图片复制到目标文件夹\n",
    "for img_path in selected_images:\n",
    "    shutil.copy(img_path, output_dir)\n",
    "\n",
    "print(f'Successfully selected and copied {num_images_to_select} images to {output_dir}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIT部分量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_ScienceQA(nsamples, seed, seqlen, processor):\n",
    "    import torch.nn.functional as F\n",
    "    dataset = datasets.load_from_disk(\"/home/workspace/dataset/ScienceQA-2\")[\"train\"]\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "    rng = random.Random(42)\n",
    "    samples, num_tokens = [], 0\n",
    "    prompts_lists = []\n",
    "    input_images_lists = []\n",
    "    for index, _data in enumerate(dataset):\n",
    "        promt = _data[\"question\"]\n",
    "        image_file = _data[\"image\"]\n",
    "        image = np.array(image_file)\n",
    "        if image_file is None:\n",
    "            nsamples = nsamples+1\n",
    "            continue\n",
    "        msgs = [{'role': 'user', 'content': \"(<image>./</image>)\\n\"+ promt}]\n",
    "        prompts_lists.append(processor.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True))\n",
    "        input_images_lists.append([image])\n",
    "        if index >= nsamples-1:\n",
    "            break\n",
    "    # return prompts_lists,input_images_lists\n",
    "     \n",
    "    inputs = processor(\n",
    "        prompts_lists,\n",
    "        input_images_lists,\n",
    "        max_slice_nums=processor.image_processor.max_slice_nums,\n",
    "        use_image_id=processor.image_processor.use_image_id,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=8192\n",
    "    )\n",
    "    # return inputs\n",
    "    traindataset = []\n",
    "    for _ in range(inputs[\"input_ids\"].size(0)):\n",
    "        input_ids = inputs[\"input_ids\"].select(0, _).unsqueeze(0) \n",
    "        attention_mask = inputs[\"attention_mask\"].select(0, _).unsqueeze(0) \n",
    "        pixel_values = inputs[\"pixel_values\"]\n",
    "        image_sizes = inputs[\"image_sizes\"]\n",
    "        image_bound = inputs[\"image_bound\"]\n",
    "        tgt_sizes = inputs[\"tgt_sizes\"]\n",
    "        traindataset.append({\"input_ids\": input_ids, \n",
    "                             \"attention_mask\": attention_mask,\n",
    "                             \"pixel_values\": pixel_values,\n",
    "                             \"image_sizes\": image_sizes,\n",
    "                             \"image_bound\": image_bound,\n",
    "                             \"tgt_sizes\": tgt_sizes})\n",
    "\n",
    "    return traindataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from datasets import load_dataset, load_from_disk\n",
    "# import random\n",
    "# import torch\n",
    "\n",
    "# def get_wikitext2(nsamples, seed, seqlen, tokenizer):\n",
    "#     # set seed\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.random.manual_seed(seed)\n",
    "\n",
    "#     # load dataset and preprocess\n",
    "#     # traindata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "#     # testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "#     traindata = load_dataset(\"/home/workspace/code/git/FlatQuant_mlm/datasets/wikitext\", split=\"train\")\n",
    "#     testdata = load_dataset(\"/home/workspace/code/git/FlatQuant_mlm/datasets/wikitext\", split=\"test\")\n",
    "#     trainenc = tokenizer(\"\\n\\n\".join(traindata[\"text\"]), return_tensors=\"pt\")\n",
    "#     testenc = tokenizer(\"\\n\\n\".join(testdata[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "#     traindataset = []\n",
    "#     for _ in range(nsamples):\n",
    "#         i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "#         j = i + seqlen\n",
    "#         inp = trainenc.input_ids[:, i:j]\n",
    "#         attention_mask = torch.ones_like(inp)\n",
    "#         traindataset.append({\"input_ids\": inp, \"attention_mask\": attention_mask})\n",
    "#     return traindataset, testenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "You are using a model of type minicpmv to instantiate a model of type minicpm. This is not supported for all configurations of models and can yield errors.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "2024-11-07 15:41:31 INFO [auto_gptq.modeling.minicpm.configuration_minicpm] vision_config is None, using default vision config\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "from auto_gptq import AutoGPTQForVIT, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "pretrained_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1\"\n",
    "quantized_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1-g128\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True, trust_remote_code=True)\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=8,  # quantize model to 4-bit\n",
    "    group_size=-1,  # it is recommended to set the value to 128\n",
    "    desc_act=True,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    ")\n",
    "# traindataset, testenc = get_wikitext2(128, 0, model.seqlen, tokenizer)\n",
    "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "model = AutoGPTQForVIT.from_pretrained(pretrained_model_dir, quantize_config)\n",
    "from transformers import AutoProcessor\n",
    "model.model.processor = AutoProcessor.from_pretrained(pretrained_model_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aircraft in the image is an Airbus A380, identifiable by its distinctive hump on the upper deck, which is characteristic of the Airbus A380 model. The A380 is a twin-engine, wide-body, four-engine jet airliner that was developed by Airbus and manufactured by Boeing. It is one of the largest aircraft in the world, capable of carrying more than 800 passengers and has a range of up to 9,500 nautical miles (17,200 km). This particular model is part of the Airbus A380 family, which includes the A380-800 and A380-900 variants.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image = Image.open('/home/workspace/code/llm-awq/awq/airplane.jpeg').convert('RGB')\n",
    "\n",
    "# First round chat \n",
    "question = \"Tell me the model of this aircraft.\"\n",
    "msgs = [{'role': 'user', 'content': [image, question]}]\n",
    "\n",
    "answer = model.model.cuda().chat(\n",
    "    image=None,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MiniCPMVTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# traindataset, testenc = get_wikitext2(128, 0, model.seqlen, tokenizer)\n",
    "traindataset = get_ScienceQA(1024, 0, model.seqlen, model.model.processor)\n",
    "# i,m = get_ScienceQA(32, 0, model.seqlen, model.model.processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/27\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/27...\n",
      "2024-11-07 16:17:12 INFO [auto_gptq.quantization.gptq] duration: 0.4007911682128906\n",
      "2024-11-07 16:17:12 INFO [auto_gptq.quantization.gptq] avg loss: 6.768801540601999e-05\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/27...\n",
      "2024-11-07 16:17:13 INFO [auto_gptq.quantization.gptq] duration: 0.2792811393737793\n",
      "2024-11-07 16:17:13 INFO [auto_gptq.quantization.gptq] avg loss: 1.185271412396105e-05\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/27...\n",
      "2024-11-07 16:17:13 INFO [auto_gptq.quantization.gptq] duration: 0.2770664691925049\n",
      "2024-11-07 16:17:13 INFO [auto_gptq.quantization.gptq] avg loss: 4.957634519087151e-05\n",
      "INFO - Quantizing self_attn.out_proj in layer 1/27...\n",
      "2024-11-07 16:18:27 INFO [auto_gptq.quantization.gptq] duration: 0.21391677856445312\n",
      "2024-11-07 16:18:27 INFO [auto_gptq.quantization.gptq] avg loss: 1.2891981668872177e-06\n",
      "INFO - Quantizing mlp.fc1 in layer 1/27...\n",
      "2024-11-07 16:19:34 INFO [auto_gptq.quantization.gptq] duration: 0.3289053440093994\n",
      "2024-11-07 16:19:34 INFO [auto_gptq.quantization.gptq] avg loss: 0.0005746465176343918\n",
      "INFO - Quantizing mlp.fc2 in layer 1/27...\n",
      "2024-11-07 16:21:15 INFO [auto_gptq.quantization.gptq] duration: 0.7139286994934082\n",
      "2024-11-07 16:21:15 INFO [auto_gptq.quantization.gptq] avg loss: 8.529113256372511e-05\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.06 GiB. GPU 0 has a total capacity of 79.33 GiB of which 1024.00 MiB is free. Process 5139 has 78.32 GiB memory in use. Of the allocated memory 69.98 GiB is allocated by PyTorch, and 7.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mquantize(traindataset)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/modeling/_base_vit.py:391\u001b[0m, in \u001b[0;36mBaseGPTQForVIT.quantize\u001b[0;34m(self, examples, batch_size, use_triton, use_cuda_fp16, autotune_warmup_after_quantized, cache_examples_on_gpu)\u001b[0m\n\u001b[1;32m    385\u001b[0m         layer_output \u001b[38;5;241m=\u001b[39m move_to_device(\n\u001b[1;32m    386\u001b[0m             layer(\u001b[38;5;241m*\u001b[39mlayer_input,attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    387\u001b[0m             cur_layer_device \u001b[38;5;28;01mif\u001b[39;00m cache_examples_on_gpu \u001b[38;5;28;01melse\u001b[39;00m CPU,\n\u001b[1;32m    388\u001b[0m         )\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m         layer_output \u001b[38;5;241m=\u001b[39m move_to_device(\n\u001b[0;32m--> 391\u001b[0m             layer(\u001b[38;5;241m*\u001b[39mlayer_input)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    392\u001b[0m             cur_layer_device \u001b[38;5;28;01mif\u001b[39;00m cache_examples_on_gpu \u001b[38;5;28;01melse\u001b[39;00m CPU,\n\u001b[1;32m    393\u001b[0m         )                \n\u001b[1;32m    394\u001b[0m     layer_outputs\u001b[38;5;241m.\u001b[39mappend([layer_output])\n\u001b[1;32m    396\u001b[0m layers[i] \u001b[38;5;241m=\u001b[39m move_to_device(layer, CPU \u001b[38;5;28;01mif\u001b[39;00m force_layer_back_to_cpu \u001b[38;5;28;01melse\u001b[39;00m cur_layer_device)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/modeling/minicpm/modeling_navit_siglip.py:670\u001b[0m, in \u001b[0;36mSiglipEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    667\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    669\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(hidden_states)\n\u001b[0;32m--> 670\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    671\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    672\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    673\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    674\u001b[0m )\n\u001b[1;32m    675\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    677\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/lmquant/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/workspace/code/git/AutoGPTQ_mlm/auto_gptq/modeling/minicpm/modeling_navit_siglip.py:399\u001b[0m, in \u001b[0;36mSiglipAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    396\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(batch_size, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    398\u001b[0m k_v_seq_len \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 399\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query_states, key_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_weights\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q_len, k_v_seq_len):\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention weights should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(batch_size,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mq_len,\u001b[38;5;250m \u001b[39mk_v_seq_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_weights\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.06 GiB. GPU 0 has a total capacity of 79.33 GiB of which 1024.00 MiB is free. Process 5139 has 78.32 GiB memory in use. Of the allocated memory 69.98 GiB is allocated by PyTorch, and 7.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model.quantize(traindataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 15:10:31 INFO [auto_gptq.modeling.minicpm.configuration_minicpm] vision_config is None, using default vision config\n"
     ]
    }
   ],
   "source": [
    "# save quantized model\n",
    "model.save_quantized(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save quantized model using safetensors\n",
    "# model.save_quantized(quantized_model_dir, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type minicpmv to instantiate a model of type minicpm. This is not supported for all configurations of models and can yield errors.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "2024-11-08 03:14:48 INFO [auto_gptq.modeling.minicpm.configuration_minicpm] vision_config is None, using default vision config\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "/opt/conda/envs/lmquant/lib/python3.11/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /home/workspace/model/MiniCPM-3o-1B-sft-v1-pc were not used when initializing MiniCPMV: ['vpm.encoder.layers.0.mlp.fc1.g_idx', 'vpm.encoder.layers.0.mlp.fc1.qweight', 'vpm.encoder.layers.0.mlp.fc1.qzeros', 'vpm.encoder.layers.0.mlp.fc1.scales', 'vpm.encoder.layers.0.mlp.fc2.g_idx', 'vpm.encoder.layers.0.mlp.fc2.qweight', 'vpm.encoder.layers.0.mlp.fc2.qzeros', 'vpm.encoder.layers.0.mlp.fc2.scales', 'vpm.encoder.layers.0.self_attn.k_proj.g_idx', 'vpm.encoder.layers.0.self_attn.k_proj.qweight', 'vpm.encoder.layers.0.self_attn.k_proj.qzeros', 'vpm.encoder.layers.0.self_attn.k_proj.scales', 'vpm.encoder.layers.0.self_attn.out_proj.g_idx', 'vpm.encoder.layers.0.self_attn.out_proj.qweight', 'vpm.encoder.layers.0.self_attn.out_proj.qzeros', 'vpm.encoder.layers.0.self_attn.out_proj.scales', 'vpm.encoder.layers.0.self_attn.q_proj.g_idx', 'vpm.encoder.layers.0.self_attn.q_proj.qweight', 'vpm.encoder.layers.0.self_attn.q_proj.qzeros', 'vpm.encoder.layers.0.self_attn.q_proj.scales', 'vpm.encoder.layers.0.self_attn.v_proj.g_idx', 'vpm.encoder.layers.0.self_attn.v_proj.qweight', 'vpm.encoder.layers.0.self_attn.v_proj.qzeros', 'vpm.encoder.layers.0.self_attn.v_proj.scales', 'vpm.encoder.layers.1.mlp.fc1.g_idx', 'vpm.encoder.layers.1.mlp.fc1.qweight', 'vpm.encoder.layers.1.mlp.fc1.qzeros', 'vpm.encoder.layers.1.mlp.fc1.scales', 'vpm.encoder.layers.1.mlp.fc2.g_idx', 'vpm.encoder.layers.1.mlp.fc2.qweight', 'vpm.encoder.layers.1.mlp.fc2.qzeros', 'vpm.encoder.layers.1.mlp.fc2.scales', 'vpm.encoder.layers.1.self_attn.k_proj.g_idx', 'vpm.encoder.layers.1.self_attn.k_proj.qweight', 'vpm.encoder.layers.1.self_attn.k_proj.qzeros', 'vpm.encoder.layers.1.self_attn.k_proj.scales', 'vpm.encoder.layers.1.self_attn.out_proj.g_idx', 'vpm.encoder.layers.1.self_attn.out_proj.qweight', 'vpm.encoder.layers.1.self_attn.out_proj.qzeros', 'vpm.encoder.layers.1.self_attn.out_proj.scales', 'vpm.encoder.layers.1.self_attn.q_proj.g_idx', 'vpm.encoder.layers.1.self_attn.q_proj.qweight', 'vpm.encoder.layers.1.self_attn.q_proj.qzeros', 'vpm.encoder.layers.1.self_attn.q_proj.scales', 'vpm.encoder.layers.1.self_attn.v_proj.g_idx', 'vpm.encoder.layers.1.self_attn.v_proj.qweight', 'vpm.encoder.layers.1.self_attn.v_proj.qzeros', 'vpm.encoder.layers.1.self_attn.v_proj.scales', 'vpm.encoder.layers.10.mlp.fc1.g_idx', 'vpm.encoder.layers.10.mlp.fc1.qweight', 'vpm.encoder.layers.10.mlp.fc1.qzeros', 'vpm.encoder.layers.10.mlp.fc1.scales', 'vpm.encoder.layers.10.mlp.fc2.g_idx', 'vpm.encoder.layers.10.mlp.fc2.qweight', 'vpm.encoder.layers.10.mlp.fc2.qzeros', 'vpm.encoder.layers.10.mlp.fc2.scales', 'vpm.encoder.layers.10.self_attn.k_proj.g_idx', 'vpm.encoder.layers.10.self_attn.k_proj.qweight', 'vpm.encoder.layers.10.self_attn.k_proj.qzeros', 'vpm.encoder.layers.10.self_attn.k_proj.scales', 'vpm.encoder.layers.10.self_attn.out_proj.g_idx', 'vpm.encoder.layers.10.self_attn.out_proj.qweight', 'vpm.encoder.layers.10.self_attn.out_proj.qzeros', 'vpm.encoder.layers.10.self_attn.out_proj.scales', 'vpm.encoder.layers.10.self_attn.q_proj.g_idx', 'vpm.encoder.layers.10.self_attn.q_proj.qweight', 'vpm.encoder.layers.10.self_attn.q_proj.qzeros', 'vpm.encoder.layers.10.self_attn.q_proj.scales', 'vpm.encoder.layers.10.self_attn.v_proj.g_idx', 'vpm.encoder.layers.10.self_attn.v_proj.qweight', 'vpm.encoder.layers.10.self_attn.v_proj.qzeros', 'vpm.encoder.layers.10.self_attn.v_proj.scales', 'vpm.encoder.layers.11.mlp.fc1.g_idx', 'vpm.encoder.layers.11.mlp.fc1.qweight', 'vpm.encoder.layers.11.mlp.fc1.qzeros', 'vpm.encoder.layers.11.mlp.fc1.scales', 'vpm.encoder.layers.11.mlp.fc2.g_idx', 'vpm.encoder.layers.11.mlp.fc2.qweight', 'vpm.encoder.layers.11.mlp.fc2.qzeros', 'vpm.encoder.layers.11.mlp.fc2.scales', 'vpm.encoder.layers.11.self_attn.k_proj.g_idx', 'vpm.encoder.layers.11.self_attn.k_proj.qweight', 'vpm.encoder.layers.11.self_attn.k_proj.qzeros', 'vpm.encoder.layers.11.self_attn.k_proj.scales', 'vpm.encoder.layers.11.self_attn.out_proj.g_idx', 'vpm.encoder.layers.11.self_attn.out_proj.qweight', 'vpm.encoder.layers.11.self_attn.out_proj.qzeros', 'vpm.encoder.layers.11.self_attn.out_proj.scales', 'vpm.encoder.layers.11.self_attn.q_proj.g_idx', 'vpm.encoder.layers.11.self_attn.q_proj.qweight', 'vpm.encoder.layers.11.self_attn.q_proj.qzeros', 'vpm.encoder.layers.11.self_attn.q_proj.scales', 'vpm.encoder.layers.11.self_attn.v_proj.g_idx', 'vpm.encoder.layers.11.self_attn.v_proj.qweight', 'vpm.encoder.layers.11.self_attn.v_proj.qzeros', 'vpm.encoder.layers.11.self_attn.v_proj.scales', 'vpm.encoder.layers.12.mlp.fc1.g_idx', 'vpm.encoder.layers.12.mlp.fc1.qweight', 'vpm.encoder.layers.12.mlp.fc1.qzeros', 'vpm.encoder.layers.12.mlp.fc1.scales', 'vpm.encoder.layers.12.mlp.fc2.g_idx', 'vpm.encoder.layers.12.mlp.fc2.qweight', 'vpm.encoder.layers.12.mlp.fc2.qzeros', 'vpm.encoder.layers.12.mlp.fc2.scales', 'vpm.encoder.layers.12.self_attn.k_proj.g_idx', 'vpm.encoder.layers.12.self_attn.k_proj.qweight', 'vpm.encoder.layers.12.self_attn.k_proj.qzeros', 'vpm.encoder.layers.12.self_attn.k_proj.scales', 'vpm.encoder.layers.12.self_attn.out_proj.g_idx', 'vpm.encoder.layers.12.self_attn.out_proj.qweight', 'vpm.encoder.layers.12.self_attn.out_proj.qzeros', 'vpm.encoder.layers.12.self_attn.out_proj.scales', 'vpm.encoder.layers.12.self_attn.q_proj.g_idx', 'vpm.encoder.layers.12.self_attn.q_proj.qweight', 'vpm.encoder.layers.12.self_attn.q_proj.qzeros', 'vpm.encoder.layers.12.self_attn.q_proj.scales', 'vpm.encoder.layers.12.self_attn.v_proj.g_idx', 'vpm.encoder.layers.12.self_attn.v_proj.qweight', 'vpm.encoder.layers.12.self_attn.v_proj.qzeros', 'vpm.encoder.layers.12.self_attn.v_proj.scales', 'vpm.encoder.layers.13.mlp.fc1.g_idx', 'vpm.encoder.layers.13.mlp.fc1.qweight', 'vpm.encoder.layers.13.mlp.fc1.qzeros', 'vpm.encoder.layers.13.mlp.fc1.scales', 'vpm.encoder.layers.13.mlp.fc2.g_idx', 'vpm.encoder.layers.13.mlp.fc2.qweight', 'vpm.encoder.layers.13.mlp.fc2.qzeros', 'vpm.encoder.layers.13.mlp.fc2.scales', 'vpm.encoder.layers.13.self_attn.k_proj.g_idx', 'vpm.encoder.layers.13.self_attn.k_proj.qweight', 'vpm.encoder.layers.13.self_attn.k_proj.qzeros', 'vpm.encoder.layers.13.self_attn.k_proj.scales', 'vpm.encoder.layers.13.self_attn.out_proj.g_idx', 'vpm.encoder.layers.13.self_attn.out_proj.qweight', 'vpm.encoder.layers.13.self_attn.out_proj.qzeros', 'vpm.encoder.layers.13.self_attn.out_proj.scales', 'vpm.encoder.layers.13.self_attn.q_proj.g_idx', 'vpm.encoder.layers.13.self_attn.q_proj.qweight', 'vpm.encoder.layers.13.self_attn.q_proj.qzeros', 'vpm.encoder.layers.13.self_attn.q_proj.scales', 'vpm.encoder.layers.13.self_attn.v_proj.g_idx', 'vpm.encoder.layers.13.self_attn.v_proj.qweight', 'vpm.encoder.layers.13.self_attn.v_proj.qzeros', 'vpm.encoder.layers.13.self_attn.v_proj.scales', 'vpm.encoder.layers.14.mlp.fc1.g_idx', 'vpm.encoder.layers.14.mlp.fc1.qweight', 'vpm.encoder.layers.14.mlp.fc1.qzeros', 'vpm.encoder.layers.14.mlp.fc1.scales', 'vpm.encoder.layers.14.mlp.fc2.g_idx', 'vpm.encoder.layers.14.mlp.fc2.qweight', 'vpm.encoder.layers.14.mlp.fc2.qzeros', 'vpm.encoder.layers.14.mlp.fc2.scales', 'vpm.encoder.layers.14.self_attn.k_proj.g_idx', 'vpm.encoder.layers.14.self_attn.k_proj.qweight', 'vpm.encoder.layers.14.self_attn.k_proj.qzeros', 'vpm.encoder.layers.14.self_attn.k_proj.scales', 'vpm.encoder.layers.14.self_attn.out_proj.g_idx', 'vpm.encoder.layers.14.self_attn.out_proj.qweight', 'vpm.encoder.layers.14.self_attn.out_proj.qzeros', 'vpm.encoder.layers.14.self_attn.out_proj.scales', 'vpm.encoder.layers.14.self_attn.q_proj.g_idx', 'vpm.encoder.layers.14.self_attn.q_proj.qweight', 'vpm.encoder.layers.14.self_attn.q_proj.qzeros', 'vpm.encoder.layers.14.self_attn.q_proj.scales', 'vpm.encoder.layers.14.self_attn.v_proj.g_idx', 'vpm.encoder.layers.14.self_attn.v_proj.qweight', 'vpm.encoder.layers.14.self_attn.v_proj.qzeros', 'vpm.encoder.layers.14.self_attn.v_proj.scales', 'vpm.encoder.layers.15.mlp.fc1.g_idx', 'vpm.encoder.layers.15.mlp.fc1.qweight', 'vpm.encoder.layers.15.mlp.fc1.qzeros', 'vpm.encoder.layers.15.mlp.fc1.scales', 'vpm.encoder.layers.15.mlp.fc2.g_idx', 'vpm.encoder.layers.15.mlp.fc2.qweight', 'vpm.encoder.layers.15.mlp.fc2.qzeros', 'vpm.encoder.layers.15.mlp.fc2.scales', 'vpm.encoder.layers.15.self_attn.k_proj.g_idx', 'vpm.encoder.layers.15.self_attn.k_proj.qweight', 'vpm.encoder.layers.15.self_attn.k_proj.qzeros', 'vpm.encoder.layers.15.self_attn.k_proj.scales', 'vpm.encoder.layers.15.self_attn.out_proj.g_idx', 'vpm.encoder.layers.15.self_attn.out_proj.qweight', 'vpm.encoder.layers.15.self_attn.out_proj.qzeros', 'vpm.encoder.layers.15.self_attn.out_proj.scales', 'vpm.encoder.layers.15.self_attn.q_proj.g_idx', 'vpm.encoder.layers.15.self_attn.q_proj.qweight', 'vpm.encoder.layers.15.self_attn.q_proj.qzeros', 'vpm.encoder.layers.15.self_attn.q_proj.scales', 'vpm.encoder.layers.15.self_attn.v_proj.g_idx', 'vpm.encoder.layers.15.self_attn.v_proj.qweight', 'vpm.encoder.layers.15.self_attn.v_proj.qzeros', 'vpm.encoder.layers.15.self_attn.v_proj.scales', 'vpm.encoder.layers.16.mlp.fc1.g_idx', 'vpm.encoder.layers.16.mlp.fc1.qweight', 'vpm.encoder.layers.16.mlp.fc1.qzeros', 'vpm.encoder.layers.16.mlp.fc1.scales', 'vpm.encoder.layers.16.mlp.fc2.g_idx', 'vpm.encoder.layers.16.mlp.fc2.qweight', 'vpm.encoder.layers.16.mlp.fc2.qzeros', 'vpm.encoder.layers.16.mlp.fc2.scales', 'vpm.encoder.layers.16.self_attn.k_proj.g_idx', 'vpm.encoder.layers.16.self_attn.k_proj.qweight', 'vpm.encoder.layers.16.self_attn.k_proj.qzeros', 'vpm.encoder.layers.16.self_attn.k_proj.scales', 'vpm.encoder.layers.16.self_attn.out_proj.g_idx', 'vpm.encoder.layers.16.self_attn.out_proj.qweight', 'vpm.encoder.layers.16.self_attn.out_proj.qzeros', 'vpm.encoder.layers.16.self_attn.out_proj.scales', 'vpm.encoder.layers.16.self_attn.q_proj.g_idx', 'vpm.encoder.layers.16.self_attn.q_proj.qweight', 'vpm.encoder.layers.16.self_attn.q_proj.qzeros', 'vpm.encoder.layers.16.self_attn.q_proj.scales', 'vpm.encoder.layers.16.self_attn.v_proj.g_idx', 'vpm.encoder.layers.16.self_attn.v_proj.qweight', 'vpm.encoder.layers.16.self_attn.v_proj.qzeros', 'vpm.encoder.layers.16.self_attn.v_proj.scales', 'vpm.encoder.layers.17.mlp.fc1.g_idx', 'vpm.encoder.layers.17.mlp.fc1.qweight', 'vpm.encoder.layers.17.mlp.fc1.qzeros', 'vpm.encoder.layers.17.mlp.fc1.scales', 'vpm.encoder.layers.17.mlp.fc2.g_idx', 'vpm.encoder.layers.17.mlp.fc2.qweight', 'vpm.encoder.layers.17.mlp.fc2.qzeros', 'vpm.encoder.layers.17.mlp.fc2.scales', 'vpm.encoder.layers.17.self_attn.k_proj.g_idx', 'vpm.encoder.layers.17.self_attn.k_proj.qweight', 'vpm.encoder.layers.17.self_attn.k_proj.qzeros', 'vpm.encoder.layers.17.self_attn.k_proj.scales', 'vpm.encoder.layers.17.self_attn.out_proj.g_idx', 'vpm.encoder.layers.17.self_attn.out_proj.qweight', 'vpm.encoder.layers.17.self_attn.out_proj.qzeros', 'vpm.encoder.layers.17.self_attn.out_proj.scales', 'vpm.encoder.layers.17.self_attn.q_proj.g_idx', 'vpm.encoder.layers.17.self_attn.q_proj.qweight', 'vpm.encoder.layers.17.self_attn.q_proj.qzeros', 'vpm.encoder.layers.17.self_attn.q_proj.scales', 'vpm.encoder.layers.17.self_attn.v_proj.g_idx', 'vpm.encoder.layers.17.self_attn.v_proj.qweight', 'vpm.encoder.layers.17.self_attn.v_proj.qzeros', 'vpm.encoder.layers.17.self_attn.v_proj.scales', 'vpm.encoder.layers.18.mlp.fc1.g_idx', 'vpm.encoder.layers.18.mlp.fc1.qweight', 'vpm.encoder.layers.18.mlp.fc1.qzeros', 'vpm.encoder.layers.18.mlp.fc1.scales', 'vpm.encoder.layers.18.mlp.fc2.g_idx', 'vpm.encoder.layers.18.mlp.fc2.qweight', 'vpm.encoder.layers.18.mlp.fc2.qzeros', 'vpm.encoder.layers.18.mlp.fc2.scales', 'vpm.encoder.layers.18.self_attn.k_proj.g_idx', 'vpm.encoder.layers.18.self_attn.k_proj.qweight', 'vpm.encoder.layers.18.self_attn.k_proj.qzeros', 'vpm.encoder.layers.18.self_attn.k_proj.scales', 'vpm.encoder.layers.18.self_attn.out_proj.g_idx', 'vpm.encoder.layers.18.self_attn.out_proj.qweight', 'vpm.encoder.layers.18.self_attn.out_proj.qzeros', 'vpm.encoder.layers.18.self_attn.out_proj.scales', 'vpm.encoder.layers.18.self_attn.q_proj.g_idx', 'vpm.encoder.layers.18.self_attn.q_proj.qweight', 'vpm.encoder.layers.18.self_attn.q_proj.qzeros', 'vpm.encoder.layers.18.self_attn.q_proj.scales', 'vpm.encoder.layers.18.self_attn.v_proj.g_idx', 'vpm.encoder.layers.18.self_attn.v_proj.qweight', 'vpm.encoder.layers.18.self_attn.v_proj.qzeros', 'vpm.encoder.layers.18.self_attn.v_proj.scales', 'vpm.encoder.layers.19.mlp.fc1.g_idx', 'vpm.encoder.layers.19.mlp.fc1.qweight', 'vpm.encoder.layers.19.mlp.fc1.qzeros', 'vpm.encoder.layers.19.mlp.fc1.scales', 'vpm.encoder.layers.19.mlp.fc2.g_idx', 'vpm.encoder.layers.19.mlp.fc2.qweight', 'vpm.encoder.layers.19.mlp.fc2.qzeros', 'vpm.encoder.layers.19.mlp.fc2.scales', 'vpm.encoder.layers.19.self_attn.k_proj.g_idx', 'vpm.encoder.layers.19.self_attn.k_proj.qweight', 'vpm.encoder.layers.19.self_attn.k_proj.qzeros', 'vpm.encoder.layers.19.self_attn.k_proj.scales', 'vpm.encoder.layers.19.self_attn.out_proj.g_idx', 'vpm.encoder.layers.19.self_attn.out_proj.qweight', 'vpm.encoder.layers.19.self_attn.out_proj.qzeros', 'vpm.encoder.layers.19.self_attn.out_proj.scales', 'vpm.encoder.layers.19.self_attn.q_proj.g_idx', 'vpm.encoder.layers.19.self_attn.q_proj.qweight', 'vpm.encoder.layers.19.self_attn.q_proj.qzeros', 'vpm.encoder.layers.19.self_attn.q_proj.scales', 'vpm.encoder.layers.19.self_attn.v_proj.g_idx', 'vpm.encoder.layers.19.self_attn.v_proj.qweight', 'vpm.encoder.layers.19.self_attn.v_proj.qzeros', 'vpm.encoder.layers.19.self_attn.v_proj.scales', 'vpm.encoder.layers.2.mlp.fc1.g_idx', 'vpm.encoder.layers.2.mlp.fc1.qweight', 'vpm.encoder.layers.2.mlp.fc1.qzeros', 'vpm.encoder.layers.2.mlp.fc1.scales', 'vpm.encoder.layers.2.mlp.fc2.g_idx', 'vpm.encoder.layers.2.mlp.fc2.qweight', 'vpm.encoder.layers.2.mlp.fc2.qzeros', 'vpm.encoder.layers.2.mlp.fc2.scales', 'vpm.encoder.layers.2.self_attn.k_proj.g_idx', 'vpm.encoder.layers.2.self_attn.k_proj.qweight', 'vpm.encoder.layers.2.self_attn.k_proj.qzeros', 'vpm.encoder.layers.2.self_attn.k_proj.scales', 'vpm.encoder.layers.2.self_attn.out_proj.g_idx', 'vpm.encoder.layers.2.self_attn.out_proj.qweight', 'vpm.encoder.layers.2.self_attn.out_proj.qzeros', 'vpm.encoder.layers.2.self_attn.out_proj.scales', 'vpm.encoder.layers.2.self_attn.q_proj.g_idx', 'vpm.encoder.layers.2.self_attn.q_proj.qweight', 'vpm.encoder.layers.2.self_attn.q_proj.qzeros', 'vpm.encoder.layers.2.self_attn.q_proj.scales', 'vpm.encoder.layers.2.self_attn.v_proj.g_idx', 'vpm.encoder.layers.2.self_attn.v_proj.qweight', 'vpm.encoder.layers.2.self_attn.v_proj.qzeros', 'vpm.encoder.layers.2.self_attn.v_proj.scales', 'vpm.encoder.layers.20.mlp.fc1.g_idx', 'vpm.encoder.layers.20.mlp.fc1.qweight', 'vpm.encoder.layers.20.mlp.fc1.qzeros', 'vpm.encoder.layers.20.mlp.fc1.scales', 'vpm.encoder.layers.20.mlp.fc2.g_idx', 'vpm.encoder.layers.20.mlp.fc2.qweight', 'vpm.encoder.layers.20.mlp.fc2.qzeros', 'vpm.encoder.layers.20.mlp.fc2.scales', 'vpm.encoder.layers.20.self_attn.k_proj.g_idx', 'vpm.encoder.layers.20.self_attn.k_proj.qweight', 'vpm.encoder.layers.20.self_attn.k_proj.qzeros', 'vpm.encoder.layers.20.self_attn.k_proj.scales', 'vpm.encoder.layers.20.self_attn.out_proj.g_idx', 'vpm.encoder.layers.20.self_attn.out_proj.qweight', 'vpm.encoder.layers.20.self_attn.out_proj.qzeros', 'vpm.encoder.layers.20.self_attn.out_proj.scales', 'vpm.encoder.layers.20.self_attn.q_proj.g_idx', 'vpm.encoder.layers.20.self_attn.q_proj.qweight', 'vpm.encoder.layers.20.self_attn.q_proj.qzeros', 'vpm.encoder.layers.20.self_attn.q_proj.scales', 'vpm.encoder.layers.20.self_attn.v_proj.g_idx', 'vpm.encoder.layers.20.self_attn.v_proj.qweight', 'vpm.encoder.layers.20.self_attn.v_proj.qzeros', 'vpm.encoder.layers.20.self_attn.v_proj.scales', 'vpm.encoder.layers.21.mlp.fc1.g_idx', 'vpm.encoder.layers.21.mlp.fc1.qweight', 'vpm.encoder.layers.21.mlp.fc1.qzeros', 'vpm.encoder.layers.21.mlp.fc1.scales', 'vpm.encoder.layers.21.mlp.fc2.g_idx', 'vpm.encoder.layers.21.mlp.fc2.qweight', 'vpm.encoder.layers.21.mlp.fc2.qzeros', 'vpm.encoder.layers.21.mlp.fc2.scales', 'vpm.encoder.layers.21.self_attn.k_proj.g_idx', 'vpm.encoder.layers.21.self_attn.k_proj.qweight', 'vpm.encoder.layers.21.self_attn.k_proj.qzeros', 'vpm.encoder.layers.21.self_attn.k_proj.scales', 'vpm.encoder.layers.21.self_attn.out_proj.g_idx', 'vpm.encoder.layers.21.self_attn.out_proj.qweight', 'vpm.encoder.layers.21.self_attn.out_proj.qzeros', 'vpm.encoder.layers.21.self_attn.out_proj.scales', 'vpm.encoder.layers.21.self_attn.q_proj.g_idx', 'vpm.encoder.layers.21.self_attn.q_proj.qweight', 'vpm.encoder.layers.21.self_attn.q_proj.qzeros', 'vpm.encoder.layers.21.self_attn.q_proj.scales', 'vpm.encoder.layers.21.self_attn.v_proj.g_idx', 'vpm.encoder.layers.21.self_attn.v_proj.qweight', 'vpm.encoder.layers.21.self_attn.v_proj.qzeros', 'vpm.encoder.layers.21.self_attn.v_proj.scales', 'vpm.encoder.layers.22.mlp.fc1.g_idx', 'vpm.encoder.layers.22.mlp.fc1.qweight', 'vpm.encoder.layers.22.mlp.fc1.qzeros', 'vpm.encoder.layers.22.mlp.fc1.scales', 'vpm.encoder.layers.22.mlp.fc2.g_idx', 'vpm.encoder.layers.22.mlp.fc2.qweight', 'vpm.encoder.layers.22.mlp.fc2.qzeros', 'vpm.encoder.layers.22.mlp.fc2.scales', 'vpm.encoder.layers.22.self_attn.k_proj.g_idx', 'vpm.encoder.layers.22.self_attn.k_proj.qweight', 'vpm.encoder.layers.22.self_attn.k_proj.qzeros', 'vpm.encoder.layers.22.self_attn.k_proj.scales', 'vpm.encoder.layers.22.self_attn.out_proj.g_idx', 'vpm.encoder.layers.22.self_attn.out_proj.qweight', 'vpm.encoder.layers.22.self_attn.out_proj.qzeros', 'vpm.encoder.layers.22.self_attn.out_proj.scales', 'vpm.encoder.layers.22.self_attn.q_proj.g_idx', 'vpm.encoder.layers.22.self_attn.q_proj.qweight', 'vpm.encoder.layers.22.self_attn.q_proj.qzeros', 'vpm.encoder.layers.22.self_attn.q_proj.scales', 'vpm.encoder.layers.22.self_attn.v_proj.g_idx', 'vpm.encoder.layers.22.self_attn.v_proj.qweight', 'vpm.encoder.layers.22.self_attn.v_proj.qzeros', 'vpm.encoder.layers.22.self_attn.v_proj.scales', 'vpm.encoder.layers.23.mlp.fc1.g_idx', 'vpm.encoder.layers.23.mlp.fc1.qweight', 'vpm.encoder.layers.23.mlp.fc1.qzeros', 'vpm.encoder.layers.23.mlp.fc1.scales', 'vpm.encoder.layers.23.mlp.fc2.g_idx', 'vpm.encoder.layers.23.mlp.fc2.qweight', 'vpm.encoder.layers.23.mlp.fc2.qzeros', 'vpm.encoder.layers.23.mlp.fc2.scales', 'vpm.encoder.layers.23.self_attn.k_proj.g_idx', 'vpm.encoder.layers.23.self_attn.k_proj.qweight', 'vpm.encoder.layers.23.self_attn.k_proj.qzeros', 'vpm.encoder.layers.23.self_attn.k_proj.scales', 'vpm.encoder.layers.23.self_attn.out_proj.g_idx', 'vpm.encoder.layers.23.self_attn.out_proj.qweight', 'vpm.encoder.layers.23.self_attn.out_proj.qzeros', 'vpm.encoder.layers.23.self_attn.out_proj.scales', 'vpm.encoder.layers.23.self_attn.q_proj.g_idx', 'vpm.encoder.layers.23.self_attn.q_proj.qweight', 'vpm.encoder.layers.23.self_attn.q_proj.qzeros', 'vpm.encoder.layers.23.self_attn.q_proj.scales', 'vpm.encoder.layers.23.self_attn.v_proj.g_idx', 'vpm.encoder.layers.23.self_attn.v_proj.qweight', 'vpm.encoder.layers.23.self_attn.v_proj.qzeros', 'vpm.encoder.layers.23.self_attn.v_proj.scales', 'vpm.encoder.layers.24.mlp.fc1.g_idx', 'vpm.encoder.layers.24.mlp.fc1.qweight', 'vpm.encoder.layers.24.mlp.fc1.qzeros', 'vpm.encoder.layers.24.mlp.fc1.scales', 'vpm.encoder.layers.24.mlp.fc2.g_idx', 'vpm.encoder.layers.24.mlp.fc2.qweight', 'vpm.encoder.layers.24.mlp.fc2.qzeros', 'vpm.encoder.layers.24.mlp.fc2.scales', 'vpm.encoder.layers.24.self_attn.k_proj.g_idx', 'vpm.encoder.layers.24.self_attn.k_proj.qweight', 'vpm.encoder.layers.24.self_attn.k_proj.qzeros', 'vpm.encoder.layers.24.self_attn.k_proj.scales', 'vpm.encoder.layers.24.self_attn.out_proj.g_idx', 'vpm.encoder.layers.24.self_attn.out_proj.qweight', 'vpm.encoder.layers.24.self_attn.out_proj.qzeros', 'vpm.encoder.layers.24.self_attn.out_proj.scales', 'vpm.encoder.layers.24.self_attn.q_proj.g_idx', 'vpm.encoder.layers.24.self_attn.q_proj.qweight', 'vpm.encoder.layers.24.self_attn.q_proj.qzeros', 'vpm.encoder.layers.24.self_attn.q_proj.scales', 'vpm.encoder.layers.24.self_attn.v_proj.g_idx', 'vpm.encoder.layers.24.self_attn.v_proj.qweight', 'vpm.encoder.layers.24.self_attn.v_proj.qzeros', 'vpm.encoder.layers.24.self_attn.v_proj.scales', 'vpm.encoder.layers.25.mlp.fc1.g_idx', 'vpm.encoder.layers.25.mlp.fc1.qweight', 'vpm.encoder.layers.25.mlp.fc1.qzeros', 'vpm.encoder.layers.25.mlp.fc1.scales', 'vpm.encoder.layers.25.mlp.fc2.g_idx', 'vpm.encoder.layers.25.mlp.fc2.qweight', 'vpm.encoder.layers.25.mlp.fc2.qzeros', 'vpm.encoder.layers.25.mlp.fc2.scales', 'vpm.encoder.layers.25.self_attn.k_proj.g_idx', 'vpm.encoder.layers.25.self_attn.k_proj.qweight', 'vpm.encoder.layers.25.self_attn.k_proj.qzeros', 'vpm.encoder.layers.25.self_attn.k_proj.scales', 'vpm.encoder.layers.25.self_attn.out_proj.g_idx', 'vpm.encoder.layers.25.self_attn.out_proj.qweight', 'vpm.encoder.layers.25.self_attn.out_proj.qzeros', 'vpm.encoder.layers.25.self_attn.out_proj.scales', 'vpm.encoder.layers.25.self_attn.q_proj.g_idx', 'vpm.encoder.layers.25.self_attn.q_proj.qweight', 'vpm.encoder.layers.25.self_attn.q_proj.qzeros', 'vpm.encoder.layers.25.self_attn.q_proj.scales', 'vpm.encoder.layers.25.self_attn.v_proj.g_idx', 'vpm.encoder.layers.25.self_attn.v_proj.qweight', 'vpm.encoder.layers.25.self_attn.v_proj.qzeros', 'vpm.encoder.layers.25.self_attn.v_proj.scales', 'vpm.encoder.layers.26.mlp.fc1.g_idx', 'vpm.encoder.layers.26.mlp.fc1.qweight', 'vpm.encoder.layers.26.mlp.fc1.qzeros', 'vpm.encoder.layers.26.mlp.fc1.scales', 'vpm.encoder.layers.26.mlp.fc2.g_idx', 'vpm.encoder.layers.26.mlp.fc2.qweight', 'vpm.encoder.layers.26.mlp.fc2.qzeros', 'vpm.encoder.layers.26.mlp.fc2.scales', 'vpm.encoder.layers.26.self_attn.k_proj.g_idx', 'vpm.encoder.layers.26.self_attn.k_proj.qweight', 'vpm.encoder.layers.26.self_attn.k_proj.qzeros', 'vpm.encoder.layers.26.self_attn.k_proj.scales', 'vpm.encoder.layers.26.self_attn.out_proj.g_idx', 'vpm.encoder.layers.26.self_attn.out_proj.qweight', 'vpm.encoder.layers.26.self_attn.out_proj.qzeros', 'vpm.encoder.layers.26.self_attn.out_proj.scales', 'vpm.encoder.layers.26.self_attn.q_proj.g_idx', 'vpm.encoder.layers.26.self_attn.q_proj.qweight', 'vpm.encoder.layers.26.self_attn.q_proj.qzeros', 'vpm.encoder.layers.26.self_attn.q_proj.scales', 'vpm.encoder.layers.26.self_attn.v_proj.g_idx', 'vpm.encoder.layers.26.self_attn.v_proj.qweight', 'vpm.encoder.layers.26.self_attn.v_proj.qzeros', 'vpm.encoder.layers.26.self_attn.v_proj.scales', 'vpm.encoder.layers.3.mlp.fc1.g_idx', 'vpm.encoder.layers.3.mlp.fc1.qweight', 'vpm.encoder.layers.3.mlp.fc1.qzeros', 'vpm.encoder.layers.3.mlp.fc1.scales', 'vpm.encoder.layers.3.mlp.fc2.g_idx', 'vpm.encoder.layers.3.mlp.fc2.qweight', 'vpm.encoder.layers.3.mlp.fc2.qzeros', 'vpm.encoder.layers.3.mlp.fc2.scales', 'vpm.encoder.layers.3.self_attn.k_proj.g_idx', 'vpm.encoder.layers.3.self_attn.k_proj.qweight', 'vpm.encoder.layers.3.self_attn.k_proj.qzeros', 'vpm.encoder.layers.3.self_attn.k_proj.scales', 'vpm.encoder.layers.3.self_attn.out_proj.g_idx', 'vpm.encoder.layers.3.self_attn.out_proj.qweight', 'vpm.encoder.layers.3.self_attn.out_proj.qzeros', 'vpm.encoder.layers.3.self_attn.out_proj.scales', 'vpm.encoder.layers.3.self_attn.q_proj.g_idx', 'vpm.encoder.layers.3.self_attn.q_proj.qweight', 'vpm.encoder.layers.3.self_attn.q_proj.qzeros', 'vpm.encoder.layers.3.self_attn.q_proj.scales', 'vpm.encoder.layers.3.self_attn.v_proj.g_idx', 'vpm.encoder.layers.3.self_attn.v_proj.qweight', 'vpm.encoder.layers.3.self_attn.v_proj.qzeros', 'vpm.encoder.layers.3.self_attn.v_proj.scales', 'vpm.encoder.layers.4.mlp.fc1.g_idx', 'vpm.encoder.layers.4.mlp.fc1.qweight', 'vpm.encoder.layers.4.mlp.fc1.qzeros', 'vpm.encoder.layers.4.mlp.fc1.scales', 'vpm.encoder.layers.4.mlp.fc2.g_idx', 'vpm.encoder.layers.4.mlp.fc2.qweight', 'vpm.encoder.layers.4.mlp.fc2.qzeros', 'vpm.encoder.layers.4.mlp.fc2.scales', 'vpm.encoder.layers.4.self_attn.k_proj.g_idx', 'vpm.encoder.layers.4.self_attn.k_proj.qweight', 'vpm.encoder.layers.4.self_attn.k_proj.qzeros', 'vpm.encoder.layers.4.self_attn.k_proj.scales', 'vpm.encoder.layers.4.self_attn.out_proj.g_idx', 'vpm.encoder.layers.4.self_attn.out_proj.qweight', 'vpm.encoder.layers.4.self_attn.out_proj.qzeros', 'vpm.encoder.layers.4.self_attn.out_proj.scales', 'vpm.encoder.layers.4.self_attn.q_proj.g_idx', 'vpm.encoder.layers.4.self_attn.q_proj.qweight', 'vpm.encoder.layers.4.self_attn.q_proj.qzeros', 'vpm.encoder.layers.4.self_attn.q_proj.scales', 'vpm.encoder.layers.4.self_attn.v_proj.g_idx', 'vpm.encoder.layers.4.self_attn.v_proj.qweight', 'vpm.encoder.layers.4.self_attn.v_proj.qzeros', 'vpm.encoder.layers.4.self_attn.v_proj.scales', 'vpm.encoder.layers.5.mlp.fc1.g_idx', 'vpm.encoder.layers.5.mlp.fc1.qweight', 'vpm.encoder.layers.5.mlp.fc1.qzeros', 'vpm.encoder.layers.5.mlp.fc1.scales', 'vpm.encoder.layers.5.mlp.fc2.g_idx', 'vpm.encoder.layers.5.mlp.fc2.qweight', 'vpm.encoder.layers.5.mlp.fc2.qzeros', 'vpm.encoder.layers.5.mlp.fc2.scales', 'vpm.encoder.layers.5.self_attn.k_proj.g_idx', 'vpm.encoder.layers.5.self_attn.k_proj.qweight', 'vpm.encoder.layers.5.self_attn.k_proj.qzeros', 'vpm.encoder.layers.5.self_attn.k_proj.scales', 'vpm.encoder.layers.5.self_attn.out_proj.g_idx', 'vpm.encoder.layers.5.self_attn.out_proj.qweight', 'vpm.encoder.layers.5.self_attn.out_proj.qzeros', 'vpm.encoder.layers.5.self_attn.out_proj.scales', 'vpm.encoder.layers.5.self_attn.q_proj.g_idx', 'vpm.encoder.layers.5.self_attn.q_proj.qweight', 'vpm.encoder.layers.5.self_attn.q_proj.qzeros', 'vpm.encoder.layers.5.self_attn.q_proj.scales', 'vpm.encoder.layers.5.self_attn.v_proj.g_idx', 'vpm.encoder.layers.5.self_attn.v_proj.qweight', 'vpm.encoder.layers.5.self_attn.v_proj.qzeros', 'vpm.encoder.layers.5.self_attn.v_proj.scales', 'vpm.encoder.layers.6.mlp.fc1.g_idx', 'vpm.encoder.layers.6.mlp.fc1.qweight', 'vpm.encoder.layers.6.mlp.fc1.qzeros', 'vpm.encoder.layers.6.mlp.fc1.scales', 'vpm.encoder.layers.6.mlp.fc2.g_idx', 'vpm.encoder.layers.6.mlp.fc2.qweight', 'vpm.encoder.layers.6.mlp.fc2.qzeros', 'vpm.encoder.layers.6.mlp.fc2.scales', 'vpm.encoder.layers.6.self_attn.k_proj.g_idx', 'vpm.encoder.layers.6.self_attn.k_proj.qweight', 'vpm.encoder.layers.6.self_attn.k_proj.qzeros', 'vpm.encoder.layers.6.self_attn.k_proj.scales', 'vpm.encoder.layers.6.self_attn.out_proj.g_idx', 'vpm.encoder.layers.6.self_attn.out_proj.qweight', 'vpm.encoder.layers.6.self_attn.out_proj.qzeros', 'vpm.encoder.layers.6.self_attn.out_proj.scales', 'vpm.encoder.layers.6.self_attn.q_proj.g_idx', 'vpm.encoder.layers.6.self_attn.q_proj.qweight', 'vpm.encoder.layers.6.self_attn.q_proj.qzeros', 'vpm.encoder.layers.6.self_attn.q_proj.scales', 'vpm.encoder.layers.6.self_attn.v_proj.g_idx', 'vpm.encoder.layers.6.self_attn.v_proj.qweight', 'vpm.encoder.layers.6.self_attn.v_proj.qzeros', 'vpm.encoder.layers.6.self_attn.v_proj.scales', 'vpm.encoder.layers.7.mlp.fc1.g_idx', 'vpm.encoder.layers.7.mlp.fc1.qweight', 'vpm.encoder.layers.7.mlp.fc1.qzeros', 'vpm.encoder.layers.7.mlp.fc1.scales', 'vpm.encoder.layers.7.mlp.fc2.g_idx', 'vpm.encoder.layers.7.mlp.fc2.qweight', 'vpm.encoder.layers.7.mlp.fc2.qzeros', 'vpm.encoder.layers.7.mlp.fc2.scales', 'vpm.encoder.layers.7.self_attn.k_proj.g_idx', 'vpm.encoder.layers.7.self_attn.k_proj.qweight', 'vpm.encoder.layers.7.self_attn.k_proj.qzeros', 'vpm.encoder.layers.7.self_attn.k_proj.scales', 'vpm.encoder.layers.7.self_attn.out_proj.g_idx', 'vpm.encoder.layers.7.self_attn.out_proj.qweight', 'vpm.encoder.layers.7.self_attn.out_proj.qzeros', 'vpm.encoder.layers.7.self_attn.out_proj.scales', 'vpm.encoder.layers.7.self_attn.q_proj.g_idx', 'vpm.encoder.layers.7.self_attn.q_proj.qweight', 'vpm.encoder.layers.7.self_attn.q_proj.qzeros', 'vpm.encoder.layers.7.self_attn.q_proj.scales', 'vpm.encoder.layers.7.self_attn.v_proj.g_idx', 'vpm.encoder.layers.7.self_attn.v_proj.qweight', 'vpm.encoder.layers.7.self_attn.v_proj.qzeros', 'vpm.encoder.layers.7.self_attn.v_proj.scales', 'vpm.encoder.layers.8.mlp.fc1.g_idx', 'vpm.encoder.layers.8.mlp.fc1.qweight', 'vpm.encoder.layers.8.mlp.fc1.qzeros', 'vpm.encoder.layers.8.mlp.fc1.scales', 'vpm.encoder.layers.8.mlp.fc2.g_idx', 'vpm.encoder.layers.8.mlp.fc2.qweight', 'vpm.encoder.layers.8.mlp.fc2.qzeros', 'vpm.encoder.layers.8.mlp.fc2.scales', 'vpm.encoder.layers.8.self_attn.k_proj.g_idx', 'vpm.encoder.layers.8.self_attn.k_proj.qweight', 'vpm.encoder.layers.8.self_attn.k_proj.qzeros', 'vpm.encoder.layers.8.self_attn.k_proj.scales', 'vpm.encoder.layers.8.self_attn.out_proj.g_idx', 'vpm.encoder.layers.8.self_attn.out_proj.qweight', 'vpm.encoder.layers.8.self_attn.out_proj.qzeros', 'vpm.encoder.layers.8.self_attn.out_proj.scales', 'vpm.encoder.layers.8.self_attn.q_proj.g_idx', 'vpm.encoder.layers.8.self_attn.q_proj.qweight', 'vpm.encoder.layers.8.self_attn.q_proj.qzeros', 'vpm.encoder.layers.8.self_attn.q_proj.scales', 'vpm.encoder.layers.8.self_attn.v_proj.g_idx', 'vpm.encoder.layers.8.self_attn.v_proj.qweight', 'vpm.encoder.layers.8.self_attn.v_proj.qzeros', 'vpm.encoder.layers.8.self_attn.v_proj.scales', 'vpm.encoder.layers.9.mlp.fc1.g_idx', 'vpm.encoder.layers.9.mlp.fc1.qweight', 'vpm.encoder.layers.9.mlp.fc1.qzeros', 'vpm.encoder.layers.9.mlp.fc1.scales', 'vpm.encoder.layers.9.mlp.fc2.g_idx', 'vpm.encoder.layers.9.mlp.fc2.qweight', 'vpm.encoder.layers.9.mlp.fc2.qzeros', 'vpm.encoder.layers.9.mlp.fc2.scales', 'vpm.encoder.layers.9.self_attn.k_proj.g_idx', 'vpm.encoder.layers.9.self_attn.k_proj.qweight', 'vpm.encoder.layers.9.self_attn.k_proj.qzeros', 'vpm.encoder.layers.9.self_attn.k_proj.scales', 'vpm.encoder.layers.9.self_attn.out_proj.g_idx', 'vpm.encoder.layers.9.self_attn.out_proj.qweight', 'vpm.encoder.layers.9.self_attn.out_proj.qzeros', 'vpm.encoder.layers.9.self_attn.out_proj.scales', 'vpm.encoder.layers.9.self_attn.q_proj.g_idx', 'vpm.encoder.layers.9.self_attn.q_proj.qweight', 'vpm.encoder.layers.9.self_attn.q_proj.qzeros', 'vpm.encoder.layers.9.self_attn.q_proj.scales', 'vpm.encoder.layers.9.self_attn.v_proj.g_idx', 'vpm.encoder.layers.9.self_attn.v_proj.qweight', 'vpm.encoder.layers.9.self_attn.v_proj.qzeros', 'vpm.encoder.layers.9.self_attn.v_proj.scales']\n",
      "- This IS expected if you are initializing MiniCPMV from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MiniCPMV from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MiniCPMV were not initialized from the model checkpoint at /home/workspace/model/MiniCPM-3o-1B-sft-v1-pc and are newly initialized: ['vpm.encoder.layers.0.mlp.fc1.weight', 'vpm.encoder.layers.0.mlp.fc2.weight', 'vpm.encoder.layers.0.self_attn.k_proj.weight', 'vpm.encoder.layers.0.self_attn.out_proj.weight', 'vpm.encoder.layers.0.self_attn.q_proj.weight', 'vpm.encoder.layers.0.self_attn.v_proj.weight', 'vpm.encoder.layers.1.mlp.fc1.weight', 'vpm.encoder.layers.1.mlp.fc2.weight', 'vpm.encoder.layers.1.self_attn.k_proj.weight', 'vpm.encoder.layers.1.self_attn.out_proj.weight', 'vpm.encoder.layers.1.self_attn.q_proj.weight', 'vpm.encoder.layers.1.self_attn.v_proj.weight', 'vpm.encoder.layers.10.mlp.fc1.weight', 'vpm.encoder.layers.10.mlp.fc2.weight', 'vpm.encoder.layers.10.self_attn.k_proj.weight', 'vpm.encoder.layers.10.self_attn.out_proj.weight', 'vpm.encoder.layers.10.self_attn.q_proj.weight', 'vpm.encoder.layers.10.self_attn.v_proj.weight', 'vpm.encoder.layers.11.mlp.fc1.weight', 'vpm.encoder.layers.11.mlp.fc2.weight', 'vpm.encoder.layers.11.self_attn.k_proj.weight', 'vpm.encoder.layers.11.self_attn.out_proj.weight', 'vpm.encoder.layers.11.self_attn.q_proj.weight', 'vpm.encoder.layers.11.self_attn.v_proj.weight', 'vpm.encoder.layers.12.mlp.fc1.weight', 'vpm.encoder.layers.12.mlp.fc2.weight', 'vpm.encoder.layers.12.self_attn.k_proj.weight', 'vpm.encoder.layers.12.self_attn.out_proj.weight', 'vpm.encoder.layers.12.self_attn.q_proj.weight', 'vpm.encoder.layers.12.self_attn.v_proj.weight', 'vpm.encoder.layers.13.mlp.fc1.weight', 'vpm.encoder.layers.13.mlp.fc2.weight', 'vpm.encoder.layers.13.self_attn.k_proj.weight', 'vpm.encoder.layers.13.self_attn.out_proj.weight', 'vpm.encoder.layers.13.self_attn.q_proj.weight', 'vpm.encoder.layers.13.self_attn.v_proj.weight', 'vpm.encoder.layers.14.mlp.fc1.weight', 'vpm.encoder.layers.14.mlp.fc2.weight', 'vpm.encoder.layers.14.self_attn.k_proj.weight', 'vpm.encoder.layers.14.self_attn.out_proj.weight', 'vpm.encoder.layers.14.self_attn.q_proj.weight', 'vpm.encoder.layers.14.self_attn.v_proj.weight', 'vpm.encoder.layers.15.mlp.fc1.weight', 'vpm.encoder.layers.15.mlp.fc2.weight', 'vpm.encoder.layers.15.self_attn.k_proj.weight', 'vpm.encoder.layers.15.self_attn.out_proj.weight', 'vpm.encoder.layers.15.self_attn.q_proj.weight', 'vpm.encoder.layers.15.self_attn.v_proj.weight', 'vpm.encoder.layers.16.mlp.fc1.weight', 'vpm.encoder.layers.16.mlp.fc2.weight', 'vpm.encoder.layers.16.self_attn.k_proj.weight', 'vpm.encoder.layers.16.self_attn.out_proj.weight', 'vpm.encoder.layers.16.self_attn.q_proj.weight', 'vpm.encoder.layers.16.self_attn.v_proj.weight', 'vpm.encoder.layers.17.mlp.fc1.weight', 'vpm.encoder.layers.17.mlp.fc2.weight', 'vpm.encoder.layers.17.self_attn.k_proj.weight', 'vpm.encoder.layers.17.self_attn.out_proj.weight', 'vpm.encoder.layers.17.self_attn.q_proj.weight', 'vpm.encoder.layers.17.self_attn.v_proj.weight', 'vpm.encoder.layers.18.mlp.fc1.weight', 'vpm.encoder.layers.18.mlp.fc2.weight', 'vpm.encoder.layers.18.self_attn.k_proj.weight', 'vpm.encoder.layers.18.self_attn.out_proj.weight', 'vpm.encoder.layers.18.self_attn.q_proj.weight', 'vpm.encoder.layers.18.self_attn.v_proj.weight', 'vpm.encoder.layers.19.mlp.fc1.weight', 'vpm.encoder.layers.19.mlp.fc2.weight', 'vpm.encoder.layers.19.self_attn.k_proj.weight', 'vpm.encoder.layers.19.self_attn.out_proj.weight', 'vpm.encoder.layers.19.self_attn.q_proj.weight', 'vpm.encoder.layers.19.self_attn.v_proj.weight', 'vpm.encoder.layers.2.mlp.fc1.weight', 'vpm.encoder.layers.2.mlp.fc2.weight', 'vpm.encoder.layers.2.self_attn.k_proj.weight', 'vpm.encoder.layers.2.self_attn.out_proj.weight', 'vpm.encoder.layers.2.self_attn.q_proj.weight', 'vpm.encoder.layers.2.self_attn.v_proj.weight', 'vpm.encoder.layers.20.mlp.fc1.weight', 'vpm.encoder.layers.20.mlp.fc2.weight', 'vpm.encoder.layers.20.self_attn.k_proj.weight', 'vpm.encoder.layers.20.self_attn.out_proj.weight', 'vpm.encoder.layers.20.self_attn.q_proj.weight', 'vpm.encoder.layers.20.self_attn.v_proj.weight', 'vpm.encoder.layers.21.mlp.fc1.weight', 'vpm.encoder.layers.21.mlp.fc2.weight', 'vpm.encoder.layers.21.self_attn.k_proj.weight', 'vpm.encoder.layers.21.self_attn.out_proj.weight', 'vpm.encoder.layers.21.self_attn.q_proj.weight', 'vpm.encoder.layers.21.self_attn.v_proj.weight', 'vpm.encoder.layers.22.mlp.fc1.weight', 'vpm.encoder.layers.22.mlp.fc2.weight', 'vpm.encoder.layers.22.self_attn.k_proj.weight', 'vpm.encoder.layers.22.self_attn.out_proj.weight', 'vpm.encoder.layers.22.self_attn.q_proj.weight', 'vpm.encoder.layers.22.self_attn.v_proj.weight', 'vpm.encoder.layers.23.mlp.fc1.weight', 'vpm.encoder.layers.23.mlp.fc2.weight', 'vpm.encoder.layers.23.self_attn.k_proj.weight', 'vpm.encoder.layers.23.self_attn.out_proj.weight', 'vpm.encoder.layers.23.self_attn.q_proj.weight', 'vpm.encoder.layers.23.self_attn.v_proj.weight', 'vpm.encoder.layers.24.mlp.fc1.weight', 'vpm.encoder.layers.24.mlp.fc2.weight', 'vpm.encoder.layers.24.self_attn.k_proj.weight', 'vpm.encoder.layers.24.self_attn.out_proj.weight', 'vpm.encoder.layers.24.self_attn.q_proj.weight', 'vpm.encoder.layers.24.self_attn.v_proj.weight', 'vpm.encoder.layers.25.mlp.fc1.weight', 'vpm.encoder.layers.25.mlp.fc2.weight', 'vpm.encoder.layers.25.self_attn.k_proj.weight', 'vpm.encoder.layers.25.self_attn.out_proj.weight', 'vpm.encoder.layers.25.self_attn.q_proj.weight', 'vpm.encoder.layers.25.self_attn.v_proj.weight', 'vpm.encoder.layers.26.mlp.fc1.weight', 'vpm.encoder.layers.26.mlp.fc2.weight', 'vpm.encoder.layers.26.self_attn.k_proj.weight', 'vpm.encoder.layers.26.self_attn.out_proj.weight', 'vpm.encoder.layers.26.self_attn.q_proj.weight', 'vpm.encoder.layers.26.self_attn.v_proj.weight', 'vpm.encoder.layers.3.mlp.fc1.weight', 'vpm.encoder.layers.3.mlp.fc2.weight', 'vpm.encoder.layers.3.self_attn.k_proj.weight', 'vpm.encoder.layers.3.self_attn.out_proj.weight', 'vpm.encoder.layers.3.self_attn.q_proj.weight', 'vpm.encoder.layers.3.self_attn.v_proj.weight', 'vpm.encoder.layers.4.mlp.fc1.weight', 'vpm.encoder.layers.4.mlp.fc2.weight', 'vpm.encoder.layers.4.self_attn.k_proj.weight', 'vpm.encoder.layers.4.self_attn.out_proj.weight', 'vpm.encoder.layers.4.self_attn.q_proj.weight', 'vpm.encoder.layers.4.self_attn.v_proj.weight', 'vpm.encoder.layers.5.mlp.fc1.weight', 'vpm.encoder.layers.5.mlp.fc2.weight', 'vpm.encoder.layers.5.self_attn.k_proj.weight', 'vpm.encoder.layers.5.self_attn.out_proj.weight', 'vpm.encoder.layers.5.self_attn.q_proj.weight', 'vpm.encoder.layers.5.self_attn.v_proj.weight', 'vpm.encoder.layers.6.mlp.fc1.weight', 'vpm.encoder.layers.6.mlp.fc2.weight', 'vpm.encoder.layers.6.self_attn.k_proj.weight', 'vpm.encoder.layers.6.self_attn.out_proj.weight', 'vpm.encoder.layers.6.self_attn.q_proj.weight', 'vpm.encoder.layers.6.self_attn.v_proj.weight', 'vpm.encoder.layers.7.mlp.fc1.weight', 'vpm.encoder.layers.7.mlp.fc2.weight', 'vpm.encoder.layers.7.self_attn.k_proj.weight', 'vpm.encoder.layers.7.self_attn.out_proj.weight', 'vpm.encoder.layers.7.self_attn.q_proj.weight', 'vpm.encoder.layers.7.self_attn.v_proj.weight', 'vpm.encoder.layers.8.mlp.fc1.weight', 'vpm.encoder.layers.8.mlp.fc2.weight', 'vpm.encoder.layers.8.self_attn.k_proj.weight', 'vpm.encoder.layers.8.self_attn.out_proj.weight', 'vpm.encoder.layers.8.self_attn.q_proj.weight', 'vpm.encoder.layers.8.self_attn.v_proj.weight', 'vpm.encoder.layers.9.mlp.fc1.weight', 'vpm.encoder.layers.9.mlp.fc2.weight', 'vpm.encoder.layers.9.self_attn.k_proj.weight', 'vpm.encoder.layers.9.self_attn.out_proj.weight', 'vpm.encoder.layers.9.self_attn.q_proj.weight', 'vpm.encoder.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO - The layer llm.model.layers.0.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.0.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.1.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.2.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.3.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.4.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.5.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.6.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.7.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.8.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.9.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.10.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.11.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.12.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.13.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.14.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.15.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.16.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.17.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.18.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.19.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.20.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.21.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.22.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.23.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.24.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.25.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.26.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.27.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.28.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.29.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.30.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.31.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.32.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.33.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.34.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.35.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.36.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.37.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.38.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.39.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.40.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.41.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.42.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.43.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.44.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.45.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.46.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.47.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.48.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.49.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.50.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.self_attn.q_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.self_attn.k_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.self_attn.v_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.self_attn.o_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.mlp.gate_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.mlp.up_proj is not quantized.\n",
      "INFO - The layer llm.model.layers.51.mlp.down_proj is not quantized.\n",
      "INFO - The layer llm.lm_head is not quantized.\n",
      "INFO - The layer vpm.embeddings.patch_embedding is not quantized.\n",
      "INFO - The layer resampler.kv_proj is not quantized.\n",
      "INFO - The layer resampler.attn.out_proj is not quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers------- {'llm.model.layers.0.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.0.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.0.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.0.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.0.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.0.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.0.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.1.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.1.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.1.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.1.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.1.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.1.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.1.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.2.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.2.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.2.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.2.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.2.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.2.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.2.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.3.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.3.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.3.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.3.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.3.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.3.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.3.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.4.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.4.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.4.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.4.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.4.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.4.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.4.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.5.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.5.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.5.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.5.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.5.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.5.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.5.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.6.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.6.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.6.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.6.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.6.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.6.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.6.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.7.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.7.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.7.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.7.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.7.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.7.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.7.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.8.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.8.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.8.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.8.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.8.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.8.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.8.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.9.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.9.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.9.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.9.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.9.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.9.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.9.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.10.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.10.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.10.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.10.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.10.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.10.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.10.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.11.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.11.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.11.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.11.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.11.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.11.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.11.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.12.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.12.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.12.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.12.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.12.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.12.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.12.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.13.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.13.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.13.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.13.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.13.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.13.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.13.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.14.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.14.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.14.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.14.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.14.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.14.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.14.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.15.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.15.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.15.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.15.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.15.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.15.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.15.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.16.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.16.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.16.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.16.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.16.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.16.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.16.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.17.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.17.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.17.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.17.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.17.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.17.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.17.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.18.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.18.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.18.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.18.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.18.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.18.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.18.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.19.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.19.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.19.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.19.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.19.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.19.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.19.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.20.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.20.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.20.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.20.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.20.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.20.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.20.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.21.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.21.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.21.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.21.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.21.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.21.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.21.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.22.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.22.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.22.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.22.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.22.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.22.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.22.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.23.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.23.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.23.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.23.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.23.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.23.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.23.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.24.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.24.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.24.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.24.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.24.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.24.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.24.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.25.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.25.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.25.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.25.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.25.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.25.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.25.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.26.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.26.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.26.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.26.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.26.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.26.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.26.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.27.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.27.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.27.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.27.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.27.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.27.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.27.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.28.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.28.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.28.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.28.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.28.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.28.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.28.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.29.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.29.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.29.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.29.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.29.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.29.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.29.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.30.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.30.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.30.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.30.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.30.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.30.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.30.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.31.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.31.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.31.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.31.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.31.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.31.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.31.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.32.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.32.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.32.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.32.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.32.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.32.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.32.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.33.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.33.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.33.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.33.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.33.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.33.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.33.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.34.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.34.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.34.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.34.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.34.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.34.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.34.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.35.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.35.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.35.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.35.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.35.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.35.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.35.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.36.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.36.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.36.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.36.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.36.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.36.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.36.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.37.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.37.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.37.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.37.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.37.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.37.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.37.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.38.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.38.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.38.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.38.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.38.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.38.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.38.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.39.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.39.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.39.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.39.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.39.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.39.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.39.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.40.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.40.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.40.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.40.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.40.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.40.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.40.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.41.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.41.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.41.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.41.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.41.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.41.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.41.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.42.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.42.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.42.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.42.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.42.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.42.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.42.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.43.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.43.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.43.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.43.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.43.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.43.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.43.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.44.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.44.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.44.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.44.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.44.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.44.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.44.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.45.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.45.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.45.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.45.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.45.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.45.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.45.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.46.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.46.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.46.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.46.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.46.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.46.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.46.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.47.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.47.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.47.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.47.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.47.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.47.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.47.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.48.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.48.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.48.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.48.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.48.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.48.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.48.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.49.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.49.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.49.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.49.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.49.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.49.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.49.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.50.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.50.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.50.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.50.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.50.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.50.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.50.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.model.layers.51.self_attn.q_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.51.self_attn.k_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.51.self_attn.v_proj': Linear(in_features=1536, out_features=512, bias=False), 'llm.model.layers.51.self_attn.o_proj': Linear(in_features=1536, out_features=1536, bias=False), 'llm.model.layers.51.mlp.gate_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.51.mlp.up_proj': Linear(in_features=1536, out_features=3840, bias=False), 'llm.model.layers.51.mlp.down_proj': Linear(in_features=3840, out_features=1536, bias=False), 'llm.lm_head': Linear(in_features=1536, out_features=73464, bias=False), 'vpm.embeddings.patch_embedding': Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid), 'vpm.encoder.layers.0.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.0.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.0.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.0.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.0.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.0.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.1.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.1.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.1.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.1.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.1.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.1.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.2.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.2.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.2.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.2.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.2.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.2.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.3.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.3.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.3.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.3.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.3.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.3.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.4.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.4.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.4.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.4.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.4.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.4.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.5.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.5.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.5.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.5.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.5.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.5.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.6.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.6.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.6.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.6.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.6.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.6.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.7.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.7.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.7.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.7.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.7.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.7.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.8.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.8.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.8.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.8.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.8.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.8.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.9.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.9.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.9.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.9.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.9.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.9.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.10.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.10.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.10.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.10.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.10.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.10.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.11.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.11.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.11.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.11.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.11.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.11.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.12.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.12.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.12.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.12.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.12.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.12.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.13.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.13.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.13.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.13.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.13.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.13.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.14.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.14.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.14.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.14.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.14.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.14.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.15.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.15.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.15.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.15.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.15.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.15.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.16.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.16.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.16.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.16.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.16.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.16.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.17.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.17.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.17.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.17.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.17.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.17.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.18.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.18.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.18.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.18.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.18.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.18.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.19.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.19.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.19.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.19.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.19.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.19.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.20.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.20.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.20.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.20.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.20.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.20.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.21.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.21.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.21.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.21.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.21.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.21.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.22.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.22.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.22.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.22.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.22.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.22.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.23.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.23.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.23.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.23.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.23.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.23.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.24.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.24.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.24.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.24.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.24.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.24.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.25.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.25.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.25.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.25.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.25.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.25.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'vpm.encoder.layers.26.self_attn.k_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.26.self_attn.v_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.26.self_attn.q_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.26.self_attn.out_proj': Linear(in_features=1152, out_features=1152, bias=True), 'vpm.encoder.layers.26.mlp.fc1': Linear(in_features=1152, out_features=4304, bias=True), 'vpm.encoder.layers.26.mlp.fc2': Linear(in_features=4304, out_features=1152, bias=True), 'resampler.kv_proj': Linear(in_features=1152, out_features=1536, bias=False), 'resampler.attn.out_proj': Linear(in_features=1536, out_features=1536, bias=True)}\n",
      "ignore_layers------- ['lm_head', 'vpm.embeddings', 'vpm.post_layernorm', 'llm.model']\n"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForVIT, BaseQuantizeConfig\n",
    "\n",
    "# quantized_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1-g128\"\n",
    "quantized_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1-pc\"\n",
    "\n",
    "model_quant = AutoGPTQForVIT.from_quantized(quantized_model_dir, device=\"cuda:0\", use_triton=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VITGPTQ(\n",
       "  (model): MiniCPMV(\n",
       "    (llm): MiniCPMForCausalLM(\n",
       "      (model): MiniCPMModel(\n",
       "        (embed_tokens): Embedding(73464, 1536)\n",
       "        (layers): ModuleList(\n",
       "          (0-51): 52 x MiniCPMDecoderLayer(\n",
       "            (self_attn): MiniCPMSdpaAttention(\n",
       "              (q_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (k_proj): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (v_proj): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (rotary_emb): MiniCPMLongRoPE()\n",
       "            )\n",
       "            (mlp): MiniCPMMLP(\n",
       "              (gate_proj): Linear(in_features=1536, out_features=3840, bias=False)\n",
       "              (up_proj): Linear(in_features=1536, out_features=3840, bias=False)\n",
       "              (down_proj): Linear(in_features=3840, out_features=1536, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MiniCPMRMSNorm()\n",
       "            (post_attention_layernorm): MiniCPMRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MiniCPMRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=73464, bias=False)\n",
       "    )\n",
       "    (vpm): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(4900, 1152)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x SiglipEncoderLayer(\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): QuantLinear()\n",
       "              (v_proj): QuantLinear()\n",
       "              (q_proj): QuantLinear()\n",
       "              (out_proj): QuantLinear()\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): QuantLinear()\n",
       "              (fc2): QuantLinear()\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (resampler): Resampler(\n",
       "      (kv_proj): Linear(in_features=1152, out_features=1536, bias=False)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      )\n",
       "      (ln_q): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (ln_kv): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (ln_post): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline,AutoProcessor\n",
    "pretrained_model_dir = \"/home/workspace/model/MiniCPM-3o-1B-sft-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True, trust_remote_code=True)\n",
    "model_quant.model.processor = AutoProcessor.from_pretrained(pretrained_model_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MiniCPMVTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aircraft in the image is an Airbus A380-800, which is a wide-body airliner. This model is part of Airbus's A380 family, which includes the A380 and A380-800 versions. The A380-800 is the largest passenger airliner ever built and can carry up to 800 passengers across its four main decks. It was introduced in 2005 and has since become a symbol of commercial aviation's capacity to transport large numbers of people over long distances.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image = Image.open('/home/workspace/code/llm-awq/awq/airplane.jpeg').convert('RGB')\n",
    "\n",
    "# First round chat \n",
    "question = \"Tell me the model of this aircraft.\"\n",
    "msgs = [{'role': 'user', 'content': [image, question]}]\n",
    "\n",
    "answer = model_quant.model.chat(\n",
    "    image=None,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vpm.embeddings', 'vpm.post_layernorm']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant.outside_layer_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniCPMModel(\n",
       "  (embed_tokens): Embedding(73464, 1536)\n",
       "  (layers): ModuleList(\n",
       "    (0-51): 52 x MiniCPMDecoderLayer(\n",
       "      (self_attn): MiniCPMSdpaAttention(\n",
       "        (q_proj): QuantLinear()\n",
       "        (k_proj): QuantLinear()\n",
       "        (v_proj): QuantLinear()\n",
       "        (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        (rotary_emb): MiniCPMLongRoPE()\n",
       "      )\n",
       "      (mlp): MiniCPMMLP(\n",
       "        (gate_proj): Linear(in_features=1536, out_features=3840, bias=False)\n",
       "        (up_proj): Linear(in_features=1536, out_features=3840, bias=False)\n",
       "        (down_proj): Linear(in_features=3840, out_features=1536, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): MiniCPMRMSNorm()\n",
       "      (post_attention_layernorm): MiniCPMRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): MiniCPMRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant.model.llm.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_inputs = []\n",
    "\n",
    "def store_input_hook(_, args,kwargs):\n",
    "    # Positional arguments.\n",
    "    layer_input = []\n",
    "    for inp in args:\n",
    "        layer_input.append(inp)\n",
    "    layer_inputs.append(layer_input)\n",
    "\n",
    "    # # Keyword arguments.\n",
    "    # if kwargs[\"attention_mask\"] is not None:\n",
    "    #     attention_masks.append(kwargs[\"attention_mask\"].to(data_device))\n",
    "    # else:\n",
    "    #     attention_masks.append(None)\n",
    "\n",
    "    # pos_ids = kwargs.get(\"position_ids\", None)\n",
    "    # if pos_ids is not None:\n",
    "    #     position_ids.append(move_to_device(pos_ids, data_device))\n",
    "    # one_kwargs = {}\n",
    "    # for (\n",
    "    #     k,\n",
    "    #     v,\n",
    "    # ) in kwargs.items():  # make sure other arguments also be captured\n",
    "    #     if k not in [\"hidden_states\", \"attention_mask\", \"position_ids\"]:\n",
    "    #         one_kwargs[k] = nested_move_to_device(v, data_device)\n",
    "    # layer_input_kwargs.append(one_kwargs)\n",
    "    # raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = model.vpm.encoder.layers[0].register_forward_pre_hook(store_input_hook, with_kwargs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in traindataset:\n",
    "    for k, v in example.items():\n",
    "        # if len(v.shape) == 1:\n",
    "        #     v = v.unsqueeze(0)\n",
    "        # if k in \"input_ids\" or k in \"attention_mask\":\n",
    "        #     example[k] = v\n",
    "        # elif k in \"pixel_values\":\n",
    "        #     example[k][0][0] =  v[0][0].cuda()\n",
    "        #     example[k][0][1] =  v[0][1].cuda()\n",
    "        #     example[k][0][2] =  v[0][2].cuda()\n",
    "        a = model(example)\n",
    "        break\n",
    "handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7efb9932ca50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vpm.encoder.layers[0].register_forward_pre_hook(store_input_hook, with_kwargs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取激活"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 09:01:37 INFO [transformers_modules.MiniCPM-V-1B-sft-v2-1B.configuration_minicpm] vision_config is None, using default vision config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer,AutoProcessor,set_seed\n",
    "\n",
    "set_seed(42)\n",
    "# torch.manual_seed(0)\n",
    "# model_path = \"/home/workspace/model/MiniCPM-Llama3-V-2_5\"\n",
    "# model_path = \"/home/workspace/model/minicpm-vit-1b-w8-lenovo-llama-w8-pergroup128\"\n",
    "# model_path = \"/home/workspace/model/minicpm-vit-1b-w8-lenovo\"\n",
    "# model_path = \"/home/workspace/model/llava-1___5-7b-hf\"\n",
    "# model_path = \"/home/workspace/model/minicpm-gptq-w4-32-perchannel-only_quant_downproj\"\n",
    "model_path = \"/home/workspace/model/MiniCPM-V-1B-sft-v2-1B\"\n",
    "model = AutoModel.from_pretrained(model_path, \n",
    "                                  trust_remote_code=True,\n",
    "                                  device_map=None) \n",
    "# model_weight_path = \"/home/workspace/model/minicpm_v_navit_250_0927.pt\"\n",
    "# model.load_state_dict(torch.load(model_weight_path))\n",
    "model = model.cuda().eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipVisionTransformer(\n",
       "  (embeddings): SiglipVisionEmbeddings(\n",
       "    (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "    (position_embedding): Embedding(4900, 1152)\n",
       "  )\n",
       "  (encoder): SiglipEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-26): 27 x SiglipEncoderLayer(\n",
       "        (self_attn): SiglipAttention(\n",
       "          (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): SiglipMLP(\n",
       "          (activation_fn): PytorchGELUTanh()\n",
       "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpm = model.vpm\n",
    "input_shape = (1, 1024, 1152)\n",
    "example_input = torch.ones(input_shape, dtype=torch.float32)\n",
    "\n",
    "# print(model_resampler)\n",
    "vpm.prepare_layernorm()\n",
    "for bolck in vpm.encoder.layers:\n",
    "    bolck.prepare_layernorm()\n",
    "    bolck.self_attn.prepare_sha()\n",
    "    bolck.mlp.prepare_conv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipVisionTransformer(\n",
       "  (embeddings): SiglipVisionEmbeddings(\n",
       "    (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "    (position_embedding): Embedding(4900, 1152)\n",
       "  )\n",
       "  (encoder): SiglipEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-26): 27 x SiglipEncoderLayer(\n",
       "        (self_attn): SiglipAttention(\n",
       "          (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          (q_proj_sha): ModuleList(\n",
       "            (0-15): 16 x Conv2d(1152, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (k_proj_sha): ModuleList(\n",
       "            (0-15): 16 x Conv2d(1152, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (v_proj_sha): ModuleList(\n",
       "            (0-15): 16 x Conv2d(1152, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (o_proj_conv): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): SiglipMLP(\n",
       "          (activation_fn): PytorchGELUTanh()\n",
       "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "          (fc1_conv): Conv2d(1152, 4304, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2_conv): Conv2d(4304, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_norm1_ane): LayerNormANE()\n",
       "        (layer_norm2_ane): LayerNormANE()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "  (post_layernorm_ane): LayerNormANE()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = vpm(example_input.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x-x1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
